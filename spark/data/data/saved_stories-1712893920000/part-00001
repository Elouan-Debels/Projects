{"aid": "40007201", "title": "How Complex Systems Fail", "url": "https://how.complexsystems.fail/", "domain": "complexsystems.fail", "votes": 2, "user": "RyeCombinator", "posted_at": "2024-04-11 21:50:33", "comments": 0, "source_title": "How Complex Systems Fail", "source_text": "How Complex Systems Fail\n\n#\n\nHow Complex Systems Fail\n\n(Being a Short Treatise on the Nature of Failure; How Failure is Evaluated;\nHow Failure is Attributed to Proximate Cause; and the Resulting New\nUnderstanding of Patient Safety)\n\nRichard I. Cook, MD Cognitive Technologies Labratory University of Chicago\n\n  1. ## Complex systems are intrinsically hazardous systems.\n\nAll of the interesting systems (e.g. transportation, healthcare, power\ngeneration) are inherently and unavoidably hazardous by the own nature. The\nfrequency of hazard exposure can sometimes be changed but the processes\ninvolved in the system are themselves intrinsically and irreducibly hazardous.\nIt is the presence of these hazards that drives the creation of defenses\nagainst hazard that characterize these systems.\n\n  2. ## Complex systems are heavily and successfully defended against failure\n\nThe high consequences of failure lead over time to the construction of\nmultiple layers of defense against failure. These defenses include obvious\ntechnical components (e.g. backup systems, \u2018safety\u2019 features of equipment) and\nhuman components (e.g. training, knowledge) but also a variety of\norganizational, institutional, and regulatory defenses (e.g. policies and\nprocedures, certification, work rules, team training). The effect of these\nmeasures is to provide a series of shields that normally divert operations\naway from accidents.\n\n  3. ## Catastrophe requires multiple failures \u2013 single point failures are not enough.\n\nThe array of defenses works. System operations are generally successful. Overt\ncatastrophic failure occurs when small, apparently innocuous failures join to\ncreate opportunity for a systemic accident. Each of these small failures is\nnecessary to cause catastrophe but only the combination is sufficient to\npermit failure. Put another way, there are many more failure opportunities\nthan overt system accidents. Most initial failure trajectories are blocked by\ndesigned system safety components. Trajectories that reach the operational\nlevel are mostly blocked, usually by practitioners.\n\n  4. ## Complex systems contain changing mixtures of failures latent within them.\n\nThe complexity of these systems makes it impossible for them to run without\nmultiple flaws being present. Because these are individually insufficient to\ncause failure they are regarded as minor factors during operations.\nEradication of all latent failures is limited primarily by economic cost but\nalso because it is difficult before the fact to see how such failures might\ncontribute to an accident. The failures change constantly because of changing\ntechnology, work organization, and efforts to eradicate failures.\n\n  5. ## Complex systems run in degraded mode.\n\nA corollary to the preceding point is that complex systems run as broken\nsystems. The system continues to function because it contains so many\nredundancies and because people can make it function, despite the presence of\nmany flaws. After accident reviews nearly always note that the system has a\nhistory of prior \u2018proto-accidents\u2019 that nearly generated catastrophe.\nArguments that these degraded conditions should have been recognized before\nthe overt accident are usually predicated on na\u00efve notions of system\nperformance. System operations are dynamic, with components (organizational,\nhuman, technical) failing and being replaced continuously.\n\n  6. ## Catastrophe is always just around the corner.\n\nComplex systems possess potential for catastrophic failure. Human\npractitioners are nearly always in close physical and temporal proximity to\nthese potential failures \u2013 disaster can occur at any time and in nearly any\nplace. The potential for catastrophic outcome is a hallmark of complex\nsystems. It is impossible to eliminate the potential for such catastrophic\nfailure; the potential for such failure is always present by the system\u2019s own\nnature.\n\n  7. ## Post-accident attribution to a \u2018root cause\u2019 is fundamentally wrong.\n\nBecause overt failure requires multiple faults, there is no isolated \u2018cause\u2019\nof an accident. There are multiple contributors to accidents. Each of these is\nnecessarily insufficient in itself to create an accident. Only jointly are\nthese causes sufficient to create an accident. Indeed, it is the linking of\nthese causes together that creates the circumstances required for the\naccident. Thus, no isolation of the \u2018root cause\u2019 of an accident is possible.\nThe evaluations based on such reasoning as \u2018root cause\u2019 do not reflect a\ntechnical understanding of the nature of failure but rather the social,\ncultural need to blame specific, localized forces or events for outcomes. ^1\n\n^1 Anthropological field research provides the clearest demonstration of the\nsocial construction of the notion of \u2018cause\u2019 (cf. Goldman L (1993), The\nCulture of Coincidence: accident and absolute liability in Huli, New York:\nClarendon Press; and also Tasca L (1990), The Social Construction of Human\nError, Unpublished doctoral dissertation, Department of Sociology, State\nUniversity of New York at Stonybrook)\n\n  8. ## Hindsight biases post-accident assessments of human performance.\n\nKnowledge of the outcome makes it seem that events leading to the outcome\nshould have appeared more salient to practitioners at the time than was\nactually the case. This means that ex post facto accident analysis of human\nperformance is inaccurate. The outcome knowledge poisons the ability of after-\naccident observers to recreate the view of practitioners before the accident\nof those same factors. It seems that practitioners \u201cshould have known\u201d that\nthe factors would \u201cinevitably\u201d lead to an accident. ^2 Hindsight bias remains\nthe primary obstacle to accident investigation, especially when expert human\nperformance is involved.\n\n^2 This is not a feature of medical judgements or technical ones, but rather\nof all human cognition about past events and their causes.\n\n  9. ## Human operators have dual roles: as producers & as defenders against failure.\n\nThe system practitioners operate the system in order to produce its desired\nproduct and also work to forestall accidents. This dynamic quality of system\noperation, the balancing of demands for production against the possibility of\nincipient failure is unavoidable. Outsiders rarely acknowledge the duality of\nthis role. In non-accident filled times, the production role is emphasized.\nAfter accidents, the defense against failure role is emphasized. At either\ntime, the outsider\u2019s view misapprehends the operator\u2019s constant, simultaneous\nengagement with both roles.\n\n  10. ## All practitioner actions are gambles.\n\nAfter accidents, the overt failure often appears to have been inevitable and\nthe practitioner\u2019s actions as blunders or deliberate willful disregard of\ncertain impending failure. But all practitioner actions are actually gambles,\nthat is, acts that take place in the face of uncertain outcomes. The degree of\nuncertainty may change from moment to moment. That practitioner actions are\ngambles appears clear after accidents; in general, post hoc analysis regards\nthese gambles as poor ones. But the converse: that successful outcomes are\nalso the result of gambles; is not widely appreciated.\n\n  11. ## Actions at the sharp end resolve all ambiguity.\n\nOrganizations are ambiguous, often intentionally, about the relationship\nbetween production targets, efficient use of resources, economy and costs of\noperations, and acceptable risks of low and high consequence accidents. All\nambiguity is resolved by actions of practitioners at the sharp end of the\nsystem. After an accident, practitioner actions may be regarded as \u2018errors\u2019 or\n\u2018violations\u2019 but these evaluations are heavily biased by hindsight and ignore\nthe other driving forces, especially production pressure.\n\n  12. ## Human practitioners are the adaptable element of complex systems.\n\nPractitioners and first line management actively adapt the system to maximize\nproduction and minimize accidents. These adaptations often occur on a moment\nby moment basis. Some of these adaptations include: (1) Restructuring the\nsystem in order to reduce exposure of vulnerable parts to failure. (2)\nConcentrating critical resources in areas of expected high demand. (3)\nProviding pathways for retreat or recovery from expected and unexpected\nfaults. (4) Establishing means for early detection of changed system\nperformance in order to allow graceful cutbacks in production or other means\nof increasing resiliency.\n\n  13. ## Human expertise in complex systems is constantly changing\n\nComplex systems require substantial human expertise in their operation and\nmanagement. This expertise changes in character as technology changes but it\nalso changes because of the need to replace experts who leave. In every case,\ntraining and refinement of skill and expertise is one part of the function of\nthe system itself. At any moment, therefore, a given complex system will\ncontain practitioners and trainees with varying degrees of expertise. Critical\nissues related to expertise arise from (1) the need to use scarce expertise as\na resource for the most difficult or demanding production needs and (2) the\nneed to develop expertise for future use.\n\n  14. ## Change introduces new forms of failure.\n\nThe low rate of overt accidents in reliable systems may encourage changes,\nespecially the use of new technology, to decrease the number of low\nconsequence but high frequency failures. These changes maybe actually create\nopportunities for new, low frequency but high consequence failures. When new\ntechnologies are used to eliminate well understood system failures or to gain\nhigh precision performance they often introduce new pathways to large scale,\ncatastrophic failures. Not uncommonly, these new, rare catastrophes have even\ngreater impact than those eliminated by the new technology. These new forms of\nfailure are difficult to see before the fact; attention is paid mostly to the\nputative beneficial characteristics of the changes. Because these new, high\nconsequence accidents occur at a low rate, multiple system changes may occur\nbefore an accident, making it hard to see the contribution of technology to\nthe failure.\n\n  15. ## Views of \u2018cause\u2019 limit the effectiveness of defenses against future events.\n\nPost-accident remedies for \u201chuman error\u201d are usually predicated on obstructing\nactivities that can \u201ccause\u201d accidents. These end-of-the-chain measures do\nlittle to reduce the likelihood of further accidents. In fact that likelihood\nof an identical accident is already extraordinarily low because the pattern of\nlatent failures changes constantly. Instead of increasing safety, post-\naccident remedies usually increase the coupling and complexity of the system.\nThis increases the potential number of latent failures and also makes the\ndetection and blocking of accident trajectories more difficult.\n\n  16. ## Safety is a characteristic of systems and not of their components\n\nSafety is an emergent property of systems; it does not reside in a person,\ndevice or department of an organization or system. Safety cannot be purchased\nor manufactured; it is not a feature that is separate from the other\ncomponents of the system. This means that safety cannot be manipulated like a\nfeedstock or raw material. The state of safety in any system is always\ndynamic; continuous systemic change insures that hazard and its management are\nconstantly changing.\n\n  17. ## People continuously create safety.\n\nFailure free operations are the result of activities of people who work to\nkeep the system within the boundaries of tolerable performance. These\nactivities are, for the most part, part of normal operations and superficially\nstraightforward. But because system operations are never trouble free, human\npractitioner adaptations to changing conditions actually create safety from\nmoment to moment. These adaptations often amount to just the selection of a\nwell-rehearsed routine from a store of available responses; sometimes,\nhowever, the adaptations are novel combinations or de novo creations of new\napproaches.\n\n  18. ## Failure free operations require experience with failure.\n\nRecognizing hazard and successfully manipulating system operations to remain\ninside the tolerable performance boundaries requires intimate contact with\nfailure. More robust system performance is likely to arise in systems where\noperators can discern the \u201cedge of the envelope\u201d. This is where system\nperformance begins to deteriorate, becomes difficult to predict, or cannot be\nreadily recovered. In intrinsically hazardous systems, operators are expected\nto encounter and appreciate hazards in ways that lead to overall performance\nthat is desirable. Improved safety depends on providing operators with\ncalibrated views of the hazards. It also depends on providing calibration\nabout how their actions move system performance towards or away from the edge\nof the envelope.\n\n## Other Materials\n\n  * Cook, Render, Woods (2000). Gaps in the continuity of care and progress on patient safety. British Medical Journal 320: 791-4.\n  * Cook (1999). A Brief Look at the New Look in error, safety, and failure of complex systems. (Chicago: CtL).\n  * Woods & Cook (1999). Perspectives on Human Error: Hindsight Biases and Local Rationality. In Durso, Nickerson, et al., eds., Handbook of Applied Cognition. (New York: Wiley) pp. 141-171.\n  * Woods & Cook (1998). Characteristics of Patient Safety: Five Principles that Underlie Productive Work. (Chicago: CtL)\n  * Cook & Woods (1994), \u201cOperating at the Sharp End: The Complexity of Human Error,\u201d in MS Bogner, ed., Human Error in Medicine , Hillsdale, NJ; pp. 255-310\n  * Woods, Johannesen, Cook, & Sarter (1994), Behind Human Error: Cognition, Computers and Hindsight , Wright Patterson AFB: CSERIAC.\n  * Cook, Woods, & Miller (1998), A Tale of Two Stories: Contrasting Views of Patient Safety, Chicago, IL: NPSF\n\nOriginal Copyright \u00a9 1998, 1999, 2000 by R.I.Cook, MD, for CtL\n\n", "frontpage": false}
