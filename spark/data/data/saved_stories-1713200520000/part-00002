{"aid": "40038890", "title": "Filesystem Error Handling (2017)", "url": "http://danluu.com/filesystem-errors/", "domain": "danluu.com", "votes": 2, "user": "herecomethefuzz", "posted_at": "2024-04-15 11:00:16", "comments": 0, "source_title": "Filesystem error handling", "source_text": "Filesystem error handling\n\nFilesystem error handling | Patreon\n\nWe\u2019re going to reproduce some results from papers on filesystem robustness\nthat were written up roughly a decade ago: Prabhakaran et al. SOSP 05 paper,\nwhich injected errors below the filesystem and Gunawi et al. FAST 08, which\nlooked at how often filesystems failed to check return codes of functions that\ncan return errors.\n\nPrabhakaran et al. injected errors at the block device level (just underneath\nthe filesystem) and found that ext3, resierfs, ntfs, and jfs mostly handled\nread errors reasonbly but ext3, ntfs, and jfs mostly ignored write errors.\nWhile the paper is interesting, someone installing Linux on a system today is\nmuch more likely to use ext4 than any of the now-dated filesystems tested by\nPrahbhakaran et al. We\u2019ll try to reproduce some of the basic results from the\npaper on more modern filesystems like ext4 and btrfs, some legacy filesystems\nlike exfat, ext3, and jfs, as well as on overlayfs.\n\nGunawi et al. found that errors weren\u2019t checked most of the time. After we\nlook at error injection on modern filesystems, we\u2019ll look at how much (or\nlittle) filesystems have improved their error handling code.\n\n### Error injection\n\nA cartoon view of a file read might be: pread syscall -> OS generic filesystem\ncode -> filesystem specific code -> block device code -> device driver ->\ndevice controller -> disk. Once the disk gets the request, it sends the data\nback up: disk -> device controller -> device driver -> block device code ->\nfilesystem specific code -> OS generic filesystem code -> pread. We\u2019re going\nto look at error injection at the block device level, right below the file\nsystem.\n\nLet\u2019s look at what happened when we injected errors in 2017 vs. what\nPrabhakaran et al. found in 2005.\n\n2005| 2017  \n---|---  \nread| write| silent| read| write| silent| read| write| silent  \nfile| mmap  \nbtrfs| prop| prop| prop| prop| prop| prop  \nexfat| prop| prop| ignore| prop| prop| ignore  \next3| prop| ignore| ignore| prop| prop| ignore| prop| prop| ignore  \next4| prop| prop| ignore| prop| prop| ignore  \nfat| prop| prop| ignore| prop| prop| ignore  \njfs| prop| ignore| ignore| prop| ignore| ignore| prop| prop| ignore  \nreiserfs| prop| prop| ignore  \nxfs| prop| prop| ignore| prop| prop| ignore  \n  \nEach row shows results for one filesystem. read and write indicating reading\nand writing data, respectively, where the block device returns an error\nindicating that the operation failed. silent indicates a read failure\n(incorrect data) where the block device didn\u2019t indicate an error. This could\nhappen if there\u2019s disk corruption, a transient read failure, or a transient\nwrite failure silently caused bad data to be written. file indicates that the\noperation was done on a file opened with open and mmap indicates that the test\nwas done on a file mapped with mmap. ignore (red) indicates that the error was\nignored, prop (yellow) indicates that the error was propagated and that the\npread or pwrite syscall returned an error code, and fix (green) indicates that\nthe error was corrected. No errors were corrected. Grey entries indicate\nconfigurations that weren\u2019t tested.\n\nFrom the table, we can see that, in 2005, ext3 and jfs ignored write errors\neven when the block device indicated that the write failed and that things\nhave improved, and that any filesystem you\u2019re likely to use will correctly\ntell you that a write failed. jfs hasn\u2019t improved, but jfs is now rarely used\noutside of legacy installations.\n\nNo tested filesystem other than btrfs handled silent failures correctly. The\nother filesystems tested neither duplicate nor checksum data, making it\nimpossible for them to detect silent failures. zfs would probably also handle\nsilent failures correctly but wasn\u2019t tested. apfs, despite post-dating btrfs\nand zfs, made the explicit decision to not checksum data and silently fail on\nsilent block device errors. We\u2019ll discuss this more later.\n\nIn all cases tested where errors were propagated, file reads and writes\nreturned EIO from pread or pwrite, respectively; mmap reads and writes caused\nthe process to receive a SIGBUS signal.\n\nThe 2017 tests above used an 8k file where the first block that contained file\ndata either returned an error at the block device level or was corrupted,\ndepending on the test. The table below tests the same thing, but with a 445\nbyte file instead of an 8k file. The choice of 445 was arbitrary.\n\n2005| 2017  \n---|---  \nread| write| silent| read| write| silent| read| write| silent  \nfile| mmap  \nbtrfs| fix| fix| fix| fix| fix| fix  \nexfat| prop| prop| ignore| prop| prop| ignore  \next3| prop| ignore| ignore| prop| prop| ignore| prop| prop| ignore  \next4| prop| prop| ignore| prop| prop| ignore  \nfat| prop| prop| ignore| prop| prop| ignore  \njfs| prop| ignore| ignore| prop| ignore| ignore| prop| prop| ignore  \nreiserfs| prop| prop| ignore  \nxfs| prop| prop| ignore| prop| prop| ignore  \n  \nIn the small file test table, all the results are the same, except for btrfs,\nwhich returns correct data in every case tested. What\u2019s happening here is that\nthe filesystem was created on a rotational disk and, by default, btrfs\nduplicates filesystem metadata on rotational disks (it can be configured to do\nso on SSDs, but that\u2019s not the default). Since the file was tiny, btrfs packed\nthe file into the metadata and the file was duplicated along with the\nmetadata, allowing the filesystem to fix the error when one block either\nreturned bad data or reported a failure.\n\n#### Overlay\n\nOverlayfs allows one file system to be \u201coverlaid\u201d on another. As explained in\nthe initial commit, one use case might be to put an (upper) read-write\ndirectory tree on top of a (lower) read-only directory tree, where all\nmodifications go to the upper, writable layer.\n\nAlthough not listed on the tables, we also tested every filesystem other than\nfat as the lower filesystem with overlay fs (ext4 was the upper filesystem for\nall tests). Every filessytem tested showed the same results when used as the\nbottom layer in overlay as when used alone. fat wasn\u2019t tested because mounting\nfat resulted in a filesystem not supported error.\n\n#### Error correction\n\nbtrfs doesn\u2019t, by default, duplicate metadata on SSDs because the developers\nbelieve that redundancy wouldn\u2019t provide protection against errors on SSD\n(which is the same reason apfs doesn\u2019t have redundancy). SSDs do a kind of\nwrite coalescing, which is likely to cause writes which happen consecutively\nto fall into the same block. If that block has a total failure, the redundant\ncopies would all be lost, so redundancy doesn\u2019t provide as much protection\nagainst failure as it would on a rotational drive.\n\nI\u2019m not sure that this means that redundancy wouldn\u2019t help -- Individual flash\ncells degrade with operation and lose charge as they age. SSDs have built-in\nwear-leveling and error-correction that\u2019s designed to reduce the probability\nthat a block returns bad data, but over time, some blocks will develop so many\nerrors that the error-correction won\u2019t be able to fix the error and the block\nwill return bad data. In that case, a read should return some bad bits along\nwith mostly good bits. AFAICT, the publicly available data on SSD error rates\nseems to line up with this view.\n\n#### Error detection\n\nRelatedly, it appears that apfs doesn\u2019t checksum data because \u201c[apfs]\nengineers contend that Apple devices basically don\u2019t return bogus data\u201d.\nPublicly available studies on SSD reliability have not found that there\u2019s a\nmodel that doesn\u2019t sometimes return bad data. It\u2019s a common conception that\nSSDs are less likely to return bad data than rotational disks, but when Google\nstudied this across their drives, they found:\n\n> The annual replacement rates of hard disk drives have previously been\n> reported to be 2-9% [19,20], which is high compared to the 4-10% of flash\n> drives we see being replaced in a 4 year period. However, flash drives are\n> less attractive when it comes to their error rates. More than 20% of flash\n> drives develop uncorrectable errors in a four year period, 30-80% develop\n> bad blocks and 2-7% of them develop bad chips. In comparison, previous work\n> [1] on HDDs reports that only 3.5% of disks in a large population developed\n> bad sectors in a 32 months period \u2013 a low number when taking into account\n> that the number of sectors on a hard disk is orders of magnitudes larger\n> than the number of either blocks or chips on a solid state drive, and that\n> sectors are smaller than blocks, so a failure is less severe.\n\nWhile there is one sense in which SSDs are more reliable than rotational\ndisks, there\u2019s also a sense in which they appear to be less reliable. It\u2019s not\nimpossible that Apple uses some kind of custom firmware on its drive that\ndevotes more bits to error correction than you can get in publicly available\ndisks, but even if that\u2019s the case, you might plug a non-apple drive into your\napple computer and want some kind of protection against data corruption.\n\n### Internal error handling\n\nNow that we\u2019ve reproduced some tests from Prabhakaran et al., we\u2019re going to\nmove on to Gunawi et al.. Since the paper is fairly involved, we\u2019re just going\nto look at one small part of the paper, the part where they examined three\nfunction calls, filemap_fdatawait, filemap_fdatawrite, and sync_blockdev to\nsee how often errors weren\u2019t checked for these functions.\n\nTheir justification for looking at these function is given as:\n\n> As discussed in Section 3.1, a function could return more than one error\n> code at the same time, and checking only one of them suffices. However, if\n> we know that a certain function only returns a single error code and yet the\n> caller does not save the return value properly, then we would know that such\n> call is really a flaw. To find real flaws in the file system code, we\n> examined three important functions that we know only return single error\n> codes: sync_blockdev, filemap_fdatawrite, and filemap_fdatawait. A file\n> system that does not check the returned error codes from these functions\n> would obviously let failures go unnoticed in the upper layers.\n\nIgnoring errors from these functions appears to have fairly serious\nconsequences. The documentation for filemap_fdatawait says:\n\n> filemap_fdatawait \u2014 wait for all under-writeback pages to complete ... Walk\n> the list of under-writeback pages of the given address space and wait for\n> all of them. Check error status of the address space and return it. Since\n> the error status of the address space is cleared by this function, callers\n> are responsible for checking the return value and handling and/or reporting\n> the error.\n\nThe comment next to the code for sync_blockdev reads:\n\n> Write out and wait upon all the dirty data associated with a block device\n> via its mapping. Does not take the superblock lock.\n\nIn both of these cases, it appears that ignoring the error code could mean\nthat data would fail to get written to disk without notifying the writer that\nthe data wasn\u2019t actually written?\n\nLet\u2019s look at how often calls to these functions didn\u2019t completely ignore the\nerror code:\n\nfn| 2008| '08 %| 2017| '17 %  \n---|---|---|---|---  \nfilemap_fdatawait| 7 / 29| 24| 12 / 17| 71  \nfilemap_fdatawrite| 17 / 47| 36| 13 / 22| 59  \nsync_blockdev| 6 / 21| 29| 7 / 23| 30  \n  \nThis table is for all code in linux under fs. Each row shows data for calls of\none function. For each year, the leftmost cell shows the number of calls that\ndo something with the return value over the total number of calls. The cell to\nthe right shows the percentage of calls that do something with the return\nvalue. \u201cDo something\u201d is used very loosely here -- branching on the return\nvalue and then failing to handle the error in either branch, returning the\nreturn value and having the caller fail to handle the return value, as well as\nsaving the return value and then ignoring it are all considered doing\nsomething for the purposes of this table.\n\nFor example Gunawi et al. noted that cifs/transport.c had\n\n    \n    \n    int SendReceive () { int rc; rc = cifs_sign_smb(); // ... rc = smb_send(); }\n\nAlthough cifs_sign_smb returned an error code, it was never checked before\nbeing overwritten by smb_send, which counted as being used for our purposes\neven though the error wasn\u2019t handled.\n\nOverall, the table appears to show that many more errors are handled now than\nwere handled in 2008 when Gunawi et al. did their analysis, but it\u2019s hard to\nsay what this means from looking at the raw numbers because it might be ok for\nsome errors not to be handled and different lines of code are executed with\ndifferent probabilities.\n\n### Conclusion\n\nFilesystem error handling seems to have improved. Reporting an error on a\npwrite if the block device reports an error is perhaps the most basic error\npropagation a robust filesystem should do; few filesystems reported that error\ncorrectly in 2005. Today, most filesystems will correctly report an error when\nthe simplest possible error condition that doesn\u2019t involve the entire drive\nbeing dead occurs if there are no complicating factors.\n\nMost filesystems don\u2019t have checksums for data and leave error detection and\ncorrection up to userspace software. When I talk to server-side devs at big\ncompanies, their answer is usually something like \u201cwho cares? All of our file\naccesses go through a library that checksums things anyway and redundancy\nacross machines and datacenters takes care of failures, so we only need error\ndetection and not correction\u201d. While that\u2019s true for developers at certain big\ncompanies, there\u2019s a lot of software out there that isn\u2019t written robustly and\njust assumes that filesystems and disks don\u2019t have errors.\n\nThis was a joint project with Wesley Aptekar-Cassels; the vast majority of the\nwork for the project was done while pair programming at RC. We also got a lot\nof help from Kate Murphy. Both Wesley (w.aptekar@gmail.com) and Kate\n(hello@kate.io) are looking for work. They\u2019re great and I highly recommend\ntalking to them if you\u2019re hiring!\n\n### Appendix: error handling in C\n\nA fair amount of effort has been applied to get error handling right. But C\nmakes it very easy to get things wrong, even when you apply a fair amount\neffort and even apply extra tooling. One example of this in the code is the\nsubmit_one_bio function. If you look at the definition, you can see that it\u2019s\nannotated with __must_check, which will cause a compiler warning when the\nresult is ignored. But if you look at calls of submit_one_bio, you\u2019ll see that\nits callers aren\u2019t annotated and can ignore errors. If you dig around enough\nyou\u2019ll find one path of error propagation that looks like:\n\n    \n    \n    submit_one_bio submit_extent_page __extent_writepage extent_write_full_page write_cache_pages generic_writepages do_writepages __filemap_fdatawrite_range __filemap_fdatawrite filemap_fdatawrite\n\nNine levels removed from submit_one_bio, we see our old friend,\n`filemap_fdatawrite, which we know often doesn\u2019t get checked for errors.\n\nThere's a very old debate over how to prevent things like this from\naccidentally happening. One school of thought, which I'll call the Uncle Bob\n(UB) school believes that we can't fix these kinds of issues with tools or\nprocesses and simply need to be better programmers in order to avoid bugs.\nYou'll often hear people of the UB school say things like, \"you can't get rid\nof all bugs with better tools (or processes)\". In his famous and well-regarded\ntalk, Simple Made Easy, Rich Hickey says\n\n> What's true of every bug found in the field?\n>\n> [Audience reply: Someone wrote it?] [Audience reply: It got written.]\n>\n> It got written. Yes. What's a more interesting fact about it? It passed the\n> type checker.\n>\n> [Audience laughter]\n>\n> What else did it do?\n>\n> [Audience reply: (Indiscernible)]\n>\n> It passed all the tests. Okay. So now what do you do? Right? I think we're\n> in this world I'd like to call guardrail programming. Right? It's really\n> sad. We're like: I can make change because I have tests. Who does that? Who\n> drives their car around banging against the guardrail saying, \"Whoa! I'm\n> glad I've got these guardrails because I'd never make it to the show on\n> time.\"\n>\n> [Audience laughter]\n\nIf you watch the talk, Rich uses \"simplicity\" the way Uncle Bob uses\n\"discipline\". They way these statements are used, they're roughly equivalent\nto Ken Thompson saying \"Bugs are bugs. You write code with bugs because you\ndo\". The UB school throws tools and processes under the bus, saying that it's\nunsafe to rely solely on tools or processes.\n\nRich's rhetorical trick is brilliant -- I've heard that line quoted tens of\ntimes since the talk to argue against tests or tools or types. But, like\nguardrails, most tools and processes aren't about eliminating all bugs,\nthey're about reducing the severity or probability of bugs. If we look at this\nparticular function call, we can see that a static analysis tool failed to\nfind this bug. Does that mean that we should give up on static analysis tools?\nA static analysis tool could look for all calls of submit_one_bio and show you\nthe cases where the error is propagated up N levels only to be dropped. Gunawi\net al. did exactly that and found a lot of bugs. A person basically can't do\nthe same thing without tooling. They could try, but people are lucky if they\nget 95% accuracy when manually digging through things like this. The sheer\nvolume of code guarantees that a human doing this by hand would make mistakes.\n\nEven better than a static analysis tool would be a language that makes it\nharder to accidentally forget about checking for an error. One of the issues\nhere is that it's sometimes valid to drop an error. There are a number of\nplaces where there's no interace that allows an error to get propagated out of\nthe filesystem, making it correct to drop the error, modulo changing the\ninterface. In the current situation, as an outsider reading the code, if you\nlook at a bunch of calls that drop errors, it's very hard to say, for all of\nthem, which of those is a bug and which of those is correct. If the default is\nthat we have a kind of guardrail that says \"this error must be checked\",\npeople can still incorrectly ignore errors, but you at least get an annotation\nthat the omission was on purpose. For example, if you're forced to\nspecifically write code that indicates that you're ignoring an error, and in\ncode that's inteded to be robust, like filesystem code, code that drops an\nerror on purpose is relatively likely to be accompanied by a comment\nexplaining why the error was dropped.\n\n### Appendix: why wasn't this done earlier?\n\nAfter all, it would be nice if we knew if modern filesystems could do basic\ntasks correctly. Filesystem developers probably know this stuff, but since I\ndon't follow LKML, I had no idea whether or not things had improved since 2005\nuntil we ran the experiment.\n\nThe papers we looked at here came out of Andrea and Remzi Arpaci-Dusseau's\nresearch lab. Remzi has a talk where he mentioned that grad students don't\nwant to reproduce and update old work. That's entirely reasonable, given the\nincentives they face. And I don't mean to pick on academia here -- this work\ncame out of academia, not industry. It's possible this kind of work simply\nwouldn't have happened if not for the academic incentive system.\n\nIn general, it seems to be quite difficult to fund work on correctness. There\nare a fair number of papers on new ways to find bugs, but that's relatively\nlittle work on applying existing techniques to existing code. In academia,\nthat seems to be hard to get a good publication out of, in the open source\nworld, that seems to be less interesting to people than writing new code.\nThat's also entirely reasonable -- people should work on what they want, and\neven if they enjoy working on correctness, that's probably not a great career\ndecision in general. I was at the RC career fair the other night and my badge\nsaid I was interested in testing. The first person who chatted me up opened\nwith \"do you work in QA?\". Back when I worked in hardware, that wouldn't have\nbeen a red flag, but in software, \"QA\" is code for a low-skill, tedious, and\npoorly paid job. Much of industry considers testing and QA to be an\nafterthought. As a result, open source projects that companies rely on are\noften woefully underfunded. Google funds some great work (like afl-fuzz), but\nthat's the exception and not the rule, even within Google, and most companies\ndon't fund any open source work. The work in this post was done by a few\npeople who are intentionally temporarily unemployed, which isn't really a\nscalable model.\n\nOccasionally, you'll see someone spend a lot of effort on immproving\ncorrectness, but that's usually done as a massive amount of free labor. Kyle\nKingsbury might be the canonical example of this -- my understanding is that\nhe worked on the Jepsen distributed systems testing tool on nights and\nweekends for years before turning that into a consulting business. It's great\nthat he did that -- he showed that almost every open source distributed system\nhad serious data loss or corruption bugs. I think that's great, but stories\nabout heoric effort like that always worry me because heroism doesn't scale.\nIf Kyle hadn't come along, would most of the bugs that he and his tool found\nstill plague open source distributed systems today? That's a scary thought.\n\nIf I knew how to fund more work on correctness, I'd try to convince you that\nwe should switch to this new model, but I don't know of a funding model that\nworks. I've set up a patreon (donation account), but it would be quite\nextraordinary if that was sufficient to actually fund a signifcant amount of\nwork. If you look at how much programmers make off of donations, if I made two\norder of magnitude less than I could if I took a job in industry, that would\nalready put me in the top 1% of programmers on patreon. If I made one order of\nmagnitude less than I'd make in industry, that would be extraordinary. Off the\ntop of my head, the only programmers who make more than that off of patreon\neither make something with much broader appeal (like games) or are Evan You,\nwho makes one of the most widely use front-end libraries in existence. And if\nI actually made as much as I can make in industry, I suspect that would make\nme the highest grossing programmer on patreon, even though, by industry\nstandards, my compensation hasn't been anything special.\n\nIf I had to guess, I'd say that part of the reason it's hard to fund this kind\nof work is that consumers don't incentivize companies to fund this sort of\nwork. If you look at \"big\" tech companies, two of them are substantially more\nserious about correctness than their competitors. This results in many fewer\nhorror stories about lost emails and documents as well as lost entire\naccounts. If you look at the impact on consumers, it might be something like\nthe difference between 1% of people seeing lost/corrupt emails vs. 0.001%. I\nthink that's pretty significant if you multiply that cost across all\nconsumers, but the vast majority of consumers aren't going to make decisions\nbased on that kind of difference. If you look at an area where correctness\nproblems are much more apparent, like databases or backups, you'll find that\neven the worst solutions have defenders who will pop into any dicussions and\nsay \"works for me\". A backup solution that works 90% of the time is quite bad,\nbut if you have one that works 90% of the time, it will still have staunch\ndefenders who drop into discussions to say things like \"I've restored from\nbackup three times and it's never failed! You must be making stuff up!\". I\ndon't blame companies for rationally responding to consumers, but I do think\nthat the result is unfortunate for consumers.\n\nJust as an aside, one of the great wonders of doing open work for free is that\nthe more free work you do, the more people complain that you didn't do enough\nfree work. As David MacIver has said, doing open source work is like doing\nnormal paid work, except that you get paid in complaints instead of cash. It's\nbasically guaranteed that the most common comment on this post, for all time,\nwill be that didn't test someone's pet filesystem because we're btrfs shills\nor just plain lazy, even though we include a link to a repo that lets anyone\nadd tests as they please. Pretty much every time I've done any kind of free\nexperimental work, people who obvously haven't read the experimental setup or\nthe source code complain that the experiment couldn't possibly be right\nbecause of [thing that isn't true that anyone could see by looking at the\nsetup] and that it's absolutely inexcusable that I didn't run the experiment\non the exact pet thing they wanted to see. Having played video games\ncompetitively in the distant past, I'm used to much more intense internet\ntrash talk, but in general, this incentive system seems to be backwards.\n\n### Appendix: experimental setup\n\nFor the error injection setup, a high-level view of the experimental setup is\nthat dmsetup was used to simulate bad blocks on the disk.\n\nA list of the commands run looks something like:\n\n    \n    \n    cp images/btrfs.img.gz /tmp/tmpeas9efr6.gz gunzip -f /tmp/tmpeas9efr6.gz losetup -f losetup /dev/loop19 /tmp/tmpeas9efr6 blockdev --getsize /dev/loop19 # 0 74078 linear /dev/loop19 0 # 74078 1 error # 74079 160296 linear /dev/loop19 74079 dmsetup create fserror_test_1508727591.4736078 mount /dev/mapper/fserror_test_1508727591.4736078 /mnt/fserror_test_1508727591.4736078/ mount -t overlay -o lowerdir=/mnt/fserror_test_1508727591.4736078/,upperdir=/tmp/tmp4qpgdn7f,workdir=/tmp/tmp0jn83rlr overlay /tmp/tmpeuot7zgu/ ./mmap_read /tmp/tmpeuot7zgu/test.txt umount /tmp/tmpeuot7zgu/ rm -rf /tmp/tmp4qpgdn7f rm -rf /tmp/tmp0jn83rlr umount /mnt/fserror_test_1508727591.4736078/ dmsetup remove fserror_test_1508727591.4736078 losetup -d /dev/loop19 rm /tmp/tmpeas9efr6\n\nSee this github repo for the exact set of commands run to execute tests.\n\nNote that all of these tests were done on linux, so fat means the linux fat\nimplementation, not the windows fat implementation. zfs and reiserfs weren\u2019t\ntested because they couldn\u2019t be trivially tested in the exact same way that we\ntested other filesystems (one of us spent an hour or two trying to get zfs to\nwork, but its configuration interface is inconsistent with all of the\nfilesystems tested; reiserfs appears to have a consistent interface but\ntesting it requires doing extra work for a filesystem that appears to be\ndead). ext3 support is now provided by the ext4 code, so what ext3 means now\nis different from what it meant in 2005.\n\nAll tests were run on both ubuntu 17.04, 4.10.0-37, as well as on arch,\n4.12.8-2. We got the same results on both machines. All filesystems were\nconfigured with default settings. For btrfs, this meant duplicated metadata\nwithout duplicated data and, as far as we know, the settings wouldn't have\nmade a difference for other filesystems.\n\nThe second part of this doesn\u2019t have much experimental setup to speak of. The\nsetup was to grep the linux source code for the relevant functions.\n\nThanks to Leah Hanson, David Wragg, Ben Kuhn, Wesley Aptekar-Cassels, Joel\nBorggr\u00e9n-Franck, Yuri Vishnevsky, and Dan Puttick for comments/corrections on\nthis post.\n\n", "frontpage": false}
