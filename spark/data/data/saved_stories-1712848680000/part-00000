{"aid": "40000104", "title": "Flow-IPC: Open-Source Toolkit for Low-Latency Inter-Process Communication in C++", "url": "https://www.linode.com/blog/open-source/flow-ipc-introduction-low-latency-cpp-toolkit/", "domain": "linode.com", "votes": 1, "user": "signa11", "posted_at": "2024-04-11 09:15:59", "comments": 0, "source_title": "Introduction to Flow-IPC: Open Source Toolkit for Low-Latency Inter-Process Communication in C++", "source_text": "Introduction to Flow-IPC: Open Source Toolkit for Low-Latency Inter-Process Communication in C++ | Akamai\n\nSkip to main content\n\n  * Espa\u00f1ol\n  * \u65e5\u672c\u8a9e\n  * Deutsch\n  * Portugu\u00eas\n  * \ud55c\uad6d\uc5b4\n  * Fran\u00e7ais\n  * Italiano\n  * \u4e2d\u6587 (\u7b80\u4f53)\n  * Portugu\u00eas Brasileiro\n\nView All Products\n\n###### Compute\n\n###### Storage\n\n###### Databases\n\n###### Networking\n\n###### Developer Tools\n\n###### Delivery\n\n###### Security\n\n###### Services\n\n###### Industries\n\n###### Pricing\n\n###### Community\n\n###### Engage With Us\n\nExplore Blog\n\nCategories\n\n14\n\n  1. Cloud Consulting Services\n\n4\n\n  2. Cloud Overviews\n\n50\n\n  3. Compute\n\n15\n\n  4. Containers (Kubernetes, Docker)\n\n33\n\n  5. Databases\n\n19\n\n  6. Developer Tools\n\n37\n\n  7. Linode\n\n254\n\n  8. Linux\n\n69\n\n  9. Multicloud\n\n4\n\n  10. Networking\n\n32\n\n  11. Open Source\n\n6\n\n  12. Partner Network\n\n7\n\n  13. Security\n\n60\n\n  14. Storage\n\n22\n\nAuthors 53\n\n  1. Austin Gil 1\n  2. Abe Massry 2\n  3. Al Newkirk 1\n  4. Alexander Newnham 2\n  5. Andrew Sauber 1\n  6. Andrew Stebbins 2\n  7. Andy Stevens 5\n  8. Bradley LaBoon 1\n  9. Blair Lyon 11\n  10. Billy Thompson 10\n  11. Christopher Aker 176\n  12. Cassie Bubnis 1\n  13. Daniele Polencic 4\n  14. David Monschein 2\n  15. David Rodriguez 1\n  16. Dan Spataro 2\n  17. Elvis Segura 1\n  18. Gardiner Bryant 1\n  19. Holden Morris 4\n  20. Hillary Wilmoth 67\n  21. Jim Ackley 5\n  22. Jessica Capuano Mora 1\n  23. Justin Cobbett 15\n  24. Jon Frederickson 1\n  25. Jonathan Hill 2\n  26. Justin Mitchel 4\n  27. James Steel 5\n  28. Jamie Sherkness 8\n  29. Linode 52\n  30. Linode Events 8\n  31. The Linode Security Team 122\n  32. The Linode Network & Security Teams 1\n  33. LINQ 1\n  34. Leslie Salazar 1\n  35. Michelle Berg 1\n  36. Mitch Donaberger 1\n  37. Mike Fischler 1\n  38. Mike Maney 10\n  39. Maddie Presland 18\n  40. Mike Quatrani 4\n  41. Nathan Melehan 2\n  42. Nigel Poulton 1\n  43. Prasoon Pushkar 1\n  44. Rick Myers 7\n  45. Richard Tubb 1\n  46. Sal Carrasco 1\n  47. Salman Iqbal 1\n  48. Shawn Michels 1\n  49. Linode Support 13\n  50. Tom Asaro 18\n  51. Travis Baka 1\n  52. Talia Nassi 8\n  53. Yuri Goldfeld 1\n\nBlogOpen SourceIntroduction to Flow-IPC: Open Source Toolkit for Low-Latency\nInter-Process Communication in C++\n\n# Introduction to Flow-IPC: Open Source Toolkit for Low-Latency Inter-Process\nCommunication in C++\n\nYuri Goldfeld\n\nApril 2, 2024\n\n### About the Author\n\nYuri Goldfeld is a Senior Principal Software Engineer at Akamai Technologies\n\n### Sign up for the \u201cIn the Node\u201d Newsletter\n\nThe form is loading\n\nPlease reload the page and try again.\n\nWe recently released Flow-IPC \u2013 an interprocess communication toolkit in C++ \u2013\nas open source under the Apache 2.0 and MIT licenses. Flow-IPC will be useful\nfor C++ projects that transmit data between application processes and need to\nachieve near-zero latency without a trade-off against simple and reusable\ncode.\n\nIn the announcement, we showed that Flow-IPC can transmit data structure\npayloads as large as 1GB just as quickly as a 100K payload\u2013 and in less than\n100 microseconds. With classic IPC, the latency depends on the payload size\nand can reach into the 1 second range. Thus the improvement can be as much as\nthree or four orders of magnitude.\n\nIn this post, we show the source code that produced those numbers. Our\nexample, which centers on the Cap\u2019n Proto integration, shows that Flow-IPC is\nboth fast and easy to use. (Note that the Flow-IPC API is comprehensive in\nthat it supports transmitting various types of payloads, but the Cap\u2019n Proto-\nbased payload transmission is a specific feature.) Cap\u2019n Proto is an open\nsource project not affiliated with Akamai whose use is subject to the license,\nas of the date of this blog\u2019s publication date, found here.\n\nWhat\u2019s Included?\n\nFlow-IPC is a library with an extensible C++17 API. It is hosted in GitHub\ntogether with full documentation, automated tests and demos, and a CI\npipeline. The example we explore below is the perf_demo test application.\nFlow-IPC currently supports Linux running on x86-64. We have plans to expand\nthis to macOS and ARM64, followed by Windows and other OS variants depending\non demand. You\u2019re welcome to contribute and port.\n\nThe Flow-IPC API is in the spirit of the C++ standard library and Boost,\nfocusing on integrating various concepts and their implementations in a\nmodular way. It is designed to be extensible. Our CI pipeline tests across a\nrange of GCC and Clang compiler versions and build configurations, including\nhardening via runtime sanitizers: ASAN (hardens against memory misuse), TSAN\n(against race conditions), and UBSAN (against miscellaneous undefined\nbehavior).\n\nAt this time, Flow-IPC is for local communication: crossing process boundaries\nbut not machine boundaries. However it has an extensible design, so expanding\nit to networked IPC is a natural next step. We think the use of Remote Direct\nMemory Access (RDMA) provides an intriguing possibility for ultra-fast LAN\nperformance.\n\nWho Should Use It?\n\nFlow-IPC is a pragmatic inter-process communication toolkit. In designing it,\nwe came from the perspective of the modern C++ systems developer, specifically\ntailoring it to the IPC tasks one faces repeatedly, particularly in server\napplication development. Many of us have had to throw together a Unix-domain-\nsocket, named-pipe, or local HTTP-based protocol to transmit something from\none process to another. Sometimes, to avoid the copying involved in such\nsolutions, one might turn to shared-memory (SHM), a notoriously touchy and\nhard-to-reuse technique. Flow-IPC can help any C++ developer faced with such\ntasks, from common to advanced.\n\nHighlights include:\n\n  * Cap\u2019n Proto integration: Tools for in-place schema-based serialization like Cap\u2019n Proto, which is best-in-class, are very helpful for inter-process work. However, without Flow-IPC, you\u2019d still have to copy the bits into a socket or pipe, etc., and then copy them again on receipt. Flow-IPC provides end-to-end zero-copy transmission of Cap\u2019n Proto-encoded structures using shared memory.\n  * Socket/FD support: Any message transmitted via Flow-IPC can include a native I/O handle (also known as a file descriptor or FD). For example, in a web server architecture, you could split the server into two processes: a process for managing endpoints and a process for processing requests. After the endpoint process completes the TLS negotiation, it can pass the connected TCP socket handle directly to the request processing process.\n  * Native C++ structure support: Many algorithms require work directly on C++ structs, most often ones involving multiple levels of STL containers and/or pointers. Two threads collaborating on such a structure is common and easy to code whereas two processes doing so via shared-memory is quite hard \u2013 even with thoughtful tools like Boost.interprocess. Flow-IPC simplifies this by enabling sharing of STL-compliant structures like containers, pointers, and plain-old-data.\n  * jemalloc plus SHM: One line of code allows you to allocate any necessary data in shared memory, whether it\u2019s for behind the scenes operations like Cap\u2019n Proto transmission or directly for native C++ data. These tasks can be delegated to jemalloc, the heap engine behind FreeBSD and Meta. This feature can be particularly valuable for projects that require intensive shared memory allocation, similar to regular heap allocation.\n  * No naming or cleanup headaches: With Flow-IPC, you need not name server-sockets, SHM segments, or pipes, or worry about leaked persistent RAM. Instead, establish a Flow-IPC session between processes: this is your IPC context. From this single session object, communication channels can be opened at will, without additional naming. For tasks that require direct shared memory (SHM) access, a dedicated SHM arena is available. Flow-IPC performs automatic cleanup, even in the case of an abnormal exit, and it avoids conflicts among resource names.\n  * Use for RPC: Flow-IPC is designed to complement, not compete with higher-level communication frameworks like gRPC and Cap\u2019n Proto RPC. There\u2019s no need to choose between them and Flow-IPC. In fact, using Flow-IPC\u2019s zero-copy features can generally enhance the performance of these protocols.\n\nExample: Sending a Multi-Part File\n\nWhile Flow-IPC can transmit data of various kinds, we decided to focus on a\ndata structure described by a Cap\u2019n Proto (capnp) schema. This example will be\nparticularly clear to those familiar with capnp and Protocol Buffers, but\nwe\u2019ll give plenty of context for those who are less familiar with these tools.\n\nIn this example, there are two apps that engage in a request-response\nscenario.\n\n  * App 1 (server): This is a memory-caching server that has pre-loaded files ranging from 100kb to 1GB into RAM. It stands by to handle get-cached file requests and issue responses.\n  * App 2 (client): This client requests a file of a certain size. App 1 (server) sends the file data in a single message broken down into a series of chunks. Each chunk includes the data along with its hash.\n\n    \n    \n    # Cap'n Proto schema (.capnp file, generates .h and .c++ source code # using capnp compiler tool): $Cxx.namespace(\"cache_demo::schema\"); struct Body { union { getCacheReq @0 :GetCacheReq; getCacheRsp @1 :GetCacheRsp; } } struct GetCacheReq { fileName @0 :Text; } struct GetCacheRsp { # We simulate the server returning file multiple parts, # each (~equally) sized at its discretion. struct FilePart { data @0 :Data; dataSizeToVerify @1 :UInt64; # Recipient can verify that `data` blob's size is indeed this. dataHashToVerify @2 :Hash; # Recipient can hash `data` and verify it is indeed this. } fileParts @0 :List(FilePart); }\n\nOur goal in this experiment is to issue a request for an N-sized file, receive\nthe response, measure the time it took to process, and check the integrity of\na part of the file. This interaction takes place through a communication\nchannel.\n\nTo do this, we need to first establish this channel. While Flow-IPC allows you\nto manually establish a channel from its low-level constituent parts (local\nsocket, POSIX message queue, and more), it is much easier to use Flow-IPC\nsessions instead. A session is simply the communication context between two\nlive processes. Once established, channels are readily available.\n\nFrom a bird\u2019s eye view, here\u2019s how the process works.\n\nProcess A on the left is called the session-server. The process boxes on the\nright \u2013 session-clients \u2013 connect to Process A in order to establish sessions.\nGenerally, any given session is completely symmetrical, so it doesn\u2019t matter\nwho initiated the connection. Both sides are equally capable and can be\nassigned any algorithmic role. Before the session is ready, though, we\u2019ll need\nto assign roles. One side will be the session-client and will perform an\ninstant connect of a single session, and the other side, the session-server,\nwill accept as many sessions as it wants.\n\nIn this example, the setup is straightforward. There are two apps with one\nsession between them and one channel in that session. The cache-client (App 2)\nplays the role of session-client, and the session-server role is taken by App\n1. However, the reverse would work fine as well.\n\nTo set this up, each application (the cache-client and cache-server) must\nunderstand the same IPC universe, which just means that they need to know\nbasic facts about the applications involved.\n\nHere\u2019s why:\n\n  * The client needs to know how to locate the server to initiate a session. If you\u2019re the connecting-app (the client), you need to know the name of the accepting-app. Flow-IPC uses the server\u2019s name to figure out details like the socket addresses and shared-memory segment names based on this name.\n  * The server app must know who\u2019s allowed to connect to it, for security reasons. Flow-IPC checks the client\u2019s details, like user/group and executable path, against the operating system to ensure everything matches up.\n  * Standard OS safety mechanisms (owners, permissions) apply to various IPC transports (sockets, MQs, SHM). A single selector enum will set the high-level policy to use and Flow-IPC will then set permissions as restrictively as possible while respecting that choice.\n\nFor us, we will just need to execute the following in each of the two\napplications. This can be in a single .cpp file linked into both the cache-\nserver and cache-client apps.\n\n    \n    \n    // IPC app universe: simple structs naming the 2 apps. // The applications should share this code. const ipc::session::Client_app CLI_APP{ \"cacheCli\", // Name the app uniquely. // From where it will run (for safety). \"/usr/bin/cache_client.exec\", CLI_UID, GID }; // The user and group ID (for safety). const ipc::session::Server_app SRV_APP{ { \"cacheSrv\", \"/usr/bin/cache_server.exec\", SRV_UID, GID }, // For the server, provide similar details --^. // Plus a few server-specific settings: // Safety: List client-app names that can connect to server-app. // So in our case this will just be { \"cacheCli\" }. { CLI_APP.m_name }, \"\", // An optional path override; don't worry about it here. // Safety/permissions selector: // We've decided to run the two apps as different users // in the same group - so we indicate that here. ipc::util::Permissions_level::S_GROUP_ACCESS };\n\nNote that more complex setups can have more of these definitions.\n\nHaving executed that code in each of our applications, we\u2019ll simply pass these\nobjects into the session object constructor so that the server will know what\nto expect when accepting sessions and the client will know which server to\nconnect to.\n\nSo let\u2019s open the session. In App 2 (the cache-client), we just want to open a\nsession and a channel within it. While Flow-IPC allows instantly opening\nchannels anytime (given a session object), it is typical to need a certain\nnumber of ready-to-go channels at the start of the session. Since we want to\nopen one channel, we can let Flow-IPC create the channel when creating the\nsession. This avoids any unnecessary asynchronicity. So, at the beginning of\nour cache-client\u2019s main() function, we can connect and open a channel with a\nsingle .sync_connect() call:\n\n    \n    \n    // Specify that we *do* want zero-copy behavior, by merely choosing our // backing-session type. // In other words, setting this alias says, \u201cbe fast about Cap\u2019n Proto things.\u201d // // Different (subsequent) capnp-serialization-backing and SHM-related behaviors // are available; just change this alias. E.g., omit `::shm::classic` to disable // SHM entirely; or specify `::shm::arena_lend::jemalloc` to employ // jemalloc-based SHM. Subsequent code remains the same! // This demonstrates a key design tenet of Flow-IPC. using Session = ipc::session::shm::classic::Client_session<...>; // Tell Session object about the applications involved. Session session{ CLI_APP, SRV_APP, /* detail omitted */ }; // Ask for 1 *channel* to be available on both sides // from the very start of the session. Session::Channels ipc_raw_channels(1); // Instantly open session - and the 1 channel. // (Fail if server is not running at this time.) session.sync_connect(session.mdt_builder(), &ipc_raw_channels); auto& ipc_raw_channel = ipc_raw_channels[0]; // (Can also instantly open more channel(s) anytime: // `session.open_channel(&channel)`.)\n\nWe should have a ipc_raw_channel now, which is a basic channel object.\nDepending on specific settings, this could represent a Unix-domain stream\nsocket, a POSIX MQ, or other types of channels. If we wanted to, we could use\nit in unstructured fashion immediately, meaning we could use it to transmit\nbinary blobs (with boundaries preserved) and/or native handles (FDs). We could\nalso directly access an SHM arena via\nsession.session_shm()->construct<T>(...). This is outside our discussion scope\nhere, but it\u2019s a powerful capability worth mentioning.\n\nFor now, we just want to speak the Cap\u2019n Proto cache_demo::schema::Body\nprotocol (from our .capnp file). So we upgrade the raw channel object to a\nstructured channel object like so:\n\n    \n    \n    // Template arg indicates capnp schema. (Take a look at the .capnp file above.) Session::Structured_channel<cache_demo::schema::Body> ipc_channel { nullptr, std::move(ipc_raw_channel), \u201cEat\u201d the raw channel: take over it. ipc::transport::struc::Channel_base::S_SERIALIZE_VIA_SESSION_SHM, &session };\n\nThat\u2019s it for our setup. We are now ready to exchange capnp messages over the\nchannel. Notice that we\u2019ve bypassed dealing with OS specific details like\nobject names, Unix permissions values, and so on. Our approach was merely to\nname our two applications. We also opted for end-to-end zero-copy transmission\nto maximize performance by leveraging shared-memory without a single\n::shm_open() or ::mmap() in sight.\n\nWe are now ready for the fun part: issuing the GetCacheReq request, receiving\nthe GetCacheRsp response, and accessing various parts of that response, namely\nthe file parts and their hashes.\n\nHere\u2019s the code:\n\n    \n    \n    // Issue request and process response. TIMING FOR LATENCY GRAPH STARTS HERE --> auto req_msg = ipc_channel.create_msg(); req_msg.body_root() // Vanilla capnp code: call Cap'n Proto-generated mutator API. ->initGetCacheReq().setFileName(\"huge-file.bin\"); // Send message; get ~instant reply. const auto rsp_msg = ipc_channel.sync_request(req_msg); // More vanilla capnp work: accessors. const auto rsp_root = rsp_msg->body_root().getGetCacheRsp(); // <-- TIMING FOR LATENCY GRAPH STOPS HERE. // ... verify_hash(rsp_root, some_file_chunk_idx); // ... // More vanilla Cap'n Proto accessor code. void verify_hash(const cache_demo::schema::GetCacheRsp::Reader& rsp_root, size_t idx) { const auto file_part = rsp_root.getFileParts()[idx]; if (file_part.getHashToVerify() != compute_hash(file_part.getData())) { throw Bad_hash_exception(...); } }\n\nIn the code above we used the simple .sync_request() which both sends a\nmessage and awaits a specific response. The ipc::transport::struc::Channel API\nprovides a number of niceties to make protocols natural to code, including\nasync receiving, demultiplexing to handler functions by message type,\nnotification versus request, and unsolicited-message versus response. There\nare no restrictions placed on your schema (cache_demo::schema::Body in our\ncase). If it is expressible in capnp, you can use it with Flow-IPC structured\nchannels.\n\nThat\u2019s it! The server side is similar in spirit and level of difficulty. The\nperf_demo source code is available.\n\nWithout Flow-IPC, replicating this setup for end-to-end zero-copy performance\nwould involve a significant amount of difficult code, including management of\nSHM segments whose names and cleanup would have to be coordinated between the\ntwo applications. Even without zero-copy \u2013 i.e., simply ::write()ing a copy of\nthe capnp serialization of req_msg to and ::read()ing rsp_msg from a Unix\ndomain socket FD \u2013 sufficiently robust code would be non-trivial to write in\ncomparison.\n\nThe graph below shows the latencies, where each point on the x-axis represents\nthe sum of all filePart.data sizes for each given test run. The blue line\nshows the latencies from the basic method where App 1 writes capnp\nserializations to a Unix domain socket using ::write(), and App 2 reads them\nwith ::read(). The orange line represents the latencies for the code using\nFlow-IPC, as discussed above.\n\nHow to Contribute\n\nFor feature requests and defect reports, please look at the Issue database on\nthe Flow-IPC GitHub site. File Issues as needed.\n\nTo contribute changes and new features, please consult the contribution guide.\nWe can be reached at the Flow-IPC discussions board on GitHub.\n\nWhat\u2019s Next?\n\nWe tried to provide a realistic experiment in the above example, skipping\nnothing significant in the code shown. We did deliberately choose a scenario\nthat lacks asynchronicity and callbacks. Accordingly, questions will arise,\nsuch as, \u201chow do I integrate Flow-IPC with my event loop?\u201d or, \u201chow do I\nhandle session and channel closing? Can I just open a simple stream socket\nwithout all this session stuff?\u201d, etc.\n\nSuch questions are addressed in the full documentation. The docs include a\nreference generated from API comments in the code, a guided Manual with a\ngentler learning curve, and installation instructions. The main repository\u2019s\nREADME will point you to all of these resources. The Manual intro and API\nsynopsis cover the breadth of available features.\n\nResources\n\n  * Announcement Blog Post\n  * Flow-IPC Project at GitHub To install, read documentation, file feature/change requests, or contribute.\n  * Flow-IPC Discussions Great way to reach us \u2013 and the rest of the community.\n\n##\n\nYou might also like...\n\n### Open Source Plays A Crucial Role In Cloud Computing Explosion | Billy Thompson \u2013 Akamai\n\nAkamai's Billy Thompson explains how open-source software isn't just an\nalternative, it's the foundation of cloud computing.\n\nOpen Source\n\n### Supercharge Your Sales with Odoo CRM | The Open Source and Enterprise Friendly CRM Tool\n\nIn this video, @TechHut checks out Odoo\u2019s CRM function and explains how to\nnavigate the pipeline, track sales leads, and more.\n\nOpen Source\n\n### Open Source Is A Core Part Of Our Ethos | Billy Thompson \u2013 Akamai\n\nIn this video, Swapnil sits down with Billy Thompson, Solutions Engineering\nManager at Akamai, to talk about the importance of open source.\n\nOpen Source\n\n## Comments\n\n### Leave a Reply Cancel reply\n\n\u00a9 2003-2024 Linode LLC. All rights reserved.\n\nCookie Preferences\n\n#### Sign up for the \u201cIn the Node\u201d Newsletter\n\nSign Up Now\n\n", "frontpage": false}
