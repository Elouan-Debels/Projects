{"aid": "40038350", "title": "AWS Hedges Its Bets with Nvidia GPUs and Homegrown AI Chips", "url": "https://www.nextplatform.com/2024/04/12/aws-hedges-its-bets-with-nvidia-gpus-and-homegrown-ai-chips/", "domain": "nextplatform.com", "votes": 1, "user": "rbanffy", "posted_at": "2024-04-15 09:27:31", "comments": 0, "source_title": "AWS Hedges Its Bets With Nvidia GPUs And Homegrown AI Chips", "source_text": "AWS Hedges Its Bets With Nvidia GPUs And Homegrown AI Chips\n\nLatest\n\n  * [ April 12, 2024 ] AWS Hedges Its Bets With Nvidia GPUs And Homegrown AI Chips Compute\n  * [ April 11, 2024 ] Looking To Adopt Generative AI Within Your Organization? AI\n  * [ April 10, 2024 ] With MTIA v2 Chip, Meta Can Do AI Inference, But Not Training AI\n  * [ April 10, 2024 ] Gelsinger: With Gaudi 3 and Xeon 6, AI Workloads Will Come Our Way AI\n  * [ April 9, 2024 ] Google Joins The Homegrown Arm Server CPU Club Compute\n  * [ April 9, 2024 ] With Gaudi 3, Intel Can Sell AI Accelerators To The PyTorch Masses AI\n  * [ April 8, 2024 ] Mixed Results For The Datacenter Thundering Thirteen In Q4 Compute\n  * [ April 4, 2024 ] Intel\u2019s Chips No Longer Pay More Than Their Fair Share Of Foundry Costs Compute\n\n# AWS Hedges Its Bets With Nvidia GPUs And Homegrown AI Chips\n\nApril 12, 2024 Jeffrey Burt Compute 1\n\nThere was a time \u2013 and it doesn\u2019t seem like that long ago \u2013 that the\ndatacenter chip market was a big-money but relatively simple landscape, with\nCPUs from Intel and AMD and Arm looking to muscle its way in and GPUs mostly\nfrom Nvidia with some from AMD and Intel looking to muscle its way in. And a\nslew of AI startups that didn\u2019t really sell much into the datacenter.\n\nThat has changed drastically in recent years.\n\nThere\u2019s still Intel, AMD, Nvidia, and Arm, but there also are a lot more\nchoices when it comes to silicon. There is a massive \u2013 and growing \u2013 amount of\ndata being generated and analyzed, and the more recent emergence of generative\nAI and large language models is giving rise to myriad chip startups looking to\ngain a foothold.\n\nThen there are hyperscalers like Amazon Web Services, Microsoft with its\nupcoming Maia 100, and Google Cloud and its Tensor Processing Units, which are\nmaking their own homegrown processors.\n\nThere is a plethora of silicon options in the market, and cloud infrastructure\nproviders will play a significant role in how all this falls together. About\n70 percent of AI workloads are in the cloud now, and that promises to grow as\nenterprises adopt the technology and expand their workloads.\n\nFor AWS, it has its own Trainium (for training AI workloads obviously) and\nInferentia (for AI inferencing obviously) \u2013 not to mention its Graviton CPUs\nand Nitro DPUs, all thanks to its 2015 acquisition of Israeli chip designer\nAnnapurna. AWS has a lot of Nvidia GPUs, too, which are cornerstones of AI\ncompute. But the rise of AI \u2013 and most recently, the accelerating innovation\nand adoption of the emerging generative AI technology \u2013 is creating a fluid\nprocessor environment that the company and other cloud providers will have to\nnavigate.\n\nAWS is set for the moment with Nvidia GPUs, Trainium, and Inferentia, but how\nthis plays out in the future is a wait-and-see game, according to Chetan\nKapoor, director of Amazon EC2 product management.\n\n\u201cWe\u2019re in the very early stages of understanding how this might settle,\u201d\nKapoor tells The Next Platform. \u201cWhat we do know is that, based on the rapid\ngrowth that you\u2019re seeing in this space, there is a lot of headroom for us to\ncontinue to grow our footprint of Nvidia-based products and at the same time,\nwe\u2019re going to continue to grow our fleet of Trainium and Inferentia capacity.\nIt\u2019s just too early to call how that market is going to be. But it\u2019s not a\nzero-sum game, the way we see it. Because of this exponential growth, there\nwill continue to be phenomenal growth in our fleet of Nvidia GPUs, but at the\nsame time, we\u2019ll continue to find the opportunistic way to land Trainium and\nInferentia for external and internal use.\u201d\n\nLike its competitors, AWS is all in on AI, but what it can do internally and\nwhat it invests in the market. AWS late last month invested another $2.75\nbillion in AI company Anthropic \u2013 bringing its total investment to $4 billion\n\u2013 an investment that came weeks after the cloud provider said Anthropic\u2019s\nClaude 3 family of models were running on Amazon Bedrock AI managed service.\nIt echoes what Microsoft\u2019s partnership with OpenAI (which includes more than\n$10 billion in investments) and Google with Anthropic (more than $2 billion\ninvested).\n\nTo run all this, AWS is sticking with what it has now with Nvidia and its own\nchips, but Kapoor, who essentially calls the shots for the EC2 hardware\nacceleration business, says the company is \u201cgoing to continue to stay engaged\nwith other providers in this space and if other providers like Intel or AMD\nhave a really compelling offering that we think can complement our Nvidia-base\nsolutions, I\u2019m more than happy to collaborate with them in that market.\u201d\n\nAWS doubled down on Nvidia at its recent GTC 2024 show, saying \u2013 as Microsoft\nAzure, Google Cloud, and Oracle Cloud Infrastructure did \u2013 that is adopting\nthe accelerator maker\u2019s new Blackwell GPUs, include the massive GB200 Grace\nBlackwell superchip, which has two B200 GPUs attached to a single Grace CPU\nvia a 600 GB/sec NVLink interconnect.\n\nWhether other AI chips can muscle their way into the AWS environment is\nunclear. Companies like Groq, Mythic, and SambaNova Systems are putting\ntogether processors for AI workloads, but Kapoor says it\u2019s about more than the\naccelerators themselves. OpenAI chief executive officer Sam Altman has floated\nthe idea of the company designing its own AI training and inferencing chips to\nsupplement a tight market that is seeing demand for Nvidia GPUs skyrocket to\nmeeting AI workload demands.\n\n\u201cIt\u2019s really hard to build chips,\u201d he says. \u201cIt\u2019s even harder to build\nservers, and manage and deploy a fleet of tens of thousands, if not hundreds\nof thousands, of these accelerators. But what is even more challenging is\nbuilding a developer ecosystem that takes advantage of this capability. In our\nexperience, it\u2019s not just about silicon. Silicon is part of the offering. But\nthen, how do we provision it as a compute platform? How do we manage and scale\nit? It matters, but what is paramount? How easy to use is that solution? What\ndeveloper ecosystem is available around your offering? Basically, how quickly\ncan customers get their job done?\u201d\n\nThe accelerating adoption of generative AI doesn\u2019t give organizations the\nluxury of spending months learning and using new hardware architectures. What\nthey use needs to be a holistic architecture that is both easy to use and\ncost-effective.\n\n\u201cIt has to have a developer community around it for it to have a traction in\nthe space,\u201d Kapoor says. \u201cIf there\u2019s a startup that is able to accomplish that\nfeat, then great, they\u2019ll be successful. But it\u2019s important to really view\nfrom that lens where it needs to be performant, needs to be cheap, it needs to\nbe broadly available, and really easy to use, which is really, really hard for\neven large corporations to actually get it right.\u201d\n\nOrganizations are under a lot of pressure to adopt AI to keep competitive\nagainst rivals. For companies, running those AI workloads typically comes down\nto performance vs. costs when considering the infrastructure they use.\n\n\u201cWe\u2019re going to see this trend where there\u2019ll be some customers who are just\nfocused on time-to-market, and they\u2019re less focused on making sure they\u2019re\noptimizing their spend,\u201d he says. \u201cThey will tend to prefer a Nvidia-based\nsolution because that gives them the ability to get to market as quickly as\npossible. On the other hand, we\u2019re starting to see this trend already where\nsome of these customers are going to look at this cost and say, \u2018Well, I don\u2019t\nhave the budget to support this,\u2019 and they\u2019re going to look for alternative\nsolutions that provide them the performance they\u2019re looking for, but at the\nsame time give them a way out to save 30 percent or 40 percent over the total\ncost it takes for them to train and deploy these models. That\u2019s where some of\nthese alternative solutions from us or from other silicon partners would come\ninto play.\u201d\n\nThat said, there will continue to be sustained demand for Nvidia products.\nMany of the new foundational models are being built on the vendor\u2019s GPUs\nbecause the research and scientific communities have a lot of experience\nbuilding and training AI models with Nvidia hardware and software, Kapoor\nsays. Also, Nvidia will continue expanding the edges in terms of raw\nperformance that a system can provide. The GPU maker is \u201creally, really good\nat not only building silicon, but these systems, but they\u2019re also phenomenal\nat optimizing performance to make sure that their customers are getting most\nout of these really, really expensive accelerators,\u201d he says.\n\nSo hyperscalers are going to have to keep a sharp ear to what organizations\nare telling them, because while some 70 percent of AI workloads are in the\ncloud now, that will grow in the coming years. The systems AWS and others have\nrunning atop Nvidia\u2019s A100 or H100 chips already are highly complex and at\nscale, and that will only increase with Blackwell, which calls for rack-\nintegrated offerings with technologies like liquid cooling and even more\ndensity.\n\n\u201cThere\u2019s just a lot more enduring complexity on what it takes to design,\nbuild, and actually deploy these kinds of machines, so we expect that\ncustomers that are okay with deploying systems previously on-prem will see a\nlot of challenges there,\u201d Kapoor says. \u201cThey may they may not have liquid\ncooling infrastructure. They may not have rack positions that supply enough\npower and they\u2019re going to gravitate towards cloud because we\u2019ll have done all\nthis hard work for them and these resources will be just available via an API\nfor them to consume and fire up. The same thing applies on the security side.\nToday we have really, really strong posture when it comes to enabling our\ncustomers to feel confident that their IP, which is typically as the\nparameters of the model, the weights and biases, are fully accessible to\nthem.\u201d\n\nThey soon also will have AI supercomputers to handle these AI and machine\nlearning workloads. AWS is working with Nvidia on its \u201cProject Ceiba\u201d to build\nsuch a system that will now include Blackwell GPUs and NVLink Switch 4\ninterconnects, as we have outlined. In addition, Microsoft and OpenAI\nreportedly are planning for the \u201cStargate\u201d supercomputer \u2013 or, as we noted,\npossibly multiple datacenters that make up supercomputer.\n\n#### Sign up to our Newsletter\n\nFeaturing highlights, analysis, and stories from the week directly from us to\nyour inbox with nothing in between. Subscribe now\n\n### Related Articles\n\nCompute\n\n### Intel Downplays Hybrid CPU-GPU Engines, Merges NNP Into GPU\n\nJune 8, 2023 Timothy Prickett Morgan Compute 7\n\nWhen Intel announced its \u201cFalcon Shores\u201d project to build a hybrid CPU-GPU\ncompute engine back in February 2022 that allowed the independent scaling of\nCPU and GPU capacity within a single socket, it looked like the chip maker was\npreparing to take on rivals Nvidia and AMD head on with ...\n\nCompute\n\n### Intel Adjusts, However Slowly, To New Realities In The Datacenter\n\nApril 29, 2022 Timothy Prickett Morgan Compute 11\n\nWhile chip designer and maker Intel has a new strategy and a new executive\nteam to implement it, it is going to take a long time for changes made last\nyear and this year to be felt and for product and process roadmap changes to\nput the company into a ...\n\nAI\n\n### A Tale Of Two Nvidia Eos Supercomputers\n\nMarch 6, 2024 Timothy Prickett Morgan AI, HPC 1\n\nNote: This story augments and corrects information that originally appeared in\nHalf Eos\u2019d: Even Nvidia Can\u2019t Get Enough H100s For Its Supercomputers, which\nwas published on February 15. So, as it turns out, there are two\nsupercomputers named Eos, and even the people in the press relations\ndepartment at Nvidia, ...\n\n#### 1 Comment\n\n  1. Hubert says:\n\nApril 13, 2024 at 7:34 am\n\nA very insightful and sensible perspective by EC2 product management director\nChetan Kapoor! I quite like his hypeless cool-headedness combined with a broad\nview of where the field (AI/ML cloud) is now, and what to expect with respect\nto standardized and home-brewed infrastructure, in terms of cost, performance\nand availability. Great interview (and very sober)!\n\nReply\n\n### Leave a Reply Cancel reply\n\nThis site uses Akismet to reduce spam. Learn how your comment data is\nprocessed.\n\n###### About\n\nThe Next Platform is published by Stackhouse Publishing Inc in partnership\nwith the UK\u2019s top technology publication, The Register.\n\nIt offers in-depth coverage of high-end computing at large enterprises,\nsupercomputing centers, hyperscale data centers, and public clouds. Read\nmore...\n\n###### Newsletter\n\nFeaturing highlights, analysis, and stories from the week directly from us to\nyour inbox with nothing in between. Subscribe now\n\nAll Content Copyright The Next Platform\n\n", "frontpage": false}
