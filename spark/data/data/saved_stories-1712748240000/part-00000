{"aid": "39987221", "title": "A faster, better way to prevent an AI chatbot from giving toxic responses", "url": "https://news.mit.edu/2024/faster-better-way-preventing-ai-chatbot-toxic-responses-0410", "domain": "news.mit.edu", "votes": 1, "user": "nomagicbullet", "posted_at": "2024-04-10 05:22:06", "comments": 0, "source_title": "A faster, better way to prevent an AI chatbot from giving toxic responses", "source_text": "A faster, better way to prevent an AI chatbot from giving toxic responses | MIT News | Massachusetts Institute of Technology\n\nSkip to content \u2193\n\nMassachusetts Institute of Technology\n\nSuggestions or feedback?\n\n## MIT News | Massachusetts Institute of Technology\n\nSubscribe to MIT News newsletter\n\n## Browse By\n\n### Topics\n\nView All \u2192\n\nExplore:\n\n  * Machine learning\n  * Social justice\n  * Startups\n  * Black holes\n  * Classes and programs\n\n### Departments\n\nView All \u2192\n\nExplore:\n\n  * Aeronautics and Astronautics\n  * Brain and Cognitive Sciences\n  * Architecture\n  * Political Science\n  * Mechanical Engineering\n\n### Centers, Labs, & Programs\n\nView All \u2192\n\nExplore:\n\n  * Abdul Latif Jameel Poverty Action Lab (J-PAL)\n  * Picower Institute for Learning and Memory\n  * Media Lab\n  * Lincoln Laboratory\n\n### Schools\n\n  * School of Architecture + Planning\n  * School of Engineering\n  * School of Humanities, Arts, and Social Sciences\n  * Sloan School of Management\n  * School of Science\n  * MIT Schwarzman College of Computing\n\nView all news coverage of MIT in the media \u2192\n\nListen to audio content from MIT News \u2192\n\nSubscribe to MIT newsletter \u2192\n\nClose\n\n#### Breadcrumb\n\n  1. MIT News\n  2. A faster, better way to prevent an AI chatbot from giving toxic responses\n\n# A faster, better way to prevent an AI chatbot from giving toxic responses\n\nResearchers create a curious machine-learning model that finds a wider variety\nof prompts for training a chatbot to avoid hateful or harmful output.\n\nAdam Zewe | MIT News\n\nPublication Date:\n\nApril 10, 2024\n\nPress Inquiries\n\n### Press Contact:\n\nAbby Abazorius\n\nEmail: abbya@mit.edu\n\nPhone: 617-253-2709\n\nMIT News Office\n\n### Media Download\n\n\u2193 Download Image\n\nCaption: Researchers from MIT and the MIT-IBM Watson AI Lab used machine\nlearning to improve safeguards on large language models.\n\nCredits: Credit: Christine Daniloff, MIT; iStock\n\n#### *Terms of Use:\n\nImages for download on the MIT News office website are made available to non-\ncommercial entities, press and the general public under a Creative Commons\nAttribution Non-Commercial No Derivatives license. You may not alter the\nimages provided, other than to crop them to size. A credit line must be used\nwhen reproducing images; if one is not provided below, credit the images to\n\"MIT.\"\n\nClose\n\nCaption:\n\nResearchers from MIT and the MIT-IBM Watson AI Lab used machine learning to\nimprove safeguards on large language models.\n\nCredits:\n\nCredit: Christine Daniloff, MIT; iStock\n\nA user could ask ChatGPT to write a computer program or summarize an article,\nand the AI chatbot would likely be able to generate useful code or write a\ncogent synopsis. However, someone could also ask for instructions to build a\nbomb, and the chatbot might be able to provide those, too.\n\nTo prevent this and other safety issues, companies that build large language\nmodels typically safeguard them using a process called red-teaming. Teams of\nhuman testers write prompts aimed at triggering unsafe or toxic text from the\nmodel being tested. These prompts are used to teach the chatbot to avoid such\nresponses.\n\nBut this only works effectively if engineers know which toxic prompts to use.\nIf human testers miss some prompts, which is likely given the number of\npossibilities, a chatbot regarded as safe might still be capable of generating\nunsafe answers.\n\nResearchers from Improbable AI Lab at MIT and the MIT-IBM Watson AI Lab used\nmachine learning to improve red-teaming. They developed a technique to train a\nred-team large language model to automatically generate diverse prompts that\ntrigger a wider range of undesirable responses from the chatbot being tested.\n\nThey do this by teaching the red-team model to be curious when it writes\nprompts, and to focus on novel prompts that evoke toxic responses from the\ntarget model.\n\nThe technique outperformed human testers and other machine-learning approaches\nby generating more distinct prompts that elicited increasingly toxic\nresponses. Not only does their method significantly improve the coverage of\ninputs being tested compared to other automated methods, but it can also draw\nout toxic responses from a chatbot that had safeguards built into it by human\nexperts.\n\n\u201cRight now, every large language model has to undergo a very lengthy period of\nred-teaming to ensure its safety. That is not going to be sustainable if we\nwant to update these models in rapidly changing environments. Our method\nprovides a faster and more effective way to do this quality assurance,\u201d says\nZhang-Wei Hong, an electrical engineering and computer science (EECS) graduate\nstudent in the Improbable AI lab and lead author of a paper on this red-\nteaming approach.\n\nHong\u2019s co-authors include EECS graduate students Idan Shenfield, Tsun-Hsuan\nWang, and Yung-Sung Chuang; Aldo Pareja and Akash Srivastava, research\nscientists at the MIT-IBM Watson AI Lab; James Glass, senior research\nscientist and head of the Spoken Language Systems Group in the Computer\nScience and Artificial Intelligence Laboratory (CSAIL); and senior author\nPulkit Agrawal, director of Improbable AI Lab and an assistant professor in\nCSAIL. The research will be presented at the International Conference on\nLearning Representations.\n\nAutomated red-teaming\n\nLarge language models, like those that power AI chatbots, are often trained by\nshowing them enormous amounts of text from billions of public websites. So,\nnot only can they learn to generate toxic words or describe illegal\nactivities, the models could also leak personal information they may have\npicked up.\n\nThe tedious and costly nature of human red-teaming, which is often ineffective\nat generating a wide enough variety of prompts to fully safeguard a model, has\nencouraged researchers to automate the process using machine learning.\n\nSuch techniques often train a red-team model using reinforcement learning.\nThis trial-and-error process rewards the red-team model for generating prompts\nthat trigger toxic responses from the chatbot being tested.\n\nBut due to the way reinforcement learning works, the red-team model will often\nkeep generating a few similar prompts that are highly toxic to maximize its\nreward.\n\nFor their reinforcement learning approach, the MIT researchers utilized a\ntechnique called curiosity-driven exploration. The red-team model is\nincentivized to be curious about the consequences of each prompt it generates,\nso it will try prompts with different words, sentence patterns, or meanings.\n\n\u201cIf the red-team model has already seen a specific prompt, then reproducing it\nwill not generate any curiosity in the red-team model, so it will be pushed to\ncreate new prompts,\u201d Hong says.\n\nDuring its training process, the red-team model generates a prompt and\ninteracts with the chatbot. The chatbot responds, and a safety classifier\nrates the toxicity of its response, rewarding the red-team model based on that\nrating.\n\nRewarding curiosity\n\nThe red-team model\u2019s objective is to maximize its reward by eliciting an even\nmore toxic response with a novel prompt. The researchers enable curiosity in\nthe red-team model by modifying the reward signal in the reinforcement\nlearning set up.\n\nFirst, in addition to maximizing toxicity, they include an entropy bonus that\nencourages the red-team model to be more random as it explores different\nprompts. Second, to make the agent curious they include two novelty rewards.\nOne rewards the model based on the similarity of words in its prompts, and the\nother rewards the model based on semantic similarity. (Less similarity yields\na higher reward.)\n\nTo prevent the red-team model from generating random, nonsensical text, which\ncan trick the classifier into awarding a high toxicity score, the researchers\nalso added a naturalistic language bonus to the training objective.\n\nWith these additions in place, the researchers compared the toxicity and\ndiversity of responses their red-team model generated with other automated\ntechniques. Their model outperformed the baselines on both metrics.\n\nThey also used their red-team model to test a chatbot that had been fine-tuned\nwith human feedback so it would not give toxic replies. Their curiosity-driven\napproach was able to quickly produce 196 prompts that elicited toxic responses\nfrom this \u201csafe\u201d chatbot.\n\n\u201cWe are seeing a surge of models, which is only expected to rise. Imagine\nthousands of models or even more and companies/labs pushing model updates\nfrequently. These models are going to be an integral part of our lives and\nit\u2019s important that they are verified before released for public consumption.\nManual verification of models is simply not scalable, and our work is an\nattempt to reduce the human effort to ensure a safer and trustworthy AI\nfuture,\u201d says Agrawal.\n\nIn the future, the researchers want to enable the red-team model to generate\nprompts about a wider variety of topics. They also want to explore the use of\na large language model as the toxicity classifier. In this way, a user could\ntrain the toxicity classifier using a company policy document, for instance,\nso a red-team model could test a chatbot for company policy violations.\n\n\u201cIf you are releasing a new AI model and are concerned about whether it will\nbehave as expected, consider using curiosity-driven red-teaming,\u201d says\nAgrawal.\n\nThis research is funded, in part, by Hyundai Motor Company, Quanta Computer\nInc., the MIT-IBM Watson AI Lab, an Amazon Web Services MLRA research grant,\nthe U.S. Army Research Office, the U.S. Defense Advanced Research Projects\nAgency Machine Common Sense Program, the U.S. Office of Naval Research, the\nU.S. Air Force Research Laboratory, and the U.S. Air Force Artificial\nIntelligence Accelerator.\n\n### Share this news article on:\n\n  * X\n  * Facebook\n  * LinkedIn\n  * Reddit\n  * Print\n\n## Paper\n\nPaper: \u201cCuriosity-Driven Red-Teaming for Large Language Models\u201d\n\n## Related Links\n\n  * Zhang-Wei Hong\n  * James Glass\n  * Pulkit Agrawal\n  * Computer Science and Artificial Intelligence Laboratory\n  * Department of Electrical Engineering and Computer Science\n  * School of Engineering\n  * MIT Schwarzman College of Computing\n  * MIT-IBM Watson AI Lab\n\n## Related Topics\n\n  * Research\n  * Computer science and technology\n  * Artificial intelligence\n  * Machine learning\n  * Algorithms\n  * Human-computer interaction\n  * Computer Science and Artificial Intelligence Laboratory (CSAIL)\n  * Electrical Engineering & Computer Science (eecs)\n  * School of Engineering\n  * MIT Schwarzman College of Computing\n  * MIT-IBM Watson AI Lab\n  * Defense Advanced Research Projects Agency (DARPA)\n\n## Related Articles\n\n### New method uses crowdsourced feedback to help train robots\n\n### Ensuring AI works with the right dose of curiosity\n\n### Large language models use a surprisingly simple mechanism to retrieve some\nstored knowledge\n\n### A new way to let AI chatbots converse all day without crashing\n\n## More MIT News\n\n### Has remote work changed how people travel in the US?\n\nA new study finds sustained pattern changes \u2014 with a lot of regional\nvariation.\n\nRead full story \u2192\n\n### Physicist Netta Engelhardt is searching black holes for universal truths\n\nShe says one question drives her work: \u201cWhich pillars of gravitational physics\nare just not true?\u201d\n\nRead full story \u2192\n\n### MIT community members gather on campus to witness 93 percent totality\n\nHundreds of observers took advantage of great weather to view the 2024 partial\neclipse.\n\nRead full story \u2192\n\n### Extracting hydrogen from rocks\n\nIwnetim Abate aims to stimulate natural hydrogen production underground,\npotentially unearthing a new path to a cheap, carbon-free energy source.\n\nRead full story \u2192\n\n### When an antibiotic fails: MIT scientists are using AI to target \u201csleeper\u201d\nbacteria\n\nMost antibiotics target metabolically active bacteria, but with artificial\nintelligence, researchers can efficiently screen compounds that are lethal to\ndormant microbes.\n\nRead full story \u2192\n\n### MIT engineers design flexible \u201cskeletons\u201d for soft, muscle-powered robots\n\nNew modular, spring-like devices maximize the work of live muscle fibers so\nthey can be harnessed to power biohybrid bots.\n\nRead full story \u2192\n\n  * More news on MIT News homepage \u2192\n\n## More about MIT News at Massachusetts Institute of Technology\n\nThis website is managed by the MIT News Office, part of the Institute Office\nof Communications.\n\n### News by Schools/College:\n\n  * School of Architecture and Planning\n  * School of Engineering\n  * School of Humanities, Arts, and Social Sciences\n  * MIT Sloan School of Management\n  * School of Science\n  * MIT Schwarzman College of Computing\n\n### Resources:\n\n  * About the MIT News Office\n  * MIT News Press Center\n  * Terms of Use\n  * Press Inquiries\n  * Filming Guidelines\n  * RSS Feeds\n\n### Tools:\n\n  * Subscribe to MIT Daily/Weekly\n  * Subscribe to press releases\n  * Submit campus news\n  * Guidelines for campus news contributors\n  * Guidelines on generative AI\n\nMassachusetts Institute of Technology\n\nMassachusetts Institute of Technology 77 Massachusetts Avenue, Cambridge, MA,\nUSA\n\nRecommended Links:\n\n  * Visit\n  * Map (opens in new window)\n  * Events (opens in new window)\n  * People (opens in new window)\n  * Careers (opens in new window)\n  * Contact\n  * Privacy\n  * Accessibility\n  *     * Social Media Hub\n    * MIT on X\n    * MIT on Facebook\n    * MIT on YouTube\n    * MIT on Instagram\n\n", "frontpage": false}
