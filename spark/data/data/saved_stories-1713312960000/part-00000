{"aid": "40055332", "title": "Validating EDB Postgres Distributed Raft Consistency with Jepsen", "url": "https://www.enterprisedb.com/blog/validating-edb-postgres-distributed-continuous-high-availability-and-consistency-active", "domain": "enterprisedb.com", "votes": 1, "user": "theadamwright", "posted_at": "2024-04-16 18:13:46", "comments": 0, "source_title": "Validating EDB Postgres Distributed Continuous High Availability and Consistency for Active/Active Architectures (Part 1)", "source_text": "Validating EDB Postgres Distributed Continuous High Availability and Consistency for Active/Active Architectures (Part 1) | EDB\n\nSkip to main content\n\n# Validating EDB Postgres Distributed Continuous High Availability and\nConsistency for Active/Active Architectures (Part 1)\n\n##### Vaijayanti Bharadwaj\n\nApril 12, 2024\n\n## Introduction\n\nEDB Postgres Distributed (PGD) provides best-in-class high availability\nrequired for always-on, enterprise-grade Postgres database application\nperformance across fully managed or self-managed deployments in any cloud.\nWith EDB\u2019s Postgres technology supporting robust, globally distributed\napplications that process thousands of transactions per second, customers\nregard EDB Postgres Distributed as their go-to for delivering up to 99.999%\nguaranteed uptime and improving the performance of data replication by up to\n5X.\n\nAs part of internal activities leading up to our latest EDB Postgres\nDistributed release, our Engineering team focused on how to deliver more\nrobust releases that enhance customer confidence in PGD. Setting correct\nexpectations for customers prompted us to enhance our internal testing\nprocesses in efforts influenced by the Jepsen database testing framework.\n\n## Validating EDB Postgres distributed safety, correctness, and consistency\n\nIn the Jepsen framework, a Jepsen control node has the Jepsen libraries,\ntogether with plugins for PGD and tests. This talks to the PGD cluster as\nshown below.\n\nThis blog is the first of a series of posts, as EDB extends our commitment to\nvalidate PGD\u2019s availability and consistency in geo-distributed Postgres data\narchitectures.\n\nIn the subsections that follow, we\u2019ll walk through our internal testing and\nhow it validates EDB Postgres Distributed\u2019s availability and consistency in\nactive/active architectures.\n\n## Background and Terminology\n\nBefore describing our internal PGD testing enhancements, let\u2019s provide some\nbackground and terminology references.\n\nOperation: An operation (e.g., read or write) is carried out by a client. This\noperation can take some time to complete. A modifying operation such as a\nwrite can take effect sometime between its invocation and completion. Multiple\noperations can be in progress simultaneously.\n\nProcess: An operation is performed by a logical process. A process can do only\none operation at a time.\n\nHistory: A history is a set of operation invocations and completions with\ntheir timestamps. An example is provided below.\n\n{:process 0, :type :invoke, :f :read, :value nil}: invocation process 0\n\n{:process 1, :type :invoke, :f :write, :value 3}: invocation process 1\n\n{:process 1, :type :info, :f :write, :value 3}: completion process 1\n\n{:process 0, :type :ok, :f :read, :value 3}: completion process 0\n\n## Testing Linearizability\n\nWe used the Knossos checker to verify that a history generated from a set of\nconcurrent operations is linearizable. It models a register that has\noperations read, write, and compare-and-swap (CAS). And given a set of\nconcurrent operations, if there is at least one possible path in the history\nthat is linearizable, then the system is validated as linearizable for that\nset.\n\n### Linearizable System\n\nA linearizable implementation implies that the following properties are met:\n\n  1. The system should support these operations:\n\n    1. read(key)\n    2. write(key, value)\n    3. compare_and_swap(key, old_value, new_value)\n  2. In the presence of multiple concurrent reads and writes from multiple nodes, a read should never return a stale value. A stale read is one that returns, say, a value V, and there is at least one modification to the key that was successful and completed before the key was read that set the value to V2.\n  3. As the system moves from one state to another, the state of the system seen should always progress forward. Returning a stale value is like going backward.\n  4. There should be a total order on all the operations.\n\nIn a distributed system, an operation is applied to the system at a certain\ntime. It may take some time to complete and if it is a modifying operation,\nthe modification takes effect sometime between start time and completion time.\nThe diagrams that follow show how operations proceed concurrently on a\ndistributed system.\n\nThe first diagram below provides an example of non-linearizable behavior.\nAfter a write has been applied and completed, a subsequent read returns an\nolder value. A linearizable system cannot go \u201cbackward\u201d.\n\nThe second diagram below shows the behavior of a linearizable system. There is\na total order on the operations because the system moves from a state of A =\n2, A = 4, A = 8... and clients see the system moving forward. An operation\ntakes some time to take effect, but once it takes effect, every node sees the\neffect of that operation. Thus, the system behaves as one system.\n\n### A look at EDB\u2019s implementation of Raft in PGD\n\nPGD uses Raft as a consensus protocol for certain key operations. While Raft\ndoes not come in the data path of transactions, it is used for configuration\nmanagement, electing a write leader for a subgroup, serializing DDL and global\nlocks, and arriving at consensus in distributed transaction reconciliation.\n\nOne of our first steps was to test the linearizability of PGD\u2019s Raft\nimplementation We used a key-value API provided by the Raft subsystem to test\nlinearizable behavior. The implementation of the read, write, and CAS calls\ngoes through Raft.\n\nA failed operation appears in pink, and a successful operation appears in\nblue. Operations may also return with uncertainty, which means it may or may\nnot have been executed, and this is factored by the Knossos checkers.\n\nUsing this API to test linearizability yields timeline charts provided in the\nsample exhibits that follow.\n\n#### Example of Correct Execution\n\n#### Example of Invalid Analysis\n\n#### Linearizable Reads and Raft\n\nThe tests showed some processes reading stale value as seen in the above\nexample. This is a bug or at least a linearizability violation. The reason is\nthat key-value Raft implementation in PGD reads from the leader. The leader\nshould have the latest value because it acknowledges the request only after\ngetting an acknowledgement from the majority of replicas that they have\ncommitted the change, and then applying the changed state locally. But, in the\nevent there is a leader change, a new leader is elected. The new leader must\nhave the change committed in its raft logs, but it may not have been applied\nto the key value store. For the new leader, the apply_index may lag the\ncommit_index. And on a read, it may return a value that is stale.\n\nPGD code that uses Raft for various use cases, such as configuration\nmanagement, waits if it needs the applied value to be seen locally. We created\nan explicit SQL call to wait until the apply index catches up with the commit\nindex, and when using this, the test succeeded.\n\n## Lessons Learned\n\nDuring this process, we realized some valuable lessons.\n\n  1. An API definition needs to be very precise and allow callers to infer if something has definitely failed or there is uncertainty. For example, throwing errors with right details, such as request failing is due to a timeout expiring, allows the caller to infer that there is uncertainty in the result of the request. It may or may not have succeeded. For tests to accurately check results, the requests need to return :ok (success), :fail (failure) and :info (uncertainty). An :info result can mean the request could have succeeded or failed. Wrongly considering a request as failed when it could have succeeded gives false alarms in the tests.\n  2. Some of the tests that use register workload with the Knossos checker have very high space and time complexities. If the test runs with a fewer number of keys, it can take a very long time to verify, and memory as well. Its duration needs to be short. If the duration is long, the verification process can go out of memory. If duration is kept short, bugs may not get caught. Also, if the number of keys is increased, the bug may not be hit in the test. Similarly, creating a lot of contention can result in the code hitting deadlocks and slowing down the progress of the test. We are working to hit a balance between these to make sure the code gets tested better.\n\n## Status and work-in-progress activities\n\nAs a result of writing and running these tests, we were able to target a few\nareas of next-stage focus, including:\n\n  1. Helping identify some rare cases when update conflicts, in the presence of write skew, causing data divergence among nodes.\n  2. Improving PGD error handling on replicas in the presence of conflicts.\n  3. Helping identify gaps in eager and async conflict resolution.\n\nWe continue to refine our efforts to validate PGD\u2019s availability and\nconsistency in geo-distributed architectures with various configurations, and\nwe\u2019ll share those findings in future blogs, focusing on:\n\n  1. Conflict resolution with CRDTs\n  2. Qualify Write Lead with Majority Commit Scope for Strict Serializability\n  3. Qualify Write lead with synchronous commit for Linearizable\n  4. Adding tests for correctness of global locks\n\nFor more information about the latest EDB Postgres Distributed release, check\nout our EDB Docs.\n\nShare this Facebook X LinkedIn Email\n\n### Relevant Blogs\n\n### EDB Releases Continuous High Availability Solution With Postgres for\nKubernetes\n\nEDB Postgres Distributed for Kubernetes is now available in preview;\nadditional Q1 releases bring better performance, storage efficiency, and data\nconsistency to Postgres In today's dynamic, data-driven economy, downtime\nis...\n\nMarch 19, 2024\n\n### Elevating Diagnostics and Troubleshooting With EDB Postgres Workload\nReports\n\nThis blog was co-authored by&nbsp;Martin Marques. To successfully migrate\napplications from Oracle to PostgreSQL, organizations need to provide tools to\nthe teams most impacted by these changes\u2014DBAs, developers...\n\nMarch 18, 2024\n\n### Improving Developer Experience: Updated EDB .NET Connector Now Published\non NuGet\n\nThis blog was co-authored by EDB Senior Principal Eric McCormack. Our recent\nupdate of the EDB .NET Connector, now available on the NuGet packet manager,\nunderscores our...\n\nMarch 15, 2024\n\n### More Blogs\n\n### Release Radar: Reducing Bloat and Improving Query Performance With EDB\nSupport for pg_squeeze\n\nEDB is pleased to announce support of the popular PostgreSQL extension\npg_squeeze on our database distributions and across different deployment\ntypes. Addressing higher infrastructure cost, availability and Postgres\nperformance issues...\n\nFebruary 28, 2024\n\n### Release Radar: Increase Resilience with EDB Postgres Distributed on\nBigAnimal\n\nIn response to increasing regional data residency requirements, we have added\na Cross-Cloud Service Provider (CSP) Witness feature for EDB Postgres\nDistributed on the fully managed EDB BigAnimal Postgres Database...\n\nFebruary 13, 2024\n\n### Release Radar: EDB Enhances Oracle Compatibility with DBMS_Scheduler\nExtension in BigAnimal\n\nEDB is committed to assuring seamless Oracle-to-Postgres transitions for\ncustomers, and we recently added support for DBMS_Scheduler extension in our\nfully managed BigAnimal Postgres cloud database service to enhance these...\n\nFebruary 07, 2024\n\nEnglish\n\nEnglish Nederlands Fran\u00e7ais Deutsch \u65e5\u672c\u8a9e \ud55c\uad6d\uc5b4 Portugu\u00eas Espa\u00f1ol\n\n\u2713\n\nThanks for sharing!\n\nAddToAny\n\nMore...\n\n", "frontpage": false}
