{"aid": "40054209", "title": "Introduction to Lighting", "url": "https://www.scratchapixel.com/lessons/3d-basic-rendering/introduction-to-lighting/introduction-to-lighting-punctual-lights.html", "domain": "scratchapixel.com", "votes": 1, "user": "ibobev", "posted_at": "2024-04-16 16:43:33", "comments": 0, "source_title": "Introduction to Lighting", "source_text": "Introduction to Lighting\n\nHome\n\n# Introduction to Lighting\n\nDistributed under the terms of the CC BY-NC-ND 4.0 License.\n\n  1. An Introduction to Lighting in 3D Rendering\n  2. Point and Spot Lights\n  3. Distant Lights\n  4. Area Lights\n  5. Source Code (external link GitHub)\n\nJoin the adventure\n\nSupport Scratchapixel with a $10/month pledge for a year. Starting March 2024\nthrough March 2025. To enable full-time content creation. Be one of the\ninitial 50 supporters for a chance to win a RenderMan Walking Teapot.\n\n$1,100 of $7,200 reached so far.\n\n## Point and Spot Lights\n\nReading time: 27 mins.\n\n## Sphere Light\n\nLet's start with one of the simpler models: the point light. As the name\nsuggests, a point light is a light source represented by a point in space.\nThis type of light radiates equally in all directions and is part of what was\nmentioned in the previous chapter as the delta category and punctual light,\nwhich can be seen as a sub-category of the delta category.\n\nDealing with sphere light in CG is very straightforward since all we need to\ndo to represent one in C++ is to create a point light class (which you can\nderive from a light class if you wish, this is a common approach) and add a\nvariable to it (in addition to the light color and light intensity that are\npart of the light base class) to store the light position. This will be\nrepresented by a Vec3.\n\n    \n    \n    class Light { public: virtual ~Light() = default; Vec3<float> color_{1.0}; float intensity_{1.0}; }; class PointLight : public Light { public: PointLight(Vec3<float> pos) : position_(pos) {} Vec3<float> position_{0.0, 0.0, 0.0}; };\n\nAs you can guess, when a primary ray intersects a surface and we want to\ndetermine whether the point is illuminated by our point light, all we need to\ndo is cast a ray from the intersection point (or depending on the papers/books\nyou are looking at) and use the point light position to define the shadow ray\ndirection. From an implementation standpoint, we do this by passing the point\nlight instance some information about the intersection point and letting the\nlight fill up the data we need to cast our shadow rays. We will call this\nmethod Sample().\n\n    \n    \n    class Light { public: virtual ~Light() = default; virtual Vec3<float> Sample(const DifferentialGeometry& dg, Vec3<float>& wi, float& pdf, float& t_max) const = 0; Vec3<float> color_{1.0}; float intensity_{1.0}; };\n\nAt this point, we cannot explain why using the Sample() method is a good\napproach. We will discuss this in detail in the chapter on area lights. Don't\nworry too much about the variables within the Sample() method either. We will\nget to that in a moment. In this version of our program, we will store the\ninformation regarding the geometry at the intersection point in a structure\ncalled DifferentialGeometry. The structure looks like this:\n\n    \n    \n    struct Hit { float u; float v; int id0{-1}; int id1{-1}; float t{std::numeric_limits<float>::max()}; operator bool() const { return id0 != -1; } }; struct DifferentialGeometry : Hit { Vec3<float> P; Vec3<float> Ng; Vec3<float> Ns; };\n\nAs you can see, it holds the intersection point P, and the geometric and\nshading normals Ng and Ns, respectively. The geometric normal is just the\nnormal of the triangle itself, while the shading normal is the smooth shading\nnormal, that is, the normal as specified at each vertex of the triangle\nweighted by the hit point's barycentric coordinates. We studied this technique\nin previous lessons. So once we find out that we have a hit for the primary\nray, we will fill up that structure and pass it on to the light source Sample\nmethod:\n\n    \n    \n    DifferentialGeometry dg; for (int i = 0; i < static_cast<int>(prims.size()); ++i) { const std::vector<Vec3<float>>& pos = prims[i]->position_; const std::vector<TriangleMesh::Triangle>& tris = prims[i]->triangles_; for (size_t j = 0; j < tris.size(); ++j) { Intersect(ray, dg, i, j, tris[j], pos.data()); } } if (dg) { prims[dg.id0]->PostIntersect(ray, dg); Vec3<float> wi; float t_max, pdf; Vec3<float> light_L = light.Sample(dg, wi, pdf, t_max); // Further processing here... }\n\nThis code snippet is part of the outer loop that iterates over each pixel in\nthe image. What it does is that it tests each object making up the scene for\nan intersection with the primary ray (we loop over every object and every\ntriangle of every object). If we find a hit (if dg returns true\u2014you can see\nwhen this happens by looking at the implementation of the Hit structure\nabove), then we call the PostIntersect method of the primitive that is\nresponsible for setting dg.P, dg.Ng, and dg.Ns based on the hit data (using\nthe primitive and triangle id that has been intersected as well as the s and t\nbarycentric coordinates of the hit point on the triangle to interpolate the\ntriangle vertex positions and normals). Once we get these set, we can then\ncall the light Sample method with dg. Then, what we are interested in is what\nhappens in that Sample method. Let's have a look.\n\n    \n    \n    class Point : public Light { public: ... Vec3<float> Sample(const DifferentialGeometry& dg, Vec3<float>& wi, float& pdf, float &t_max) const override { Vec3<float> d = position_ - dg.P; float distance = d.Length(); wi = d / distance; pdf = distance * distance; t_max = distance; return color_ * intensity_; } ... };\n\nFigure 1: A point light irradiates light in all directions. The direction to\nthe light, is normalized. Any intersection with triangles that occurs at a\ndistance greater than t_max can be ignored.\n\nWhat you can see from the code is that we first calculate a vector d that\nextends from the intersection point dg.P to the light position position_. We\nstore the distance between these two points into the variable distance for a\nreason we will explain soon. This distance is important. Then we set the\nvector wi as d divided by the distance, effectively normalizing the vector\npointing towards the light position as seen from the point of intersection. We\nthen set the pdf variable as the square of the distance, and the variable\nt_max to distance.\n\nWhy are we doing all this, and what are these wi, pdf, and t_max useful for?\n\n  * wi is a variable name we've already used in the previous chapter. It is the variable name most rendering applications, papers, or books use to designate the direction to the light. Note that this is not the direction from the light source to the point, but rather from the point to the light. This is always the case. While this might seem counterintuitive as it goes in the opposite direction to which light travels in the real world, we do this by mere convenience as it facilitates the calculation of things such as the dot product between the surface normal and the vector . The same applies to as shown in Figure 1. As wi is a vector, it's being normalized\u2014nothing unusual here.\n\n  * t_max: we set our shadow ray's maximum distance to this value. The astute reader will have already noticed that, contrary to other lights such as the sun light which we will discuss in the next chapter, there's no point in casting a shadow that goes past the point light. For this reason, we will bound our shadow ray's parametric distance from 0 (not really 0, but more on this later) to t_max. This guarantees that while testing all triangles in the scene for an intersection, we ignore triangles for which the intersection lies beyond the light, as shown in Figure 1.\n\n  * pdf: this one is a bit more peculiar to handle. The name pdf is poorly chosen for this one, but the pdf variable merely serves as a placeholder here for us to store the squared distance between the hit point and the light position. Why is the squared distance of interest to us? Well, we are touching upon a matter of primordial importance. For that, we will need to take a bit of space.\n\nThe reason we need the square of the distance is that there's a relationship\nbetween the energy received by a point illuminated by a light source and the\ndistance between the point light source and that point. This becomes more\nunderstandable when we observe what happens to a series of 'light rays'\nemitted by a point light source as they travel through space. In Figure 2, we\nhave represented only a few of these rays. Note that these rays are all\ncontained within a small cone of direction intersecting an imaginary sphere\nwith a radius , centered on the point of emission. The intersection of the\ncone with the sphere defines a small spherical cap with area . Now let's\nimagine an imaginary sphere whose radius is twice as large as the first one\nand see how many of these rays would now pass through a cone narrower than the\nfirst one so that its intersection with the larger sphere sustains a spherical\ncap with the same area as the area of the first cap. As you can see by looking\nat Figures 2 and 3, only a smaller number of these rays are contained within\nthat cone when considering the larger sphere. We can intuitively deduce that\nthe larger the imaginary sphere, the fewer the number of rays that pass\nthrough a spherical cap of constant area. What happens in reality is that for\na surface area , the ratio of this area to the surface area of the sphere is\ninversely proportional to the surface area of the imaginary sphere. And as you\nknow, the surface area of the sphere is with representing the radius of the\nsphere. Thus, for a sphere with radius , we have a total surface area of . For\na sphere with radius , the surface area of the sphere is . Thus, the surface\narea ratio between the two spheres is . For a sphere with radius , the ratio\nbetween the first and the third sphere is . Thus, we can see that the ratio is\n. The ratio of energy for a constant surface area passing through a sphere\ncentered around the point of emission is therefore inversely proportional to\nthe square of the distance between the illuminated point and the light source,\nas we have shown in Figure 4.\n\nFigure 2: The amount of light energy per surface area, here represented by\njust a few light rays, diminishes as the imaginary sphere centered around the\nlight source increases in size. This image intuitively illustrates why the\namount of incoming radiance from a point light source diminishes with the\nsquare of the distance between the point being illuminated and the\nsource.Figure 3: Same as Figure 2 but as a 2D view. You cleary see that an\nequal sphercical cap area, the number of rays passing through the cap\ndecreases as the size of the imaginary sphere increases. THis is another\nillustration of the inverse square law.\n\nThis is known in computer graphics as the inverse square law and is something\nthat we will be using almost constantly when dealing with lights. When the\nradius of an imaginary sphere centered around the light source doubles, the\nsurface area quadruples because of the relationship between the surface area\nof the sphere and the radius squared (). If doubles, the surface area through\nwhich the same amount of light is spread is four times larger, thereby making\nthe light intensity one-fourth as much at twice the distance.\n\nThis is also referred to as the square falloff. Note that artists (if you are\none) sometimes don't like or don't understand why using a square falloff on a\npoint light source (and in fact, almost all light sources, except the sun)\nshould be the norm. By not doing so, you're not following the laws of physics,\nand by not doing so, you cannot achieve physically accurate lighting. In other\nwords, if you don't use a square falloff, your lighting will be incorrect, and\nit's likely you will be struggling with it in order to get a result that looks\nnice, particularly if you try to match the lighting of a live-action plate. Of\ncourse, some software (most of them) gives you the ability to turn it off or\ncontrol the falloff exponent itself. However, we do recommend that you stick\nwith the default and physically accurate behaviors and falloff values (which\nis the distance raised to the power of 2).\n\nBefore seeing the actual implementation of how light contributes to the\nillumination of our point, it's important to remember that while we've been\nrepresenting light as rays in these illustrations, radiometry actually deals\nwith solid angles. Consider a cone whose apex is located at the light origin;\nthe profile of the cone becomes larger as the distance from the light source\nto the base of the cone increases. However, it's crucial to consider that the\ncone is, first of all, considered to be infinitesimally small. What we are\nreally considering is the footprint of the cone on the surface, but within a\nsmall surface area centered around point (or ). Therefore, one can develop\nanother intuition about this square falloff by realizing that while the size\nof the footprint on the surface increases as the distance to the light\nincreases, the ratio between the surface area around and the footprint\ndecreases (as visible in Figure 4). This effectively means that less light is\nbeing collected within as the distance between the light and increases.\n\nWhile I know my choice of variable name here might be a bit confusing, it's\nimportant not to mix up the differential area that we just mentioned with the\nspherical area that we've been using before. The differential area relates to\nthe surface being illuminated, specifically, the surface of the object. It's a\ndifferential area centered around the point of illumination. We do so in\nrendering because, like everything else, the real-world is not made out of\ninfinitesimally small points, but is continuous. Thus, our equation needs to\nfunction with surfaces rather than points, which have no physicality. Though\nwe understand that over a surface the intensity of light impinging on that\nsurface might vary, which is why we always take a super small surface area ,\nconsidering it so small that over that area, we assume the lighting impinging\non its surface is constant. While one might argue that this is another\nconstruct of the mind, no more valid than using infinitesimally small points,\nwhich I can understand, the former has yet more physical/mathematical\ngrounding than the latter. And yet, yes, is a mind construct since we don't\nreally specify what size this thing is; we just assume that it is small enough\nthat the maths that \"decouples\" from using it are \"correct and sound\". As for\n, this relates to the surface area of the spherical cap defined by the\nintersection of a cone of direction with an imaginary sphere centered around\nthe point light. Note that both and are related to each other because in\nradiometry, what we're really considering is how this spherical cap projects\nonto the object surface around . This is where Lambert's cosine law comes into\nplay, as it defines that the amount of light projected from into (in short,\nhow much of is subject to receive light from ) is proportional to the cosine\nbetween the surface normal and the light direction (cone orientation).\n\nFigure 4: Another way of understanding the square falloff is to realize that\nthe size of the cone footprint, when thought of in terms of solid angle, gets\nlarger as the distance between and the light source increases. This means that\nthe ratio between , the infinitesimally small area around , and the cone\nfootprint on the surface, decreases as this distance increases. Consequently,\nless light is being 'collected' by as the distance increases, explaining the\nfalloff.\n\nNow you understand why we need the squared distance. As suggested, we store\nthis value in the pdf member variable. Not a great choice, but the reason we\nhave a variable called pdf will be explained when we get to area lights. For\nnow, just remember that what the pdf variable holds in the case of the point\nlight source is the distance to the light source squared. Let's use the data\nset in Sample() to calculate the contribution of this light to our point. Here\nis the code:\n\n    \n    \n    if (dg) { prims[dg.id0]->PostIntersect(ray, dg); Vec3<float> wi; float t_max, pdf; Vec3<float> light_L = light.Sample(dg, wi, pdf, t_max); bool in_shadow = Occluded({dg.P, wi, 0.01f, t_max - 0.01f}, prims); if (in_shadow) continue; Vec3<float> L = light_L * std::max(0.f, dg.Ns.Dot(wi)) / (M_PI * pdf); pbuf[0] = static_cast<uint8_t>(std::min(1.f, L.x) * 255); pbuf[1] = static_cast<uint8_t>(std::min(1.f, L.y) * 255); pbuf[2] = static_cast<uint8_t>(std::min(1.f, L.z) * 255); }\n\nAfter calling Sample(), we use the light vector to cast a shadow ray. This is\ndone by a call to Occluded, which is similar to Intersect() but, as mentioned\nearlier, is optimized for shadow rays. Remember, we can return from the\nfunction as soon as we have found one valid intersection. A valid intersection\nis one that occurs within a distance from that is less than t_max, the\ndistance from to the light source. Check the code on GitHub to see the\ndifferences in implementation between Intersect() and Occluded(). Then, if is\nin shadow, the light doesn't contribute to the illumination of and we then\nskip the rest of the code block. If the point is not in shadow, we set L, the\nradiance at point , as the light intensity multiplied by the light color\n(light_L, which is returned by Sample()) multiplied by the dot product between\nthe surface normal and the light direction -- our famous Lambert cosine term\n-- divided by , the normalization factor of a diffuse surface, and the pdf\nparameter, which holds the squared distance from to the light.\n\nWe can formalize this with the following equation:\n\nWhere:\n\n  * : This should technically be our BRDF (which we discussed in the previous chapter). For a purely diffuse surface, this should be , as explained in the lesson 'A Creative Dive into BRDF, Linearity, and Exposure'. Generally, BRDF models use the incoming light and outgoing view directions, but diffuse materials reflect light in every direction regardless of its incoming direction, and consequently, for these types of surfaces, and can be ignored.\n\n  * : is the light intensity, which combines its color and intensity, as coded.\n\n  * : is the distance from to the light. And, thanks to the inverse square law, we need to square this distance.\n\n  * and : are respectively the surface normal and the light direction. The last term in the equation (the dot product) represents the Lambertian cosine law.\n\n  * : stands for our outgoing radiance. This is the light that has interacted with the surface and is being reflected or scattered back towards the observer. It depends on the properties of the surface material (modeled by the BRDF), the incoming light (its intensity and direction), and the geometry of the surface (normal and viewing direction).\n\nOnce you have calculated L in the code, you just need to convert the floating-\npoint value to a range within [0, 255] and store the result into the image\nbuffer at the current pixel location. That's it. Note that we assume in the\ncode above that the object's albedo is white, and so it's not explicitly\ndefined. However, note the division by in the code. Since the object's albedo\nis white, all we need is to multiply the blue part of the equation by . The\nfollowing image shows the output of the program (left) compared to the same\nscene rendered in Maya with Arnold (right). As you can see (and hopefully so),\nthe two images match perfectly. Maths never lie.\n\nThe complete code of the program that was used to produce that image can be\nfound on the GitHub repo (use the link from the table of contents at the top\nof the page).\n\nThe following video shows how the shape of the shadows changes as the sphere\ngets closer to the surface. To understand why the shadow gets bigger as the\npoint source moves closer to the sphere, we have represented the projection of\nthe sphere onto the plane as a triangle, as seen from the source. The fact\nthat the shadow size changes with the proximity of the light source to the\ngeometry is an important visual cue that differs with distant light, as we\nwill explain in the next chapter. Note also how the surface gets brighter as\nthe point light source gets closer. This is due to the inverse square law: the\nsmaller the distance to the source, the more light impinges on the surface at\nany point whose illumination is being calculated.\n\nThe astute reader may again ask themselves what happens when that distance is\nless than 1? Indeed, dividing the light intensity by a value approaching zero\nshould lead to resulting values approaching infinity! That's clearly not\nphysically accurate. Well, this is where we pay the price for our model's\nsimplicity. As mentioned in the previous chapter, point lights do not have a\nphysical size, unlike any real-world light source. The lack of size means that\nwhile the model works well when the distance to the light source is greater\nthan 1, it fails at accurately simulating light interaction as the distance\napproaches 0. For more accurate modeling, you would need to use a different\nmodel that either simulates a physical size for the sphere or just use geo\nlights (or area lights, which are simplified versions of geo lights). We will\ncover that in the chapter on area lights. Note that some applications, notably\nvideo game engines, address this problem (though they don't really solve it\nper se) by capping the distance squared like so:\n\nFeel free to do the same if you want to. Another approach that some game\nengines take is to add a radius of influence to point light sources. The issue\nwith these light sources is that even though their contribution can be very\nsmall when far away, they still need to be computed within the light loop. If\nyou have 10 lights and only 1 of them significantly contributes to the scene\nwhile the others are too far to make a visual difference, 9 of them induce a\ncomputation penalty that we would be happy to avoid if possible. So one way to\nsolve this problem is to add a sphere of influence beyond which we decide that\nthe light doesn't contribute to the illumination of point . A common formula\nbeing used is the following:\n\nWhere is the radius of influence of the point light. What happens is that\nlight contribution quickly drops off to 0 as gets close to . We won't be using\nthis method in our code, but we are just mentioning it here for the sake of\ncompleteness.\n\n## Spot Light\n\nOnce we understand how to implement point lights, implementing spotlights\nbecomes straightforward. Indeed, spotlights are essentially a subset of point\nlight sources. You can envision a spotlight as a cone whose apex is at the\nposition of the point light, as shown in Figure 5. Like point lights,\nspotlights are defined by their location, color, and intensity parameters,\nwhich are common to all lights. Additionally, we define two parameters: one to\nspecify the aperture of the inner cone and another to specify the aperture of\nthe outer cone, defined respectively as the inner and outer cone angles\n(Figure 5). The bounded area defined by the two cones allows control over the\nlight falloff, with the intensity diminishing between the inner and outer\ncones. Since the projector can also be arbitrarily oriented around its\nposition, we need to add a direction vector. Ideally, it's best to define the\nprojector's default direction (generally (0,0,-1), similar to the default\nposition and orientation of cameras) and use a 4x4 transformation matrix to\ncontrol the spotlight's orientation (and position).\n\nFigure 5: xx\n\nSince spotlights are subsets of point lights, here again, we will need to\napply the inverse square law (obviously).\n\nThe mathematics behind simulating a spotlight are a bit more involved than\nthose for a point light, but nothing too complicated. We need to determine\nwhether the ray connecting to the light source falls within the falloff\nregion. To do this, we calculate the dot product between the spotlight\ndirection and , the normalized vector from to the light. Note that the light\ndirection and point in opposite directions, since goes from to the light\nsource (as shown in Figure 6). Thus, we can either negate the light direction\nor negate the result of the dot product (we will choose the latter). Remember\nthat the dot product between two vectors is equal to the cosine of the angle\nbetween them:\n\n  * If the vectors are pointing in the same direction, the dot product is 1. The angle between the vectors is 0, and .\n\n  * If the vectors are perpendicular to each other, the angle between the vectors is 90 degrees, and . This also implies that the outer and inner angles of the spotlight cannot be greater than 90 degrees.\n\nNow, to determine whether is inside the inner cone, outside the outer cone\n(where no light is received from the spotlight), or in between (in the falloff\nregion), all we need to do is compare this dot product against the cosines of\nthe inner and outer cone angles:\n\n  * If the dot product between the light direction and (or rather , as mentioned above\u2014don't forget to negate the result) is higher than the cosine of the inner angle, then is within the inner cone and receives full illumination without any falloff.\n\n  * If the dot product is less than the cosine of the outer angle, then is outside the outer cone and no contribution is received from the light.\n\n  * For any dot product within the two cosines, is in the falloff region. Here, all we need to do is remap the dot product from 1 to 0 (1 at the edge of the inner cone and 0 at the edge of the outer cone). This is a simple linear remapping problem, which we achieve using:\n\nWhere:\n\n  * is the inner cone angle.\n\n  * is the outer cone angle.\n\n  * is the light direction.\n\n  * is the normalized vector from to the light position.\n\nRemember that is greater than since . When you want to normalize a value\n(remap a value from the range to the range ), the equation is:\n\nThis formula adjusts the value so that when it is at the lower bound of the\noriginal range (), it maps to 0, and when it is at the upper bound (), it maps\nto 1. Don't forget to clamp this value in the range [0:1]. In code, this would\nbe implemented as:\n\n    \n    \n    float cos_inner_angle = std::cos(inner_angle); // in radians float cos_outer_angle = std::cos(outer_angle); float falloff = std::clamp((-light_dir.Dot(wi) - cos_inner_angle) / (cos_inner_angle - cos_outer_angle), 0.f, 1.f);\n\nFigure 6: We calculate the angle between the light direction and to compare\nthis value against the angles and . Note that in the image, we've been using\nangles for comparison, but in practice, it suffices to use the dot product\nbetween the two vectors, which is equal to the cosine of the angle.\n\nHere is the complete code for the Sample() method of our SpotLight class:\n\n    \n    \n    class SpotLight : public Light { public: SpotLight(float angle_inner = 20.f, float angle_outer = 25.f) { cos_angle_inner_ = std::cos(DegToRadians(angle_inner)); cos_angle_outer_ = std::cos(DegToRadians(angle_outer)); } Vec3<float> Sample(const DifferentialGeometry& dg, Vec3<float>& wi, float& pdf, float &t_max) const { Vec3<float> d = pos_ - dg.P; float distance = d.Length(); wi = d / distance; pdf = distance * distance; t_max = distance; float falloff = std::clamp((-wi.Dot(dir_) - cos_angle_outer_) / (cos_angle_inner_ - cos_angle_outer_), 0.f, 1.f); return color_ * intensity_ * falloff; } Vec3<float> pos_{0,0,0}; Vec3<float> dir_{0,0,-1}; float cos_angle_inner_; float cos_angle_outer_; };\n\nSurprisingly simple, right? Here is the result, which, as expected, is similar\nto what other production renderers produce. Note that you might see\ndifferences with other rendering engines, which can be attributed to how\nthey've decided to implement the falloff. This is the case with the Arnold\nrenderer, for instance, which shows a result different from what the Maya\nnative renderer produces.\n\nNote that in the implementation of the spotlight, we've decided to use a\nmatrix to rotate the light from its default direction (0,0,-1) to the desired\ndirection: in our case, (0,-1,0), which required a rotation of -90 degrees\nalong the x-axis. We've set the light position directly, though. In a\nproduction renderer, you should use the light transformation matrix to set\nboth the light direction and position. We haven't done this in our demo just\nto show different ways of achieving the same goal.\n\n    \n    \n    SpotLight light; Matrix44<float> m; m.SetAxisAngle(Vec3<float>(1,0,0), DegToRadians(-90)); m.MultDirMatrix(light.dir_, light.dir_); // set the spotlight direction light.dir_.Normalize(); std::cerr << \"Spotlight direction: \" << light.dir_ << std::endl; light.pos_ = Vec3<float>(0,6,-22); // set the spotlight position, could have used the matrix as well\n\nNote that some engines also like to square the falloff, as shown here:\n\n    \n    \n    Vec3<float> Sample(const DifferentialGeometry& dg, Vec3<float>& wi, float& pdf, float &t_max) const { ... return color_ * intensity_ * falloff * falloff; }\n\nFigure 7: Squaring the falloff values can help produce a more subtle and\ngradual falloff effect.\n\nThis approach has no real physical justification but can provide a falloff\nthat feels more natural and is thus more pleasing to the eye, as it makes the\ntransition from light to dark more gradual and subtle. Figure 7 illustrates\nthe difference. Be mindful not to confuse this type of falloff with the\ninverse square law \"falloff.\" While on this topic, note that the code for\nadding up the contribution of the spotlight is not different from the code\nused for the point light. We multiply the light intensity (a mix of color,\nintensity, and falloff here) by the dot product (cosine) between and the\nsurface normal, divided by for the BRDF (assuming the object's albedo = 1 in\nthis case, and the factor accounts for the normalization required when dealing\nwith diffuse surfaces as explained above) and the square of the distance\n(stored in the variable pdf) from to the light.\n\n    \n    \n    Vec3<float> L = light_L * std::max(0.f, dg.Ns.Dot(wi)) / (M_PI * pdf);\n\n## Conclusion & Takeaways\n\nThis concludes our chapter on implementing punctual lights. So far, it hasn\u2019t\nbeen too difficult, but you\u2019ve learned a critical aspect of lighting in this\nchapter, which is the inverse square law. In the next chapter, we will learn\nhow to implement another type of delta light: the distant light. Before we go,\nlet's pay a quick tribute to Pixar, who used spotlights creatively in their\nrevolutionary short animated film Luxo Jr., produced in 1986.\n\npreviousnext\n\n", "frontpage": false}
