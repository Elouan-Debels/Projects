{"aid": "39988441", "title": "Implementing \"Seen by\" Functionality with Postgres", "url": "https://supabase.com/blog/seen-by-in-postgresql", "domain": "supabase.com", "votes": 1, "user": "liumaiyi", "posted_at": "2024-04-10 08:50:11", "comments": 0, "source_title": "Implementing \"seen by\" functionality with Postgres", "source_text": "Implementing \"seen by\" functionality with Postgres\n\nJoin us for a Special Announcement\n\n05d\n\n:\n\n05h\n\n:\n\n39m\n\n:\n\n59s\n\nClaim your ticket\n\nBack\n\nBlog\n\n# Implementing \"seen by\" functionality with Postgres\n\n2022-07-18\n\n\u2022\n\n33 minute read\n\nVictorGuest Author\n\ntl;dr: Use HyperLogLog, it's a reasonable approach with great trade-offs and\nno large architectural liabilities. For a quick & dirty prototype, use hstore,\nwhich also performs the best with integer IDs.\n\nThe year is 2022. You're head DBA at the hot new social site, SupaBook... Your\nstartup is seeing eye-boggling growth because everyone loves fitting their\nhot-takes in posts restricted to VARCHAR(256).\n\nWhy VARCHAR(256)? No particular reason, but you don't have time to get hung up\non that or ask why -- you just found out that the priority this quarter is\ntracking content views across all posts in the app.\n\n\"It sounds pretty simple\" a colleague at the meeting remarks -- \"just an\nincrement here and an increment there and we'll know which posts are seen the\nmost on our platform\". You start to explain why it will be non-trivial, but\nthe meeting ends before you can finish.\n\nWell, it's time to figure out how you're going to do it. There's been a\ncomplexity freeze at the company, so you're not allowed to bring in any new\ntechnology, but you don't mind that because for v1 you would have picked\nPostgres anyway. Postgres's open source pedigree, robust suite of features,\nstable internals, and awesome mascot Slonik make it a strong choice, and it's\nwhat you're already running.\n\n(insert record scratch here)\n\nSure, this scenario isn't real, but it could be - that last part about\nPostgres definitely is. Let's see how you might solve this problem, as that\nimaginary DBA.\n\n## Experiment setup#\n\nWe've got the following simple table layout:\n\nIn SQL migration form:\n\nhideCopy\n\n1\n\nCREATE EXTENSION IF NOT EXISTS uuid-ossp;\n\n2\n\nCREATE EXTENSION IF NOT EXISTS citext;\n\n3\n\n4\n\n\\-- Create a email domain to represent and constraing email addresses\n\n5\n\nCREATE DOMAIN email\n\n6\n\nAS citext\n\n7\n\nCHECK ( LENGTH(VALUE) <= 255 AND value ~\n'^[a-zA-Z0-9.!#$%&''*+/=?^_`{|}~-]+@[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?(?:\\\\.[a-zA-Z0-9](?:[a-zA-Z0-9-]{0,61}[a-zA-Z0-9])?)*$'\n);\n\n8\n\n9\n\nCOMMENT ON DOMAIN email is 'lightly validated email address';\n\n10\n\n11\n\n\\-- Create the users table\n\n12\n\nCREATE TABLE users (\n\n13\n\nid bigserial PRIMARY KEY GENERATED BY DEFAULT AS IDENTITY,\n\n14\n\nuuid uuid NOT NULL DEFAULT uuid_nonmc_v1(),\n\n15\n\n16\n\nemail email NOT NULL,\n\n17\n\nname text,\n\n18\n\nabout_html text,\n\n19\n\n20\n\ncreated_at timestamptz NOT NULL DEFAULT NOW()\n\n21\n\n);\n\n22\n\n23\n\n\\-- Create the posts table\n\n24\n\nCREATE TABLE posts (\n\n25\n\nid bigserial PRIMARY KEY GENERATED BY DEFAULT AS IDENTITY,\n\n26\n\nuuid uuid NOT NULL DEFAULT uuid_nonmc_v1(),\n\n27\n\n28\n\ntitle text,\n\n29\n\ncontent text,\n\n30\n\nmain_image_src text,\n\n31\n\nmain_link_src text,\n\n32\n\n33\n\ncreated_by bigint REFERENCES users(id),\n\n34\n\n35\n\nlast_hidden_at timestamptz,\n\n36\n\nlast_updated_at timestamptz,\n\n37\n\ncreated_at timestamptz NOT NULL DEFAULT NOW()\n\n38\n\n);\n\nThis basic setup has taken the (imaginary) company quite far -- even though\nthe posts table has millions and millions of entries, Postgres chugs along and\nserves our queries with impressive speed and reliability. Scaling up is the\nnew (and old) scaling out.\n\n## How should we do it?#\n\nWell we can't pat ourselves for our miraculous and suspiciously simple DB\narchitecture all day, let's move on to the task at hand.\n\nLike any good tinkerer we'll start with the simplest solutions and work our\nway up in complexity to try and get to something outstanding, testing our\nnumbers as we go.\n\n### Try #1: The naive way, a simple counter on every Post#\n\nThe easiest obvious way to do this is to maintain a counter on every tuple in\nthe posts table. It's obvious, and it's almost guaranteed to work -- but maybe\nnot work well.\n\nThe migration to make it happen isn't too difficult:\n\nhideCopy\n\n1\n\nBEGIN;\n\n2\n\n3\n\nALTER TABLE posts ADD COLUMN seen_by_count;\n\n4\n\n5\n\nCOMMENT ON COLUMN posts.seen_by_count\n\n6\n\nIS 'simple count of users who have seen the post';\n\n7\n\n8\n\nCOMMIT;\n\nThere's one obvious glaring issue here -- what if someone sees the same post\ntwice? Every page reload would cause inflated counts in the seen_by_count\ncolumn, not to mention a lot of concurrent database updates (which isn't\nnecessarily Postgres's forte to begin with).\n\nClearly there's a better way to do things but before that...\n\n## Writing a test suite before the CPUs get hot and heavy#\n\nHow will we know which approach is better without numbers?! Measuring\ncomplexity and feeling can only get us so far -- we need to get some numbers\nthat tell us the performance of the solution at the stated tasks -- we need\nbenchmarks.\n\nBefore we can declare any solution the best, in particular we need a\nbaseline!. The simplest possible incorrect solution (simply incrementing a\ncounter on the Post) is probably a reasonable thing to use as a benchmark, so\nlet's take a moment to write our testing suite.\n\nLet's do this the simplest one might imagine:\n\n  * Generate a large amount of users\n\n    * Lets model for 1000, 10k, 100K, 1MM, and 10MM users\n  * Generate an even larger amount of fake posts attributed to those users\n\n    * This is a bit harder -- we need to define a general distribution for our users that's somewhat informed by real life...\n    * An average/normalized distribution doesn't quite work here -- on sites like twitter 10% of users create 80% of the tweets!\n  * Generate a description of \"events\" that describe which post was seen by whom, which we can replay.\n\n    * We want the equivalent of an effect system or monadic computation, which is easier than it sounds -- we want to generate an encoding (JSON, probably) of what to do, without actually doing it\n    * We'll just do consistent \"as fast as we can\" execution (more complicated analysis would burst traffic to be ab it closer to real life)\n\nOK, let's roll our hands up and get it done:\n\n### Script: User seeding#\n\nHere's what that looks like:\n\n1\n\n/**\n\n2\n\n* Generate a list of synthetic users to be loaded into Postgres\n\n3\n\n*\n\n4\n\n* @param {object} args\n\n5\n\n* @param {number} [args.count] number of users to generate\n\n6\n\n* @param {number} [args.aboutHTMLWordCount] number of words to generate (lorem ipsum) for about_html (serves to add heft to tuples)\n\n7\n\n* @param {string} [args.outputFilePath] output file path, if present this functoin returns void\n\n8\n\n* @returns {any[][]} List of generated synthetic users\n\n9\n\n*/\n\n10\n\nexport async function generateUsers(args) {\n\n11\n\nconst count = args.count || DEFAULT_USER_COUNT\n\n12\n\nconst aboutHTMLWordCount = args.aboutHTMLWordCount ||\nDEFAULT_ABOUT_HTML_WORD_COUNT\n\n13\n\n14\n\nconst outputFilePath = args.outputFilePath\n\n15\n\nif (!outputFilePath) {\n\n16\n\nthrow new Error('output file path must be specified')\n\n17\n\n}\n\n18\n\n19\n\nfor (var id = 0; id < count; id++) {\n\n20\n\nconst user = {\n\n21\n\nid,\n\n22\n\nemail: `user${id}@example.com`,\n\n23\n\nname: `user ${id}`,\n\n24\n\nabout_html: fastLoremIpsum(aboutHTMLWordCount, 'w'),\n\n25\n\n}\n\n26\n\n27\n\n// Write the entries to disk (returning nothing)\n\n28\n\nif (args.outputFilePath) {\n\n29\n\nawait appendFile(outputFilePath, `${JSON.stringify(user)}\\n`)\n\n30\n\n}\n\n31\n\n}\n\n32\n\n}\n\nNothing too crazy in there -- we generate a bunch of JSON, and force it out to\ndisk. It's best to avoid trying to keep it in memory so we can handle much\nlarger volumes than we might be able to fit in memory.\n\nIf you'd like to see the code, check out scripts/generate/users.js in the\nrepo.\n\n### Script: Post seeding#\n\nAlong with users, we need to generate posts that they can view. We'll keep it\nsimple and take an amount of posts to make, generating from 0 to count of\nthose.\n\nIt's very similar to the user generation code, with the caveat that we can\ntake into account the 80/20 lurker/poster rule. here's what that looks like:\n\nIt's a bit long so if you'd like to see the code, check out\nscripts/generate/posts.js in the repo.\n\n### Script: action (API call) seeding/generation#\n\nThis script is a bit tricky -- we need to inject some randomness in the\nperforming of the following actions:\n\n  * Record a new view of a post\n  * Retrieve just the count of a single post\n  * Retrieve all the users who saw a post\n\nI've chosen to use autocannon so I needed to write a request generation script\nwhich looks like this:\n\n1\n\nconst process = require('process')\n\n2\n\n3\n\nconst POST_COUNT = process.env.TEST_POST_COUNT\n\n4\n\n? parseInt(process.env.TEST_POST_COUNT, 10)\n\n5\n\n: undefined\n\n6\n\nconst USER_COUNT = process.env.TEST_USER_COUNT\n\n7\n\n? parseInt(process.env.TEST_USER_COUNT, 10)\n\n8\n\n: undefined\n\n9\n\n10\n\n/**\n\n11\n\n* Request setup function for use with autocannon\n\n12\n\n*\n\n13\n\n* @param {Request} request\n\n14\n\n* @returns {Request}\n\n15\n\n*/\n\n16\n\nfunction setupRequest(request) {\n\n17\n\n// ENsure we have counts to go off of\n\n18\n\nif (!POST_COUNT || !USER_COUNT) {\n\n19\n\nthrow new Error('Cannot setup request without valid post/user count!')\n\n20\n\n}\n\n21\n\n22\n\n// Pick a random post to do an operation on\n\n23\n\nconst postId = Math.floor(Math.random() * POST_COUNT)\n\n24\n\n25\n\n// Choose pseudo-randomly whether to register a seen by or read seenby status\n\n26\n\nconst operationChoice = Math.floor(Math.random() * 10)\n\n27\n\nif (operationChoice < 1) {\n\n28\n\n// 10% of the time, get *all* the users\n\n29\n\nrequest.method = 'GET'\n\n30\n\nrequest.path = `/posts/${postId}/seen-by/users`\n\n31\n\n} else if (operationChoice < 7) {\n\n32\n\n// 60% of the time, get the count of seenby on a post\n\n33\n\nrequest.method = 'GET'\n\n34\n\nrequest.path = `/posts/${postId}/seen-by/count`\n\n35\n\n} else {\n\n36\n\n// 30% of the time, add a new seen-by entry\n\n37\n\nconst userId = Math.floor(Math.random() * USER_COUNT)\n\n38\n\n39\n\n// Most of the time we'll be *setting* seen-by\n\n40\n\n// And we'll get the count (so we can show it) later as well\n\n41\n\nrequest.method = 'POST'\n\n42\n\nrequest.path = `/posts/${postId}/seen-by/${userId}`\n\n43\n\n}\n\n44\n\n45\n\nreturn request\n\n46\n\n}\n\n47\n\n48\n\nmodule.exports = setupRequest\n\nNothing too crazy here, and some back of the envelope estimations on how often\neach operation would normally be called. These numbers could be tweaked more,\nbut we should see a difference between approaches even if we messed up\nmassively here.\n\nIf you'd like to see the code, check out scripts/setup-request.cjs in the\nrepo.\n\n### Glue it all together#\n\nOnce we're done we need to glue this all together into one script, with\nroughly this format:\n\n1\n\nexport default async function runBenchmark() {\n\n2\n\n// Start the server\n\n3\n\n// Reset before test\n\n4\n\n// Generate & insert users\n\n5\n\n// Generate & insert posts\n\n6\n\n// Generate actions (API Calls) to run\n\n7\n\n// Execute the API calls\n\n8\n\n// Write JSON results to tmpdir\n\n9\n\n// Stop the server\n\n10\n\n}\n\nIf you want to see what the code actually ended up looking like, check out\nscripts/bench.js in the repo.\n\nAlong with the benchmark, we'll standardize on the following settings:\n\n1\n\nexport SEEN_BY_STRATEGY=simple-counter # or: simple-hstore, assoc-table, hll\n\n2\n\nexport TEST_USERS_JSON_PATH=/tmp/supabase-seen-by.users.json\n\n3\n\nexport TEST_POSTS_JSON_PATH=/tmp/supabase-seen-by.posts.json\n\n4\n\nexport TEST_POST_COUNT=1000\n\n5\n\nexport TEST_USER_COUNT=100000\n\n6\n\nexport TEST_DURATION_SECONDS=60\n\n7\n\n8\n\n## Use custom postgres image built with hll extension\n(https://github.com/citusdata/postgresql-hll)\n\n9\n\n## NOTE: `make db-custom-image` must be run beforehand\n\n10\n\n#export DB_IMAGE=postgres-14.4-alpine-hll\n\n11\n\n#export DB_IMAGE_TAG=latest\n\n### Our first run, on the naive solution#\n\nAlright, finally we're ready. Let's see what we get on our naive solution. We\nexpect this to be pretty fast, because not only is it wrong, but it's just\nabout the simplest thing you could do.\n\nOn my local machine, here's our baseline (output from autocannon):\n\n1\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\n2\n\n\u2502 Stat \u2502 2.5% \u2502 50% \u2502 97.5% \u2502 99% \u2502 Avg \u2502 Stdev \u2502 Max \u2502\n\n3\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n4\n\n\u2502 Latency \u2502 0 ms \u2502 2 ms \u2502 6 ms \u2502 6 ms \u2502 2.03 ms \u2502 1.82 ms \u2502 23 ms \u2502\n\n5\n\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n6\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\n7\n\n\u2502 Stat \u2502 1% \u2502 2.5% \u2502 50% \u2502 97.5% \u2502 Avg \u2502 Stdev \u2502 Min \u2502\n\n8\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n9\n\n\u2502 Req/Sec \u2502 297 \u2502 318 \u2502 389 \u2502 500 \u2502 391.24 \u2502 47.87 \u2502 297 \u2502\n\n10\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n11\n\n\u2502 Bytes/Sec \u2502 54.1 kB \u2502 57.9 kB \u2502 70.8 kB \u2502 91.1 kB \u2502 71.3 kB \u2502 8.72 kB \u2502 54.1\nkB \u2502\n\n12\n\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n13\n\n14\n\nReq/Bytes counts sampled once per second.\n\n15\n\n# of samples: 60\n\n16\n\n17\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\n18\n\n\u2502 Percentile \u2502 Latency (ms) \u2502\n\n19\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n20\n\n\u2502 0.001 \u2502 0 \u2502\n\n21\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n22\n\n\u2502 0.01 \u2502 0 \u2502\n\n23\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n24\n\n\u2502 0.1 \u2502 0 \u2502\n\n25\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n26\n\n\u2502 1 \u2502 0 \u2502\n\n27\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n28\n\n\u2502 2.5 \u2502 0 \u2502\n\n29\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n30\n\n\u2502 10 \u2502 0 \u2502\n\n31\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n32\n\n\u2502 25 \u2502 0 \u2502\n\n33\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n34\n\n\u2502 50 \u2502 2 \u2502\n\n35\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n36\n\n\u2502 75 \u2502 3 \u2502\n\n37\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n38\n\n\u2502 90 \u2502 5 \u2502\n\n39\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n40\n\n\u2502 97.5 \u2502 6 \u2502\n\n41\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n42\n\n\u2502 99 \u2502 6 \u2502\n\n43\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n44\n\n\u2502 99.9 \u2502 9 \u2502\n\n45\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n46\n\n\u2502 99.99 \u2502 16 \u2502\n\n47\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n48\n\n\u2502 99.999 \u2502 23 \u2502\n\n49\n\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n50\n\n51\n\n23k requests in 60.02s, 4.28 MB read\n\nAs you might imagine, pretty darn good latency across all the requests.\n\n## Back to trying things out#\n\nNow that we've got a basic baseline of our tests, let's continue trying out\nideas:\n\n### Try #2: Storing the users who did the \"see\"ing, with hstore#\n\nThe next obvious thing (and probably a core requirement if we'd asked around),\nis knowing who viewed each post. Well if we need to know who, then we probably\nneed to store some more information!\n\nPostgres has native support for arrays and a data structure called a hstore,\nso let's try those. It's pretty obvious that having hundreds, thousands, or\nmillions of entries in one of these data structures, inside a tuple isn't the\ngreatest idea, but let's try it anyway and let the numbers speak for\nthemselves.\n\nHere's what the migration would look like:\n\nhideCopy\n\n1\n\nBEGIN;\n\n2\n\n3\n\nCREATE EXTENSION IF NOT EXISTS hstore;\n\n4\n\n5\n\nALTER TABLE posts ADD COLUMN seen_count_hstore hstore\n\n6\n\nNOT NULL DEFAULT ''::hstore;\n\n7\n\n8\n\nCOMMENT ON COLUMN posts.seen_count_hstore\n\n9\n\nIS 'count of users that have seen the post, with hstore';\n\n10\n\n11\n\nCOMMIT;\n\nhstore provides support for both GIST and GIN indices, but after reading the\ndocumentation we can conclude that we don't necessarily need those for the\ncurrent set of functionality.\n\n#### Caveats#\n\nWell as you might have imagined, this is obviously pretty bad and will\neventually be hard to scale. If you expect only 0-50 entries in your column\ntext[] is perfectly fine, but thousands or millions is another ballgame.\n\nThinking of how to scale this, a few ideas pop to mind:\n\n  * Compress our columns with LZ4 which is newly supported TOAST column compression (I first heard about this thanks to Fujitsu's fantastic blog post)\n  * PARTITION our posts table\n\n#### Performance#\n\nOK, time to get on with it, let's see how it performs with an hstore:\n\n1\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\n2\n\n\u2502 Stat \u2502 2.5% \u2502 50% \u2502 97.5% \u2502 99% \u2502 Avg \u2502 Stdev \u2502 Max \u2502\n\n3\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n4\n\n\u2502 Latency \u2502 0 ms \u2502 2 ms \u2502 5 ms \u2502 6 ms \u2502 2.15 ms \u2502 1.67 ms \u2502 16 ms \u2502\n\n5\n\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n6\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\n7\n\n\u2502 Stat \u2502 1% \u2502 2.5% \u2502 50% \u2502 97.5% \u2502 Avg \u2502 Stdev \u2502 Min \u2502\n\n8\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n9\n\n\u2502 Req/Sec \u2502 287 \u2502 305 \u2502 348 \u2502 504 \u2502 369.12 \u2502 58.8 \u2502 287 \u2502\n\n10\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n11\n\n\u2502 Bytes/Sec \u2502 53.9 kB \u2502 56.9 kB \u2502 64.5 kB \u2502 92.5 kB \u2502 68.3 kB \u2502 10.7 kB \u2502 53.8\nkB \u2502\n\n12\n\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n13\n\n14\n\nReq/Bytes counts sampled once per second.\n\n15\n\n# of samples: 60\n\n16\n\n17\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\n18\n\n\u2502 Percentile \u2502 Latency (ms) \u2502\n\n19\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n20\n\n\u2502 0.001 \u2502 0 \u2502\n\n21\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n22\n\n\u2502 0.01 \u2502 0 \u2502\n\n23\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n24\n\n\u2502 0.1 \u2502 0 \u2502\n\n25\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n26\n\n\u2502 1 \u2502 0 \u2502\n\n27\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n28\n\n\u2502 2.5 \u2502 0 \u2502\n\n29\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n30\n\n\u2502 10 \u2502 0 \u2502\n\n31\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n32\n\n\u2502 25 \u2502 1 \u2502\n\n33\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n34\n\n\u2502 50 \u2502 2 \u2502\n\n35\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n36\n\n\u2502 75 \u2502 3 \u2502\n\n37\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n38\n\n\u2502 90 \u2502 5 \u2502\n\n39\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n40\n\n\u2502 97.5 \u2502 5 \u2502\n\n41\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n42\n\n\u2502 99 \u2502 6 \u2502\n\n43\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n44\n\n\u2502 99.9 \u2502 9 \u2502\n\n45\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n46\n\n\u2502 99.99 \u2502 9 \u2502\n\n47\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n48\n\n\u2502 99.999 \u2502 16 \u2502\n\n49\n\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n50\n\n51\n\n22k requests in 60.02s, 4.1 MB read\n\nNot too far off! While we didn't try the pathological case(s) of millions of\npeople liking the same post to hit breaking point, a slightly more random\ndistribution seems to have done decently -- we actually have lower 99.999th\npercentile latency versus the simple counter.\n\nAn average of 2.15ms versus 2.05ms with the simpler counter is a ~4% increase\nin the average latency (though of course, the p99.999 is lower!).\n\n### Try #3: An Association table for remembering who liked what#\n\nA likely requirement from the original scenario that we've completely ignored\nis remembering which users liked a certain post to. The easiest solution here\nis an \"associative\" table like this one:\n\nIn SQL:\n\nhideCopy\n\n1\n\nbegin;\n\n2\n\n3\n\ncreate table posts_seen_by_users (\n\n4\n\npost_id bigint references posts (id),\n\n5\n\nuser_id bigint references users (id),\n\n6\n\nseen_count bigint not null default 0 check (seen_count > 0),\n\n7\n\nprimary key (post_id, user_id)\n\n8\n\n);\n\n9\n\n10\n\ncommit;\n\n#### Caveats#\n\nIn production, you're going to want to do a few things to make this even\nremotely reasonable long term:\n\n  * PARTITION the table (consider using partition-friendly pg_partman)\n  * Move old partitions off to slower/colder storage and maintain snapshots\n  * Summarize older content that might be seen lots\n  * Consider a partitioning key up front -- post IDs are probably a reasonable thing to use if they're sufficiently randomly distributed\n\nThese are good initial stop-gaps, but a realistic setup will have many\nproblems and many more solutions to be discovered.\n\n(It will be a recurring theme but this is a spot where we probably don't\nnecessarily want to use stock Postgres but instead want to use tools like\nCitus Columnar Storage, ZedStore, or an external choice like ClickHouse).\n\n#### Performance#\n\nAlright, enough dilly dally, let's run our test bench against this setup:\n\n1\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\n2\n\n\u2502 Stat \u2502 2.5% \u2502 50% \u2502 97.5% \u2502 99% \u2502 Avg \u2502 Stdev \u2502 Max \u2502\n\n3\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n4\n\n\u2502 Latency \u2502 0 ms \u2502 2 ms \u2502 8 ms \u2502 8 ms \u2502 2.5 ms \u2502 2.45 ms \u2502 30 ms \u2502\n\n5\n\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n6\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\n7\n\n\u2502 Stat \u2502 1% \u2502 2.5% \u2502 50% \u2502 97.5% \u2502 Avg \u2502 Stdev \u2502 Min \u2502\n\n8\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n9\n\n\u2502 Req/Sec \u2502 238 \u2502 254 \u2502 321 \u2502 464 \u2502 326.52 \u2502 48.14 \u2502 238 \u2502\n\n10\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n11\n\n\u2502 Bytes/Sec \u2502 43.4 kB \u2502 46.3 kB \u2502 58.5 kB \u2502 84.5 kB \u2502 59.5 kB \u2502 8.77 kB \u2502 43.3\nkB \u2502\n\n12\n\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n13\n\n14\n\nReq/Bytes counts sampled once per second.\n\n15\n\n# of samples: 60\n\n16\n\n17\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\n18\n\n\u2502 Percentile \u2502 Latency (ms) \u2502\n\n19\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n20\n\n\u2502 0.001 \u2502 0 \u2502\n\n21\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n22\n\n\u2502 0.01 \u2502 0 \u2502\n\n23\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n24\n\n\u2502 0.1 \u2502 0 \u2502\n\n25\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n26\n\n\u2502 1 \u2502 0 \u2502\n\n27\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n28\n\n\u2502 2.5 \u2502 0 \u2502\n\n29\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n30\n\n\u2502 10 \u2502 0 \u2502\n\n31\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n32\n\n\u2502 25 \u2502 1 \u2502\n\n33\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n34\n\n\u2502 50 \u2502 2 \u2502\n\n35\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n36\n\n\u2502 75 \u2502 4 \u2502\n\n37\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n38\n\n\u2502 90 \u2502 7 \u2502\n\n39\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n40\n\n\u2502 97.5 \u2502 8 \u2502\n\n41\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n42\n\n\u2502 99 \u2502 8 \u2502\n\n43\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n44\n\n\u2502 99.9 \u2502 11 \u2502\n\n45\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n46\n\n\u2502 99.99 \u2502 25 \u2502\n\n47\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n48\n\n\u2502 99.999 \u2502 30 \u2502\n\n49\n\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n50\n\n51\n\n20k requests in 60.02s, 3.57 MB read\n\nA little bit more divergence here -- 99.999%ile latency @ 30 which is almost\ndouble what it was for simple-hstore.\n\nAverage is coming in at 2.50ms which is 16% slower than simple-hstore and 21%\nslower than simple-counter.\n\n### Try #4: Getting a bit more serious: bringing out the HyperLogLog#\n\nWe'll just draw the rest of the owl now.\n\nWhat's HyperLogLog you ask? Well it's just a probabilistic data structure!\nDon't worry if you've never heard of it before, it's a reasonably advanced\nconcept.\n\nYou may have heard of Bloom Filters and they're somewhat related but they're\nnot quite a great fit for the problem we're solving since we want to know how\nmany people have seen a particular post. Knowing whether one user has seen a\nparticular post is useful too -- but not quite what we're solving for here\n(and we'd have to double-check our false positives anyway if we wanted to be\nabsolutely sure).\n\nHyperLogLog provides a probabilistic data structure that is good at counting\ndistinct entries, so that means that the count will not be exact, but be\nreasonably close (depending on how we tune). We won't have false positives\n(like with a bloom filter) -- we'll have a degree of error (i.e. the actual\ncount may be 1000, but the HLL reports 1004).\n\nWe have to take this into account on the UI side but and maybe retrieve the\nfull count if anyone ever really needs to know/view individual users that have\nseen the content, so we can fall back to our association table there.\n\nGiven that every second there are about 6000 tweets on Twitter(!), this is\nprobably one of the only solutions that could actually work at massive scale\nwith the limitations we've placed on ourselves.\n\nHere's what that looks like in SQL:\n\nhideCopy\n\n1\n\nBEGIN;\n\n2\n\n3\n\nCREATE EXTENSION IF NOT EXISTS hll;\n\n4\n\n5\n\nALTER TABLE posts ADD COLUMN seen_count_hll hll\n\n6\n\nNOT NULL DEFAULT hll_empty();\n\n7\n\n8\n\nCOMMENT ON COLUMN posts.seen_count_hll\n\n9\n\nIS 'HyperLogLog storing user IDs';\n\n10\n\n11\n\nCOMMIT;\n\nHere we need the citus/postgresql-hll extension, which is generously made\n(truly) open source by citusdata.\n\nNOTE that we still have access to the association table -- and while we still\ninsert rows into it, we can drop the primary key index, and simply update our\nHLL (and leave ourselves a note on when we last updated it).\n\n### Caveats#\n\nThere's not much to add to this solution, as the heavy lifting is mostly done\nby postgresql-hll, but there's one big caveat:\n\n  * This approach will need a custom Postgres image for this, since hll is not an official contrib module\n\nThere are also a few optimizations that are easy to imagine:\n\n  * Batching inserts to the association table (storing them in some other medium in the meantime -- local disk, redis, etc)\n  * Writing our association table entries in a completely different storage medium altogether (like object storage) and use Foreign Data Wrappers and pg_cron and delay or put off processing all together\n\n### Performance#\n\nThe most complicated solution by far, let's see how it fares:\n\n1\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\n2\n\n\u2502 Stat \u2502 2.5% \u2502 50% \u2502 97.5% \u2502 99% \u2502 Avg \u2502 Stdev \u2502 Max \u2502\n\n3\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n4\n\n\u2502 Latency \u2502 0 ms \u2502 2 ms \u2502 6 ms \u2502 6 ms \u2502 2.28 ms \u2502 2.03 ms \u2502 59 ms \u2502\n\n5\n\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n6\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\n7\n\n\u2502 Stat \u2502 1% \u2502 2.5% \u2502 50% \u2502 97.5% \u2502 Avg \u2502 Stdev \u2502 Min \u2502\n\n8\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n9\n\n\u2502 Req/Sec \u2502 272 \u2502 285 \u2502 351 \u2502 456 \u2502 353.05 \u2502 45.13 \u2502 272 \u2502\n\n10\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n11\n\n\u2502 Bytes/Sec \u2502 49.5 kB \u2502 51.9 kB \u2502 63.9 kB \u2502 83.1 kB \u2502 64.3 kB \u2502 8.22 kB \u2502 49.5\nkB \u2502\n\n12\n\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n13\n\n14\n\nReq/Bytes counts sampled once per second.\n\n15\n\n# of samples: 60\n\n16\n\n17\n\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\n18\n\n\u2502 Percentile \u2502 Latency (ms) \u2502\n\n19\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n20\n\n\u2502 0.001 \u2502 0 \u2502\n\n21\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n22\n\n\u2502 0.01 \u2502 0 \u2502\n\n23\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n24\n\n\u2502 0.1 \u2502 0 \u2502\n\n25\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n26\n\n\u2502 1 \u2502 0 \u2502\n\n27\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n28\n\n\u2502 2.5 \u2502 0 \u2502\n\n29\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n30\n\n\u2502 10 \u2502 0 \u2502\n\n31\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n32\n\n\u2502 25 \u2502 1 \u2502\n\n33\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n34\n\n\u2502 50 \u2502 2 \u2502\n\n35\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n36\n\n\u2502 75 \u2502 4 \u2502\n\n37\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n38\n\n\u2502 90 \u2502 6 \u2502\n\n39\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n40\n\n\u2502 97.5 \u2502 6 \u2502\n\n41\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n42\n\n\u2502 99 \u2502 6 \u2502\n\n43\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n44\n\n\u2502 99.9 \u2502 9 \u2502\n\n45\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n46\n\n\u2502 99.99 \u2502 28 \u2502\n\n47\n\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\n48\n\n\u2502 99.999 \u2502 59 \u2502\n\n49\n\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n50\n\n51\n\n21k requests in 60.03s, 3.86 MB read\n\nAnother somewhat nuanced degradation in performance -- while the 99.99%ile\nlatency was nearly 2x higher, the average latency was actually lower than the\nassoc-table approach @ 2.28ms.\n\nThe average latency on the HLL approach is 11% worse than simple-counter, 6%\nworse than simple-hstore, and faster than assoc-table alone, which is an\nimprovement.\n\n### Oh, the other places we could go#\n\nOne of the great things about Postgres is it's expansive ecosystem -- while\nPostgres may (and frankly should not) beat the perfect specialist tool for\nyour use case, it often does an outstanding job in the general case.\n\nLet's look into some more experiments that could be run -- maybe one day in\nthe future we'll get some numbers behind these (community contributions are\nwelcome!).\n\n#### Incremental view maintenance powered by pg_ivm#\n\nIf you haven't heard about pg_ivm it's an extension for handling Incremental\nView Maintenance -- updating VIEWs when underlying tables change.\n\nIVM is a hotly requested feature whenever views (particularly materialized\nviews) are mentioned, so there has been much fanfare to it's release.\n\nThere are a couple advantages we could gain by using pg_ivm:\n\n  * Ability to time constrain calculations (newer posts which are more likely to be seen can exist in instant-access views)\n  * We could theoretically remove the complicated nature of the HLL all together by using COUNT with IVM\n\npg_ivm is quite new and cutting edge but looks to be a great solution -- it's\nworth giving a shot someday.\n\n### Doing graph computations with AGE#\n\nAs is usually the case in academia and practice, we can make our problem\ndrastically easier by simply changing the data structures we use to model our\nproblem!\n\nOne such reconfiguration would be storing the information as a graph:\n\nAs you might imagine, finding the number of \"seen-by\" relations would simply\nbe counting the number of edges out of one of the nodes!\n\nWell, the Postgres ecosystem has us covered here too! AGE is an extension that\nallows you to perform graph related queries in Postgres.\n\nWe won't pursue it in this post but it would be a great way to model this\nproblem as well -- thanks to the extensibility of Postgres, this data could\nlive right next to our normal relational data as well.\n\n## So what's the best way to do it?#\n\nOK, so what's the answer at the end of the day? What's the best way to get to\nthat useful v1? Here are the numbers:\n\nIn tabular form:\n\nApproach| Avg (ms)| 99%ile (ms)| 99.999%ile (ms)  \n---|---|---|---  \nsimple-counter| 2.03| 6| 23  \nsimple-hstore| 2.15| 6| 16  \nassoc-table| 2.5| 8| 30  \nhll| 2.16| 7| 27  \n  \nIf we go strictly with the data, the best way looks to be the hstore-powered\nsolution, but I think the HLL is probably the right choice.\n\nThe HLL results were quite variable -- some runs were faster than others, so\nI've taken the best of 3 runs.\n\nEven though the data says hstore, knowing that posts will be seen by more and\nmore people over time, I might choose the HLL solution for an actual\nimplementation. It's far less likely to pose a bloated row problem, and it has\nthe absolute correctness (and later recall) of the assoc-table solution, while\nperforming better over all (as you can imagine, no need to COUNT rows).\n\nAnother benefit of the HLL solution is that PostgreSQL tablespaces allow us to\nput the association table on a different, slower storage mechanism, and keep\nour posts table fast. Arguably in a real system we might have the HLL in\nsomething like redis but for a v1, it looks like Postgres does quite well!\n\n# Wrap-up#\n\nI hope you enjoyed this look down the trunk hole, and you've got an idea of\nhow to implement solutions to surprisingly complex problems like this one with\nPostgres.\n\nAs usual, Postgres has the tools to solve the problem reasonably well (if not\ncompletely) before you reach out for more complicated/standalone solutions.\n\nSee any problems with the code, solutions that haven't been tried? -- reach\nout, or open an issue!\n\n## More Postgres resources#\n\n  * Partial data dumps using Postgres Row Level Security\n  * Postgres Views\n  * Postgres Auditing in 150 lines of SQL\n  * Cracking PostgreSQL Interview Questions\n  * What are PostgreSQL Templates?\n  * Realtime Postgres RLS on Supabase\n\nShare this article\n\nLast post\n\n#### Supabase Flutter SDK 1.0 Developer Preview\n\n2 August 2022\n\nNext post\n\n#### Revamped Auth Helpers for Supabase (with SvelteKit support)\n\n13 July 2022\n\npostgres\n\nplanetpg\n\nOn this page\n\n  * Experiment setup\n  * How should we do it?\n\n    * Try #1: The naive way, a simple counter on every Post\n  * Writing a test suite before the CPUs get hot and heavy\n\n    * Script: User seeding\n    * Script: Post seeding\n    * Script: action (API call) seeding/generation\n    * Glue it all together\n    * Our first run, on the naive solution\n  * Back to trying things out\n\n    * Try #2: Storing the users who did the \"see\"ing, with hstore\n    * Try #3: An Association table for remembering who liked what\n    * Try #4: Getting a bit more serious: bringing out the HyperLogLog\n    * Caveats\n    * Performance\n    * Oh, the other places we could go\n    * Doing graph computations with [AGE][age]\n  * So what's the best way to do it?\n\n  * Wrap-up\n\n    * More Postgres resources\n\nShare this article\n\n## Build in a weekend, scale to millions\n\n## Footer\n\nWe protect your data.More on Security\n\n  * SOC2 Type 2 Certified\n  * HIPAA Compliant\n\nTwitter\n\nGitHub\n\nDiscord\n\nYoutube\n\n###### Product\n\n  * Database\n\n  * Auth\n\n  * Functions\n\n  * Realtime\n\n  * Storage\n\n  * Vector\n\n  * Pricing\n\n  * Special Announcement\n\n###### Resources\n\n  * Support\n\n  * System Status\n\n  * Become a Partner\n\n  * Integrations\n\n  * Experts\n\n  * Brand Assets / Logos\n\n  * Security and Compliance\n\n  * DPA\n\n  * SOC2\n\n  * HIPAA\n\n###### Developers\n\n  * Documentation\n\n  * Changelog\n\n  * Contributing\n\n  * Open Source\n\n  * SupaSquad\n\n  * DevTo\n\n  * RSS\n\n###### Company\n\n  * Blog\n\n  * Customer Stories\n\n  * Careers\n\n  * Company\n\n  * Terms of Service\n\n  * Privacy Policy\n\n  * Acceptable Use Policy\n\n  * Support Policy\n\n  * Service Level Agreement\n\n  * Humans.txt\n\n  * Lawyers.txt\n\n  * Security.txt\n\n\u00a9 Supabase Inc\n\nWe only collect analytics essential to ensuring smooth operation of our\nservices. Learn more\n\nLearn more\n\n", "frontpage": false}
