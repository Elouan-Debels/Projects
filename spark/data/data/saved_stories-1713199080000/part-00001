{"aid": "40038729", "title": "Your Graph Data Fits in Memory", "url": "https://jazco.dev/2024/04/15/in-memory-graphs/", "domain": "jazco.dev", "votes": 1, "user": "goranmoomin", "posted_at": "2024-04-15 10:36:49", "comments": 0, "source_title": "Your (Graph) Data Fits in Memory", "source_text": "Your (Graph) Data Fits in Memory \u00b7 Jaz's Blog\n\n### Jaz's Blog A space where I rant about computers\n\n# Your (Graph) Data Fits in Memory\n\n15 Apr 2024\n\nI recently shipped a new revision of Bluesky\u2019s global AppView at the start of\nFebruary and things have been going very well. The system scales and handles\nmillions of users without breaking a sweat, the ScyllaDB-backed Data Plane\nservice sits at under 5% DB load in the most intense production workloads, and\nthings are going great. You know what that means, time to add some new\nfeatures that absolutely don\u2019t fit the existing scalable data model!\n\nA recent feature I\u2019ve been working on is something we\u2019ve referred to as\n\u201cSocial Proof\u201d, the feature you see on Facebook or Twitter that shows you how\nmany of your friends also follow this user.\n\n## The Query-time Conundrum\n\nIn our existing architecture, we handle graph lookups by paging over entire\npartitions of graph data (i.e. all the follows created by user A) or by\nlooking for the existence of a specific graph relationship (i.e. does A follow\nB).\n\nThat\u2019s working pretty well for things like fanning out posts someone makes to\nthe timelines of their followers or showing that you follow the different\nauthors of posts in a thread.\n\nIn the above examples, the \u201cexpensive\u201d mode of loading (i.e. paging over all\nyour follows) is done in a paginated manner or as part of an async job during\ntimeline fanout etc.\n\nIf we want to show you \u201cpeople you follow who also follow user B\u201d when you\nview user B\u2019s profile, we need a fast way to query multiple potentially large\nsets of data on-demand at interactive speeds.\n\nYou might recognize this feature as a Set Intersection problem:\n\nWhen user A views user B\u2019s profile, we want to compute the intersection of the\ntwo sets shown in the image above to get the users that A follows who also\nfollow user B so we can show a social proof of user B.\n\nThe easiest way to do this is to grab the list of people that User A follows\nfrom Scylla, then walk over each of those people and check if they follow user\nB.\n\nWe can reverse this problem and grab the list of people who follow user B and\nwalk the list and check if user A follows them as well, but either way we\u2019re\ndoing a potentially large partition scan to load one of the entire sets, then\npotentially LOTs of one-row queries to check for the existence of specific\nfollows.\n\nImagine user A follows 1,000 people and user B has 50,000 followers, that\u2019s\none expensive query and then 1,000 tiny queries every time we hydrate User B\u2019s\nprofile for user A and those queries will be different for every user\ncombination we need to load.\n\nA more efficient way to tackle this problem would be to load both sets (A\u2019s\nfollows and followers of B) and then intersect them in-memory in our service.\n\nIf we store both sets in-memory as Hash Maps we can iterate over the smaller\nset and perform fast lookups for membership in the other set. Some programming\nlanguages (i.e. rust) even have Set data structures that natively support\nefficient intersection methods.\n\nBut can we even fit this data in memory?\n\n## How Much Memory does a Graph Take?\n\nIn our network, each user is assigned a DID that looks something like\ndid:plc:q6gjnaw2blty4crticxkmujt which you might notice is a 32 character\nstring. Not all DIDs are this long, they can be longer or shorter but the vast\nmajority (>99.9%) of DIDs on AT Proto are 32 character strings.\n\nThe AT Proto network currently has ~160M follow records for ~5.5M users. If we\nwere to store each of these follows in a pair of HashMaps (one to lookup by\nthe actor, one to lookup by the subject) how much memory would we need?\n\n    \n    \n    Keys: 32 Bytes * 5.5M Users * 2 Maps = ~352MB Values: 160M Follows * 32 Bytes * 2 Maps = ~10.24GB\n\nJust the raw keys and values total around 10.5GB with some wiggle room for\nHashMap provisioning overhead we\u2019re looking at something like 12-14GB of RAM\nto store the follow graph. With modern computers that\u2019s actually not too crazy\nand could fit in-memory on a production server no problem, but we can do one\nstep better.\n\nIf we convert each DID into a uint64 (a process referred to as \u201cinterning\u201d),\nwe can significantly compress the size of our graph and make it faster since\nour hashing functions will have fewer bytes they need to work with.\n\n    \n    \n    UID-Lookup-Maps: (32 Bytes * 5.5M Users) + (8 Bytes * 5.5M Users) = 177MB + 44MB = ~221MB Keys: 8 Bytes * 5.5M Users * 2 Maps = 88MB Values: 160M Follows * 8 Bytes * 2 Maps = ~2.56GB\n\nOur new in-memory graph math works out to under 3GB, maybe closer to 4-5 GB\nincluding provisioning overhead. This looks even more achievable for our\nservice!\n\n## How Fast is it?\n\nTo prove this concept can power production-scale features, I built an\nimplementation in Rust that loads a CSV adjacency list of follows on startup\nand provides HTTP endpoints for adding new follows, unfollowing, and a few\ndifferent kinds of queries.\n\nThe main structure of the graph is quite simple:\n\n    \n    \n    pub struct Graph { follows: RwLock<HashMap<u64, HashSet<u64>>>, followers: RwLock<HashMap<u64, HashSet<u64>>>, uid_to_did: RwLock<HashMap<u64, String>>, did_to_uid: RwLock<HashMap<String, u64>>, next_uid: RwLock<u64>, pending_queue: RwLock<Vec<QueueItem>>, pub is_loaded: RwLock<bool>, }\n\nWe keep track of follows in two directions, from the actor side and from the\nsubject side. Additionally we provide two lookup maps, one that turns DIDs to\nu64s and one that turns u64s back into DIDs.\n\nFinally we keep a variable to know which ID we will assign to the next DID we\nlearn about, and two variables that enqueue follows while we\u2019re loading our\ngraph from the CSV so we don\u2019t drop any events in the meantime.\n\nTo perform our Social Proof check, we can make use of this function:\n\n    \n    \n    // `get_following` and `get_followers` simply acquire a read lock // on their respective sets and return a copy of the HashSet pub fn intersect_following_and_followers(&self, actor: u64, target: u64) -> HashSet<u64> { self.get_following(actor) .intersection(&self.get_followers(target)) .cloned() .collect() }\n\nTo test the validity of this solution, we can use K6 to execute millions of\nsemi-random requests against the service locally.\n\nFor this service, we want to test a worst-case scenario to prove it\u2019ll hold\nup, so we will intersect the following set of many random users against the\n500 largest follower accounts on the network.\n\nRunning this test over the course of an hour at a rate of ~41.5k req/sec we\nsee the following results:\n\nWe\u2019re consuming ~6.6GB of resident RAM to support the graph and request load,\nand our service is responding to these worst-case requests with a p99 latency\nof ~1.2ms while keeping up with writes from the event firehose and utilizing\naround 7.5 CPU cores.\n\nCompared to a solution that depends on Redis sets, we\u2019re able to utilize\nmultiple CPU cores to handle requests since we leverage RWLocks that don\u2019t\nforce sequential access for reads.\n\nThe best part is, we don\u2019t need to hit our Scylla database at all in order to\nanswer these queries!\n\nWe don\u2019t need expensive concurrent fanout or to hammer Scylla partitions to\nkeep fresh follow data in sync to perform set intersections.\n\nWe can backfill and then iteratively maintain our follow graph in-memory for\nthe cost of a little bit of startup time (~5 minutes) and a few GB of RAM.\nSince it\u2019s so cheap, we could even run a couple instances of the service for\nhigher availability and rolling updates.\n\nAfter this proof of concept, I went back and performed a more realistic\nsustained load test at 2.65k req/sec for 5 hours to see what memory usage and\nCPU usage look like over time.\n\nUnder a realistic throughput (but worst-case query) production load we keep\np99s of under 1ms and consume 0.5 CPU cores while memory utilization trends\nslowly upward with the growth of the follow graph (+16MiB over 5 hours).\n\nThere\u2019s further optimization left to be made by locking individual HashSets\ninstead of the entire follows or following set, but we can leave that for a\nlater day.\n\nIf you\u2019re interested in solving problems like these, take a look at our open\nBackend Developer Job Rec.\n\n### Related posts\n\n  * Scaling Go to 192 Cores with Heavy I/O 10 Jan 2024\n  * Solving Thundering Herds with Request Coalescing in Go 28 Sep 2023\n  * Speeding Up Massive PostgreSQL Joins with Common Table Expressions 10 Aug 2023\n\n\u00a9 2024. All rights reserved.\n\n", "frontpage": false}
