{"aid": "39980554", "title": "Show HN: DiscoGrad \u2013 Automatically differentiate across branches in C++ programs", "url": "https://github.com/DiscoGrad/DiscoGrad", "domain": "github.com/discograd", "votes": 1, "user": "frankling_", "posted_at": "2024-04-09 15:34:26", "comments": 0, "source_title": "GitHub - DiscoGrad/DiscoGrad: DiscoGrad - automatically differentiate across conditional branches in C++ programs", "source_text": "GitHub - DiscoGrad/DiscoGrad: DiscoGrad - automatically differentiate across\nconditional branches in C++ programs\n\nSkip to content\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nDiscoGrad / DiscoGrad Public\n\n  * Notifications\n  * Fork 0\n  * Star 4\n\nDiscoGrad - automatically differentiate across conditional branches in C++\nprograms\n\n### License\n\nMIT license\n\n4 stars 0 forks Branches Tags Activity\n\nStar\n\nNotifications\n\n# DiscoGrad/DiscoGrad\n\nThis commit does not belong to any branch on this repository, and may belong\nto a fork outside of the repository.\n\n2 Branches\n\n0 Tags\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\njustinnkUpdate README.md640f84e \u00b7\n\n## History\n\n22 Commits  \n  \n### backend\n\n|\n\n### backend\n\n| further cleanup  \n  \n### docs\n\n|\n\n### docs\n\n| update animation  \n  \n### programs\n\n|\n\n### programs\n\n| further cleanup  \n  \n### transformation\n\n|\n\n### transformation\n\n| estimator improvements, add ICCS'24 experiments, drop SI  \n  \n### .gitignore\n\n|\n\n### .gitignore\n\n| initial commit  \n  \n### CITATION.bib\n\n|\n\n### CITATION.bib\n\n| further cleanup  \n  \n### LICENSE.txt\n\n|\n\n### LICENSE.txt\n\n| estimator improvements, add ICCS'24 experiments, drop SI  \n  \n### README.md\n\n|\n\n### README.md\n\n| Update README.md  \n  \n### build_docs.sh\n\n|\n\n### build_docs.sh\n\n| initial commit  \n  \n### compile_flags.txt\n\n|\n\n### compile_flags.txt\n\n| initial commit  \n  \n### doxygen.conf\n\n|\n\n### doxygen.conf\n\n| estimator improvements, add ICCS'24 experiments, drop SI  \n  \n### smooth_compile\n\n|\n\n### smooth_compile\n\n| estimator improvements, add ICCS'24 experiments, drop SI  \n  \n## Repository files navigation\n\n# DiscoGrad\n\nAutomatically differentiate across conditional branches in C++ programs.\n\nAutomatic Differentiation (AD) is a popular method to obtain the gradients of\ncomputer programs, which are extremely useful for adjusting program parameters\nusing gradient descent to solve optimization, control, and inference problems.\nUnfortunately, AD alone often yields unhelpful (zero-valued and/or biased)\ngradients for programs involving both parameter-dependent branching control\nflow such as if-else statements and randomness, including various types of\nsimulations.\n\nDiscoGrad automatically transforms C++ programs to a version that efficiently\ncalculates smoothed gradients across branches. Smoothing via external\nperturbations is supported, but is not required if the target program itself\ninvolves randomness. DiscoGrad includes several gradient estimation backends\nas well as the possibility to integrate neural networks via Torch. The tool\nsupports basic C++ constructs, but is still a research prototype.\n\nThe repository includes a number of sample applications from domains such as\ntransportation, crowd management, and epidemiology.\n\n## \ud83d\udcbe Installation\n\nTested on Ubuntu 22.04.4 LTS, Arch Linux and Fedora 38 Workstation\n\nTo compile the transformation code, you need the following packages (or their\nanalogues provided by your Linux distribution):\n\n  * clang, clang-devel (version 13 or higher)\n  * llvm, llvm-devel (version 13 or higher)\n  * cmake\n\n    \n    \n    cd transformation cmake . make -j\n\n## \ud83d\ude80 Quickstart\n\nYou can use the code contained in programs/hello_world/hello_world.cpp as a\nquickstart template and reference. The programs folder also contains a number\nof more complex programs.\n\nTo compile the hello world example, which implements the Heaviside step\nfunction as shown in the video above:\n\n    \n    \n    discograd$ ./smooth_compile programs/hello_world/hello_world.cpp\n\nsmooth_compile is a shell script that invokes the commands for transforming\nand compiling the code for the different backends. Here, it will create a\nbinary for each backend in the programs/hello_world folder.\n\nAD on the original (crisp) C++ program yields a 0 derivative:\n\n    \n    \n    DiscoGrad$ echo 0.0 | ./programs/hello_world/hello_world_crisp_ad --var 0.0 --ns 1 expectation: 1 derivative: 0\n\nOur estimator DiscoGrad Gradient Oracle (DGO) calculates a non-zero derivative\nuseful for optimization:\n\n    \n    \n    DiscoGrad$ echo 0.0 | ./programs/hello_world/hello_world_dgo --var 0.25 --ns 1000 expectation: 0.527 derivative: -0.7939109206\n\nYou can run ./programs/hello_world/hello_world_{crisp,dgo,pgo,reinforce} -h\nfor CLI usage information.\n\n## \u2754Usage\n\n### Use of the DiscoGrad API\n\nThe use of our API requires some boilerplate, as detailed below. Please refer\nto the programs folder for some example usages.\n\n  1. At the top of your source file, define how many inputs your program has and include the discograd header (in this order).\n\n    \n    \n    const int num_inputs = 1; #include \"discograd.hpp\"\n\n  2. Implement your entry function, by prepending _DiscoGrad_ to the name and using the differentiable type adouble as return value. An object of the type aparams holds the program inputs. As in traditional AD libraries, the type adouble represents a double precision floating point variable. In addition to differentiating through adoubles, DiscoGrad allows branching on (functions of) adoubles and generates gradients that reflect the dependence of the branch taken on the condition.\n\n    \n    \n    adouble _DiscoGrad_my_function(DiscoGrad& _dg, aparams p) { adouble inputs[num_inputs]; for (int i = 0; i < num_inputs; i++) inputs[i] = p[i]; adouble output = 0.0; ... // calculations and conditional branching based on inputs return output; }\n\n  3. In the main function, create an instance of the DiscoGrad class and a wrapper for your smooth function. Call .estimate(func) on the DiscoGrad instance to invoke the backend-specific gradient estimator.\n\n    \n    \n    int main(int argc, char** argv) { // interface with backend and provide the CLI arguments, such as the variance DiscoGrad<num_inputs> dg(argc, argv); // create a wrapper for the smooth function DiscoGradFunc<num_inputs> func(_DiscoGrad_my_function); // call the estimate function of the backend (chosen during compilation) dg.estimate(func); }\n\n### Compilation\n\nTo compile a program in the folder programs/my_program/my_program.cpp with\nevery backend:\n\n    \n    \n    discograd$ ./smooth_compile programs/my_program/my_program.cpp\n\nCustom compiler or linker flags can be set in the smooth_compile script.\n\nYou can find a list of backends below.\n\n### Executing a Smoothed Program\n\nTo run a smoothed program and compute its gradient, simply invoke the binary\nwith the desired CLI arguments, for example\n\n    \n    \n    discograd$ ./programs/my_program/my_program_dgo --var 0.25 --ns 100\n\nif you want to use the DGO backend. Parameters are entered via stdin, for\nexample by piping the output of echo as shown in the quickstart guide. The\noutput to stdout after expectation and derivative will provide the smoothed\noutput and partial derivatives.\n\n## Backends\n\nThis is an overview of all the current backends. More detailed explanations\ncan be found in the following sections.\n\nExecutableSuffix| Description  \n---|---  \ncrisp| The original program with optional input perturbations and AD  \ndgo| DiscoGrad Gradient Oracle, DiscoGrad's own gradient estimator based on\nautomatic differentiation and Monte Carlo sampling.  \npgo| Polyak's Gradient-Free Oracle presented by Polyak and further analysed by\nNesterov et al.  \nreinforce| Application of REINFORCE to programs with artificially introduced\nGaussian randomness.  \n  \nAdditionally, an implementation of gradient estimation via Chaudhuri and\nSolar-Lezama's method of Smooth Interpretation can be found in the branch\n'discograd_ieee_access'.\n\nNote: When all branches occur directly on discrete random variables drawn from\ndistributions of known shape, StochasticAD may be a well-suited alternative to\nthe above estimators.\n\nReferences:\n\n  * Chaudhuri, Swarat, and Armando Solar-Lezama. \"Smooth interpretation.\" ACM Sigplan Notices 45.6 (2010): 279-291.\n  * Boris T Polyak. \"Introduction to optimization.\" 1987. (Chapter 3.4.2)\n  * Nesterov, Yurii, and Vladimir Spokoiny. \"Random gradient-free minimization of convex functions.\" Foundations of Computational Mathematics 17 (2017): 527-566.\n\n## \u2696\ufe0f License\n\nThis project is licensed under the MIT License. The DiscoGrad tool includes\nsome parts from third parties, which are licensed as follows:\n\n  * backend/ankerl/unordered_dense.h, MIT license\n  * backend/genann.hpp, zlib license\n  * backend/discograd_gradient_oracle/kdepp.hpp, MIT license\n  * backend/args.{h,cpp}, MIT license\n  * Doxygen Awesome theme, MIT license\n\n## \ud83d\udcc4 Cite\n\n    \n    \n    @article{kreikemeyer2023smoothing, title={Smoothing Methods for Automatic Differentiation Across Conditional Branches}, author={Kreikemeyer, Justin N. and Andelfinger, Philipp}, journal={IEEE Access}, year={2023}, publisher={IEEE}, volume={11}, pages={143190-143211}, doi={10.1109/ACCESS.2023.3342136} }\n\n## About\n\nDiscoGrad - automatically differentiate across conditional branches in C++\nprograms\n\n### Resources\n\nReadme\n\n### License\n\nMIT license\n\n### Citation\n\nActivity\n\nCustom properties\n\n### Stars\n\n4 stars\n\n### Watchers\n\n2 watching\n\n### Forks\n\n0 forks\n\nReport repository\n\n## Releases\n\nNo releases published\n\n## Packages 0\n\nNo packages published\n\n## Contributors 2\n\n  * philipp-andelfinger\n  * justinnk Justin\n\n## Languages\n\n  * C++ 95.6%\n  * Shell 2.9%\n  * Python 1.1%\n  * CMake 0.4%\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": false}
