{"aid": "39994727", "title": "Playing Telephone with ChatGPT and DALLE3", "url": "https://blog.dagworks.io/p/playing-telephone-with-chatgpt-and", "domain": "dagworks.io", "votes": 2, "user": "krawczstef", "posted_at": "2024-04-10 19:22:13", "comments": 0, "source_title": "Playing Telephone with ChatGPT and DallE", "source_text": "Playing Telephone with ChatGPT and DallE\n\n# DAGWorks\u2019s Substack\n\nShare this post\n\n#### Playing Telephone with ChatGPT and DallE\n\nblog.dagworks.io\n\n#### Discover more from DAGWorks\u2019s Substack\n\nThought posts, and updates on Hamilton and the DAGWorks Platform.\n\nContinue reading\n\nSign in\n\n# Playing Telephone with ChatGPT and DallE\n\n### An exploration of latent space\n\nElijah ben Izzy\n\n,\n\nDAGWorks Inc.\n\n,\n\nThierry Jean\n\n, and\n\nStefan Krawczyk\n\nApr 09, 2024\n\n4\n\nShare this post\n\n#### Playing Telephone with ChatGPT and DallE\n\nblog.dagworks.io\n\nShare\n\nNote: This project found itself on the front page of HN a while ago. It got\nenough interesting comments/engagement that we decided to do a more in-depth\nwrite-up. Enjoy!\n\n##\n\nFun with APIs\n\nA few months ago I had just gained access to the multi-modal ChatGPT API, and\npromptly went down a rabbit hole.\n\nThanks for reading DAGWorks\u2019s Substack! Subscribe to receive new updates!\n\nI wanted to generate pretty pictures with DALL\u00b7E 3, but was too lazy to come\nup with prompts or ideas. So, rather than laboring over crafting prompts, what\nif I used ChatGPT to do it for me? And, to make up for a lack of creativity,\nwhat if I used a stochastic process?\n\nI started playing around with an idea that was inspired by an old childhood\npastime \u2013 the game of telephone. People gathered in a circle, whispering a\nsecret word from one to another, until it traveled all the way around. To the\nsurprise of seven-year-old me, the word would almost always change, often\nincomprehensibly so. The goal of this game was two-fold \u2013 not only did the\nobserved change provide an abundance of amusement, but as children we were\nable to learn about how information travels through lossy mediums. This\nprovides a profound view into multiple fundamental concepts, ranging from\nrumor spreading to lossy compression.\n\nIn the world of AI, a plethora of information is both lost and created through\ngenerative processes. The goal of image telephone is to capture and observe\nthis, ideally using it to learn something about the way models \u201cthink\u201d (in\nquotes, as thinking is an overly anthropomorphic description of what LLMs/LVMs\ndo).\n\nThe rules are simple:\n\n  1. Start with an image\n\n  2. Ask ChatGPT for a {somewhat, very, excessively, obsessively} detailed caption for that image\n\n  3. Ask DALL\u00b7E 3 for a picture from that caption\n\n  4. GOTO (2)\n\nThrough a series of strange transformations, the classic Windows XP hill and\nlogo morphed into a celestial cube on a warped checkerboard...\n\nI started playing around with this in the web UI, and quickly realized that,\nwhile the results were really fun, to scale up we needed automation and rigor.\nAs DAGWorks Inc. builds and maintains Hamilton (a library for building self-\ndocumenting, modular, and shareable dataflows), and Burr (a library for\nbuilding & debugging applications that carry state), I realized that I could\nboth implement this and share best practices, all while helping get Hamilton &\nBurr into people\u2019s hands.\n\nIn this post we\u2019re going to:\n\n  1. Outline how we implemented this\n\n  2. Show how you can get started/customize your own workflows, and contribute them back to the community\n\n  3. Opine a bit on the data we generated through playing the telephone game\n\n  4. Share a few more ideas for those of you who have too much time on your hands or too many OpenAI credits...\n\nIf you don\u2019t care much about the specifics (or don\u2019t want to write/read code),\nand want to look at pretty pictures, we\u2019ve packaged together the results in a\nstreamlit app you can explore https://image-telephone.streamlit.app/.\n\nYou can also skip down to \u201cThinking Deeper\u201d if you\u2019re not interested in the\ntechnical design, and just want pretty to see more pretty pictures (no\njudgement!).\n\n##\n\nStructuring our workflows\n\nWe\u2019re going to break our code into two workflows:\n\n  1. Generate a caption for an image\n\n  2. Generate an image for a caption\n\nThese will both be simple \u2013 they\u2019ll do some prompt/image manipulation, reach\nout to the corresponding OpenAI APIs, and return the results.\n\nAs we want these to be easy to read/track, we\u2019ll be writing them using\nHamilton \u2013 a lightweight layer over python functions for building and\nmaintaining dataflows. Hamilton allows you to express computation as a\ndirected acyclic graph (DAG) of python functions that specify their\ndependencies. Each function corresponds to an output variable, and the\nparameter names correspond to upstream dependencies (other functions or\ninputs). You use a \u201cdriver\u201d to run the code \u2013 it ensures that just the nodes\nyou need (and those upstream) are executed given the output variables you\nrequest.\n\nExample definition of a dataflow in Hamilton. The code on the left mirrors the\nDAG on the right. What\u2019s not shown is the Driver code.\n\nWhile we\u2019ll be using Hamilton to express individual dataflows, we need to tie\nthem together so we can run them in a loop. To do that, we\u2019ll be using Burr.\nIf you\u2019re familiar with LangChain tooling, you can think of the Hamilton DAGs\nas chains, and the Burr code encapsulating the \u201cagent\u201d like behavior that is\nour telephone game.\n\nLet\u2019s start by digging into the dataflows themselves:\n\n###\n\nImage -> Caption\n\nThe code for this is straightforward, but we added a few requirements to make\npost-hoc analysis easier. The high-level goals are:\n\n  1. Control how descriptive ChatGPT captions the image\n\n  2. Use the multi-modal API to generate a caption\n\n  3. Compute and store embeddings of the caption for later analysis (you\u2019ll see why in a bit)\n\n  4. Load images from and save them to a variety of sources\n\nIf you look at the DAG that Hamilton generates, you can see a few things \u2013 we\ndo a bit of prompt manipulation, process the image URL to be friendly to the\nOpenAI API, call out to get embeddings from the generated caption, and join it\nall together in a simple metadata dict.\n\nThe DAG has a variety of inputs with optional parameters \u2013 the only required\nones are the max_tokens, the model, the embeddings model, and the image URL.\n\nThe DAG for generating captions + associated metadata. Hamilton has tooling\nthat allows you to generate these images to form documentation for your\npipelines.\n\nYou can find the code here.\n\n###\n\nCaption -> Image\n\nThe code for this is far simpler \u2013 all we have to do is:\n\n  1. Create some prompt for image generation (\u201cgenerate an image based on this prompt\u201d)\n\n  2. Use DALL\u00b7E 3 to generate the image\n\n  3. Save the results\n\nThe DAG echoes the additional simplicity \u2013 all you need to do is pass the\nimage prompt (if you look at the code you\u2019ll see the others have defaults).\n\nThe DAG for generating an image for a prompt, also generated with Hamilton\n\nYou can find the code here.\n\n###\n\nTying it together\n\nFinally, we want to run this all in a loop. This is where we use Burr. Burr is\na Python micro-framework for easily expressing and debugging applications\nwhere application state drives what should happen next. Burr was built to\ncomplement Hamilton precisely to easily enable applications such as this\ntelephone application.\n\nTo use Burr we just need to define a few functions that constitute the actions\nwe want to take, and then how they are linked together and when to terminate.\n\n  * Actions:\n\n    * (1) Caption: Caption the image\n\n    * (2) Analyze: Generate analysis of the caption\n\n    * (3) Generate: Generate a new image based on the caption generated\n\n    * (4) Terminal: Final action we go to to end things.\n\n  * Edges:\n\n    * Caption \u2192 Analysis\n\n    * Analyze \u2192 Terminal: if max number of iterations reached.\n\n    * Analyze \u2192 Generate\n\n    * Generate \u2192 Caption\n\nEach of the \u201cCaption\u201d, \u201cAnalyze\u201d, \u201dGenerate\u201d actions will use our Hamilton\nDAGs. Note: we could generate the caption analysis during the Caption action,\nbut split it out for visibility. For the Analyze action we\u2019ll use Hamilton\u2019s\noverrides functionality to run different portions of the DAG to enable this.\nHamilton allows you to only compute the things that you want, and then easily\noverride portions of the DAG with prior computed values.\n\nWe then define the Burr application using these actions, specifying how we\ntransition between them, and when we want to terminate. For the actual\napplication we need to save the data to S3 and there are several avenues to do\nso (via materializers in Hamilton, in a Burr action, via Burr hooks, or\noutside the application) but we omit discussion of that in this post for\nbrevity.\n\nHere\u2019s the outline of the Burr actions & application that will run locally, or\nwithin a notebook (open this one or this one for a working example):\n\nThis file contains bidirectional Unicode text that may be interpreted or\ncompiled differently than what appears below. To review, open the file in an\neditor that reveals hidden Unicode characters. Learn more about bidirectional\nUnicode characters\n\nShow hidden characters\n\n\"\"\"  \n---  \nThis module demonstrates a telephone application  \nusing Burr that:  \n\\- captions an image  \n\\- creates caption embeddings (for analysis)  \n\\- creates a new image based on the created caption  \n\"\"\"  \nimport os  \nimport uuid  \nfrom hamilton import dataflows, driver  \nimport requests  \nfrom burr.core import Action, ApplicationBuilder, State, default, expr  \nfrom burr.core.action import action  \nfrom burr.lifecycle import PostRunStepHook  \n# import hamilton modules  \ncaption_images = dataflows.import_module(\"caption_images\", \"elijahbenizzy\")  \ngenerate_images = dataflows.import_module(\"generate_images\", \"elijahbenizzy\")  \n@action(  \nreads=[\"current_image_location\"],  \nwrites=[\"current_image_caption\", \"image_location_history\"],  \n)  \ndef image_caption(state: State, caption_image_driver: driver.Driver) ->\ntuple[dict, State]:  \n\"\"\"Action to caption an image.\"\"\"  \ncurrent_image = state[\"current_image_location\"]  \nresult = caption_image_driver.execute(  \n[\"generated_caption\"], inputs={\"image_url\": current_image}  \n)  \nupdates = {  \n\"current_image_caption\": result[\"generated_caption\"],  \n}  \n# could save to S3 here.  \nreturn result,\nstate.update(**updates).append(image_location_history=current_image)  \n@action(  \nreads=[\"current_image_caption\"],  \nwrites=[\"caption_analysis\"],  \n)  \ndef caption_embeddings(state: State, caption_image_driver: driver.Driver) ->\ntuple[dict, State]:  \nresult = caption_image_driver.execute(  \n[\"metadata\"],  \noverrides={\"generated_caption\": state[\"current_image_caption\"]}  \n)  \n# could save to S3 here.  \nreturn result, state.append(caption_analysis=result[\"metadata\"])  \n@action(  \nreads=[\"current_image_caption\"],  \nwrites=[\"current_image_location\", \"image_caption_history\"],  \n)  \ndef image_generation(state: State, generate_image_driver: driver.Driver) ->\ntuple[dict, State]:  \n\"\"\"Action to create an image.\"\"\"  \ncurrent_caption = state[\"current_image_caption\"]  \nresult = generate_image_driver.execute(  \n[\"generated_image\"], inputs={\"image_generation_prompt\": current_caption}  \n)  \nupdates = {  \n\"current_image_location\": result[\"generated_image\"],  \n}  \n# could save to S3 here.  \nreturn result,\nstate.update(**updates).append(image_caption_history=current_caption)  \n@action(  \nreads=[\"image_location_history\", \"image_caption_history\", \"caption_analysis\"],  \nwrites=[]  \n)  \ndef terminal_step(state: State) -> tuple[dict, State]:  \nresult = {\"image_location_history\": state[\"image_location_history\"],  \n\"image_caption_history\": state[\"image_caption_history\"],  \n\"caption_analysis\": state[\"caption_analysis\"]}  \n# could save to S3 here.  \nreturn result, state  \ndef build_application(starting_image: str = \"statemachine.png\",  \nnumber_of_images_to_caption: int = 4):  \n\"\"\"This shows how one might define functions to be nodes.\"\"\"  \n# instantiate hamilton drivers and then bind them to the actions.  \ncaption_image_driver = (  \ndriver.Builder()  \n.with_config({\"include_embeddings\": True})  \n.with_modules(caption_images)  \n.build()  \n)  \ngenerate_image_driver = (  \ndriver.Builder()  \n.with_config({})  \n.with_modules(generate_images)  \n.build()  \n)  \napp = (  \nApplicationBuilder()  \n.with_state(  \ncurrent_image_location=starting_image,  \ncurrent_image_caption=\"\",  \nimage_location_history=[],  \nimage_caption_history=[],  \ncaption_analysis=[],  \n)  \n.with_actions(  \ncaption=image_caption.bind(caption_image_driver=caption_image_driver),  \nanalysis=caption_embeddings.bind(caption_image_driver=caption_image_driver),  \ngenerate=image_generation.bind(generate_image_driver=generate_image_driver),  \nterminal=terminal_step,  \n)  \n.with_transitions(  \n(\"caption\", \"analysis\", default),  \n(\"analysis\", \"terminal\",  \nexpr(f\"len(image_caption_history) == {number_of_images_to_caption}\")),  \n(\"analysis\", \"generate\", default),  \n(\"generate\", \"caption\", default),  \n)  \n.with_entrypoint(\"caption\")  \n.with_tracker(project=\"image-telephone\")  \n.build()  \n)  \nreturn app  \nif __name__ == \"__main__\":  \napp = build_application()  \napp.visualize(  \noutput_file_path=\"statemachine\", include_conditions=True, view=True,\nformat=\"png\"  \n)  \nlast_action, result, state = app.run(halt_after=[\"terminal\"])  \n# save to S3 / download images etc.  \nprint(state)  \n  \nview raw telephone-burr-application.py hosted with \u2764 by GitHub\n\nThe state machine that powers Burr, connecting the various Hamilton DAGs\ntogether\n\nFor full code we direct you to:\n\n  * Hamilton repository with full code to replicate the streamlit application.\n\n  * Burr repository for a simplified version of this game.\n\n##\n\nSharing Code with the Hamilton Hub\n\nThe exciting thing about writing this code is that anyone can use it \u2013 not\nonly is it open-source, but it also lives on the Hamilton hub, where it is\ndocumented and displayed, along with a variety of other user-contributed\ndataflows. You can see the two dataflows we\u2019ve drawn above in the hub:\n\n  * Image captioning\n\n  * Image generation\n\nIt is easy to get started with these \u2013 you can (as we\u2019ve shown in our script\nabove) download and run them without installing a new library.\n\nThis file contains bidirectional Unicode text that may be interpreted or\ncompiled differently than what appears below. To review, open the file in an\neditor that reveals hidden Unicode characters. Learn more about bidirectional\nUnicode characters\n\nShow hidden characters\n\ncaption_images = dataflows.import_module(\"caption_images\", \"elijahbenizzy\")  \n---  \ngenerate_images = dataflows.import_module(\"generate_images\", \"elijahbenizzy\")  \ndr = driver.Driver({\"include_embeddings\" : True}, caption_images,\ngenerate_images)  \n  \nview raw driver.py hosted with \u2764 by GitHub\n\nYou can easily contribute your own as well \u2013 see the guidelines to get\nstarted!\n\n##\n\nThinking Deeper\n\nNow that we\u2019ve shown you how to build and run these workflows yourself, it\nshould be straightforward to get started. We ran quite a few iterations of\nthis algorithm, and found some really interesting results...\n\nIf you want to go ahead and explore before reading our musings, you can play\nwith the results here \u2013 I recommend cycling through that to get a sense before\ndigging in.\n\n###\n\nA Method for Exploring\n\nWhen we captioned the image, we fired off a secondary request to the OpenAI\nAPI to get embeddings for the generated caption, using the text-embeddings-\nada-002 model. While this is suboptimal compared to gathering image\nembeddings, it was an easy way to gather some interesting data.\n\nAs embeddings are very high dimensional (1500 dimensions for OpenAI!), the\ndata itself means nothing at first glance. Thus we use a tool called TSNE\n(t-distributed stochastic neighbor embedding) to project these onto 2\ndimensions. There\u2019s some complex math/tuning that goes into it, but you want\nto think of it as roughly distance-preserving \u2013 if two points in high-\ndimensional space are close to each other, they\u2019ll likely be close together in\nlow-dimensional space when TSNE is applied.\n\nUsing this allows us to plot the progression over time. Let\u2019s take a look at\nwhat happens when we feed Salvador Dali\u2019s timeless painting, The Persistence\nof Memory, through our algorithm:\n\nWe see an initial large jump as a particularly lazy caption causes the subject\nmatter to jump from melting clocks (a) to space (b). It meanders around a bit\nwith the ballerina (c)/(d), until we end up at a nighttime scene with the\nmoon, a mystical building, the same lady (who has given up dancing), and a cat\n(.\n\nNote that this is far from a perfect science \u2013 just as embeddings are a\nprojection from extremely high dimensional space to a simple array of vectors,\nTSNE is yet another dimensionality reduction, and almost all information is\nlost. That said, it does provide some interesting insights.\n\nPlotting a variety of executions together on a single TSNE plot. While the\ndata is a bit of a mess, you can see clusters form between different series.\nAs shown, the ominous city explosion image is terrifyingly common...\n\n###\n\nSome Head Scratchers\n\nA few of the decisions ChatGPT made in captioning (and DALL\u00b7E 3 made in\ngenerating) were somewhat bizarre:\n\nWhile this was my best latte to date, I recognize that HN user MCBrit\u2019s B-\nLatte Art description may have been a tad more realistic. Luckily DALL\u00b7E 3 was\nable to turn this into something beautiful.\n\nDALL\u00b7E 3 decided to switch cultures and add a pagoda here \u2013 which gradually\nbecame more ornate and eventually dominated the image itself.\n\nBlurring the line between birds and birds-of-paradise...\n\nThese are just a few of the examples of strange augmentations,\nmisinterpretations, and removals \u2013 please look through and leave a comment if\nthey interest you!\n\n###\n\nLocal Minima\n\nInterestingly, DALL\u00b7E 3 would get stuck in a few patterns \u2013 these would\noccasionally run across series as well...\n\nLatte art morphed into tea sets for a while\n\nDALL\u00b7E 3 loves galaxies\n\nOriental pagodas dominated multiple series\n\nWhile there was no one-size-fits-all explanation for these, similar patterns\nkept cropping up. Digging in more would require quite a bit of analysis, but\nwe\u2019ve theorized that:\n\n  * DALL\u00b7E 3 gives preference to its training data and tends to sit around that (space-images, pagodas, etc...)\n\n  * DALL\u00b7E 3 works with reduced information \u2013 in the embeddings space in which DallE operates, tea sets, gothic mansions, and galaxies are all closer to a basis vector than the more transient subjects\n\n  * The way we prompted it (often, describe this image \u201cobsessively\u201d) was associated with a certain style that promoted dramatic flourishes. These are often unstable when projected between GPT-4 and DALL\u00b7E 3\u2019s vector space, but the clusters + the description combine, in these cases, to form a more invertible projection, due to the topology of the space.\n\n###\n\nSymmetry, Reflections, Fractals, and Repetition\n\nWe found a few interesting meta-patterns present throughout the groups of\nimages. Symmetry and reflections played a large role in multiple images:\n\nLeft-right symmetry, reflective symmetry, rotational symmetry, and facial\nsymmetry were all common\n\nTo take symmetry multiple steps further, we have fractals (self-symmetric\nshapes). While it is not particularly novel to claim that fractals can\ndescribe most of the world around us, they do feel particularly well-suited to\nDALL\u00b7E 3\u2019s artistic style\n\nThe emergence and evolution of a fractal from an amphibious cathedral\n(abridged and selected from a separate, offline series)\n\nThe emergence and evolution of a fractal from an amphibious cathedral\n(abridged and selected from a separate, offline series)\n\nWe also found that DALL\u00b7E 3 was very dramatic (in no small part due to our\nprompting approach) \u2013 it often just happily added *more* of anything\ninteresting.\n\nDALL\u00b7E 3 keeps adding waterfalls until it gets distracted and switches to a\nnew focus\n\nThese basic compositional properties \u2013 symmetry, reflections, fractals, and\nrepetition (among more) likely show up for a variety of reasons:\n\n  * They are common in the training data\n\n  * They are visually interesting and pleasing to the eye \u2013 the RLHF approach has suggested these to be stronger\n\n  * They contain less information (due to symmetry), and are thus easier to draw\n\nWithout a deeper dive into DALL\u00b7E 3\u2019s architecture, these are all just\nconjectures. And, by no means, an exhaustive set. If you have insights,\npredictions, or wish to generate more data, please reach out/post in the\ncomments! Happy to discuss.\n\nShare\n\n##\n\nIn Summation\n\nWe did a lot in this post \u2013 with the aim of having something for everyone.\n\nFor the builders among us:\n\n  * We demonstrated how you can use Hamilton to build and adapt well-organized dataflows.\n\n  * We demonstrated how you can build a simple application using Burr to orchestrate Hamilton DAGs.\n\n  * We walked through the code, at a high level, that we used to caption and generate images.\n\n  * We shared the Hamilton Hub \u2013 making it easier for you to get started and share your own dataflows.\n\nFor the philosophers among us:\n\n  * We walked through a simple framework to analyze the transition of photographs through this process.\n\n  * We highlighted and dug into a few of the stranger transitions we saw through the image telephone game.\n\n  * We hypothesized about why DALL\u00b7E 3/ChatGPT made the decisions it did.\n\nPerhaps the strangest and most profound learning, however, came from the\nlaziness that prompted this dive into the rabbit hole in the first place.\nWhile crafting prompts is a lot of work, getting ChatGPT to do it for you is\nquite easy. When you allow a basic stochastic process (such as projecting\nbetween the two models) to guide the way, you\u2019ll find that DALL\u00b7E 3 has\npreferences. Perhaps these preferences represent its best self \u2013 the images\nthat it wants to draw, of which it can seamlessly create variation upon\nvariation.\n\nThe exciting thing is that there are a lot more things you can do with this\napproach and this tooling. Using Hamilton to build, the Hamilton hub to share,\nthe OpenAI APIs to compute, and a stochastic method to iterate, you can build\na lot! Two apps we\u2019ve thought of but have not yet built:\n\n  1. Make it more \u2013 start with an image of a \u201chard-working-cat\u201d (generically, an Xing Y). Then, make it more Xing, and more Xing, and continue until the results look absurd. This was a popular meme on twitter recently. The way to really make this easy would be to ask ChatGPT for the next caption.\n\n  2. Craft a story \u2013 start with an image of your choice, and ask ChatGPT to caption it as panel N of a 20-panel cartoon, and to give the next panel. Ask DALL\u00b7E 3 to generate that, and continually pass back. You should find some interesting things\n\nI\u2019m sure you can come up with even more innovative ideas \u2013 the world is your\noyster here.\n\n##\n\nWe want to hear from you!\n\nIf you\u2019re excited by any of this, or have strong opinions, drop by our Slack\nchannel / leave some comments here! Some resources to help you get started:\n\n\ud83d\udce3 join our Hamilton community on Slack \u2014 need help with Hamilton? Ask here.\n\n\ud83d\udce3 join our Burr community on Discord \u2014 need help with Burr? Ask here.\n\n\u2b50\ufe0f Hamilton on GitHub\n\n\u2b50\ufe0f Burr on GitHub\n\n\ud83d\udcdd leave us an issue if you find something\n\n\ud83d\udcc8 check out the DAGWorks platform and sign up for a free trial\n\n\ud83d\udcc8 sign up for Burr Cloud.\n\nOther Hamilton posts you might be interested in:\n\n  * tryhamilton.dev \u2013 an interactive tutorial in your browser!\n\n  * Production Prompt Engineering Patterns with Hamilton\n\n  * Build a modular LLM stack with Hamilton\n\n  * From Dev to Prod: a ML Pipeline Reference Post\n\n  * Hamilton + Airflow (GitHub repo)\n\n  * Hamilton + Feast (GitHub repo)\n\n  * Pandas data transformations in Hamilton in 5 minutes\n\n  * Lineage + Hamilton in 10 minutes\n\nThanks for reading DAGWorks\u2019s Substack! Subscribe for free to receive new\nposts and support my work.\n\n4 Likes\n\n4\n\nShare this post\n\n#### Playing Telephone with ChatGPT and DallE\n\nblog.dagworks.io\n\nShare\n\nComments\n\nWinning over hearts and minds at work: ADKAR my favorite change management\napproach\n\nA post on five simple steps to help you affect change in your data\norganization, using Hamilton as an example.\n\nDec 19, 2023 \u2022\n\nDAGWorks Inc.\n\nand\n\nStefan Krawczyk\n\n8\n\nShare this post\n\n#### Winning over hearts and minds at work: ADKAR my favorite change\nmanagement approach\n\nblog.dagworks.io\n\nHow well-structured should your data code be?\n\nYou need more structure than you think but less than you fear.\n\nJan 9 \u2022\n\nElijah ben Izzy\n\n,\n\nDAGWorks Inc.\n\n, and\n\nStefan Krawczyk\n\n8\n\nShare this post\n\n#### How well-structured should your data code be?\n\nblog.dagworks.io\n\nRetrieval augmented generation (RAG) with Streamlit, FastAPI, Weaviate, and\nHamilton!\n\nA scalable reference architecture for RAG applications\n\nSep 8, 2023 \u2022\n\nThierry Jean\n\n,\n\nDAGWorks Inc.\n\n, and\n\nStefan Krawczyk\n\n6\n\nShare this post\n\n#### Retrieval augmented generation (RAG) with Streamlit, FastAPI, Weaviate,\nand Hamilton!\n\nblog.dagworks.io\n\nReady for more?\n\n\u00a9 2024 DAGWorks Inc.\n\nPrivacy \u2219 Terms \u2219 Collection notice\n\nStart WritingGet the app\n\nSubstack is the home for great writing\n\nShare\n\n## Create your profile\n\n## Only paid subscribers can comment on this post\n\nAlready a paid subscriber? Sign in\n\n#### Check your email\n\nFor your security, we need to re-authenticate you.\n\nClick the link we sent to , or click here to sign in.\n\n", "frontpage": false}
