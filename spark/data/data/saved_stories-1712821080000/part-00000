{"aid": "39997573", "title": "MiniGPT4-Video", "url": "https://vision-cair.github.io/MiniGPT4-video/", "domain": "vision-cair.github.io", "votes": 3, "user": "essamsleiman", "posted_at": "2024-04-11 01:36:57", "comments": 0, "source_title": "MiniGPT4-Video", "source_text": "MiniGPT4-Video\n\n# MiniGPT4-Video:\n\n## Advancing Multimodal LLMs for Video Understanding with Interleaved Visual-\nTextual Tokens\n\nKirolos Ataallah , Xiaoqian Shen, Eslam Abdelrahman, Essam Sleiman, Deyao Zhu,\nJian Ding, Mohamed Elhoseiny\n\n\u25b6 King Abdullah University of Science and Technology \u25b6 Harvard University;\n\nPaper Code Video Model Demo\n\n## Abstract\n\nThis paper introduces MiniGPT4-Video, a multimodal Large Language Model (LLM)\ndesigned specifically for video understanding. The model is capable of\nprocessing both temporal visual and textual data, making it adept at\nunderstanding the complexities of videos. Building upon the success of\nMiniGPT-v2, which excelled in translating visual features into the LLM space\nfor single images and achieved impressive results on various image-text\nbenchmarks, this paper extends the model's capabilities to process a sequence\nof frames, enabling it to comprehend videos. MiniGPT4-video does not only\nconsider visual content but also incorporates textual conversations, allowing\nthe model to effectively answer queries involving both visual and text\ncomponents. The proposed model outperforms existing state-of-the-art methods,\nregistering gains of 4.22%, 1.13%, 20.82%, and 13.1% on the MSVD, MSRVTT,\nTGIF, and TVQA benchmarks respectively.\n\n## Video Demo\n\n## Model\n\nMiniGPT-Video consists of a vision encoder (EVA-CLIP), a single linear\nprojection layer, and large language model (LLama2 or Mistral).:\n\n###\n\nThe architecture of MiniGPT4-Video.\n\n## Results\n\nFor a comprehensive evaluation of our proposed architecture, we assessed its\nperformance across three bench-mark types: Video-ChatGPT, Open-ended\nQuestions, and Multiple-Choice Questions (MCQs). In the Video-ChatGPT\nbenchmark, depicted in Table 1, our model is comparable with the previous\nmethods without subtitles. When we add the subtitles as input, our model\nachieves the state-of-the- art in all five dimensions, which verified that our\nmodel can utilize the subtitle information to improve the video understanding.\nIn the zero-shot evaluation of open-ended and multiple-choice question\nbenchmarks, our proposed MiniGPT4-Video sig- nificantly outperforms existing\nstate-of-the-art methods. It achieves notable margins of improvement 4.22%,\n1.13%, 20.82%, and 13.1% on the MSVD, MSRVTT, TGIF, and TVQA benchmarks,\nrespectively. The results, both with and without subtitles, further\ndemonstrate that integrating subtitle information alongside visual cues\nsignificantly enhances performance, with accuracy rising from 33.9% to 54.21%\non TVQA. While subtitles contribute substantially to performance improvements\non TVQA, their inclusion doesn\u2019t offer added value for datasets like MSVD- QA,\nMSRVTT-QA, TGIF-QA, and ActivityNet, where ques- tions are exclusively vision-\nbased.:\n\n## Examples\n\n## Acknowledgement\n\nThis website is adapted from Nerfies, licensed under a Creative Commons\nAttribution-ShareAlike 4.0 International License.\n\n", "frontpage": false}
