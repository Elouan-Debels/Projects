{"aid": "40054669", "title": "CodeQwen1.5", "url": "https://qwenlm.github.io/blog/codeqwen1.5/", "domain": "qwenlm.github.io", "votes": 1, "user": "tosh", "posted_at": "2024-04-16 17:20:55", "comments": 0, "source_title": "Code with CodeQwen1.5", "source_text": "Code with CodeQwen1.5 | Qwen\n\n# Code with CodeQwen1.5\n\nApril 16, 2024 \u00b7 6 min \u00b7 1151 words \u00b7 Qwen Team | Translations:\n\n  * \u7b80\u4f53\u4e2d\u6587\n\nGITHUB HUGGING FACE MODELSCOPE DEMO DISCORD\n\n# Introduction#\n\nThe advent of advanced programming tools, which harnesses the power of large\nlanguage models (LLMs), has significantly enhanced programmer productivity and\naccuracy. Notwithstanding these advancements, dominant coding assistants like\nGithub Copilot, built upon proprietary LLMs, pose notable challenges in terms\nof cost, privacy, security, and potential copyright infringement. Recognizing\nthe imperative for a more transparent and accessible alternative, the open-\nsource community has embarked on a concerted endeavor to develop open\ncodeLLMs. This initiative has already given rise to several promising open-\nsource models, including StarCoder2, CodeLlama, and DeepSeek-Coder, offering a\npath forward, albeit one that necessitates continued refinement.\n\nToday, we are delighted to introduce a new member of the Qwen1.5 open-source\nfamily, the CodeQwen1.5-7B, a specialized codeLLM built upon the Qwen1.5\nlanguage model. CodeQwen1.5-7B has been pretrained with around 3 trillion\ntokens of code-related data. It supports an extensive repertoire of 92\nprogramming languages, and it exhibits exceptional capacity in long-context\nunderstanding and generation with the ability to process information of 64K\ntokens. In terms of performance, CodeQwen1.5 demonstrates impressive\ncapabilities in basic code generation, long-context modeliing, code editation\nand SQL. We believe this model can significantly enhance developer\nproductivity and streamline software development workflows within diverse\ntechnological environments.\n\n# CodeQwen are Basic Coders#\n\nCode generation is a key competence for large language models, as they are\ntasked with translating natural language instructions into executable code\nwith unwavering precision. CodeQwen1.5, with only 7 billion parameters, has\nsurpassed larger models in basic code generation capabilities, further\nnarrowing the gap in coding proficiency between GPT-4 and opensource code\nLLMs. We conducted a thorough evaluation on HumanEval and MBPP to provide a\nclear and fair comparison as follows.\n\nModel| Size| HumanEval0-shot| HumanEval+0-shot| MBPP0-shot| MBPP+0-shot|\nMBPP3-shot  \n---|---|---|---|---|---|---  \nBase Model  \nCodeLlama-Base| 7B| 33.5| 25.6| 52.1| 41.6| 38.6  \nStarCoder2| 7B| 35.4| 29.9| 54.4| 45.6| 51.0  \nDeepSeek-Coder-Base| 6.7B| 47.6| 39.6| 70.2| 56.6| 60.6  \nCodeQwen1.5| 7B| 51.8| 45.7| 72.2| 60.2| 61.8  \nChat Model  \nGPT-3.5-Turbo| -| 76.8| 70.7| 82.5| 69.7| 70.8  \nGPT-4-Turbo (Nov 2023)| -| 85.4| 81.7| 83.5| 70.7| 80.0  \nDeepSeek-Coder-Instruct| 6.7B| 78.6| 70.1| 73.2| 63.4| 65.4  \nCodeQwen1.5-Chat| 7B| 83.5| 78.7| 77.7| 67.2| 70.6  \n  \nIn addition to the widely recognized HumanEval and MBPP benchmarks, we\nexplored LiveCodeBench. This benchmark assesses code performance by\nintroducing fresh challenges sourced from coding competitions such as\nLeetCode, AtCoder, and CodeForces over time. Our evaluation of CodeQwen1.5 on\nLiveCodeBench spanned from September 1, 2023, to April 1, 2024. The findings\nindicate that CodeQwen1.5 ranks among the top open-access models currently\navailable. Note: it is possible that the inclusion of LeetCode data in our\npretraining corpus may contribute to the performance in LiveCodeBench.\n\nThe evaluations mentioned primarily revolve around Python capabilities;\nhowever, CodeQwen1.5 is not merely a Python specialist but also an expert\nacross multiple programming languages. We conducted a comprehensive evaluation\nof CodeQwen1.5 in the eight mainstream languages featured in MultiPL-E,\nincluding Python, C++, Java, PHP, TypeScript, C#, Bash, and JavaScript. The\nresults highlight the exceptional programming capabilities of CodeQwen1.5.\n\n## CodeQwen are Long Context Coders#\n\nLong context capability is crucial for code LLMs, serving as the core skill\nfor understanding repository-level code and becoming a code agent. However,\ncurrent code models still have very limited support for length, which hinders\ntheir potential for practical application. CodeQwen1.5 aims to further advance\nthe progress of open-source code models in long context modeling. To achieve\nthis, we have collected and constructed long sequence code data at the\nrepository level for pre-training. Through careful data proportioning and\norganization, we have enabled it to support input lengths of up to 64K tokens.\n\nEvaluation 1: We collected high-quality repo from GitHub Trending repositories\non 2024-3-28 that were not included in CodeQwen1.5\u2019s training data to observe\nthe effectiveness of long context modeling. The following figure demonstrates\nthat as the sequence length increases, CodeQwen1.5\u2019s Perplexity (PPL) still\nmanages to maintain a downward trend.\n\nEvaluation 2: We created a synthetic task called Needle in the Code, inspired\nby popular long-context evaluations in the text domain. In this task, we\ninserted a very simple custom function at various positions within a longer\ncodebase (we chose Megatron to honor its contributions to open-source LLMs!)\nand tested whether the model could replicate this function at the end of the\ncodebase. The figure below shows that CodeQwen is capable of successfully\ncompleting this task within a 64k length range.\n\nBoth Evaluation 1 and Evaluation 2 serve as initial and foundational\nassessments. For the Chat model, we aim to evaluate its long context\ncapabilities with more practical tasks. However, our objective is to examine\nthe Chat model\u2019s capability to handle long contexts through more pragmatic,\nreal-world evaluation tasks.\n\nEvaluation 3: SWE Bench is a benchmark designed to assess the ability of Large\nLanguage Models (LLMs) or agents to tackle practical software development\nchallenges. It presents contestants with a code repository and an associated\nissue, tasking them with generating a commit patch that resolves the issue\neffectively. The benchmark uniquely emphasizes the long-context processing\ncapabilities of code LLMs, necessitating both deep comprehension of the given\ncodebase and the generation of extensive, unit-test-passing code.\n\nCurrently, participants in the SWE Bench competition predominantly are\nproprietary models. We introduce CodeQwen1.5 as an open-source model entry.\nDespite achieving a score of 0.89, CodeQwen1.5 surpasses ChatGPT-3.5,\ndemonstrating the nascent yet promising competitiveness of open-source code\nmodels against their proprietary counterparts.\n\n## CodeQwen are Debuggers#\n\nAn effective code assistant must demonstrate proficiency in both generating\ncode in response to given specifications and adeptly modifying or debugging\nexisting code to accommodate evolving requirements or rectify errors. In\nassessing CodeQwen1.5\u2019s proficiency in code modification tasks, we\nconcentrated our evaluation on the CodeEditorBench suite, encompassing four\ndistinct dimensions: Debugging, Translation, Language Switching, and Code\nPolishing. The results indicate that CodeQwen1.5 achieves the SOTA performance\nat the 7 billion parameter scale.\n\n## CodeQwen are SQLers#\n\nCodeQwen1.5 serves as a solution to bridge the gap between non-programming\nprofessionals and efficient data interaction. It alleviates the steep learning\ncurve associated with SQL by enabling users without coding expertise to query\ndatabases through natural language. We evaluated CodeQwen1.5-Chat\u2019s\nperformance on two popular Text-to-SQL benchmarks, Spider and Bird.\nExperimental results pose CodeQwen1.5 a second position close to GPT-4\n(results come from DIN-SQL, a SOTA prompting method). This outstanding\nperformance is attributed to the utilization of synthetic data throughout both\npre-training and fine-tuning stages. Synthetic data, characterized by its\nscalability, verifiability, and variety, emerges as a compelling area for\nfuture research due to its proven effectiveness in enhancing CodeQwen1.5\u2019s SQL\ncapabilities.\n\n## Develop with CodeQwen1.5#\n\nCodeQwen1.5 is part of the Qwen1.5 open-source family. We advise you to read\nour blog for Qwen1.5 to figure out the usages with Transformers, vLLM,\nllama.cpp, Ollama, etc.\n\n## Conclusion#\n\nWe have released CodeQwen1.5-7B and CodeQwen1.5-7B-Chat, an open and versatile\ncode LLM. The models are intended to aid progress in code assistance and code\nagents, benefiting the research community. We\u2019ll keep investing heavily in\nsmart code development, with the ultimate goal of creating AI programmers.\n\n\u00a9 2024 Qwen Powered by Hugo\n\n", "frontpage": false}
