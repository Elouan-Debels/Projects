{"aid": "40054645", "title": "Lucene: The Good Parts", "url": "https://www.parse.ly/lucene/", "domain": "parse.ly", "votes": 4, "user": "Tomte", "posted_at": "2024-04-16 17:18:57", "comments": 0, "source_title": "Lucene: The Good Parts", "source_text": "Lucene: The Good Parts | Parse.ly\n\nSkip to content\n\n# Lucene: The Good Parts\n\nBefore MongoDB, before Cassandra, before \u201cNoSQL\u201d, there was Lucene.\n\nDid you know that Doug Cutting wrote the first versions of Lucene in 1999? To\nput things in context, this was around the time Google was more a research\nproject than an actual trusted application. Google\u2019s proof-of-concept search\nengine was still a sprawling set of desktop computers in Stanford\u2019s research\nlabs.\n\nI worked on my first Lucene project around 2005. It was a document management\nsystem. It didn\u2019t have any real issues of scale \u2014 it was a web application\nmeant to be run on-premise and to provide a view of data that could safely fit\nin a hard drive or NAS.\n\nBut, even though the total dataset measured in the hundreds of gigabytes,\nsearching through all the data efficiently was still a challenge. SQL was not\nthen, and is still not now, a very good blob or document storage system. Yet,\nthere seemed to be no alternative to SQL for durability, short of relying\ndirectly upon the filesystem. To boot, the primary use case of the application\nI was working on was actually document search. People needed to find things.\nAll SQL databases stink at unstructured search, so that\u2019s why I started\nresearching Lucene.\n\nLucene was a Java library you had to learn, and then manually integrate into\nyour app. Thankfully, this wasn\u2019t as hopeless as it sounds now.\n\nAmong Java projects, Lucene was exceptionally well-documented. Further, Lucene\nin Action had been published in 2004, and the book went into a lot of depth on\nhow the whole library worked. I remember purchasing my copy and devouring the\nbook in a weekend. I remember thinking, at the time, that it was probably one\nof the best technical books I had read \u2014 not just about Lucene, but in\ngeneral!\n\nLucene in Action is now available in a 2nd Edition that updates it for Lucene\n3.0 \u2014 though, it could probably use yet another update, as Lucene 5.0 was\nrecently released!\n\nA couple of things struck me about Lucene after my first project working with\nit. First, Lucene approaches problems of data exploration from the vantage\npoint of \u201cinformation retrieval\u201d, not from the vantage point of \u201cdatabase\nmanagement theory\u201d. This meant Lucene was less concerned with things like\nMVCC, ACID, and 3-NF, and was instead concerned with much more practical\nconcerns, like how to build a fast and humane interface for unstructured data.\n\nLucene\u2019s creator pondered: How do we support queries that normal users will\nactually type? How do we rapidly search all the data we have, in one fell\nswoop? How do we order the results when there is more than one likely match?\nHow do we summarize the full result set, even if we only have enough space to\ndisplay part of the result set?\n\nAt the time, Solr and Elasticsearch didn\u2019t yet exist. Solr would be released\nin one year by the team at CNET. With that release would come a very important\napplication of Lucene: faceted search. Elasticsearch would take another 5\nyears to be released. With its recent releases, it has brought another\nimportant application of Lucene to the world: aggregations. Over the last\ndecade, the Solr and Elasticsearch packages have brought Lucene to a much\nwider community. Solr and Elasticsearch are now being considered alongside\ndata stores like MongoDB and Cassandra, and people are genuinely confused by\nthe differences.\n\nGoogle Trends shows declining search interest in Lucene, even as Solr and\nElasticsearch interest rises. How odd \u2014 Lucene is what makes it all work!\n\nSo, I thought it might be fun to go back to basics. What\u2019s so good about\nLucene? How does it work under the hood? And why does that give a system like\nElasticsearch a leg up on a system like Cassandra in certain applications?\nFinally, what can we learn from Lucene even if we don\u2019t care about full text\nsearch?\n\n## Jargon terms\n\nSo, let\u2019s start with a de-jargoning exercise. Here are some terms you see\nthrown around in the Lucene and Information Retrieval communities which are\nnot nearly as common in the SQL and database communities. Lucene even re-\ndefines the term \u201cterm\u201d \u2014 so, please, pay attention!\n\n  * document: a record; the unit of search; the thing returned as search results (\u201cnot a row\u201d)\n  * field: a typed slot in a document for storing and indexing values (\u201cnot a column\u201d)\n  * index: a collection of documents, typically with the same schema (\u201cnot a table\u201d)\n  * corpus: the entire set of documents in an index\n  * inverted index: internal data structure that maps terms to documents by ID\n  * term: value extracted from source document, used for building the inverted index\n  * vocabulary: the full set of distinct terms in a corpus\n  * uninverted index: aka \u201cfield data\u201d, array of all field values per field, in document order\n  * doc values: alternative way of storing the uninverted index on-disk (Lucene-specific)\n\nOK, that gets some jargon out of the way.\n\n## Inverting our corpus\n\nLet\u2019s start with a simple corpus of: two documents, doc1 and doc2, both\ncontain the field \u201ctag\u201d, type \u201cstring\u201d, with the text \u201cbig data\u201d. There is\nalso doc3, same structure, but its tag contains the text \u201csmall data\u201d.\n\nWith this small corpus, how can we find things?\n\nInstead of storing:\n\n    \n    \n    doc1={\"tag\": \"big data\"} doc2={\"tag\": \"big data\"} doc3={\"tag\": \"small data\"}\n\nWe can store the \u201cinverted index\u201d. What\u2019s that?\n\n    \n    \n    big=[doc1,doc2] data=[doc1,doc2,doc3] small=[doc3]\n\nAh, so it\u2019s not an index of documents to terms, it\u2019s an index of terms to\ndocuments. Clever. If we organize the data this way, we can find documents by\nvalue more quickly. When I search for \u201cbig\u201d, I get back doc1 and doc2. If I\nsearch for \u201csmall\u201d, I get back doc3. If I search for \u201cdata\u201d, I get back all\ndocuments. This is basically the core data structure in Lucene and in search\nin general. Yay for the inverted index!\n\nLucene spreads its index across several on-disk files, each format purpose-\nbuilt for a specific use case. These files are organized into logical\n\u201csegments\u201d that represents subsets of documents across the corpus.\n\n## Not in my vocabulary\n\nIn the above documents, I have 3 \u201cterms\u201d, and the assumption is that I\ngenerated them by doing basic whitespace tokenization. So, my original corpus\nhad the field values [\"big data\", \"small data\"], but my generated terms are\n[\"big\", \"small\", \"data\"].\n\nThis already suggests something interesting about terms. If information is\nrepeated in your field values, it will be compressed by pulling out the terms.\n\nBy the way, if I were to leave those fields unanalyzed, I\u2019d have two terms:\nthey\u2019d be \u201cbig data\u201d and \u201csmall data\u201d. If I decide not to analyze a field, but\nI decide to store it (in Lucene \u201cstored\u201d field or in Elasticsearch \u201c_source\u201d\nfield), then I am essentially storing the data twice. Once, in the inverted\nindex, and once in the \u201cfield storage\u201d (wherever that is), as well.\n\nTerms are interesting when you have data that repeats frequently among your\ndocuments. In this small example, the term \u201cdata\u201d is repeated in both\ndocuments, but only requires one entry in the inverted index. Imagine the same\nkind of corpus as above, but where you have 1,000 total documents, half tagged\nwith \u201cbig data\u201d and half tagged with \u201csmall data\u201d. In this case you might\nhave:\n\n    \n    \n    data=[1,2,3,...,1000] big=[1,3,5,7,9,...,999] small=[2,4,6,8,...,1000]\n\nHere, the inverted index stores one entry for \u201cdata\u201d, even though data appears\nin 1,000 documents. It stores one entry for \u201cbig\u201d, even though it occurs in\n500 documents. Likewise for \u201csmall\u201d.\n\n## Big data concordance\n\nYou might do a quick back-of-the-envelope calculation at how much more\nefficient it is to store an inverted index of this data than to store a normal\ndocument-to-term index.\n\nStoring the document IDs repeatedly isn\u2019t free, but it\u2019s certainly cheaper\nthan storing the whole document repeatedly. The vocabulary of a large data set\nwill tend to be much smaller than the record storage of that same data set,\nand we can take advantage of this at scale.\n\nBy the way, this is a pretty ancient technique of mining data. The first\ncomplete vocabulary of a complex text was constructed in the year 1262, by 500\nvery patient monks. The document in question was, of course, the Bible, and\nthe vocabulary was called a concordance. How does that proverb go? \u201cThere is\nnothing new under the sun.\u201d\n\nAn example biblical concordance, this one a bit more modern than the one the\nmonks may have worked on.\n\n## Discretely numerical\n\nHave a lot of text data you need to make sense of? You clearly have a \u201csearch\u201d\nproblem. Have a lot of numeric data you need to make sense of? Well, now, of\ncourse, you have an \u201canalytics\u201d problem. Different problem, right?\n\nWell, maybe. The benefits of the inverted index, terms, and vocabularies apply\nequally well to numeric data. It just requires some lateral thinking to get\nthere.\n\nThe reason fields need to have types is because the way we index field values\ninto terms can dramatically affect how we can query those fields. Text is not\nthe only thing that can be broken into terms \u2014 numeric and date field values\ncan, as well. This is a bit mind-bending, as terms feel like a text-only\nconcept.\n\nHere\u2019s a snippet from Lucene in Action on the topic: \u201cIf you indexed your\nfield with NumericField, you can efficiently search a particular range for\nthat field using NumericRangeQuery. Under the hood, Lucene translates the\nrequested range into the equivalent set of brackets in the indexed trie\nstructure.\u201d\n\nThe equivalent set of brackets in the indexed trie structure? Sounds fancy. To\nstart with, what trie structure are we talking about?\n\nAn example trie data structure storing numeric data.\n\nHere\u2019s an example. So, let\u2019s suppose I add a new field to my documents called\n\u201cviews\u201d. It is a numeric field that contains the number of views each document\nreceived on some website. The section above explained how we might find\ndocuments that have certain ranges of views, e.g. views between 50 and 100.\n\nIf I convert the \u201cviews\u201d field into terms, I\u2019ll have something that looks like\nthis, perhaps:\n\n    \n    \n    49=[doc31] 50=[doc40,doc41] 51=[doc53] ...\n\nThis isn\u2019t very helpful. To query for a range of views from 50 to 100, I\u2019d\nhave to construct 50-part query, one for each discrete term:\n\n    \n    \n    50 OR 51 OR 52 ... OR 100\n\nThe solution, as mentioned above, is a \u201ctrie structure of brackets\u201d. Lucene\nwill automatically generate terms that look more like this:\n\n    \n    \n    49=[doc31] 50=[doc40,doc41] 50x75=[doc40,doc41,doc53,doc78,doc99,...] 51=[doc53] ...\n\nNotice that 50x75 is a special term that encompasses a bracket of 25 discrete\nvalues, and thus points to a lot of documents. This allows for smaller queries\nto cover ranges, and a quicker retrieval of documents over large ranges. The\nidea is to reduce the discrete numeric data set to a number of lumpier \u201cterm\nranges\u201d. So now, we might be able to cover our 50-100 range with a query like\nthis:\n\n    \n    \n    50x75 OR 76x99 OR 100\n\nThe key thing is to select these term ranges automatically \u2014 and Lucene has an\nalgorithm for that which ensures that there are enough terms to cover all\nranges with good average speed.\n\nPretty magical, huh? Here\u2019s the other clever thing: because numeric values can\nbe converted to term ranges, this same magic works on dates, as well. The\ndates are converted to numbers, the numbers are then converted into term\nranges. Thus, even though you might be searching through 1 million \u201cminutes\u201d\nof data, you would only be searching through a few hundred \u201cminute ranges\u201d in\nthe inverted index. We could even call these \u201cminute ranges\u201d, well, \u201cdays\u201d!\n\nIf you need a more visual explanation of numeric range queries, check out this\nslideshow.\n\nThe UNIX philosophy introduced the abstraction that \u201ceverything is a file\u201d,\nand it certainly required some lateral thinking to make devices like printers\nand network sockets feel like files. The Lucene philosophy equivalent is,\n\u201ceverything is a term\u201d. Numbers, dates, text, identifiers, all can be mapped\nto behave like text terms, with all the same benefits.\n\n## Just uninvert what you\u2019ve inverted\n\nBut, we\u2019d still like to do something with the values within this field. For\nexample, we might like to aggregate up the total views across all of our\ndocuments, what in SQL might be a sum() aggregate. We might also want to find\nthe document with the most views, that is, to sort our documents by their\nnumber of views.\n\nTo do this, our inverted index is no help. We might have values ranging from 0\nto 100 million in there, with each discrete value (or synthetic range)\npointing to the right document IDs. We don\u2019t want to find the documents with\ncertain values; we want to instead calculate summaries (aka \u201canalytics\u201d) over\nour corpus or some subset thereof.\n\nA new problem demands another lateral thinking solution. Why don\u2019t we uninvert\nthe inverted index? Huh?\n\nIn other words, why don\u2019t we store, per field, an array of field values, in\ndocument order? An index, not of terms to document IDs, but an index of field\nvalues that we know (by their order) correspond to specific documents.\n\n    \n    \n    views=[1,1000,5000,1000000,200,...]\n\nWhen we need to do calculations across the whole corpus, we can slurp this\narray into memory (and, perhaps, keep it there for later). We can then run\ncalculations as fast as computationally possible. If you need to execute a\nsum() on some subset of this array, we can use another trick, Bitsets, for\nfiltering down the array as we go.\n\n## The quickest bit\n\nA quick detour. I first heard of Bitsets in one of my favorite programming\nbooks, an oldie but goodie passed down to me by my Dad. It\u2019s called\nProgramming Pearls.\n\nIts first problem, entitled \u201cCracking the Oyster\u201d, involves solving a specific\nfile sorting problem by representing the lines of the file as an array of\nbits, where each bit represents one of the possible line values. As described\nin that chapter, the Bitset is \u201ca dense set over a finite domain when each\nelement occurs at most once and no other data is associated with the element\u201d.\nThe author observes that programmers should seek cases where \u201creducing a\nprogram\u2019s space requirements also reduces its run time,\u201d something he refers\nto as \u201cmutual improvement\u201d. Properly applying a Bitset to a sorting problem is\none such example.\n\nAdmit it \u2014 part of the reason you went into software is because you thought\nworking in binary and doing bit arithmetic was pretty cool. After all, there\nare only 10 kinds of people in this world, those who understand binary, and\nthose who don\u2019t.\n\nLet\u2019s return to our uninverted index. Let\u2019s say that you want to only sum\nviews from documents that match a specific author. In this case, the full\narray of views will be compared against a Bitset that might look as follows:\n\n    \n    \n    views=[1,1000,5000,1000000,200,...] specific_author=[0,1,0,1,0,...] filtered_views=[0,1000,0,1000000,0,...]\n\nAs you can see, the views array was gated through the specific_author Bitset,\nand the result was an array of filtered_views. This might even be a sparse\narray, where most of the values are 0 and the only actual values come from\nmatching documents, but you don\u2019t need to worry about that because Lucene uses\na compressed Bitset that handles this case nicely. (See Roaring Bitmaps for\nthe most modern implementation.)\n\nRoaring Bitmaps, one modern and popular Bitset implementation, achieves\nstriking compression rates on typical data sets. From \u201cBetter bitmap\nperformance with Roaring bitmaps\u201d.\n\nIn any case, this can be done very efficiently in-memory and the result is a\nfiltered set of field values that matches what we need exactly. Now all we\nneed to do is sum those filtered values.\n\nThis makes it clear why it\u2019s valuable to have the uninverted index in-memory.\nSpeed. That\u2019s why it\u2019s often called the field cache.\n\nBut in-memory isn\u2019t an option for truly big data sets. This leads us to the\nfinal chapter.\n\n## The solution is obviously... flat files\n\nStoring every single field value in memory is fast, but prohibitive. Lucene\u2019s\n\u201cdoc values\u201d is basically a hack that takes advantage of Cassandra-style\n\u201ccolumnar\u201d data storage.\n\nWe store all the document values in a simple format on-disk. Basically, in\nflat files. Oh, the humanity.\n\nI know what you\u2019re thinking. Flat files, how pedestrian! But in this case, we\nbenefit from a few other lateral thoughts. Let\u2019s look back at our views array.\nRather than storing:\n\n    \n    \n    views=[1,1000,5000,1000000,200,...]\n\nWe now store the same kind of data in a file that basically just has the\nvalues splatted out in column-stride format:\n\n    \n    \n    1 1000 5000 1000000 200 ...\n\nWe can also be smart and only store binary representation so we can quickly\nslurp this data into arrays in memory. If the file format on-disk is aligned\nwith the document IDs in our corpus, then we achieve random access to any\nspecific document by seeking into this file. This is the same trick that all\ncolumnar, disk-backed key-value stores utilize, for the most part.\n\nWhen we need to perform a sum() on this data, we can simply do a\nstraightforward sequential read of the file. Though it won\u2019t put all the data\nin memory, this scan will signal to the Linux kernel that the disk data is\nhot, and Linux will start caching.\n\nTo quote a kernel developer, \u201cwhen you read a 100-megabyte file twice, once\nafter the other, the second access will be quicker, because the file blocks\ncome directly from the page cache in memory and do not have to be read from\nthe hard disk again.\u201d Flat files, for the win!\n\nOf course, even if the whole file doesn\u2019t fit into memory, we can still\nsmartly load large chunks, and know exactly what document range our field\nvalues correspond to, thanks to the strict order. This can let us re-use the\nBitsets from the earlier section to filter these subsets appropriately.\n\n## Lucene: Nice index, OK database\n\nLucene is not a database \u2014 as I mentioned earlier, it\u2019s just a Java library.\nIt\u2019s coming from the world of information retrieval, which cares about finding\nand describing data, not the world of database management, which cares about\nkeeping it.\n\nThat said, Lucene is an excellent building block for high-performance indices\nof your data. Solr and Elasticsearch are essentially wrappers on Lucene that\nuse its good parts for information retrieval, and then try to build their own\nlayer atop for persistence. Solr takes advantage of Lucene\u2019s built-in \u201cfield\nstorage\u201d for this, while Elasticsearch stores JSON blobs inside a Lucene\nfield, called \u201c_source\u201d.\n\nLucene goes even deeper than that, though: using Lucene\u2019s API, you can build\nyour own index format (see its Codecs API). Since Lucene\u2019s data model is so\nflexible, when you squint, systems built with Lucene often look like \u201cNoSQL\u201d\ndatabases themselves.\n\nWith the rise of NoSQL, I\u2019ve noticed another trend: NIH. No, no, not that NIH\n\u2014 I\u2019m talking about Not Indexed Here. The rise of MongoDB and Cassandra has\nalso led developers to \u201croll their own index\u201d, mainly out of necessity.\n\nFor example, Cassandra encourages you to \u201cdetermine exactly what queries you\nneed to support\u201d, and then store your data in a way to support those queries.\nCassandra only really has one index \u2014 the partition index. So, you have to map\nall your problems into that one query pattern. Bummer.\n\nBefore MongoDB added full text search to its core, it encouraged developers to\n\u201cuse keywords stored in an array in the same document as the text field\u201d. Same\ndeal. An indexed keyword array lets you leverage MongoDB\u2019s one-trick pony, the\nBTree index. What\u2019s worse, the actual implementation of their built-in full-\ntext search support doesn\u2019t introduce any new indexing techniques. It just\ntakes care of generating that keywords array for you, and then stuffing it in\nthe BTree.\n\nOK, I get it \u2014 every distributed database has to re-invent its approach to a\nconsistent hash ring for your data. But, do they have to re-invent indexing,\ntoo?\n\nIn both of these cases, and in many others, you\u2019d be better off using a Lucene\nindex on your data. Invert your thinking, invert your index. Store your data\nwhere you wish, but then build a corpus of Lucene documents with fields\ncorresponding to the data you actually need to find. Anything you put in a\nfield will be indexed and queryable in ad-hoc ways. You just need to come to\nterms with your terms. But, as we\u2019ve learned, anything can be a term. Convert\nthose into a vocabulary you can actually understand. Then defy comprehension\nby converting it all into compressed Bitsets. Impress your friends once more\nby uninverting your inversion. When your sysadmin complains of memory usage,\nreveal that you\u2019ve rebuilt the fancy database using none other than flat\nfiles. Marvel at how well your OS optimizes for them.\n\nThen, query your Lucene index with pride \u2014 a decade-old technology, built on a\ncentury of computer science research, and a millennium of monk-like wisdom.\n\nIn other words, cutting-edge stuff.\n\nInterested in Lucene, next-generation databases, and storing lots of data at\nscale? Especially if you\u2019re a Python programmer who likes remote teams, you\nshould look at Parse.ly \u2014 we\u2019re hiring!\n\nThank you to Sal Gionfriddo, Didier Deshommes, and Otis Gospodneti\u0107 for\nreviewing an earlier draft of this post.\n\nPlatform\n\n  * Content Analytics Dashboard\n  * Content Conversion Engine\n  * Content API for Developers\n  * Data Pipeline for Analysts\n  * Enterprise Trusted Data Partner\n  * Pricing\n  * Data Privacy\n\nResources\n\n  * Blog\n  * Content Library\n  * Customers\n  * Documentation\n  * Glossary\n  * Changelog\n  * Partners\n\nAbout Parse.ly\n\n  * About\n  * Careers\n  * Contact\n  * Status\n  * Homepage\n\nContent analytics for everyone.\n\n  * LinkedIn\n  * Twitter\n  * GitHub\n\n\u00a9 All Rights Reserved\n\nSite Terms\n\nProduct Terms\n\nPrivacy Policy\n\nDo not sell or share my personal information.\n\nPrivacy Notice for California Users\n\nAn\n\nCreation\n\nAs an open source company, we take your privacy seriously and want to be as\ntransparent as possible. So: We use cookies to collect some personal data from\nyou (like your browsing data, IP addresses, and other unique identifiers).\nSome of these cookies we absolutely need in order to make things work, and\nothers you can choose in order to optimize your experience while using our\nsite and services.\n\n", "frontpage": false}
