{"aid": "40055269", "title": "Bloomberg's analysis didn't show that ChatGPT is racist", "url": "https://interviewing.io/blog/refuting-bloombergs-analysis-chatgpt-isnt-racist", "domain": "interviewing.io", "votes": 38, "user": "leeny", "posted_at": "2024-04-16 18:08:10", "comments": 24, "source_title": "Refuting Bloomberg's analysis: ChatGPT isn't racist. But it is bad at recruiting.", "source_text": "Refuting Bloomberg's analysis: ChatGPT isn't racist. But it is bad at\nrecruiting.\n\ninterviewing.io\n\n  * For employers\n  * Gift mock interviews\n  * Blog\n  * FAQ\n  * Log in\n\n# Refuting Bloomberg's analysis: ChatGPT isn't racist. But it is bad at\nrecruiting.\n\nBy Aline Lerner | Published: April 11, 2024\n\nNote: We\u2019ve reached out to Bloomberg, asking them to share their data and\nclarify their findings, ahead of publishing this piece. If it turns out that\nthey used a different method for statistical significance testing or if we\nmissed something, we\u2019ll gladly retract the part of this post that\u2019s about\ntheir results.\n\nRecently, Bloomberg published an article called \u201cOpenAI\u2019s GPT is a recruiter\u2019s\ndream tool. Tests show there\u2019s racial bias.\u201d In this piece, the Bloomberg team\nran a clever test where they had ChatGPT review nearly identical resumes with\njust the names changed to include typically Black, White, Asian, and Hispanic\nnames. Their analysis uncovered racial bias.\n\nBloomberg had published their numbers on GitHub, so we were able to check\ntheir work. When we re-ran the numbers, we saw that they hadn\u2019t done\nstatistical significance testing and that there was, in fact, no racial bias.\nHowever, when we ran our own tests, we discovered that ChatGPT is indeed bad\nat judging resumes. It\u2019s not bad because it\u2019s racist. It\u2019s bad because it\u2019s\nprone to a different kind of bias, the same kind of bias as human recruiters \u2014\nover-indexing on candidates\u2019 pedigrees: whether they\u2019ve worked at a top\ncompany and/or whether they attended a top school. Pedigree can be somewhat\npredictive (especially where people worked), but ChatGPT is significantly\noverestimating its importance and doing a disservice to candidates from non-\ntraditional backgrounds as a result.\n\n## The Bloomberg study\n\nHere\u2019s what the team at Bloomberg did (taken verbatim from their piece):\n\n> We used demographically-distinct names as proxies for race and gender, a\n> common practice used to audit algorithms... Altogether we produced 800\n> demographically-distinct names: 100 names each for males and females who are\n> either Black, White, Hispanic or Asian...\n>\n> To test for name-based discrimination, Bloomberg prompted OpenAI\u2019s GPT-3.5\n> and GPT-4 to rank resumes for a real job description for four different\n> roles from Fortune 500 companies: HR specialist, software engineer, retail\n> manager and financial analyst.\n>\n> For each role, we generated eight nearly-identical resumes using GPT-4. The\n> resumes were edited to have the same educational background, years of\n> experience, and last job title. We removed years of education, as well as\n> any objectives or personal statements.\n>\n> We then randomly assigned a distinct name from each of the eight demographic\n> groups [Black, White, Hispanic, Asian, and men and women for each] to each\n> of the resumes.\n>\n> Next, we shuffled the order of resumes, to account for order effects, and\n> asked GPT to rank the candidates.\n\nThe authors reported that ChatGPT shows racial bias across all groups, except\nfor retail managers ranked by GPT-4.\n\nMore specifically:\n\n> [We] found that resumes labeled with names distinct to Black Americans were\n> the least likely to be ranked as the top candidates for financial analyst\n> and software engineer roles. Those with names distinct to Black women were\n> top-ranked for a software engineering role only 11% of the time by GPT \u2014 36%\n> less frequently than the best-performing group.\n>\n> The analysis also found that GPT\u2019s gender and racial preferences differed\n> depending on the particular job that a candidate was evaluated for. GPT does\n> not consistently disfavor any one group, but will pick winners and losers\n> depending on the context. For example, GPT seldom ranked names associated\n> with men as the top candidate for HR and retail positions, two professions\n> historically dominated by women. GPT was nearly twice as likely to rank\n> names distinct to Hispanic women as the top candidate for an HR role\n> compared to each set of resumes with names distinct to men. Bloomberg also\n> found clear preferences when running tests with the less-widely used GPT-4 \u2014\n> OpenAI\u2019s newer model that the company has promoted as less biased.\n\nThe team also, commendably, published their results on GitHub, so we tried our\nhand at reproducing them. What we found was starkly different from what\nBloomberg reported.\n\nBefore we get into what we found, here\u2019s what we did.\n\nIn their results, Bloomberg published the rates at which both GPT-3.5 and\nGPT-4 chose each demographic as the top candidate. The Bloomberg analysts ran\na lot of trials for each job: ChatGPT was asked 1,000 times to rank 8 resumes\nfor the HR specialist job, for example. And if ChatGPT had gender or racial\nbias, each group should in general be the top pick 125 times (or 12.5% of the\ntime, given that there were 1000 data points).\n\nWhere are we going with this? Stats nerds might have noticed a glaring\nomission from the Bloomberg piece: statistical significance testing and\np-values. Why is that important? Even with 1,000 trials, a perfectly unbiased\nresume sorter would not give you exactly equal proportions, picking each group\nprecisely 125 times. Instead, purely random variation could mean that one\ngroup is picked 112 times and another is picked 128 times, without there\nactually being any bias. Therefore, you need to run some tests to see if the\nresults you got were by chance or because there truly is a pattern of some\nkind. Once you run the test, the p-value tells you the probability that a\ncertain set of selection rates are consistent with chance, and in this case,\nconsistent with a random (and, therefore, unbiased) sorting of resumes.\n\nWe calculated the p-values for each group^1. What we found was starkly\ndifferent from what Bloomberg reported.\n\n## Where the Bloomberg study went wrong\n\nGiven the nature of our business, we looked at software engineers first. Here\nare the results of Bloomberg having run software engineering resumes through\nGPT-4 for all 8 groups (the column titled \u201cobsfreq\u201d) as well as our calculated\np-value.\n\nA_M = Asian man, A_W = Asian woman, etc. 12.5% would be the expected rate at\nwhich a candidate from each group would be the top choice (\u201cexpperc\u201d). Out of\n1000 candidates, that\u2019s 125 each (\u201cexpfreq\u201d). Finally, \u201cobsfreq\u201d is the\nobserved frequency of the top candidate being from each group, taken from\nBloomberg\u2019s results.\n\nIt\u2019s convention that you want your p-value to be less than 0.05 to declare\nsomething statistically significant \u2013 in this case, that would mean less than\n5% chance that the results were due to randomness. This p-value of 0.2442 is\nway higher than that. As it happened, we couldn\u2019t reproduce statistical\nsignificance for software engineers when using GPT-3.5 either. Using\nBloomberg\u2019s numbers, ChatGPT does NOT appear to have a racial bias when it\ncomes to judging software engineers\u2019 resumes.^2 The results appear to be more\nnoise than signal.\n\nWe then re-ran the numbers for the eight race/gender combinations, using the\nsame method as above. In the table below, you can see the results. TRUE means\nthat there is a racial bias. FALSE obviously means that there isn't. We also\nshared our calculated p-values. The TL;DR is that GPT-3.5 DOES show racial\nbias for both HR specialists and financial analysts but NOT for software\nengineers or retail managers. Most importantly, GPT-4 does not show racial\nbias for ANY of the race/gender combinations.^3\n\nOccupation| GPT 3.5 (Statistically significant? \u2016 p-value)| GPT 4\n(Statistically significant? \u2016 p-value)  \n---|---|---  \nFinancial analyst| TRUE \u2016 0.0000| FALSE \u2016 0.2034  \nSoftware engineer| FALSE \u2016 0.4736| FALSE \u2016 0.1658  \nHR specialist| TRUE \u2016 0.0000| FALSE (but it\u2019s close) \u2016 0.0617  \nRetail manager| FALSE \u2016 0.2229| FALSE \u2016 0.6654  \n  \nThat\u2019s great, right? Well, not so fast. Before we deemed ChatGPT as competent\nat judging resumes, we wanted to run a test of our own, specifically for\nsoftware engineers (because, again, that\u2019s our area of expertise). The results\nof this test were not encouraging.\n\n## How we tested ChatGPT\n\ninterviewing.io is an anonymous mock interview platform, where our users get\npaired with senior/staff/principal-level FAANG engineers for interview\npractice. We also connect top performers with top-tier companies, regardless\nof how they look on paper. In our lifetime, we\u2019ve hosted >100k technical\ninterviews, split between the aforementioned mock interviews and real ones. In\nother words, we have a bunch of useful, indicative historical performance data\nabout software engineers.^4 So we decided to use that data as a sanity check.\n\n### The setup\n\nWe asked ChatGPT (GPT4, specifically gpt-4-0125-preview) to grade several\nthousand LinkedIn profiles belonging to people who have practiced on\ninterviewing.io before. For each profile, we asked ChatGPT to give the person\na coding score between 1 and 10, where someone with a 10 would be a top 10%\ncoder. In order to improve the quality of the response, we asked it to first\ngive its reasoning followed by the coding score.\n\nWe want to be very clear here that we did not share any performance data with\nChatGPT or share with ChatGPT any information about our users \u2014 we just asked\nit to make value judgments about publicly available LinkedIn profiles. Then we\ncompared those value judgments with our data on our end.\n\n## How ChatGPT performed\n\nThere is a correlation between what ChatGPT says and how the coder performed\non a real technical screen. The tool performs better than a random guess...\nbut not by much. To put these results in perspective, overall, 47% of coders\npass the screen. The ChatGPT score can split them into two groups: one that\nhas a 45% chance and one with a 50% chance. So it gives you a bit more\ninformation on whether someone will succeed, but not much.\n\nBelow are two more granular ways of looking at ChatGPT\u2019s performance. The\nfirst is a modified calibration plot, and the second is a ROC curve.\n\n### Calibration plot\n\nIn this plot, we take each predicted probability from ChatGPT (e.g., 0.4112)\nand assign it to one of 10 equally-spaced deciles. Decile 1 is the 10% of\nprofiles with the lowest probability. Decile 10 is the 10% of people with the\nhighest probability.\n\nThen, for each decile, we plot the actual probability of those candidates\nperforming well in interviews (i.e., what portion of them actually passed\ninterviews on interviewing.io). As you can see, the plot is something of a\nmess \u2014 for all deciles that ChatGPT came up with, those candidates actually\npassed about half the time. The ideal plot (\u201cAn excellent model\u201d) would have a\nmuch steeper slope, with way fewer passing in the bottom decile than the top\ndecile.\n\nWe asked GPT-4 to judge several thousand LinkedIn profiles belonging to people\nwho have practiced on interviewing.io before. Then we split up its predictions\ninto deciles \u2014 10% buckets and compared to how those users actually performed.\nA great model would have bad performance in the first decile and then improve\nsharply and steadily.\n\n### ROC curve\n\nAnother way to judge ChatGPT\u2019s performance at this task is to look at a ROC\ncurve. This curve graphs the true positive rate of a model against the false\npositive rate. It\u2019s a standard way of judging how accurate an ML model is\nbecause it lets the viewer see how it performs at different acceptable false\npositive rates \u2014 for cancer diagnostics, you may be OK with a very high false\npositive rate, for instance. For eng recruiting, you likely will not be!\n\nRelated to ROC curves is the AUC, or the area under the curve. A perfect model\nwould have a 100% true positive rate for every possible false positive rate,\nand so the area under the curve would be 1. A model that\u2019s basically the same\nas guessing would have a true positive rate that equals the false positive\nrate (AUC = 0.5). With that in mind, here\u2019s the ROC curve and the AUC for\nChatGPT judging resumes \u2014 with an overall AUC of about 0.55, it\u2019s only barely\nbetter than random guesses.\n\nWe asked GPT-4 to judge several thousand LinkedIn profiles belonging to people\nwho have practiced on interviewing.io before. It performed barely better than\nguessing.\n\nSo, no matter how you measure it, while ChatGPT does not appear to have racial\nbias when judging engineers\u2019 profiles, it\u2019s not particularly good at the task\neither.\n\n## ChatGPT is biased against non-traditional candidates\n\nWhy did ChatGPT perform poorly on this task? Perhaps it\u2019s because there may\nnot be that much signal in a resume in the first place. But there\u2019s another\npossible explanation as well.\n\nYears ago, I ran an experiment where I anonymized a bunch of resumes and had\nrecruiters try to guess which candidates were the good ones. They did terribly\nat this task, about as well as random guessing. Not surprisingly, they tended\nto over-index on resumes that had top companies or prestigious schools on\nthem. In my data set of candidates, I happened to have a lot of non-\ntraditional, good candidates \u2014 people who were strong engineers but didn\u2019t\nattend a highly ranked school or work at a top company. That threw recruiters\nfor a loop.\n\nIt looks like the same thing happened to ChatGPT, at least in part. We went\nback and looked at how ChatGPT treated candidates with top-tier schools on\ntheir LinkedIn profiles vs. those without. It turns out that ChatGPT\nconsistently overestimates the passing rate of engineers with top schools and\ntop companies on their resumes. We also saw that ChatGPT consistently\nunderestimates the performance of candidates without those elite \u201ccredentials\u201d\non their resumes. Both of these differences are statistically significant. In\nthe graph below, you can see exactly how much ChatGPT overestimates and\nunderestimates in each case.\n\nTo ChatGPT\u2019s credit, we did not find the same bias when it came to top\ncompanies, which is funny because, in our experience, having worked at a top\ncompany carries some predictive signal, whereas where someone went to school\ndoes not carry much.\n\n## ChatGPT isn't racist, but its biases still make it bad at recruiting\n\nIn recruiting, we often talk about unconscious bias. Though it\u2019s no longer en\nvogue, companies have historically spent tens of thousands of dollars on\nunconscious bias trainings designed to stop recruiters from making decisions\nbased on candidates\u2019 gender and race. At the same time, recruiters were\ntrained to exhibit a different, conscious bias: to actively select candidates\nfrom elite schools and top companies.\n\nThe same conscious bias against candidates who didn\u2019t attend a top-tier school\nappears to be codified in ChatGPT.\n\nThat decision is rational \u2014 in the absence of a better signal, you have to use\nproxies, and those proxies seem as good as any. Unfortunately, as you can see\nfrom these results (and from other studies we\u2019ve done in the past; see the\nfootnote for the full list^5), it\u2019s not particularly accurate... and it\u2019s\ndefinitely not accurate enough to codify into our AI tools.\n\nIn a market where recruiter jobs are tenuous, where the dwindling number of\nrecruiters is dealing with more applicants than before and are pressured, more\nthan ever, to make the aforementioned rapid decisions, and where companies are\nembracing AI as a tempting and productive cost-cutting measure^6, we\u2019re in\nrather dangerous territory.\n\nA few months ago, we published a long piece called, \u201cWhy AI can\u2019t do hiring\u201d.\nThe main two points of the piece were that 1) it\u2019s hard to extract signal from\na resume because there\u2019s not much there in the first place, and 2) even if you\ncould, you\u2019d need proprietary performance data to train an AI \u2014 without that\ndata, you\u2019re doing glorified keyword matching.\n\nUnfortunately, most, if not all, of the AI tools and systems that claim to\nhelp recruiters make better decisions do not have this kind of data and are 1)\neither built on top of GPT (or one of its analogs) without fine-tuning or 2)\nare glorified keyword matchers masquerading as AI, or both.\n\nThough human recruiters aren\u2019t particularly good at judging resumes, and\nthough we, as a society, don\u2019t yet have a great solution to the problem of\neffective candidate filtering, it\u2019s clear that off-the-shelf AI solutions are\nnot the magic pill we\u2019re looking for \u2014 they\u2019re flawed in the same ways as\nhumans. They just do the wrong thing faster and at scale.\n\nFootnotes:\n\n## Footnotes\n\n  1. We used a Chi-squared goodness of fit test, a type of statistical significance test that you use for discrete data, like yes or no votes on resumes. \u21a9\n\n  2. Another way to see this is to simulate the same process with a perfectly unbiased resume sorter, i.e., a bot that picks from the 8 resumes at random. If you run 1,000 imaginary versions of the Bloomberg experiment, it\u2019s pretty common for a particular group to have a round where they\u2019re the top pick just 11% of the time. This distribution is in the histogram below. This is another way of saying what the p-values are saying: the disparities are consistent with random chance. [INSERT HISTOGRAM] \u21a9\n\n  3. A caveat is that these tests might show evidence of bias if the sample size were increased to, say, 10,000 rather than 1,000. That is, with a larger sample size, the p-value might show that ChatGPT is indeed more biased than random chance. The thing is, we just don\u2019t know from their analysis, and it certainly rules out extreme bias. In fact, the most recent large-scale resume audit study found that resumes with distinctively Black names were 2.1 percentage points less likely to get callbacks from human recruiters. Based on Bloomberg\u2019s data, ChatGPT\u2019s was less biased against resumes from Black candidates than human recruiters \u2014 according to our calculations, in the Bloomberg data set, there was 1.5 percentage point drop. \u21a9\n\n  4. Mock interview performance, especially in aggregate, is very predictive of real interview performance. We have data from both mock and real interviews, spanning ~6 years, and the candidates who\u2019ve done well in mock interviews on our platform have consistently been 3X more likely to pass real interviews. \u21a9\n\n  5. Here is a list of our past studies that show how top schools are not particularly predictive and top companies are only somewhat predictive:\n\n     * Lessons from a year\u2019s worth of hiring data\n     * We looked at how a thousand college students performed in technical interviews to see if where they went to school mattered. It didn't.\n     * Lessons from 3,000 technical interviews... or how what you do after graduation matters way more than where you went to school\n\n\u21a9\n\n  6. A significant portion of employers, ranging from 35-55% depending on the source (Zippia, Forbes, USC), are currently using AI to screen job candidates. The adoption of AI in hiring appears to be particularly high among large enterprises and recruiting firms. Given that large enterprises see a disproportionately high volume of candidates, the % of candidates who are screened by AI is likely much higher than the 35-55% number. \u21a9\n\nLife is chaos and pain. Interview prep doesn't have to be.\n\nGet instant access to anonymous mock interviews, salary negotiation, and the\nworld's largest library of interview replays.\n\n#### Related posts\n\nThe Eng Hiring Bar: What the hell is it?\n\nLessons from a year\u2019s worth of hiring data\n\nNo engineer has ever sued a company because of constructive post-interview\nfeedback. So why don\u2019t employers do it?\n\nTechnical interview performance is kind of arbitrary. Here\u2019s the data.\n\n#### Stuff we write about\n\nRecessionSalary negotiationCompany NewsData Deep DivesDiversityGuest\nPostsHiring is brokenInterview tipsFor employers, how to hire better\n\n### Have interviews coming up? Study up on common questions and topics.\n\nMEDIUM\n\nData Structures and Algorithms\n\n### LRU Cache\n\nImplement an LRU Cache LRU = Least recently used cache\n\nMEDIUM\n\nData Structures and Algorithms\n\n### Most Frequent Element in an Array\n\nGiven an array of integers, find the most frequent element in the array. Write\na method that takes an array of integers and returns an integer. If there is a\ntie, you can just return any.\n\nMEDIUM\n\nMathematics\n\n### Reverse Integer\n\nGiven a 32-bit signed integer, reverse digits of the integer.\n\nPriority Queues\n\nQuestions & tips\n\nArrays\n\nQuestions & tips\n\nMemoization\n\nQuestions & tips\n\nUnion Find\n\nQuestions & tips\n\nBuckets\n\nQuestions & tips\n\nBinary Search\n\nQuestions & tips\n\n## We know exactly what to do and say to get the company, title, and salary\nyou want.\n\nInterview prep and job hunting are chaos and pain. We can help. Really.\n\ninterviewing.io\n\nInterview Replays\n\nSystem design mock interviewGoogle mock interviewJava mock interviewPython\nmock interviewMicrosoft mock interview\n\nInterview Questions by Language/Company\n\nJava interview questionsPython interview questionsJavaScript interview\nquestionsAmazon interview questionsGoogle interview questionsMeta interview\nquestionsApple interview questionsNetflix interview questionsMicrosoft\ninterview questions\n\nPopular Interview Questions\n\nReverse stringLongest substring without repeating charactersLongest common\nsubsequenceContainer with most waterReverse linked listK closest points to\noriginKth smallest elementReverse words in a string\n\nGuides\n\nAmazon Leadership PrinciplesSystem Design Interview GuideFAANG Hiring Process\nGuide\n\nCompany\n\nFor engineersFor employersBlogPressFAQLog in\n\n\u00a92024 Interviewing.io Inc. Made with <3 in San Francisco.\n\nPrivacy PolicyTerms of Service\n\n", "frontpage": true}
