{"aid": "39979649", "title": "TinyTimeMixer: Open-source time series LLM by IBM", "url": "https://huggingface.co/ibm/TTM", "domain": "huggingface.co", "votes": 2, "user": "milliondreams", "posted_at": "2024-04-09 14:13:14", "comments": 0, "source_title": "ibm/TTM \u00b7 Hugging Face", "source_text": "ibm/TTM \u00b7 Hugging Face\n\nHugging Face\n\n#\n\nibm\n\n/\n\nTTM\n\nTransformers Safetensors tinytimemixer Inference Endpoints\n\nModel card Files Files and versions Community\n\nEdit model card\n\n# TinyTimeMixer (TTM) Model Card\n\nTinyTimeMixers (TTMs) are compact pre-trained models for Multivariate Time-\nSeries Forecasting, open-sourced by IBM Research. With less than 1 Million\nparameters, TTM introduces the notion of the first-ever \u201ctiny\u201d pre-trained\nmodels for Time-Series Forecasting.\n\nTTM outperforms several popular benchmarks demanding billions of parameters in\nzero-shot and few-shot forecasting. TTM is pre-trained on diverse public time-\nseries datasets which can be easily fine-tuned on your multi-variate target\ndata. Refer to our paper for more details.\n\nThe current open-source version supports point forecasting use-cases ranging\nfrom minutely to hourly resolutions (Ex. 10 min, 15 min, 1 hour, etc.)\n\nNote that zeroshot, fine-tuning and inference tasks using TTM can easily be\nexecuted in 1 GPU machine or in laptops too!!\n\n## Benchmark Highlights:\n\n  * TTM (with less than 1 Million parameters) outperforms the following popular Pre-trained SOTAs demanding several hundred Million to Billions of parameters paper:\n\n    * GPT4TS (NeurIPS 23) by 7-12% in few-shot forecasting.\n    * LLMTime (NeurIPS 23) by 24% in zero-shot forecasting.\n    * SimMTM (NeurIPS 23) by 17% in few-shot forecasting.\n    * Time-LLM (ICLR 24) by 2-8% in few-shot forecasting\n    * UniTime (WWW 24) by 27% in zero-shot forecasting.\n  * Zero-shot results of TTM surpass the few-shot results of many popular SOTA approaches including PatchTST (ICLR 23), PatchTSMixer (KDD 23), TimesNet (ICLR 23), DLinear (AAAI 23) and FEDFormer (ICML 22).\n  * TTM (1024-96, released in this model card with 1M parameters) outperforms pre-trained MOIRAI-Small (14M parameters) by 10%, MOIRAI-Base (91M parameters) by 2% and MOIRAI-Large (311M parameters) by 3% on zero-shot forecasting (horizon = 96). [notebook]\n  * TTM quick fine-tuning also outperforms the competitive statistical baselines (Statistical ensemble and S-Naive) in M4-hourly dataset which existing pretrained TS models are finding difficult to outperform. [notebook]\n  * TTM takes only a few seconds for zeroshot/inference and a few minutes for finetuning in 1 GPU machine, as opposed to long timing-requirements and heavy computing infra needs of other existing pre-trained models.\n\n## Model Description\n\nTTM falls under the category of \u201cfocused pre-trained models\u201d, wherein each\npre-trained TTM is tailored for a particular forecasting setting (governed by\nthe context length and forecast length). Instead of building one massive model\nsupporting all forecasting settings, we opt for the approach of constructing\nsmaller pre-trained models, each focusing on a specific forecasting setting,\nthereby yielding more accurate results. Furthermore, this approach ensures\nthat our models remain extremely small and exceptionally fast, facilitating\neasy deployment without demanding a ton of resources.\n\nHence, in this model card, we plan to release several pre-trained TTMs that\ncan cater to many common forecasting settings in practice. Additionally, we\nhave released our source code along with our pretraining scripts that users\ncan utilize to pretrain models on their own. Pretraining TTMs is very easy and\nfast, taking only 3-6 hours using 6 A100 GPUs, as opposed to several days or\nweeks in traditional approaches.\n\nEach pre-trained model will be released in a different branch name in this\nmodel card. Kindly access the required model using our getting started\nnotebook mentioning the branch name.\n\n## Model Releases (along with the branch name where the models are stored):\n\n  * 512-96: Given the last 512 time-points (i.e. context length), this model can forecast up to next 96 time-points (i.e. forecast length) in future. Recommended for hourly and minutely resolutions (Ex. 10 min, 15 min, 1 hour, etc) (branch name: main)\n\n  * 1024-96: Given the last 1024 time-points (i.e. context length), this model can forecast up to next 96 time-points (i.e. forecast length) in future. Recommended for hourly and minutely resolutions (Ex. 10 min, 15 min, 1 hour, etc) (branch name: 1024-96-v1)\n\n  * Stay tuned for more models !\n\n## Model Details\n\nFor more details on TTM architecture and benchmarks, refer to our paper.\n\nTTM-1 currently supports 2 modes:\n\n  * Zeroshot forecasting: Directly apply the pre-trained model on your target data to get an initial forecast (with no training).\n\n  * Finetuned forecasting: Finetune the pre-trained model with a subset of your target data to further improve the forecast.\n\nSince, TTM models are extremely small and fast, it is practically very easy to\nfinetune the model with your available target data in few minutes to get more\naccurate forecasts.\n\nThe current release supports multivariate forecasting via both channel\nindependence and channel-mixing approaches. Decoder Channel-Mixing can be\nenabled during fine-tuning for capturing strong channel-correlation patterns\nacross time-series variates, a critical capability lacking in existing\ncounterparts.\n\nIn addition, TTM also supports exogenous infusion and categorical data which\nis not released as part of this version. Stay tuned for these extended\nfeatures.\n\n## Recommended Use\n\n  1. Users have to externally standard scale their data indepedently for every channel before feeding it to the model (Refer to TSP, our data processing utility for data scaling.)\n  2. Enabling any upsampling or prepending zeros to virtually increase the context length for shorter length datasets is not recommended and will impact the model performance.\n\n### Model Sources\n\n  * Repository: https://github.com/IBM/tsfm/tree/main/tsfm_public/models/tinytimemixer\n  * Paper: https://arxiv.org/pdf/2401.03955.pdf\n\n## Uses\n\n    \n    \n    # Load Model from HF Model Hub mentioning the branch name in revision field model = TinyTimeMixerForPrediction.from_pretrained( \"https://huggingface.co/ibm/TTM\", revision=\"main\" ) # Do zeroshot zeroshot_trainer = Trainer( model=model, args=zeroshot_forecast_args, ) ) zeroshot_output = zeroshot_trainer.evaluate(dset_test) # Freeze backbone and enable few-shot or finetuning: # freeze backbone for param in model.backbone.parameters(): param.requires_grad = False finetune_forecast_trainer = Trainer( model=model, args=finetune_forecast_args, train_dataset=dset_train, eval_dataset=dset_val, callbacks=[early_stopping_callback, tracking_callback], optimizers=(optimizer, scheduler), ) finetune_forecast_trainer.train() fewshot_output = finetune_forecast_trainer.evaluate(dset_test)\n\n## How to Get Started with the Model\n\n  * Getting Started Notebook\n  * 512-96 Benchmarks\n  * 1024-96 Benchmarks\n  * Script for Finetuning with cross-channel correlation support - to be added soon\n\n## Training Data\n\nThe TTM models were trained on a collection of datasets from the Monash Time\nSeries Forecasting repository. The datasets used include:\n\n  * Australian Electricity Demand: https://zenodo.org/records/4659727\n  * Australian Weather: https://zenodo.org/records/4654822\n  * Bitcoin dataset: https://zenodo.org/records/5122101\n  * KDD Cup 2018 dataset: https://zenodo.org/records/4656756\n  * London Smart Meters: https://zenodo.org/records/4656091\n  * Saugeen River Flow: https://zenodo.org/records/4656058\n  * Solar Power: https://zenodo.org/records/4656027\n  * Sunspots: https://zenodo.org/records/4654722\n  * Solar: https://zenodo.org/records/4656144\n  * US Births: https://zenodo.org/records/4656049\n  * Wind Farms Production data: https://zenodo.org/records/4654858\n  * Wind Power: https://zenodo.org/records/4656032\n\n## Citation [optional]\n\nKindly cite the following paper, if you intend to use our model or its\nassociated architectures/approaches in your work\n\nBibTeX:\n\n    \n    \n    @article{ekambaram2024ttms, title={TTMs: Fast Multi-level Tiny Time Mixers for Improved Zero-shot and Few-shot Forecasting of Multivariate Time Series}, author={Ekambaram, Vijay and Jati, Arindam and Nguyen, Nam H and Dayama, Pankaj and Reddy, Chandra and Gifford, Wesley M and Kalagnanam, Jayant}, journal={arXiv preprint arXiv:2401.03955}, year={2024} }\n\nAPA:\n\nEkambaram, V., Jati, A., Nguyen, N. H., Dayama, P., Reddy, C., Gifford, W. M.,\n& Kalagnanam, J. (2024). TTMs: Fast Multi-level Tiny Time Mixers for Improved\nZero-shot and Few-shot Forecasting of Multivariate Time Series. arXiv preprint\narXiv:2401.03955.\n\n## Model Card Authors\n\nVijay Ekambaram, Arindam Jati, Pankaj Dayama, Nam H. Nguyen, Wesley Gifford\nand Jayant Kalagnanam\n\n## IBM Public Repository Disclosure:\n\nAll content in this repository including code has been provided by IBM under\nthe associated open source software license and IBM is under no obligation to\nprovide enhancements, updates, or support. IBM developers produced this code\nas an open source project (not as an IBM product), and IBM makes no assertions\nas to the level of quality nor security, and will not be maintaining this code\ngoing forward.\n\nDownloads last month\n\n    484\n\nSafetensors\n\nModel size\n\n805k params\n\nTensor type\n\nF32\n\n\u00b7\n\nUnable to determine this model\u2019s pipeline type. Check the docs .\n\n", "frontpage": false}
