{"aid": "40042815", "title": "How to manage a quantum computing emergency", "url": "https://educatedguesswork.org/posts/pq-emergency/", "domain": "educatedguesswork.org", "votes": 4, "user": "yuedongze", "posted_at": "2024-04-15 16:39:27", "comments": 0, "source_title": "How to manage a quantum computing emergency", "source_text": "How to manage a quantum computing emergency\n\n# Educated Guesswork\n\n  * Archive\n  * About Me\n  * Contact\n  * Follow on Twitter\n  * Subscribe\n\n# How to manage a quantum computing emergency\n\nYou go to war with the algorithms you have, not the ones you wish you had\n\nPosted by ekr on 15 Apr 2024\n\nIllustration by Kate Hudson with MidJourney and Photoshop AI.\n\nRecently, I wrote about how the Internet community is working towards post-\nquantum algorithms in case someone develops a cryptographically relevant\nquantum computer (CRQC). That's still what everyone is hoping for, but nobody\nreally know when or even if a CRQC is developed, and even in the best case the\ntransition is going to take a really long time, so what happens if someone\nbuilds a CRQC well in advance of when that transition is complete? Clearly,\nthis takes the situation that is somewhere between non-urgent and urgent to\none that is outright emergent but that doesn't mean that all is lost. In this\npost, I want to look at what we would do if a CRQC were to appear sooner\nrather than later. As with the previous post, this post primarily focuses on\nTLS and the Web, though I do touch on some other protocols.\n\nObviously there are a lot of scenarios to consider and \"cryptographically\nrelevant\" is doing a lot of work here. For instance, we typically assume that\nthe strength of X25519 is approximately 2^128 bits. A technique which brought\nthe strength down to 2^80 would be a pretty big improvement as an attack and\nwould definitely be \"cryptographically relevant\" but would also still leave\nattack quite expensive; it probably wouldn't be worth using this kind of CRQC\nto attack connections carrying people's credit cards, especially if each\nconnection had to be attacked individually, at a cost of 2^80 operations each\ntime. This would obviously be a strong incentive to accelerate the PQ\ntransition, but probably wouldn't be an outright emergency unless you had\nparticularly high value communications.\n\nFor the purpose of this post, let's assume that:\n\n  1. This is a particularly severe attack, bringing the existing algorithms within range of commercial attackers in a plausible time frame, whether that's days or real time.^[1]\n\n  2. It happens at some point in the next few years, while there is significant deployment but by no means universal deployment of PQ key establishment and minimal if any deployment of PQ signatures and certificates.\n\nThis is close to a worst-case scenario in that our existing cryptography is\nseverely weakened but it's not practical to just disable it and switch to PQ\nalgorithms. In other words isn't an emergency and we leaves us with a fairly\nlimited set of options.\n\n## Key Establishment #\n\nThe first order of business is to do something about key establishment.\nObviously if you haven't already implemented a PQ-hybrid or pure PQ algorithm,\nyou'll want to do that ASAP, selecting whichever one is more widely deployed\n(or potentially doing both if some peers do one and some the other).\n\nOnce you've added support for some PQ algorithm, the question is whether you\nshould disable the classical algorithm. The naive answer is \"no\": even if the\nclassical algorithm severely weakened, any encryption is better than no\nencryption. In reality, the situation is a bit more complicated.\n\nRecall that in TLS, the client proposes a set of algorithms and the server\nselects one, as shown below:\n\nTLS handshake sketch\n\nThe idea here is that the server gets to see what algorithms the client\nsupports and pick the best algorithm. As long as the client and server agree\non the algorithm ranking, then this will generally work fine. However, it's\npossible that the servers and clients will disagree, in which case the\nserver's preferences will win.^[2]\n\nThis actually happened during the transition away from the RC4 symmetric\ncipher. After a series of papers showed significant weaknesses in RC4, the\nbrowsers decided they preferred AES-GCM. Unfortunately, many servers preferred\nRC4, and so the result was that even when both clients and servers supported\nRC4 and AES-GCM, many servers selected RC4. In response, browsers (starting\nwith IE^[3] adopted a system in which they first tried to connect without\noffering RC4, and if that failed they then retried with it, as shown below:\n\nTLS fallback to RC4\n\nThe result was that any server which supported AES-GCM would negotiate it, but\nif the server only supported RC4, the client could still connect. This also\nmade it possible to measure the fraction of servers which supported AES-GCM,\nthus providing information about about how practical it was to disable RC4.\n\n### Downgrade Attacks #\n\nSo far we've only considered a passive attacker, but what about an active\nattacker? TLS 1.3 is designed so that the signature from the server protects\nthe handshake, so as long as the weakest signature algorithm supported by the\nclient is strong, an active attacker can't tamper with the results of the\nnegotiation.^[4] The fallback system described above weakens this guarantee a\nlittle bit in that the attacker can forge an error and force the client into\nthe fallback handshake. However, the client will still offer both algorithms\nin the fallback handshake, so the attacker can't stop the server from picking\nits preferred algorithm; it can just stop the client from getting the client's\npreferred algorithm by manipulating the first handshake.\n\nOf course, if the server's signature isn't strong\u2014or more properly the weakest\nsignature algorithm the client will accept isn't strong\u2014then the the attacker\ncan tamper with the negotiated key establishment algorithm. However, an\nattacker who can do that can just impersonate the server directly, so it\ndoesn't matter what key establishment algorithms the client supports.\n\n### Maybe it's better to fail open #\n\nThe bottom line here is that as long as you're not under active attack, TLS\nwill deliver the strongest^[5] algorithm that's jointly supported by the\npeers, and, if you're under active attack by an attacker who can break\nsignature algorithms, then all bets are off. That's probably the best you can\ndo if you're determined to connect to the server anyway. But the alternative\nis, don't connect.\n\nThe basic question here is how sensitive the communication with the site is.\nIf you're just looking up some recipes or reading the news, then it's probably\nnot that big a deal if your connection isn't secure (in fact, people used to\nregularly argue that it wasn't necessary at all, though that's obviously not a\nposition I agree with). On the other hand, if you're doing your banking or\nreading your e-mail, you probably really don't want to do that unencrypted.\nThis isn't to say that we don't want ubiquitous encryption\u2014we do\u2014or that it's\nnot possible for even innocuous seeming communications to be sensitive\u2014it\nis\u2014but to recognize that this scenario would force us to make some hard\nchoices about whether we're willing to communicate insecurely if that's the\nonly option. These are hard choices for a human and even harder for a piece of\nsoftware like a browser (it's much easier for a standalone mail client,\nobviously).\n\nThis is actually a situation where ubiquitous encryption makes things rather\nmore difficult. Back when encryption was rare, it was a reasonable bet that if\na site was encrypted then the operators thought it was particularly sensitive.\nBut now that everything is encrypted, it's much harder to distinguish whether\nit's really important for this particular connection to be protected versus\njust that it's good general practice (which, again, it is!).\n\nOne thing that may not be immediately obvious is that an insecure connection\ncan threaten not just the data that you are sending over it, but other data as\nwell. For example, if you are reading your email, you're probably\nauthenticating with either a password (with a normal mail client) or a cookie\n(with Webmail). Both of these are just replayable credentials, so an attacker\nwho can decrypt your connection can impersonate you to the server and download\nall your email, not just the messages you are reading now As discussed above,\nan attacker who recorded your traffic in the past might still be able to\nrecover your password, but this is a lot more work than just getting it off\nthe wire in real time.\n\n## Signature Algorithms #\n\nOf course, none of this does anything to authenticate the server, which is\ncritical for protecting against active attack. For that we need the server to\nhave a certificate with a PQ algorithm and the client to refuse to trust\ncertificates that either (1) are signed with a classical algorithm or (2)\ncontain keys for a classical algorithm. Importantly, it's not enough for the\nserver to stop using a PQ certificate, because the server doesn't have to be\npart of the connection at all. In fact, even if the server doesn't have a PQ\ncertificate, attack is still possible because the attacker can just forge the\nentire certificate chain.\n\nAs described in my previous post, the first thing that has to happen is that\nservers have to deploy PQ certificates. Without that, there's not much the\nclients can do to defend themselves. In this case, I would expect there to be\na huge amount of pressure to do that ASAP, despite the serious size overhead\nissues with PQ certificates noted by Bas Westerban and David Adrian. After\nall, it's better to have a slow web site than one that's not secure or that\npeople can't connect to.\n\nFor the same reason, I would expect there to be a lot less concern about the\navailability of hardware security modules (HSMs) for the new PQ algorithms or\nwhether the algorithms in question have gone through the entire IETF standards\nprocess. Those are both good things, but having PQ safe certificates is more\nimportant, so I would expect the industry to converge pretty fast on a way\nforward.\n\nOnce there is some level of PQ deployment, clients can start distrusting the\nclassical algorithms (before that, there's not much point). However, as with\nkey establishment: if the client distrusts classical algorithms than it won't\nbe able to connect to any server that doesn't have a PQ certificate, which\nwill initially be most of them, even in the best case. This is frustrating\nbecause it means that you have to choose between failure to connect or having\nprotection against active attack. What you'd really like is to have the best\nprotection you can get, i.e.,\n\n  * Only trust PQ algorithms for sites that have PQ certificates (so you aren't subject to active attack).\n  * Allow classical algorithms for sites without PQ certificates (so you at least get protection against passive attack).\n\nActually, there are three categories here:\n\n  1. Sites which are so sensitive that you shouldn't connect to them without a PQ certificate (e.g., your bank).\n  2. Sites which are known to have a PQ certificate and so you shouldn't accept a classical certificate (probably big sites like Google).\n  3. Sites that aren't that sensitive and so you'd be willing to connect to them with a classical certificate (e.g., the newspaper).\n\nThe problem is being able to distinguish which category a site falls into.\nUsually, we don't try to draw this kind of distinction, and just let the site\ntell us if it wants TLS, but this isn't a usual situation, so it's worth\nexploring some inconvenient things.\n\n### PQ Lock #\n\nThe most obvious thing is to have the client remember when the server has a PQ\ncertificate and thereafter refuse to accept a classical certificate.\nUnfortunately, this idea doesn't work well as-is, because server\nconfigurations aren't that stable. For instance:\n\n  1. A site might roll out PQ and then have problems and disable it.\n  2. A site might have multiple servers and gradually roll out PQ certificates on one of them.\n  3. A site might be served by more than one CDN with different configurations.\n\nNote that in cases (2) and (3) the client will not generally be aware that\nthere are different servers, as they have the same domain name, and IP\naddresses aren't reliable for this purpose (and, in any case, are likely under\ncontrol of the attacker because DNS isn't very secure). An in case (1) it's\nactually the same server.\n\nIn any of these situations you could have a situation where the client\ncontacts the server, get a PQ certificate, and then come back and get a\nclassical certificate, so if the client just forbids any use of classical\nafter PQ, this would create a lot of failures. Fortunately, we've been in this\nsituation before with the transition to HTTPS from HTTP, so we know the\nsolution: the server tells the client \"from now on, insist on the new thing,\"\nand the client remembers that.\n\nWith HTTP/HTTPS, this is a header called HTTP Strict Transport Security (HSTS)\nand has the semantics \"just do HTTPS from now on with this domain\". It would\nbe straightforward to introduce a new feature that had the semantics \"just\ninsist on PQ from now on with this domain\". In fact, the HSTS specification is\nextensible, so if you wanted to also insist on HTTPS (a good idea!), you could\nprobably just add a new directive saying \"also require PQ\". It would also be\neasy to add a new HTTP header that said \"if you do HTTPS, require PQ\", as HTTP\nis nicely extensible and unknown headers are just ignored.\n\nOne of the obvious problems with an HSTS-like header\u2014and in fact with HSTS\nitself\u2014is that it relies on the client at some point connecting to the server\nwhile not under attack. If the attacker is impersonating the server then they\njust don't send the new header. They can even connect to the real server and\nsend valid data otherwise, but just strip the header. This is still a real\nimprovement, though, as the attacker needs to be much more powerful: if the\nclient is ever able to form a secure connection to the true server, then it\nwill remember that PQ is needed and be protected against attack from then on,\neven if it's not protected from the beginning.\n\n### Preloading #\n\nIt's possible to protect the user from active attack from the very beginning\nby having the client software know in advance which servers support PQ. There\nis already something that browsers do with HSTS, where it's called \"HSTS\npreloading\". Chrome operates a site where server operators can request that\ntheir sites be added to the \"HSTS preload list\". The site does some checking\nto make sure that the server is properly configured and then Chrome adds it to\ntheir list. In principle, other browsers could do this themselves, but in\npractice, I think they all start from Chrome's list.\n\nIn principle, we could use a system like this for PQ preloading as well, but\nthere are scaling issues. The HSTS preload list is fairly sizable (~160K\nentries as of this writing), but this only represents a small fraction of the\ndomains on the Internet. For example, Let's Encrypt is currently issuing\ncertificates for more than 100 million registered domains and over 400 million\nfully qualified domains. If we assume that sites which have moved to PQ are\naggressive about preloading\u2014which they should be for security reasons\u2014we could\nbe talking about 10s of millions of entries. The current Firefox download is\nabout 134 MB, so we're probably looking at a nontrivial expansion in the size\nof a browser download to carry the entire preload list, even with compact data\nstructures. On the other hand, it's probably not totally prohibitive,\nespecially in the early years when there is likely to not be that much\npreloading.\n\nThere may also be ways to avoid downloading the entire database. For instance,\nyou could us a system like Safe Browsing which combines an imperfect summary\ndata structure with a query mechanism, so that you can get offline answers for\nmost sites, but then will need to check with the server to be sure. The Safe\nBrowsing database has about 4 million entries\u2014or at least did back in 2022\u2014so\nyou probably could repurpose SB-style techniques for something like this, at\nleast until PQ certificates got a lot more popular.^[6] The privacy properties\nof SB-style systems aren't as good as just preloading the entire list, so\nthere's a tradeoff here, so it would be a matter of figuring out the best of a\nset of not-great options.\n\nOf course, browser vendors don't need to wait for servers to ask to be\npreloaded; they could just add them proactively, for instance by scanning to\nsee which sites advertise the PQ-only header, or even which sites just support\nPQ algorithms. Obviously there's some risk of prematurely recording a site as\nPQ-only, but there's also a risk in allowing non-PQ connections in this\nsituation, The higher the proportion of servers that support these algorithms,\nthe more aggressive browser vendors can be about requiring PQ support, and the\nmore readily they can add servers to the list, even if the server hasn't\nreally directly signaled that it wants to be included.\n\n### Site Categorization #\n\nThere are other indicators that can be used to determine whether a site is\nespecially sensitive and so needs to be reached over a PQ-secure connection or\nnot at all. This could happen both browser side or server side based on a\nvariety of indicia such as requiring a password or being a medical or\nfinancial site. One could even imagine building some kind of statistical or\nmachine learning model to determine whether sites were sensitive. This doesn't\nhave to be perfect as long as it's significantly better than static\nconfiguration.\n\n### Reducing overhead #\n\nObviously, we would be in a better position if it weren't so expensive to use\nPQ signature algorithms. Mostly, this is about the size of the signatures. As\nnoted in Bas's post, there are a number of possible options for reducing the\nsize overhead, these include:\n\n  * Removing known intermediate and root certificates\n  * Smart compression of certificates based on a database of known certificates\n  * Completely reworking the entire structure of certificates.\n\nAll of these mechanisms are designed to be be backward compatible, meaning\nthat the client and the server can detect if they both support the\noptimization and use it, but can fall back to the more traditional mechanisms\nif not. The first two mechanisms work with existing WebPKI certificates, and\nwould work with PQ certificates as well, requiring only that the client and\nserver software be updated to support the optimization.\n\nThe last mechanism (\"Merkle tree certificates\") replaces existing WebPKI\ncertificates, and so would require servers to get both a PQ WebPKI certificate\nand a PQ Merkle tree certificate, and conditionally serve the right one\ndepending on the client's capabilities. This is obviously more work for the\nserver operator (the same for the browser user). On the other hand, if server\noperators are already going to have to change their processes to get both PQ\nand classical certificates, it would be a convenient time to also change to\nget a Merkle tree certificate.\n\n### HTTP Public Key Pinning #\n\nObviously, in addition to recording that the server supported PQ algorithms\nyou could remember the server's PQ signature key and insist that the server\npresent that in the future (this is how SSH works). In the past the TLS\ncommunity explored more flexible versions of this approach with a technique\ncalled HTTP Public Key Pinning. HPKP was eventually retired, in part due to\nconcerns about how easy it was to render your site totally unusable by pinning\nthe wrong key and in part because mechanisms like Certificate Transparency\nseemed to make it less important.\n\nOne might imagine resurrecting some variant of HPKP for a PQ transition as a\nstopgap during a period where sites are prepared to deploy PQ but CAs can't\nissue them yet. This wouldn't be quite the same because the server would have\nto authenticate with its classical certificate but then pin the PQ key, which\nwould be accepted without a certificate chain, which HPKP doesn't support. My\nsense is that we could probably manage to get some issuance of PQ certificates\nfaster than we could design a new HPKP type mechanism and get it widely\ndeployed, but it's probably still an option worth remembering in case we need\nit.\n\n## What about TLS 1.2? #\n\nOne challenge with the story I told above is that PQ support is only available\nin TLS 1.3, not TLS 1.2.^[7] This means that anyone who wants to add PQ\nsupport will also have to upgrade to TLS 1.3. On the one hand, people will\nobviously have to upgrade anyway to add the PQ algorithms, so what's the big\ndeal. On the other hand, upgrading more stuff is always harder than upgrading\nless. After all, the TLS working group could define new PQ cipher suites for\nTLS 1.2, and it's an emergency so why not just let use people use TLS 1.2 with\nPQ rather than trying to force people to move to TLS 1.3. On the gripping\nhand, TLS 1.3 is very nearly a drop-in replacement for TLS 1.2. There is one\nTLS 1.2 use case that it TLS 1.3 didn't cover (by design), namely the ability\nto passively decrypt connections if you have the server's private key\n(sometimes called \"visibility\", which is used for server side monitoring in\nsome networks. However, this technique won't work with PQ key establishment\neither, so it's not a regression if you convert to TLS 1.3.\n\n## Non-TLS systems #\n\nMuch of what I've written above applies just as well to many other interactive\nsecurity protocols such as IPsec or SSH,^[8] which are designed along\nessentially the same pattern. Any non-Web interactive protocol is likely to\nhave an easier time because there will be a fairly limited number of endpoints\nyou need to connect to, so you can more readily determine whether the other\nside has upgraded or not. As a concrete example, SSH depends on manual\nconfiguration of the keys (the server's key is usually done on a \"trust on\nfirst use\" basis when the client initially connects). Once that setup is done,\nyou don't need to discover the peer's capabilities. By contrast, a Web browser\nhas to be able to connect to any server, including ones it has no prior\ninformation about.\n\nThere is a huge variety of other cryptographic protocols and our ability to\nrecover from a CRQC would vary a lot. Especially impacted will be anything\nwhich relies on long-term digital signatures, as they are hard to replace. A\ngood example here is cryptocurrency systems like Bitcoin which rely on\nsignatures to effect the transfer of tokens: if I can forge a signature from\nyou then I can steal your money. The right defense against this is to replace\nyour classical key with a PQ key (effectively to transfer money to yourself),\nbut we can assume that a lot of people won't do that in time, and as soon as a\nCRQC is available, any future transaction becomes questionable.\n\nThe situation around Bitcoin seems to actually be pretty interesting. The\nmodern way to do Bitcoin transfers is to transfer them not to a public key but\nthe hash of a public key (called pay to public key hash (p2pkh)). As long as\nthe public key isn't revealed, then you can't use a quantum computer to forge\na signature. The public key has to be revealed in order to transfer the coin,\nbut if you don't reuse the key, then there is only a narrow window of\nvulnerability between the signature and when the payment is incorporated into\nthe blockchain (which doesn't depend on public key cryptography). However,\naccording to this study by Deloitte, about 25% of Bitcoins are vulnerable to a\nCRQC, so that's not a great situation.\n\n## What if the PQ algorithms aren't secure? #\n\nAll of the above assumes that we have public key algorithms that are in fact\nsecure against both classical and quantum computers. In that case, our problem\nis \"just\" transitioning from our insecure classical algorithms to their more-\nor-less interface compatible PQ replacements. But what happens if those\nalgorithms turn out to be secure after all. In that case we are in truly deep\ntrouble. Obviously the world got on OK for centuries without public key\ncryptography, but now we have an enormous ecosystem based on public key\ncryptography that would be rendered insecure.\n\nSome of those applications may just get abandoned (maybe we don't really need\ncryptocurrencies...) but it would obviously be very bad if nobody was able to\nsafely buy anything on Amazon, use Google docs, or that your health care\nrecords couldn't be transmitted securely, so there's obviously going to be a\nlot of incentive to do something. The options are pretty thin, though.\n\n### Signature #\n\nWe do have at least one signature algorithm which we have reasonably high\nconfidence is secure: hash signatures, which NIST is standardizing as \"SLH-\nDSA\". Unfortunately, the performance is extremely bad (we're talking 8KB\nsignatures). On the other hand, slow and big signature algorithms are better\nthan no signature algorithms at all, so there are probably some applications\nwhere we'd see some use of SLH-DSA.\n\n### Key Establishment #\n\nWhile the signature story is bad, but the key establishment story is really\ndire. The main option people seem to be considering is some variant of what\nI've been calling intergalactic Kerberos. Kerberos is a security protocol\ndesigned at MIT back in the 80s and in its original form works by having each\nendpoint (user, server) share a pairwise symmetric^[9] key with a key\ndistribution server (KDC).\n\nA high level view of Kerberos\n\nAt a high level, when Alice wants to talk to Bob, she contacts the KDC using a\nmessage encrypted with her pairwise key K_a and tells it that it wants to\ncontact Bob. The KDC creates a new random key R_ab and then sends Alice two\nvalues:\n\n  * R_ab\n  * A copy of R_ab encrypted under Bob's key (K_b), i.e., E(K_b, {Alice, K_ab}). In Kerberos terms this is called a \"ticket\".\n\nAlice can then contact Bob and present the ticket. Bob decrypts the ticket and\nrecovers K_ab. Now Alice and Bob share a key they can use to communicate. Note\nthat this all uses symmetric cryptography, so it's not vulnerable to attacks\non our PQ algorithms. You can wire up this kind of key establishment mechanism\ninto protocols like TLS (TLS 1.2 actually has Kerberos integration, but it\nwasn't ported into TLS 1.3) and use them in something approximating the usual\nfashion, albeit in a much clunkier fashion.\n\n#### Merkle Puzzle Boxes #\n\nIt turns out that there actually sort of is a public key system that doesn't\ndepend on any fancy math and so we can have reasonable confidence in how\nsecure it is. In fact, it's the original public key system, invented by Ralph\nMerkle. This post is already pretty long, so if you're interested check out\nthe Wikipedia page. The TL;DR is that it's probably not that practical because\n(1) public key sizes are enormous and (2) it only offers the defender a\nquadratic level of security (if the defender does work N the attacker does\nwork N^2 to break it), which isn't anywhere near as good as other algorithms.\nThere seem to be some quantum attacks on puzzle boxes (though I'm not sure how\ngood they are in practice), but there is also a PQ variant.\n\nThis kind of design has a number of challenges. First, it's much harder to\nmanage. In a public-key based system clients don't need to have any direct\nrelationship with the CA, because they just need the CA's public key. In a\nsymmetric key system, however, each client needs a relationship with the KDC\nin order to establish the shared key. This is obviously a huge operational\nchallenge.\n\nThe basic challenge with this kind of design is that the KDC is able to\ndecrypt K_ab and hence any traffic between Alice and Bob. This is because the\nKDC is providing both authentication and key establishment, unlike with a\npublic key system like the WebPKI where the CA provides authentication but the\nendpoints perform key establishment using asymmetric algorithms. This is just\nan inherent property of symmetric-only systems, and it's what we're reduced to\nif we don't have any CRQC-safe asymmetric algorithms.\n\nOne potential mitigation is to have multiple KDCs and then Alice and Bob use a\nkey derived from exchanges with those KDCs. In such a system, the attacker\nwould need to compromise all of the KDCs in use for a connection in order to\neither (1) impersonate one of the endpoints or (2) decrypt traffic. Recently\nwe've started to see some interest in symmetric key type solutions along these\nlines, including a draft at the IETF and a recent blog post by Adam\nLangley.^[10] My sense is that due to the drawbacks mentions above, this kind\nof system isn't likely to take off as long as we have PQ algorithms, even if\nthey're not that efficient. However, if the worst happens and we don't have\nasymmetric PQ algorithms at all, we're going to have to do something, and\nsymmetric-based systems will be one of the options on the table.\n\n## The Bigger Picture #\n\nAs I mentioned in the previous post, we shouldn't expect the PQ transition to\nhappen very quickly, both because the algorithms aren't all that we'd like and\nbecause even with better algorithms the transition is very disruptive.\nHowever, because the Internet is so dependent on cryptography and in\nparticular public key cryptography, there would be enormous demand to do\nsomething if a CRQC were to be developed any time soon. When compared to the\nalternative of no secure communications at all, a lot of options that we would\nhave previously considered unattractive or even totally non-viable would\nsuddenly look a lot better, and I would expect the industry to have to make a\nlot of tough choices to get anything at all to work while we worked out what\nto do in the long term.\n\n  1. This distinction does matter for some attacks, but even if it's days, the situation is really bad. \u21a9\ufe0e\n\n  2. Unless the server decides to defer to the client, of course. \u21a9\ufe0e\n\n  3. Thanks to David Benjamin for help with the history of this technique. \u21a9\ufe0e\n\n  4. This is a new feature of TLS 1.3. In TLS 1.2, the security of the handshake depended on the weakest common key establishment algorithm, which left it vulnerable to attacks if the weakest algorithm was breakable in real-time. \u21a9\ufe0e\n\n  5. Again with the caveats above about preferences \u21a9\ufe0e\n\n  6. The worst case is when about 1/2 of the sites want to be preloaded; once you get to well over 50%, you can instead publish the list of non-preloaded sites, though this is logistically a bit trickier, as you'd need a list of every site. You can get this list from Certificate Transparency, though, which is what CRLite does. \u21a9\ufe0e\n\n  7. Obviously, there's an element of \"we're trying to avoid maintaining TLS 1.2 and we want people to upgrade\" going on here, but there's also a small technical advantage here: although TLS 1.2 and TLS 1.3 both authenticate the server by having the server sign something, in TLS 1.2 the signature only covers part of the handshake (specifically, the random nonces and the server's key), which means that the signature doesn't cover the key establishment algorithm negotiation. This means that an attacker who can break the weakest joint key establishment algorithm can mount a downgrade attack, forcing you back to that weakest algorithm. However, we could presumably address this by remembering that both key establishment and authentication are PQ only. \u21a9\ufe0e\n\n  8. Note: QUIC uses the TLS 1.3 handshake under the hood, so it has roughly the same properties as TLS 1.3 \u21a9\ufe0e\n\n  9. In original Kerberos, a DES key. \u21a9\ufe0e\n\n  10. Langley's design actually assumes that PQ algorithms work but are too inefficient to use all the time, so you use it to bootstrap the symmetric keys with the KDC. \u21a9\ufe0e\n\n### Keep Reading\n\n  * Previous: Design choices for post-quantum TLS\n\n## More from Educated Guesswork\n\nTech, tech policy, and sometimes running by Eric Rescorla. Subscribe to the\nnewsletter for more like this:\n\n### Recent Posts\n\n  * How to manage a quantum computing emergency 15 Apr 2024 networking security post-quantum\n  * Design choices for post-quantum TLS 30 Mar 2024 networking security post-quantum\n  * Sean O'Brien 100K Race Report (2024) 13 Mar 2024 running race report\n  * A hard look at Certificate Transparency: CT in Reality 25 Dec 2023 privacy crypto security\n  * A hard look at Certificate Transparency, Part I: Transparency Systems 13 Dec 2023 privacy crypto security transparency\n\nFull archives ...\n\nSubscribe to the newsletter for more like this:\n\n", "frontpage": false}
