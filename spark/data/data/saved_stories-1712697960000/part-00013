{"aid": "39979402", "title": "Thoughts on the xz backdoor: an lzma-rs perspective", "url": "https://gendignoux.com/blog/2024/04/08/xz-backdoor.html", "domain": "gendignoux.com", "votes": 1, "user": "kzrdude", "posted_at": "2024-04-09 13:48:29", "comments": 0, "source_title": "Thoughts on the xz backdoor: an lzma-rs perspective", "source_text": "Thoughts on the xz backdoor: an lzma-rs perspective | Blog | Guillaume Endignoux\n\n# Thoughts on the xz backdoor: an lzma-rs perspective\n\nopen-source rust\n\nApril 8, 2024\n\nby Guillaume Endignoux @gendx | RSS\n\nMany discussions about open source dependencies and maintenance happened in\nthe last month. Two posts caught my eye in the Rust ecosystem: Sudo-rs\ndependencies: when less is better about the Rust rewrite of sudo trimming its\ndependency graph, and On Tech Debt: My Rust Library is now a CDO about a Rust\npackage being flagged as unmaintained, triggering complaints across downstream\nprojects failing CI. And by now, you\u2019ve likely heard about the backdoor in the\nxz-utils compression project.\n\nAs the author of a pure-Rust implementation of the XZ compression format, here\nare my series of thoughts on the topic.\n\nWould a Rust rewrite have helped compared to the existing C implementation?\nWhat maintainance model can work for critical dependencies? Is compression\nconsidered critical enough in the computing stack?\n\nBefore we dive in, here\u2019s a reminder to check if your systems contain the\nmalicious package. On Debian, use dpkg -l to check the version of installed\npackages.\n\n    \n    \n    $ dpkg -l xz-utils liblzma5 Desired=Unknown/Install/Remove/Purge/Hold | Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend |/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad) ||/ Name Version Architecture Description +++-==============-============-============-================================= ii liblzma5:amd64 5.6.0-0.2 amd64 XZ-format compression library ii xz-utils 5.6.0-0.2 amd64 XZ-format compression utilities\n\nAfter apt update && apt upgrade, you should have the xz packages in version\n5.6.1+really5.4.5-1.\n\n    \n    \n    $ dpkg -l xz-utils liblzma5 ... ||/ Name Version Architecture Description +++-==============-===================-============-================================= ii liblzma5:amd64 5.6.1+really5.4.5-1 amd64 XZ-format compression library ii xz-utils 5.6.1+really5.4.5-1 amd64 XZ-format compression utilities\n\nOn MacOS with Homebrew, you can check installed packages with brew list.\n\n    \n    \n    $ brew list xz /usr/local/Cellar/xz/5.6.1/bin/lzcat /usr/local/Cellar/xz/5.6.1/bin/lzcmp ...\n\nIf you see any 5.6.x version here, it\u2019s high time to \u201cupgrade\u201d back to a safe\nprevious version with brew update && brew upgrade.\n\n    \n    \n    $ brew update ... ==> Outdated Formulae xz ...\n    \n    \n    $ brew upgrade ... ==> Upgrading xz 5.6.1 -> 5.4.6\n\n  * Rewrite it in Rust?\n\n    * Rewritten in Rust: lzma-rs\n    * State-of-the-art: xz2 bindings\n    * Build systems and portability\n  * A maintenance model for critical dependencies?\n\n    * Maintained or not maintained?\n  * A wake up call: compression is security critical\n\n    * Compressed code archives\n    * Don\u2019t roll your own\n  * Further reading\n\n## Rewrite it in Rust?\n\nIt\u2019s a common meme that enthusiatic Rust programmers just rewrite software in\nRust, as it will be \u201cblazingly fast\u201d, memory safe, or just for the sake of it.\nBeyond the meme, would this have helped?\n\n### Rewritten in Rust: lzma-rs\n\nI\u2019m probably one of the best (or worst) placed to talk about it, having\nwritten an LZMA decompressor purely in Rust. Armin Ronacher\u2019s post on tech\ndebt and CDOs hit close to home, as I litterally \u201clearned Rust this way\u201d by\nwriting this LZMA library between jobs back in 2017. I published it on\ncrates.io just before starting the next job, and since then mostly moved on to\nother things. Over the years, a few dozen packages started depending on it.\n\nReleases have been occasional, with some bug fixes and community-contributed\nfeatures like a streaming API. I wouldn\u2019t call it unmaintained, there just\naren\u2019t new features being actively developed. Notably, the decoder is complete\nfor the LZMA and LZMA2 formats, and should also work for baseline XZ files\nthat don\u2019t contain filters (although there\u2019s a pull request to add delta\nfiltering, which I should go back to when I get the time). The encoder is not\ncompressing much: as a rule of thumb for compression formats, writing a\ndecompressor is 10 times simpler than writing an efficient compressor, so I\nfocused my time on the former.\n\nHow would I evaluate the backdooring risk compared with xz-utils?\n\nOn the technical side, similarly to the backdoored xz-utils, I once accepted a\nlarge non-audited test file (597 KB), but applied basic due diligence to\nunderstand where the file came from. This wouldn\u2019t be enough to exclude a\nbackdoor, but importantly the published package excludes artifacts like test\nfiles \u2013 which keeps the size small but is also a good defense-in-depth measure\nand helps auditing. Additionally, my code doesn\u2019t use any custom build script,\nonly the standard Cargo.toml manifest, which is likewise easy to audit. Using\na modern language with a standard package manager makes auditing easier!\n\nOn the human side, I fortunately don\u2019t have to triage too many requests, as my\npackage isn\u2019t that popular and works for what it does. I haven\u2019t been very\nquick in replying on contributions (sorry for that, although most pull\nrequests end up merged eventually), which perhaps helps setting expectations\nthat future contributions will not be merged within a day. That said, if\nsomeone wanted to be adversarial, they could try to file a RustSec advisory\nand leverage sock puppet accounts to flag my package as \u201cunmaintained\u201d before\nI notice \u2013 but the RustSec policy is a good defense-in-depth. Transfering\nownership of the package without my knowledge or consent would be very\nunlikely given crates.io\u2019s policy \u2013 the best an attacker could do would be to\ncreate a backdoored fork and convince others to depend on it, which would be\nan expensive attack.\n\n### State-of-the-art: xz2 bindings\n\nA much more commonly used XZ library in Rust is xz2, which provides bindings\nover xz-utils via the lzma-sys crate. Does it make it vulnerable to the\nbackdoor? That would be quite serious given that it\u2019s used at the heart of the\nRust compiler\u2019s bootstrapping process, to download the previous version of the\ncompiler.\n\nLooking at the source code on GitHub, lzma-sys contains an archived mirror of\nxz-utils version 5.2.5, well before the backdoor was injected. Of course,\nthat\u2019s a lie, there is no guarantee that these two GitHub repositories match\nthe real uploaded lzma-sys crate, that you can browse on docs.rs instead\n(assuming docs.rs isn\u2019t itself compromised).\n\nOf course that\u2019s also a lie, the devil is in the details! The included C code\nis only used if you opt into the \u201cstatic\u201d feature or as a fallback if liblzma\nisn\u2019t found on your system. So chances are that if your system contains the\nbackdoored liblzma then xz2 will pick it up too.\n\nThere are pros and cons to vendoring a C library in a Rust crate: if upstream\npushes a rogue update you don\u2019t pull it, but if upstream pushes a security fix\nyou don\u2019t get it either. One reassuring thing is that the build.rs file to\ncompile the C code is only a hundred lines^1 of readable Rust code, in\ncontrast with the complex build files in the upstream xz-utils project.\n\n### Build systems and portability\n\nOne thing that indeed struck me is how complicated xz-utils\u2019 build system is.\nThe CMakeLists.txt that contained a sneaky dot is more than 2000 lines long,\nin a domain-specific scripting format that few people are proficient in. Add\nto that 1400 lines in the configure.ac file and more build files in the cmake/\nand m4/ folders \u2013 not mentioning the malicious build-to-host.m4 script that\nonly appears on the Debian repository.\n\nIn comparison, the whole lzma-rs project only contains about 4k lines of code!\nOne clear benefit of modern languages with a built-in package manager is\navoiding a lot of boilerplate.\n\nBut why do we even need all these build scripts in C? This I think stems from\nthe myth that C is a portable language: in reality build systems have to\nresort to terrible hacks like check_c_source_compiles to work around\ninconsistencies between various C compilers. Some detractors would mention\nthat Rust isn\u2019t portable enough to exotic platforms \u2013 as happened when\nPython\u2019s cryptography package started depending on Rust in 2021. My take is\nthat Rust works on a growing list of hundreds of platforms, and if an exotic\nplatform isn\u2019t on this list then the C compiler for it is likely outdated and\nbuggy.\n\nHaving a single supported implementation of the compiler really simplifies the\nbuild system, no need to manage unspecified behavior. Rust does support build\nscripts which could be used by an attacker to sneakily modify the code, but\nmost Rust packages don\u2019t need them. When they do, the scripts are usually\nsmall as we saw with the lzma-sys example \u2013 a common use case is in fact to\ncompile C code and generate bindings over it.\n\nAdditionally, these build scripts are also written in Rust. This makes them\neasier to audit, as a reviewer doesn\u2019t need to be proficient in another\nlanguage like CMake or m4. This also makes them more robust because type\nerrors won\u2019t compile, whereas text-based scripts may gladly expand unescaped\nvariables into unintended commands.\n\nLastly, reactive-style checks like check_c_source_compiles are unthinkable in\nRust: you declaratively state your dependencies in the Cargo.toml manifest and\nthe language provides conditional compilation to directly check platform\nspecifics within your code.\n\n## A maintenance model for critical dependencies?\n\nThe xz-utils backdoor was made possible by an attacker taking over maintenance\nof the package. An important question is therefore how to ensure sustainable\nmaintenance of long-term critical dependencies, especially when new features\nare not actively developed anymore and the original author is moving on.\n\nOne observation is that modern package managers make it easy for anyone to\ncreate and publish a new library, which is a double-edged sword. On the one\nhand, anyone can prototype something new, or extend an existing project beyond\nits original scope by simply creating a new package. On the other hand, this\nfragments the ecosystem: someone might create a gzip package, someone else an\nlzma package, and another an xz package. Concretely, the gzip, bzip2 and xz-\nutils Debian packages are all independent packages maintained by different\npeople, despite all being similar compression tools.\n\nUnfortunately, with fragmentation comes more opportunities for attackers to\ninsert a backdoor, by targeting the weakest package.\n\nOne model that I think could work is to bundle together similar libraries or\ntools once they reach enough maturity. For example, the GNU coreutils manages\ncommon Unix command-line tools like ls, cat and mkdir. Having all of these\ntools be part of a common project strenghens and de-risks the maintenance.\nIt\u2019s also probably easier for a wider project to obtain funding or\ncontributions from those who depend on it. As an example, I learned that\nGermany\u2019s Sovereign Tech Fund is investing in various critical open-source\nprojects, such as systemd and the Rust rewrite of coreutils.\n\nAn example from the Rust ecosystem is the RustCrypto organization which\ndevelops cryptographic libraries. Importantly, a unified project doesn\u2019t mean\na monolithic library: RustCrypto precisely releases a small package for each\nalgorithm. This means that when one of these packages has a vulnerability,\nonly those who precisely depend on it have to remediate the issue, rather than\nanyone depending on anything managed by the project. The recently merged RFC\non package namespaces will undoubtedly help promoting this model of\norganizations owning related packages within the Rust ecosystem.\n\nFollowing this model, it would be natural to have a \u201ccompression tools\u201d\norganization that manages implementations of widespread compression\nalgorithms, in a sustainable way. It turns out that such an organization\nalready exists: the libarchive project \u2013 which also got potentially targeted\nby the malicious actor.\n\nAnother model is for larger projects to absorb their small critical\ndependencies. For example, the Hashbrown library is an efficient hash table\nimplementation that got integrated into the Rust standard library. It started\nas a one-person project, but got moved into the rust-lang organization, which\nmeans that its long-term maintenance is de-risked \u2013 even if the original\nauthor is still the main contributor today.\n\nThis model is quite natural: if a large project or company depends on open-\nsource packages, it\u2019s up to this large project to make sure their dependencies\nare well-maintained and well-funded, not the other way around. If not, the\nlarge project should step up, audit its dependencies, fork, or trim its\ndependency graph.\n\nIn fact, a project like the Rust compiler has a large dependency graph. I took\na look a few years ago, and rustc required no less than 265 packages, most of\nwhich are external dependencies: regular expression engine, data structures,\ngraph manipulation and of course compression algorithms to name a few. That\u2019s\nonly counting the core Rust compiler, the toolchain as a whole including the\npackage manager pulls more dependencies. All of these are certainly critical\nto the entire Rust ecosystem, and it may be time to make sure they are in a\ngood state.\n\n### Maintained or not maintained?\n\nOne question comes up more and more in the Rust ecosystem: is it worth\nlabeling packages as \u201cunmaintained\u201d? This label is supported by the RustSec\nadvisory database with clear guidelines. Unfortunately, despite \u201cunmaintained\u201d\nbeing informational, it is reported by default by tools like cargo audit\n(which many downstream projects use in CI), triggering chaos across the\necosystem whenever a commonly-used package receives the label.\n\nAlthough some projects may find it useful to know the maintenance status of\ntheir dependencies, realistically most packages are not highly maintained\nanyway. The majority are single-person projects developed years ago. On the\nother side of the spectrum, some widely-used crates were developed by prolific\ncontributors who own dozens (if not hundreds) of crates. These contributors\nreally command a lot of respect for holding the ecosystem on their shoulders,\nbut realistically one person cannot actively maintain that many packages\nregularly, and may one day move on.\n\nIt would be more useful to shift the mindset and not expect any particular\nmaintenance nor support from packages, unless explicitly labeled otherwise.\nAfter all, open-source licenses state that software is provided \u201cas is\u201d. If\nyou expect all your dependencies to be well maintained, then it\u2019s on you to\nfund these projects, bear the burden of auditing them, trim your dependency\ngraph, and be ready to maintain a fork if the upstream project doesn\u2019t satisfy\nyour needs.\n\n## A wake up call: compression is security critical\n\nA lot has been said about the technical (build systems) and social\n(maintenance model) aspects of the xz backdoor, but I think it\u2019s also a\nreminder that compression is critical to modern computing.\n\nWhen we think of secure communication between computers, the first thing that\ncomes to mind is cryptography. It provides confidentiality and authentication,\nand is ubiquitous via protocols like HTTPS \u2013 built upon TLS. Cryptography\nbeing security-critical has been well understood for a while in research and\nindustry. New cryptographic algorithms are standardized after careful multi-\nyear processes. Protocols like TLS are formally verified. Software and\nhardware implementations are expected to resist side-channel attacks,\notherwise they make the news with fancy vulnerability websites.\n\nAt first glance, compression isn\u2019t security-critical: it\u2019s only turning some\nbytes into fewer bytes. In reality, compression is everywhere as most\napplications wouldn\u2019t be practical without it. As such, the compression\nalgorithm is part of the secure channel. Having perfectly secure cryptography\nisn\u2019t useful if every decrypted byte is then decompressed by an implementation\nthat is either backdoored, vulnerable to buffer overflows or incorrect. In\nfact, the combined compression-encryption channel has been exploited by\nseveral attacks.\n\nBy extension, file formats are also part of the communication channels between\nprograms. What good are bytes if they are interpreted incorrectly or trigger\nbuffer overflows in C parsers?\n\n### Compressed code archives\n\nA particular case where compression is critical is the distribution of code\narchives (in source or binary form), which are essential to the software\nsupply chain. A few examples:\n\n  * Debian packages contain .tar archives that can be compressed with common algorithms such as gzip, bzip2, xz or zstd (Wikipedia).\n  * GitHub creates releases with the source code packaged as .zip and .tar.gz archives.\n  * Every Rust crate is packaged as a .tar.gz archive of the source code.\n  * The Rust compiler is distributed as .tar.xz binaries, as can be seen with rustup update in verbose mode:\n\n    \n    \n    $ rustup -v update ... verbose: downloading file from: 'https://static.rust-lang.org/dist/2024-03-28/rustc-1.77.1-x86_64-apple-darwin.tar.xz'\n\nThese archives are manipulated twice by compression programs: first to create\nan archive from source code (typically on some server infrastructure) and\nsecond to unpack the code (on developer or user machines). At each step, a\ncompromised compressor/decompressor could leverage that to tamper with the\narchive and inject malicious code, or directly run other malicious operations\non the system. This puts compression tools in a privileged position to insert\npersistent backdoors of the type of Ken Thompson\u2019s Reflections on Trusting\nTrust (see also Russ Cox\u2019s article 40 years later).\n\nGitHub archives are interesting, because the generated .tar.gz archives are\nnot stable. They are created on demand and the compression parameters can\nchange, which can lead to different compressed archives for the same\ndecompressed code. This means that you can\u2019t rely on a known cryptographic\nhash of the archive to avoid tampering, which has caused real issues \u2013 another\nsubtle way in which cryptography and compression interact. Additionally,\nGitHub allows uploading release assets which may not match the code in the git\nrepository.\n\n### Don\u2019t roll your own\n\nUnfortunately, I think that compression and file formats don\u2019t receive the\nsame amount of security attention, scrutiny and formalism as cryptography, and\nthe xz-utils backdoor is only an example of that. Compression algorithms are\nnot always standardized: Deflate and Brotli have RFCs, LZMA and XZ don\u2019t. Most\nprojects are decades-old and don\u2019t receive enough maintenance. In my previous\nblog post I was complaining about poor support for Brotli in Nginx.\n\nAbout file formats, the mantra \u201cdon\u2019t roll your own crypto\u201d unfortunately\ndoesn\u2019t translate to \u201cdon\u2019t roll your own encoding format\u201d. The theory of\nparsing has been well researched for decades, but it isn\u2019t systematically\napplied in the industry \u2013 a notable exception being Protocol Buffers and other\nIDLs which abstract away the serialization details. As a result, parsing JSON\nis still a minefield, with each implementation behaving differently on edge\ncases. It\u2019s only in the last 10 years that the LangSec approach has attempted\nto bridge the gap between parsing theory and security. There is of course a\nrecent push towards memory-safe languages \u2013 for example with a successful\nrewrite of an SVG rendering library in Rust already in 2019 \u2013 but a lot of\nlegacy code in compression and encoding formats is still written in C/C++.\n\nBut I digress, how does this relate to the xz-utils backdoor, which was\nexploiting test files enabled by malicious build scripts and maintainer burn-\nout? One observation is that because cryptography is seen as security-\ncritical, large organizations have dedicated teams developing in-house\ncryptography libraries, which prevents an external actor from taking over\nmaintainance, or a maintainer from pushing malicious code without review.\n\nCompression and parsing deserve more libraries like that.\n\n## Further reading\n\n  * Everything I Know About the XZ Backdoor, Evan Boehs.\n  * Timeline of the xz open source attack, Russ Cox.\n  * xz/liblzma: Bash-stage Obfuscation Explained, Gynvael Coldwind.\n\n  1. Not exactly though: there are also a thousand lines in the pkg-config support crate, that pretty much any Rust crate linking to a C library via a build.rs script depends on. \u21a9\n\n## Comments\n\nTo react to this blog post please check the Mastodon thread and the Reddit\nthread.\n\nRSS | Mastodon | GitHub | Reddit | Twitter\n\n## You may also like\n\nMaking my website 10x smaller in 2024, with a dark mode Five years of Rust - a\nfull-stack programming language for the next decade(s) Detecting SIMD support\non ARM with Android (and patching the Rust compiler for it) STV-rs: Single\nTransferable Vote implementation in Rust And 29 more posts on this blog!\n\nPrevious Making my website 10x smaller in 2024, with a dark mode\n\nBlog by Guillaume Endignoux. This work is licensed under a Creative Commons\nAttribution-NonCommercial-ShareAlike 4.0 International License. Design derived\nfrom original templates by Xiaoying Riley and Rohan Chandra.\n\n", "frontpage": false}
