{"aid": "39996705", "title": "Deepfakes, elections, and shrinking the liar's dividend", "url": "https://www.brennancenter.org/our-work/research-reports/deepfakes-elections-and-shrinking-liars-dividend", "domain": "brennancenter.org", "votes": 1, "user": "anigbrowl", "posted_at": "2024-04-10 23:21:37", "comments": 0, "source_title": "Deepfakes, Elections, and Shrinking the Liar\u2019s Dividend", "source_text": "Deepfakes, Elections, and Shrinking the Liar\u2019s Dividend | Brennan Center for Justice\n\nSkip Navigation\n\nStay up to date\n\n### Informed citizens are our democracy\u2019s best defense.\n\nFacebook X Youtube Instagram\n\nFacebook X Youtube Instagram\n\n  * Home\n  * Our Work\n  * Research & Reports\n  * Deepfakes, Elections, and Shrinking the Liar\u2019s Dividend\n\nResource\n\n# Deepfakes, Elections, and Shrinking the Liar\u2019s Dividend\n\nHeightened public awareness of the power of generative AI could give\npoliticians an incentive to lie about the authenticity of real content.\n\n  * Josh A. Goldstein\n\n  * Andrew Lohn\n\nPublished: January 23, 2024\n\nView the entire AI and Democracy series\n\nIn August 2023, the survey firm YouGov asked Americans how concerned they are\nabout various potential consequences arising from artificial intelligence\n(AI). Topping the list, 85 percent of respondents said that they are \u201cvery\nconcerned\u201d or \u201csomewhat concerned\u201d about the spread of misleading video and\naudio deepfakes. This finding is unsurprising given frequent news headlines\nsuch as \u201cAI \u2018Deepfakes\u2019 Poised to Wreak Havoc on 2024 Election\u201d and\n\u201cDeepfaking It: America\u2019s 2024 Election Collides with AI Boom.\u201d As the\nintroduction to the AI and Democracy essay series notes, \u201cincreasing awareness\nof the power of artificial intelligence coincides with growing public anxiety\nabout the future of democracy.\u201d\n\nProblematically, however, concern about deepfakes poses a threat of its own:\nunscrupulous public figures or stakeholders can use this heightened awareness\nto falsely claim that legitimate audio content or video footage is\nartificially generated and fake. Law professors Bobby Chesney and Danielle\nCitron call this dynamic the liar\u2019s dividend. They posit that liars aiming to\navoid accountability will become more believable as the public becomes more\neducated about the threats posed by deepfakes. The theory is simple: when\npeople learn that deepfakes are increasingly realistic, false claims that real\ncontent is AI-generated become more persuasive too.\n\nThis essay explores these would-be liars\u2019 incentives and disincentives to\nbetter understand when they might falsely claim artificiality, and the\ninterventions that can render those claims less effective. Politicians will\npresumably continue to use the threat of deepfakes to try to avoid\naccountability for real actions, but that outcome need not upend democracy\u2019s\nepistemic foundations. Establishing norms against these lies, further\ndeveloping and disseminating technology to determine audiovisual content\u2019s\nprovenance, and bolstering the public\u2019s capacity to discern the truth can all\nblunt the benefits of lying and thereby reduce the incentive to do so.\nGranted, politicians may instead turn to less forceful assertions, opting for\nindirect statements to raise uncertainty over outright denials or allowing\ntheir representatives to make direct or indirect claims on their behalf. But\nthe same interventions can hamper these tactics as well.\n\nDeepfakes in Politics\n\nManipulating audiovisual media is no new feat, but advancements in deep\nlearning have spawned tools that anyone can use to produce deepfakes quickly\nand cheaply. Research scientist Shruti Agarwal and coauthors write of three\ncommon deepfake video approaches, which they call face swap, lip sync, and\npuppet master. In a face swap, one person\u2019s face in a video is replaced with\nanother\u2019s. In a lip sync, a person\u2019s mouth is altered to match an audio\nrecording. And in a puppet master\u2013style deepfake, a target person is actually\nanimated by a performer in front of a camera. Audio-only deepfakes, which do\nnot involve a visual element, are also becoming more prevalent.\n\nAlthough a review of the technical literature falls outside the scope of this\nessay, suffice it to say that technical innovations are yielding deepfakes\never more able to fool viewers. Not every deepfake will be convincing; in many\ncases, they will not be. Yet malcontents have successfully used deepfakes to\nscam banks and demand ransoms for purportedly kidnapped family members.\n\nDeepfakes have gained traction in the political domain too. In September 2023,\na fake audio clip went viral depicting Michal \u0160ime\u010dka, leader of the pro-\nWestern Progressive Slovakia party, discussing with a journalist how to rig\nthe country\u2019s election. Deepfakes of President Joe Biden, former President\nDonald Trump, and other U.S. political leaders have circulated. Broader\nefforts to track the use of deepfakes \u2014 such as the AI Incident Database and\nthe Deepfakes in the 2024 Presidential Election website \u2014 are underway and\nwill undoubtedly grow as cases and investigative research accumulate.\n\nIn Comes the Liar\u2019s Dividend\n\nDeepfakes amplify uncertainty. As deepfake technology improves, Northeastern\nUniversity professor Don Fallis writes, it may become \u201cepistemically\nirresponsible to simply believe that what is depicted in a video actually\noccurred.\u201d In other words, people may find themselves inherently questioning\nwhether the events they see portrayed in a video actually occurred, which may\nundermine their uptake of new (true) information. And indeed, according to\nrecent reporting about the Israel-Hamas war, the \u201cmere possibility that A.I.\ncontent could be circulating is leading people to dismiss genuine images,\nvideo and audio as inauthentic.\u201d\n\nIn a world where deepfakes are prevalent and uncertainty is widespread, public\nfigures and private citizens alike can capitalize on that uncertainty to sow\ndoubt in real audio content or video footage and potentially benefit from the\nliar\u2019s dividend. In the courtroom, lawyers have attempted the \u201cdeepfake\ndefense,\u201d asserting that real audiovisual evidence against a defendant is\nfake. Guy Reffitt, allegedly an anti-government militia member, was charged\nwith bringing a handgun to the January 6, 2021, Capitol riots and assaulting\nlaw enforcement officers; Reffitt\u2019s lawyer maintained that the prosecution\u2019s\nevidence was deepfaked. Likewise, Tesla lawyers have argued that Elon Musk\u2019s\npast remarks on the safety of self-driving cars should not be used in court\nbecause they could be deepfakes.\n\nSimilar claims have been made in politics. A forthcoming journal article by\nKaylyn Jackson Schiff, Daniel Schiff, and Natalia Bueno interrogates the\nliar\u2019s dividend, highlighting several examples of political figures denying\nthe authenticity of audiovisual content:\n\nAs a few notable examples amongst many, former Spanish Foreign Minister\nAlfonso Dastis claimed that images of police violence in Catalonia were \u2018fake\nphotos\u2019 . . . and American Mayor Jim Fouts called audio tapes of him making\nderogatory comments toward women and black people \u2018phony, engineered tapes\u2019 .\n. . despite expert confirmation.\n\nIn July 2023, journalist Nilesh Christopher covered a case in which an Indian\npolitician insisted that embarrassing audio of him was AI-generated, though\nresearch teams agreed that at least one of the clips was authentic.\n\nThe section below builds on the existing literature by asking what the liar\u2019s\ndividend might look like in the upcoming U.S. presidential race (or in another\ndemocracy\u2019s election). While we cannot know whether any of the situations\ndescribed will come to pass, we think that the liar\u2019s dividend dynamic\nwarrants further attention given the vital importance of accountability in\ndemocratic systems.\n\nEmploying the Liar\u2019s Dividend\n\nMany references to the liar\u2019s dividend describe a scenario where a public\nfigure tries to stem reputational damage from real audio or video content that\nsurfaces by falsely representing that content as AI-generated (and therefore\nfake). For an example of how this might play out in politics, consider the\nnow-infamous Access Hollywood video that came to light in October 2016. In the\nclip, then-candidate Trump boasts about groping and kissing women without\nconsent. Trump apologized hours after it was released, but he reportedly\nsuggested the following year that the clip was not authentic. One could easily\nimagine that if a similar clip emerged in 2024, a candidate might try to\ndismiss it as AI-generated from the start.\n\nA candidate falsely denying footage of them as AI-generated could very well be\nhow the liar\u2019s dividend plays out in future elections. However, a much broader\nrange of scenarios could emerge too. Both who makes a false claim and what\nspecifically they contend could take different forms.\n\nOf course, political candidates themselves might deny the veracity of damaging\ncontent by claiming it is AI-generated. But those with established proxy\nrelationships (such as campaign staffers), supporters, or even unaffiliated or\nanonymous voices could also seek to abet the politician in question. When\ncompromising audio or video surfaces, it would be a natural time to enlist\nvalidators to defend the candidate to the public.\n\nFrom the perspective of a would-be liar, deciding who delivers the (false)\nmessage that content is AI-generated surely includes weighing trade-offs. On\nthe one hand, candidates could benefit from asserting artificiality directly\nwithout relying on proxies. Given their high-profile standing, denying\ndamaging content themselves might allow candidates to maximize media and\npublic attention and maintain electoral viability, especially if the content\nis extremely prejudicial. On the other hand, a candidate\u2019s direct denial\ncarries the risk of reputational backlash if their lie is disproven.\n\nRelying instead on official or unofficial proxies to repudiate incriminating\nmaterial could mitigate that risk: if the content is established as genuine\nand the candidate did not deny it personally, then a reputational hit for\ntrying to capitalize on the liar\u2019s dividend would be much less likely. Someone\nwith no official connection to the candidate would maximize this plausible\ndeniability, which could be especially useful when a recorded event might have\nhad eyewitnesses (raising the odds of getting caught for lying). This strategy\nis analogous to nation-states using proxies for covert action or propaganda to\nbenefit from plausible deniability: if the behavior is linked to a proxy but\ncannot be tied directly to the sponsor, then the sponsor may face less\nreputational backlash. However, as political scientist Scott Williamson notes,\ndelegating to proxies may have limited effect if proxies \u201clack credible\nseparation from their sponsors\u201d \u2014 true for political candidates and states\nalike. Moreover, as noted above, denials through proxies may receive less\nattention and thus have smaller upside potential. Figure 1 portrays the\naffiliation between a candidate and a potential proxy as a spectrum, with\nvariation in candidate exposure on the x-axis.\n\nThe nature of a false claim of artificiality could also vary. Direct claims\ninvolve someone asserting that a specific piece or pieces of media are AI-\ngenerated; indirect claims are broader \u2014 for example, statements like \u201cwe\ncannot know what to trust these days\u201d or \u201cthe opposition will do anything to\ndiscredit the candidate\u201d in response to compromising content surfacing.\nSchiff, Schiff, and Bueno describe these strategies as \u201cinformational\nuncertainty\u201d and \u201coppositional rallying,\u201d respectively. Figure 1 shows this\nvariation in claim directness on the y-axis.\n\nFigure 1\n\nThe liar\u2019s dividend can be employed through a candidate or through proxies\n(including affiliated or unaffiliated parties). It can also take the form of\ndirect versus indirect claims.\n\nFigure 1 summarizes two dimensions: whether the relationship between the liar\nand the candidate is official or unofficial (assuming that the liar isn\u2019t the\ncandidate themself) and whether the denial is direct or indirect. These are\nspectra, and the binary presentation is illustrative; figure 1 depicts only\ntwo pieces of a much more complicated puzzle. The dimensions depict that the\nliar\u2019s dividend can go beyond a candidate simply claiming that damaging\ncontent is AI-generated. Future research could build on figure 1 by applying a\ngame-theoretic approach to consider conditions under which pursuing strategies\nin the different quadrants is optimal for a prospective liar, or by testing\nhow the misinformation purveyor\u2019s proximity to the candidate affects public\nperceptions of the content.\n\nBelow, we move from describing the nature of the lie itself to additional\nconsiderations that may render false claims of artificiality more or less\nattractive for a potential liar.\n\nShifting a Candidate\u2019s Calculus\n\nWhen would political candidates \u2014 either themselves or through proxies \u2014 seek\nto benefit from the liar\u2019s dividend? The expected utility of falsely\nrepresenting audio clips or video footage as AI-generated rests on judgments\nabout a number of factors, including social and technical means for\ndistinguishing content as AI-generated or authentic, public perceptions of AI\ncapabilities, public trust in those claiming that content is AI-generated or\nauthentic, and the candidate\u2019s expectation of backlash if their lie is not\nbelieved.\n\nThe remainder of this essay discusses this (albeit non-exhaustive) list of\nimportant considerations. Throughout the discussion, we highlight features\nthat shrink the liar\u2019s dividend by making a lie less believable or raising\npossible reputational costs. These mitigations are not mutually exclusive.\nSome of them target different facets of the potential liar\u2019s calculation.\nOthers may have longer odds. Taken together, however, they may have a bigger\neffect in reducing the threat of the liar\u2019s dividend.\n\nMeans of Discovery\n\nDeepfake detectors alone are insufficient to deter a false claim of\nartificiality, but enhanced content provenance standards could make such a lie\nless believable.\n\nWhen an event has many eyewitnesses or it happens in a hard-to-deny\ncircumstance like on a debate stage or in a press briefing, evidence to refute\nclaims of inauthenticity may be readily available. But when a moment is\ncaptured by one or only a few people, a candidate may wonder if they can\nrefute the proof. The natural hope is that AI might be able to save us from\nitself.\n\nImagine if AI detection models were able to identify deepfakes with perfect\naccuracy. A would-be liar would then have little room to maneuver. If a\nstakeholder claims that authentic footage of a candidate is AI-generated,\nskeptics could run the piece of content through a detection model. The\ndetection model would not flag the piece of content as AI-generated, and the\nlack of a positive flag would be determinative proof of a lie. Unfortunately,\nhowever, no such silver bullet tool exists.\n\nResearch into AI-based deepfake detectors has yielded impressive accuracies\nwell over 90 percent on data sets of known deepfakes. However, there are many\ncaveats. For one, detectors trained to identify deepfakes made using existing\nmethods will be less effective against those produced using brand-new\ntechniques. Deepfake generators can also add specially crafted edits to their\nimages and videos that, though imperceptible to humans, can trick computer-\nbased deepfake detectors.\n\nThese cat-and-mouse games, wherein novel techniques make deepfakes difficult\nto expose until detection methods improve, will likely continue for several\nyears. Of course, once an offending video is produced, it stays the same while\ndetection methods evolve. As such, the truth about whether or not a video is a\ndeepfake may eventually become more certain. In the meantime, even as current\ndeepfake detectors can flag some ungenuine content, some portion will\ncertainly evade detection.\n\nEven if only a small percent of deepfakes go undetected, the math may still be\non the liar\u2019s side. Many deepfakes could be blocked by hosting platforms,\nrejected by media outlets, or discredited by impartial observers, but some\nwould slip through. These could cast enough doubt on detection tools that an\nunscrupulous actor who makes a false claim of artificiality cannot be\ndisproven. Just because something is not flagged as a deepfake does not\nnecessarily mean it is authentic.\n\nAlthough deepfake detectors may not yet be reliable enough to deter a would-be\nliar alone, detection is not the only technical defense. A more promising\nalternative than trying to prove that videos are fake is proving them\nauthentic, and efforts to provide tools to do so are underway. For instance,\nauthenticating cameras can now be designed to imprint tamperproof signatures\nin the metadata of an image or video at the moment of generation. This could\nrecord where and when an image was taken, for instance. Some implementations\ncan go further to test and ensure that images are made by light and not by\ncapturing screens or other pictures. Changes such as cropping or brightening\nwould then change or remove the original signature, revealing that the image\nor video was modified. This method requires little new technology; the main\nchallenge is boosting its adoption. There must be consistent implementation\nacross the chain of entities in which a piece of media is passed (e.g., the\nweb browser, a social media platform) to retain provenance information.\n\nThe Coalition for Content Provenance and Authenticity (C2PA) has put forth an\nopen standard that the Content Authenticity Initiative (CAI) and Project\nOrigin are promoting. Together, these groups include many of the major\nstakeholders that would need to be involved in any widely adopted standard:\nimage originators like Canon, Leica, Nikon, and Truepic; software companies\nlike Adobe and Microsoft; and media companies including the Associated Press,\nthe BBC, the New York Times, and the Wall Street Journal. For high-profile\nevents (i.e., those worthy of candidate lies), these organizations have both\nthe impetus and the technology to prove that their content is genuine. But\nauthentication will be more challenging for images, video, and audio captured\nby those who have yet to adopt this technology.\n\nAs politicians consider whether to falsely represent that incriminatory\naudiovisual content is AI-generated, they will invariably weigh not only the\nstrength of the evidence against them but also the likelihood of getting\ncaught. Technology for detection and provenance is advancing, but widespread\nimplementation will take time \u2014 as will establishing public trust in the\nexperts and their tools to deter prospective liars or catch them in the act.\n\nPublic Perceptions of AI Capabilities\n\nStrengthening the public\u2019s ability to discern the truth and helping voters see\nthrough false artificiality claims can reduce the incentive to lie. Media\noutlets should prepare for scenarios of uncertainty, educate the public about\nthe liar\u2019s dividend, and avoid AI threat inflation.\n\nFrom the perspective of a would-be liar, the benefits of falsely claiming that\ncontent is AI-generated depend on whether people will believe the lie. Tagging\nand tracing mechanisms like those described above will ultimately go a long\nway to diminish such claims\u2019 credibility, but bolstering the public\u2019s and the\nmedia\u2019s ability to discern \u2014 over and above mere skepticism \u2014 deepfakes from\ngenuine content would also shrink the dividend.\n\nFirst off, that humans (even now) often do detect deepfakes as AI-generated is\nworth noting. One study from 2022 presented 15,000 participants with authentic\nvideos and deepfakes and asked them to identify which was which. The\nresearchers found that ordinary people\u2019s accuracy rates were similar to those\nof leading computer vision detection models. The fact is, many deepfakes are\npoorly made and easy for humans to spot.\n\nStill, human ability to detect AI-generated content should not be considered a\nsafety net. The same study found that in some pairs of videos, less than 65\npercent of participants could correctly identify which one was AI-generated.\nIn the real world, unlike in survey experiments that evaluate participants in\nisolation, people may benefit from the wisdom of the crowds when evaluating\nwhat they see and hear. But deepfakes are improving, and detecting something\nis AI-generated may also be more challenging than proving that something is\nreal \u2014 the task at hand to hold a liar accountable.\n\nMoving forward, research that strengthens truth discernment will be critical.\nThe distinction between skepticism and truth discernment is simple but\nsignificant. As an example, take the question of whether the general public\ncan recognize fake news online. If people are told that information online is\noften false and unreliable, they may be more apt to correctly deem\nmisinformation fake news. They may, however, become more skeptical of all\nonline content and therefore more apt to deem real information fake. Efforts\nto counter disinformation should aim to increase truth discernment, not merely\nskepticism.\n\nAlthough few studies have explored as much directly, we suspect that the\ncurrent AI hype and rhetoric around deepfakes risk inducing skepticism over an\nactual ability to discern true content. A valuable research direction would be\nhow best to educate the public about deepfakes in a way that strengthens truth\ndiscernment without increasing overall skepticism. Anyone can learn common\ntells, but those tells become unreliable as technology improves. Another\nworthwhile tactic would be to instruct people on the logic of the liar\u2019s\ndividend so they\u2019re less inclined to take a possible lie uncritically and at\nface value. Other efforts should incorporate lateral reading into digital\nliteracy education to reinforce the habit of checking content against reliable\nsources.\n\nMedia organizations have a leading role to play in these efforts. Akin to\nresponding to hacks and disinformation, they should prepare the public for\nclaims by political candidates that content is AI-generated before these\nclaims occur. They should also develop plans for how they will report on\nallegations that content is AI-generated in cases where the material\u2019s\nveracity is not known.\n\nPublic Trust in the Different Messengers\n\nPublic belief in false claims of artificiality will depend on who publishes\nthe audio or video in question, who asserts its inauthenticity, and what the\ncontent contains. Media outlets should take care to avoid accidentally\npublishing deepfakes, as doing so could foster trust in subsequent claims that\nreal content is fake.\n\nWhether people believe a false artificiality claim depends not just on their\nknowledge of AI\u2019s ability to generate content and the evidence for or against\nthe claim but also on the different sources weighing in \u2014 namely, who\npublishes the material in question and who alleges its faux provenance.\n\nIf trustworthy organizations have fact-checked and published the content, then\nfalse claims of artificiality may not be believed. Problematically, however,\ntrust in different U.S. media outlets is already highly polarized. To minimize\nthe benefits of misrepresenting real content as fake, media outlets must\ncarefully verify audio and video material to avoid inadvertently releasing\ndeepfakes. Publishing material that is subsequently established as AI-\ngenerated can lend credence to those who later falsely claim that new content\nfrom that outlet is also AI-generated.\n\nWho professes that the content is AI-generated matters too. Above, we outlined\nthat peddlers of false claims could vary in their public proximity to\ncandidates \u2014 from anonymous accounts (with no clear relationship) to\ncandidates themselves (the most direct relationship). At the distal extreme,\nanonymous accounts are unlikely to be reliable, though they may still foment\nconfusion on online platforms. Claims from candidates themselves may be more\nbelievable because politicians have a higher degree of accountability if\ncaught. However, the public may recognize a candidate\u2019s incentive to disclaim\nunflattering audio content or video footage, and so may view denials with\nsuspicion from the outset. Alternatives in between \u2014 particularly from\nmessengers who appear objective \u2014 might be most believed. Further research\ncould assess this question.\n\nOf course, whether people believe claims that content is AI-generated depends\nnot just on the messenger but also on the substance and the quality of the\ncontent itself. For example, take a politician who has a strong reputation for\nfidelity. If an audio clip circulates of that candidate admitting to an\nextramarital affair, a denial would be more believable than for a candidate\nwho is already perceived as unfaithful. Thus, ironically, politicians who have\nthe most to lose from damaging audio or video content might be the ones most\nlikely to get away with lying.\n\nExpectation of Backlash\n\nStronger norms against falsely representing content as AI-generated could help\nhold liars accountable.\n\nWhat political candidates hope to gain from falsely claiming artificiality\ndepends on whether they expect backlash if their lies are disproven. Above, we\ndiscussed how messengers can range from candidates themselves to proxies to\nanonymous accounts. The risk of backlash inevitably increases as the messenger\nmoves closer along the axis to the candidate (see figure 1). Yet convincing a\ncandidate that they will face backlash for misrepresenting content as AI-\ngenerated may be challenging. For one thing, politicians do not always face\nrepercussions for lying. Research shows that partisan-motivated reasoning can\nlead voters to change their views when a candidate lies or to develop\nrationales for why a particular candidate\u2019s lie was permissible. Even if\npotential voters would disapprove of a candidate\u2019s lie, the expected backlash\nmay not be sufficient to sway the candidate from lying. Furthermore, research\nhas demonstrated that only a small fraction of Americans prioritize democratic\nvalues and behavior in their electoral choices. Voters may be inclined to vote\nfor their party\u2019s candidate despite that candidate lying about the\nauthenticity of damaging content.\n\nEven if voters would condemn candidates for falsely claiming that content is\nAI-generated, two issues remain in terms of backlash expectation. The first is\nthe anticipated damage of the truthful material that the candidate is denying.\nIf the content is sufficiently harmful, a politician may be incentivized to\nlie because forgoing the lie would mean a near-certain decline in support. In\nother words, a false artificiality claim may be a last-ditch strategy, but it\nmay also be a rational one. The second issue is a fundamental gray area:\npolitical candidates exist in an environment with imperfect information and\nmay misjudge the consequences of lying about content authenticity until norms\nand precedents are in place.\n\nActivists, thought leaders, and other members of the public should establish\nnorms around the acceptable use of AI in politics \u2014 and voice criticism when\ncandidates stray from them. Such norms could be a potent signal for elected\nofficials; at a minimum, they might limit the benefit of lying. These\nstandards could take a number of forms: major parties could pledge to withdraw\nsupport from candidates who intentionally make false claims that true content\nis AI-generated and fake; citizens could express disapproval directly; or a\ngroup of thought leaders or notable donors could commit to calling out\ncandidates who falsely declare true content to be AI-generated.\n\nAny one of these steps alone is unlikely to shift a prospective liar\u2019s\ncalculation substantially. Together, however, they could help to institute\nnorms of disapproval before fake artificiality claims become even more\nprofuse.\n\nLiars Will Lie\n\nWhen we talk about AI-generated disinformation in elections today, the focus\noften falls on the proliferation of deepfakes, whereby politicians are\ndiscredited by things they never said or did. But another risk arises for the\nvoting populace and the system we form when ever-more sophisticated AI tools\ninvoke a world in which the truth is fungible. \u201cThe epistemic function of a\ndeliberative system,\u201d political scientist Jane Mansbridge et al. write, \u201cis to\nproduce preferences, opinions, and decisions that are appropriately informed\nby facts and logic and are the outcome of substantive and meaningful\nconsideration of relevant reasons.\u201d If politicians or their proxies can\nsuccessfully use false claims to deceive the public, then they can undermine\nthe public\u2019s ability to affect those informed preferences, opinions, and\ndecisions. In the starkest of terms, disinformation becomes a threat to\ndeliberative democracy itself.\n\nOur foray into the liar\u2019s dividend conundrum included a broader range of\nscenarios than a political candidate lying. We showed that the logic of the\nliar\u2019s dividend can apply to a range of messages and messengers. We also\nidentified central factors that would determine, in the would-be liar\u2019s mind,\nthe expected utility of falsely claiming that true content is fake: detection\nand provenance, public perceptions of AI, the pros and cons of different\nmessengers, and expectations of backlash. Unfortunately, in cases where\nauthentic audio or video of a candidate is particularly damaging or where a\ncandidate does not believe that they would lose support from likely voters,\nthey may calculate that lying is the best way forward. Moreover, some\npoliticians may not make decisions from the expected utility approach in the\nfirst place. In such cases, the goal is to limit the benefit of lying as much\nas to deter the behavior.\n\nCandidates, officials, and their proxies will almost certainly use the coming\ninnovations in AI technology to make false claims that real events never\nhappened. That reality does not need to mean that they can escape\naccountability. Taking proactive steps to improve detection methods, track\nauthentic content, prepare the public to reject false artificiality claims,\nand set strong norms against these deceits can help us preserve the epistemic\nfoundation on which democracy rests.\n\nAcknowledgments\n\nThe authors thank the following individuals for their review and feedback:\nJohn Bansemer, Matthew Burtell, Mounir Ibrahim, Lindsay Hundley, Jessica Ji,\nJenny Jun, Daniel Kang, Lawrence Norden, Mekela Panditharatne, Daniel Schiff,\nand Stephanie Sykes. All mistakes are our own.\n\nDr. Lohn completed this work before starting at the National Security Council.\nThe views expressed are the author\u2019s personal views and do not necessarily\nreflect the views of the White House or the administration.\n\nJosh A. Goldstein is a research fellow at Georgetown\u2019s Center for Security and\nEmerging Technology (CSET), where he works on the CyberAI Project. Andrew Lohn\nis a senior fellow at CSET and the director for emerging technology on the\nNational Security Council Staff, Executive Office of the President, under an\nInterdepartmental Personnel Act agreement with CSET.\n\nLohn completed this work before starting at the National Security Council. The\nviews expressed are the author\u2019s own personal views and do not necessarily\nreflect the views of the White House or the Biden administration.\n\nMore from the AI and Democracy series\n\n  * Image\n\nResource\n\n### Congress Must Keep Pace with AI\n\nLawmakers need to both take advantage of AI advances and create needed\nsafeguards.\n\nMaya Kornberg, Marci Harris, Aubrey Wilson\n\nFebruary 8, 2024\n\nRead More about Congress Must Keep Pace with AI\n\n  * Image\n\nResource\n\n### Artificial Intelligence and Election Security\n\nTo protect election infrastructure and personnel from AI-generated threats,\nelection offices and vendors must implement the best practices that experts\nhave been urging for over a decade.\n\nLawrence Norden, Gowri Ramachandran\n\nOctober 5, 2023\n\nRead More about Artificial Intelligence and Election Security\n\n  * Image\n\nResource\n\n### Regulating AI Deepfakes and Synthetic Media in the Political Arena\n\nPolicymakers must prevent manipulated media from being used to undermine\nelections and disenfranchise voters.\n\nDaniel I. Weiner, Lawrence Norden\n\nDecember 5, 2023\n\nRead More about Regulating AI Deepfakes and Synthetic Media in the Political\nArena\n\nStay up to date\n\n###\n\nInformed citizens are democracy\u2019s best defense\n\nFacebook X Youtube Instagram Threads TikTok Substack LinkedIn\n\nNew York Office 120 Broadway Suite 1750 New York, NY 10271\n\nWashington DC Office 1140 Connecticut Ave., NW 11th Floor, Suite 1150\nWashington, D.C. 20036\n\n\u00a9 2024 Brennan Center for Justice at NYU Law\n\nPrivacy Policy\n\nAccessibility\n\n", "frontpage": false}
