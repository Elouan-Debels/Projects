{"aid": "40005775", "title": "You can't build a moat with AI", "url": "https://generatingconversation.substack.com/p/you-cant-build-a-moat-with-ai", "domain": "generatingconversation.substack.com", "votes": 16, "user": "vsreekanti", "posted_at": "2024-04-11 19:19:21", "comments": 25, "source_title": "You can't build a moat with AI", "source_text": "You can't build a moat with AI\n\n# Generating Conversation\n\nShare this post\n\n#### You can't build a moat with AI\n\ngeneratingconversation.substack.com\n\n#### Discover more from Generating Conversation\n\nThe latest in generative AI & LLMs across research and industry.\n\nOver 1,000 subscribers\n\nContinue reading\n\nSign in\n\n# You can't build a moat with AI\n\n### It's all about the data\n\nVikram Sreekanti\n\nand\n\nJoseph E. Gonzalez\n\nApr 11, 2024\n\nShare this post\n\n#### You can't build a moat with AI\n\ngeneratingconversation.substack.com\n\nShare\n\nDifferentiating AI applications is a hot topic, and it\u2019s hard. Is everything\njust a RAG application? (No.) If a company gets good enough at RAG in one\ndomain, does it neatly transfer to another domain? (Maybe! We\u2019re not sure.)\nAre all AI applications just going to be built by OpenAI? (Probably not.)\n\nManaging technical and go-to-market differentiation is an eternal question for\nstartups. Some believe it\u2019s all in the technology, and others believe it\u2019s all\nin the execution. Whatever the case may be, it\u2019s become clear to us that\nthere\u2019s not very much you can do to differentiate yourself using just LLMs1.\nThe real differentiation lies in your data you feed into your models.\n\nSource: DALL-E 3.\n\nThis might sound surprising at first; after all, it\u2019s the LLMs that have\nkicked off the current AI hype cycle. If they don\u2019t matter, then... what are\nwe all doing here?\n\nLLMs are obviously incredibly powerful, but we believe the core models and\ntheir use will effectively be commoditized across competitive products in the\nsame space. In that world, having a clever use of LLMs (especially if you\u2019re\nrelying on a single LLM call!) is simply not going to cut it. Instead, you\u2019ll\nneed to think carefully about what data you\u2019re using to build your\napplication. Here\u2019s why:\n\nEveryone\u2019s using the same model. Regardless of what you believe the best LLM\nis \u2014 we mostly think it\u2019s GPT-4 \u2014 there\u2019s probably a model that outperforms\nits competitors for each major category of task. If you had to bet, that model\nis probably still GPT-4 for most use cases, but Claude is catching up quickly.\nEveryone today can easily switch between models and experiment with what works\nbest today for their application. That means that whatever you\u2019re building,\nthe advantage you get by picking the right model is small.\n\nDeviating from the norm is also dangerous \u2014 it might get you unexpectedly good\nanswers in some cases and unexpectedly bad answers in other cases. When a lot\nof AI startups are working to build trust, that\u2019s a risky proposition, and as\na result, most teams default back to using the safe models. That means that\nwhat you put into the model matters more than ever.\n\nAnd your prompts aren\u2019t IP. It might feel like your applications\u2019 prompts or\nprompt templates are a good form of differentiation. After all, your top-notch\nengineering team has invested days into tuning them to have the right response\ncharacteristics, tone, and output style. Of course, giving your competitors\nyour prompts would probably accelerate their progress, but any good\nengineering team will figure out the right changes quickly. The main reason is\nthat the experimentation (with the right evaluation data!) is quick and easy \u2014\ntrying a new prompt template isn\u2019t much harder than writing it out. All it\nreally takes is a little bit of patience, some creativity, and extra Azure\nOpenAI credits.\n\nYour prompts might be better today, but we can almost guarantee you that\nadvantage won\u2019t last. You\u2019ll have to look elsewhere.\n\nSo (as always) it comes down to the data. If the models are commoditized, and\nthe prompts are commoditized, all that\u2019s left to differentiate your AI\napplication is the data you feed into your LLMs. Thankfully, the data makes a\nhuge difference. Any application you build \u2014 especially if you\u2019re working with\nenterprises \u2014 is going require using your customers\u2019 data to drive your\napplications\u2019 responses and decisions. Every team is going to have slightly\ndifferent data and slightly different preferences about how it\u2019s used.\n\nThe nice thing about LLMs is that the they do well with arbitrary,\nunstructured data. The catch is that your customers have a ton of arbitrary,\nunstructured data. Some of it will be absolute gold; some of it will be\ncompletely useless. As an application builder, it\u2019s on you to figure out how\nto use that data.\n\nAnd of course... garbage in, garbage out. LLMs as with any AI model (or any\nsystem?) are garbage in, garbage out. That puts a huge burden on your ability\nto find and use the right data. For reference, our team at RunLLM has spent\nroughly 70% of our engineering cycles in the last quarter on data engineering\n\u2014 everything from pre-processing data at ingestion time to implementing hybrid\nsearch to reranking results. Whenever we get a customer complaint or request,\nthe first solution has typically been to improve the data engineering process.\n\nThe result is a fairly complex data pipeline that can account for customer\npriorities, process a wide variety of data types, and generate metadata for\ndownstream complex reasoning tasks. It certainly isn\u2019t something that can be\nreproduced with a little bit of experimentation. It\u2019s directly driven by\ngetting hands-on customer feedback. The result is what\u2019s allowed us to\nconsistently differentiate ourselves with straightforward, grounded responses,\nand minimal hallucinations.\n\nWe firmly believe the moat for AI application is in the data and the data\nengineering today. At some point, the process of building custom LLMs might\nget so fast and easy that we\u2019ll all return to building our own models. That\nsimply isn\u2019t the case today.\n\nWhat that also means is that you don\u2019t need to be an AI genius to succeed in\nbuilding applications. In fact, focusing on the AI might be a detriment to\nyour differentiation at a certain point. With thoughtful software engineering\nand a focus on customer data, you\u2019ll build a moat over time.\n\nWe firmly believe the moat for AI application is in the data and the data\nengineering today. At some point, the process of building custom LLMs might\nget so fast and easy that we\u2019ll all return to building our own models. That\nsimply isn\u2019t the case today.\n\nWhat that also means is that you don\u2019t need to be an AI genius to succeed in\nbuilding applications today. In fact, focusing on the AI might be a detriment\nto your differentiation at a certain point. With thoughtful software\nengineering and a focus on customer data, you\u2019ll build a moat over time.\n\n1\n\nThis, of course, doesn\u2019t apply to model providers; those companies have plenty\nof room to play with different kinds of expertise or functionality within\ntheir models. If things play out the way we expect, we\u2019ll likely see each\nmodel provider excel at certain skills.\n\n### Subscribe to Generating Conversation\n\nLaunched 8 months ago\n\nThe latest in generative AI & LLMs across research and industry.\n\n5 Likes\n\nShare this post\n\n#### You can't build a moat with AI\n\ngeneratingconversation.substack.com\n\nShare\n\nComments\n\nOpenAI is too cheap to beat\n\nData matters; infrastructure matters more\n\nOct 12, 2023 \u2022\n\nVikram Sreekanti\n\nand\n\nJoseph E. Gonzalez\n\n46\n\nShare this post\n\n#### OpenAI is too cheap to beat\n\ngeneratingconversation.substack.com\n\n15\n\nThe Easiest Part of LLM Applications is the LLM\n\nLLMs have brought thousands of developers into the machine learning world. The\nmodels themselves are, of course, impressive and key to the explosion of...\n\nAug 24, 2023 \u2022\n\nJoseph E. Gonzalez\n\nand\n\nVikram Sreekanti\n\n22\n\nShare this post\n\n#### The Easiest Part of LLM Applications is the LLM\n\ngeneratingconversation.substack.com\n\n2\n\nAn introduction to evaluating LLMs\n\nAs more and more LLMs have been released over the last 6 months, comparing\nmodel quality has become a favorite pastime. We each have personal...\n\nJan 25 \u2022\n\nVikram Sreekanti\n\nand\n\nJoseph E. Gonzalez\n\n8\n\nShare this post\n\n#### An introduction to evaluating LLMs\n\ngeneratingconversation.substack.com\n\nReady for more?\n\n\u00a9 2024 Joseph E. Gonzalez\n\nPrivacy \u2219 Terms \u2219 Collection notice\n\nStart WritingGet the app\n\nSubstack is the home for great writing\n\nShare\n\n## Create your profile\n\n## Only paid subscribers can comment on this post\n\nAlready a paid subscriber? Sign in\n\n#### Check your email\n\nFor your security, we need to re-authenticate you.\n\nClick the link we sent to , or click here to sign in.\n\n", "frontpage": true}
