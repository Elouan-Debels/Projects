{"aid": "39981623", "title": "ScreenAI: A visual LLM for UI and visually-situated language understanding", "url": "https://research.google/blog/screenai-a-visual-language-model-for-ui-and-visually-situated-language-understanding/", "domain": "research.google", "votes": 28, "user": "gfortaine", "posted_at": "2024-04-09 17:15:53", "comments": 2, "source_title": "ScreenAI: A visual language model for UI and visually-situated language understanding", "source_text": "ScreenAI: A visual language model for UI and visually-situated language\nunderstanding\n\nresearch.google uses cookies from Google to deliver and enhance the quality of\nits services and to analyze traffic. Learn more.\n\nJump to Content\n\nResearch\n\nResearch\n\n# ScreenAI: A visual language model for UI and visually-situated language\nunderstanding\n\nMarch 19, 2024\n\nPosted by Srinivas Sunkara and Gilles Baechler, Software Engineers, Google\nResearch\n\nWe introduce ScreenAI, a vision-language model for user interfaces and\ninfographics that achieves state-of-the-art results on UI and infographics-\nbased tasks. We are also releasing three new datasets: Screen Annotation to\nevaluate the layout understanding capability of the model, as well as ScreenQA\nShort and Complex ScreenQA for a more comprehensive evaluation of its QA\ncapability.\n\n## Quick links\n\n  * Paper\n  *     * \u00d7\n\nScreen user interfaces (UIs) and infographics, such as charts, diagrams and\ntables, play important roles in human communication and human-machine\ninteraction as they facilitate rich and interactive user experiences. UIs and\ninfographics share similar design principles and visual language (e.g., icons\nand layouts), that offer an opportunity to build a single model that can\nunderstand, reason, and interact with these interfaces. However, because of\ntheir complexity and varied presentation formats, infographics and UIs present\na unique modeling challenge.\n\nTo that end, we introduce \u201cScreenAI: A Vision-Language Model for UI and\nInfographics Understanding\u201d. ScreenAI improves upon the PaLI architecture with\nthe flexible patching strategy from pix2struct. We train ScreenAI on a unique\nmixture of datasets and tasks, including a novel Screen Annotation task that\nrequires the model to identify UI element information (i.e., type, location\nand description) on a screen. These text annotations provide large language\nmodels (LLMs) with screen descriptions, enabling them to automatically\ngenerate question-answering (QA), UI navigation, and summarization training\ndatasets at scale. At only 5B parameters, ScreenAI achieves state-of-the-art\nresults on UI- and infographic-based tasks (WebSRC and MoTIF), and best-in-\nclass performance on Chart QA, DocVQA, and InfographicVQA compared to models\nof similar size. We are also releasing three new datasets: Screen Annotation\nto evaluate the layout understanding capability of the model, as well as\nScreenQA Short and Complex ScreenQA for a more comprehensive evaluation of its\nQA capability.\n\n## ScreenAI\n\nScreenAI\u2019s architecture is based on PaLI, composed of a multimodal encoder\nblock and an autoregressive decoder. The PaLI encoder uses a vision\ntransformer (ViT) that creates image embeddings and a multimodal encoder that\ntakes the concatenation of the image and text embeddings as input. This\nflexible architecture allows ScreenAI to solve vision tasks that can be recast\nas text+image-to-text problems.\n\nOn top of the PaLI architecture, we employ a flexible patching strategy\nintroduced in pix2struct. Instead of using a fixed-grid pattern, the grid\ndimensions are selected such that they preserve the native aspect ratio of the\ninput image. This enables ScreenAI to work well across images of various\naspect ratios.\n\nThe ScreenAI model is trained in two stages: a pre-training stage followed by\na fine-tuning stage. First, self-supervised learning is applied to\nautomatically generate data labels, which are then used to train ViT and the\nlanguage model. ViT is frozen during the fine-tuning stage, where most data\nused is manually labeled by human raters.\n\nplay silent looping video pause silent looping video\n\nScreenAI model architecture.\n\n## Data generation\n\nTo create a pre-training dataset for ScreenAI, we first compile an extensive\ncollection of screenshots from various devices, including desktops, mobile,\nand tablets. This is achieved by using publicly accessible web pages and\nfollowing the programmatic exploration approach used for the RICO dataset for\nmobile apps. We then apply a layout annotator, based on the DETR model, that\nidentifies and labels a wide range of UI elements (e.g., image, pictogram,\nbutton, text) and their spatial relationships. Pictograms undergo further\nanalysis using an icon classifier capable of distinguishing 77 different icon\ntypes. This detailed classification is essential for interpreting the subtle\ninformation conveyed through icons. For icons that are not covered by the\nclassifier, and for infographics and images, we use the PaLI image captioning\nmodel to generate descriptive captions that provide contextual information. We\nalso apply an optical character recognition (OCR) engine to extract and\nannotate textual content on screen. We combine the OCR text with the previous\nannotations to create a detailed description of each screen.\n\n### LLM-based data generation\n\nWe enhance the pre-training data's diversity using PaLM 2 to generate input-\noutput pairs in a two-step process. First, screen annotations are generated\nusing the technique outlined above, then we craft a prompt around this schema\nfor the LLM to create synthetic data. This process requires prompt engineering\nand iterative refinement to find an effective prompt. We assess the generated\ndata's quality through human validation against a quality threshold.\n\nYou only speak JSON. Do not write text that isn\u2019t JSON. You are given the\nfollowing mobile screenshot, described in words. Can you generate 5 questions\nregarding the content of the screenshot as well as the corresponding short\nanswers to them?\n\nThe answer should be as short as possible, containing only the necessary\ninformation. Your answer should be structured as follows: questions: [\n{{question: the question, answer: the answer }}, ... ]\n\n{THE SCREEN SCHEMA}\n\nA sample prompt for QA data generation.\n\nBy combining the natural language capabilities of LLMs with a structured\nschema, we simulate a wide range of user interactions and scenarios to\ngenerate synthetic, realistic tasks. In particular, we generate three\ncategories of tasks:\n\n  * Question answering: The model is asked to answer questions regarding the content of the screenshots, e.g., \u201cWhen does the restaurant open?\u201d\n  * Screen navigation: The model is asked to convert a natural language utterance into an executable action on a screen, e.g., \u201cClick the search button.\u201d\n  * Screen summarization: The model is asked to summarize the screen content in one or two sentences.\n\n## Experiments and results\n\nAs previously mentioned, ScreenAI is trained in two stages: pre-training and\nfine-tuning. Pre-training data labels are obtained using self-supervised\nlearning and fine-tuning data labels comes from human raters.\n\nWe fine-tune ScreenAI using public QA, summarization, and navigation datasets\nand a variety of tasks related to UIs. For QA, we use well established\nbenchmarks in the multimodal and document understanding field, such as\nChartQA, DocVQA, Multi page DocVQA, InfographicVQA, OCR VQA, Web SRC and\nScreenQA. For navigation, datasets used include Referring Expressions, MoTIF,\nMug, and Android in the Wild. Finally, we use Screen2Words for screen\nsummarization. Along with the fine-tuning datasets, we evaluate the fine-tuned\nScreenAI model using three novel benchmarks:\n\n  1. Screen Annotation: Enables the evaluation model layout annotations and spatial understanding capabilities.\n  2. ScreenQA Short: A variation of ScreenQA, where its ground truth answers have been shortened to contain only the relevant information that better aligns with other QA tasks.\n  3. Complex ScreenQA: Complements ScreenQA Short with more difficult questions (counting, arithmetic, comparison, and non-answerable questions) and contains screens with various aspect ratios.\n\nThe fine-tuned ScreenAI model achieves state-of-the-art results on various UI\nand infographic-based tasks (WebSRC and MoTIF) and best-in-class performance\non Chart QA, DocVQA, and InfographicVQA compared to models of similar size.\nScreenAI achieves competitive performance on Screen2Words and OCR-VQA.\nAdditionally, we report results on the new benchmark datasets introduced to\nserve as a baseline for further research.\n\nNext, we examine ScreenAI\u2019s scaling capabilities and observe that across all\ntasks, increasing the model size improves performances and the improvements\nhave not saturated at the largest size.\n\n## Conclusion\n\nWe introduce the ScreenAI model along with a unified representation that\nenables us to develop self-supervised learning tasks leveraging data from all\nthese domains. We also illustrate the impact of data generation using LLMs and\ninvestigate improving model performance on specific aspects with modifying the\ntraining mixture. We apply all of these techniques to build multi-task trained\nmodels that perform competitively with state-of-the-art approaches on a number\nof public benchmarks. However, we also note that our approach still lags\nbehind large models and further research is needed to bridge this gap.\n\n## Acknowledgements\n\nThis project is the result of joint work with Maria Wang, Fedir Zubach, Hassan\nMansoor, Vincent Etter, Victor Carbune, Jason Lin, Jindong Chen and Abhanshu\nSharma. We thank Fangyu Liu, Xi Chen, Efi Kokiopoulou, Jesse Berent, Gabriel\nBarcik, Lukas Zilka, Oriana Riva, Gang Li,Yang Li, Radu Soricut, and Tania\nBedrax-Weiss for their insightful feedback and discussions, along with Rahul\nAralikatte, Hao Cheng and Daniel Kim for their support in data preparation. We\nalso thank Jay Yagnik, Blaise Aguera y Arcas, Ewa Dominowska, David Petrou,\nand Matt Sharifi for their leadership, vision and support. We are very\ngrateful toTom Small for helping us create the animation in this post.\n\nLabels:\n\n  * Human-Computer Interaction and Visualization\n\n  * Machine Intelligence\n\n## Quick links\n\n  * Paper\n  *     * \u00d7\n\n### Other posts of interest\n\n  * March 28, 2024\n\nAutoBNN: Probabilistic time series forecasting with compositional bayesian\nneural networks\n\n    * Algorithms & Theory \u00b7\n    * Machine Intelligence \u00b7\n    * Open Source Models & Datasets\n\n  * March 20, 2024\n\nComputer-aided diagnosis for lung cancer screening\n\n    * Health & Bioscience \u00b7\n    * Human-Computer Interaction and Visualization \u00b7\n    * Machine Intelligence\n\n  * March 20, 2024\n\nComputer-aided diagnosis for lung cancer screening\n\n    * Health & Bioscience \u00b7\n    * Human-Computer Interaction and Visualization \u00b7\n    * Machine Intelligence\n\nFollow us\n\n", "frontpage": true}
