{"aid": "39981640", "title": "Efficient Transformers: A Survey", "url": "https://dl.acm.org/doi/full/10.1145/3530811", "domain": "acm.org", "votes": 2, "user": "teleforce", "posted_at": "2024-04-09 17:16:43", "comments": 0, "source_title": "Efficient Transformers: A Survey | ACM Computing Surveys", "source_text": "Efficient Transformers: A Survey | ACM Computing Surveys\n\n## This website uses cookies\n\nWe occasionally run membership recruitment campaigns on social media channels\nand use cookies to track post-clicks. We also share information about your use\nof our site with our social media, advertising and analytics partners who may\ncombine it with other information that you\u2019ve provided to them or that they\u2019ve\ncollected from your use of their services. Use the check boxes below to choose\nthe types of cookies you consent to have stored on your device.\n\nUse necessary cookies only Allow all cookies Show details\n\nOK\n\nUse necessary cookies only Allow selected cookies Allow all cookies\n\nShow details\n\nCookie declaration [#IABV2SETTINGS#] About\n\nNecessary (7) Preferences (5) Statistics (16) Marketing (24) Unclassified (0)\n\nNecessary cookies help make a website usable by enabling basic functions like\npage navigation and access to secure areas of the website. The website cannot\nfunction properly without these cookies. These cookies do not gather\ninformation about you that could be used for marketing purposes and do not\nremember where you have been on the internet.\n\nName| Provider| Purpose| Expiry| Type  \n---|---|---|---|---  \n__jid| c.disquscdn.com| Used to add comments to the website and remember the\nuser's Disqus login credentials across websites that use said service.|\nSession| HTTP  \ndisqusauth| c.disquscdn.com| Registers whether the user is logged in. This\nallows the website owner to make parts of the website inaccessible, based on\nthe user's log-in status.| Session| HTTP  \n__cf_bm| ACM| This cookie is used to distinguish between humans and bots. This\nis beneficial for the website, in order to make valid reports on the use of\ntheir website.| 1 day| HTTP  \n_cfuvid| ACM| This cookie is a part of the services provided by Cloudflare -\nIncluding load-balancing, deliverance of website content and serving DNS\nconnection for website operators.| Session| HTTP  \nCookieConsent| Cookiebot| Stores the user's cookie consent state for the\ncurrent domain| 1 year| HTTP  \n1.gif| Cookiebot| Used to count the number of sessions to the website,\nnecessary for optimizing CMP product delivery.| Session| Pixel  \nVISITOR_PRIVACY_METADATA| YouTube| Stores the user's cookie consent state for\nthe current domain| 180 days| HTTP  \n  \nPreference cookies enable a website to remember information that changes the\nway the website behaves or looks, like your preferred language or the region\nthat you are in.\n\nName| Provider| Purpose| Expiry| Type  \n---|---|---|---|---  \naet-dismiss| c.disquscdn.com| Necessary for the functionality of the website's\ncomment-system.| Persistent| HTML  \ndrafts.queue| c.disquscdn.com| Necessary for the functionality of the\nwebsite's comment-system.| Persistent| HTML  \nsubmitted_posts_cache| c.disquscdn.com| Necessary for the functionality of the\nwebsite's comment-system.| Persistent| HTML  \nmopDeploy| Mopinion| Pending| Session| HTML  \nMACHINE_LAST_SEEN| ACM| Pending| 300 days| HTTP  \n  \nStatistic cookies help website owners understand how visitors interact with\nwebsites by collecting and reporting information anonymously.\n\nName| Provider| Purpose| Expiry| Type  \n---|---|---|---|---  \n_ga| Google| Registers a unique ID that is used to generate statistical data\non how the visitor uses the website.| 2 years| HTTP  \n_ga_#| Google| Used by Google Analytics to collect data on the number of times\na user has visited the website as well as dates for the first and most recent\nvisit.| 2 years| HTTP  \n_gat| Google| Used by Google Analytics to throttle request rate| 1 day| HTTP  \n_gid| Google| Registers a unique ID that is used to generate statistical data\non how the visitor uses the website.| 1 day| HTTP  \n_hjSession_#| Hotjar| Collects statistics on the visitor's visits to the\nwebsite, such as the number of visits, average time spent on the website and\nwhat pages have been read.| 1 day| HTTP  \n_hjSessionUser_#| Hotjar| Collects statistics on the visitor's visits to the\nwebsite, such as the number of visits, average time spent on the website and\nwhat pages have been read.| 1 year| HTTP  \n_hjTLDTest| Hotjar| Registers statistical data on users' behaviour on the\nwebsite. Used for internal analytics by the website operator.| Session| HTTP  \n_hp2_#| Heap Analytics| Collects data on the user\u2019s navigation and behavior on\nthe website. This is used to compile statistical reports and heatmaps for the\nwebsite owner.| 1 day| HTTP  \n_hp2_hld#.#| Heap Analytics| Collects data on the user\u2019s navigation and\nbehavior on the website. This is used to compile statistical reports and\nheatmaps for the website owner.| 1 day| HTTP  \n_hp2_id.#| Heap Analytics| Collects data on the user\u2019s navigation and behavior\non the website. This is used to compile statistical reports and heatmaps for\nthe website owner.| 1 year| HTTP  \n_hp2_ses_props.#| Heap Analytics| Collects data on the user\u2019s navigation and\nbehavior on the website. This is used to compile statistical reports and\nheatmaps for the website owner.| 1 day| HTTP  \ndisqus_unique| c.disquscdn.com| Collects statistics related to the user's\nvisits to the website, such as number of visits, average time spent on the\nwebsite and loaded pages.| Session| HTTP  \ncollect| Google| Used to send data to Google Analytics about the visitor's\ndevice and behavior. Tracks the visitor across devices and marketing\nchannels.| Session| Pixel  \nhjActiveViewportIds| Hotjar| This cookie contains an ID string on the current\nsession. This contains non-personal information on what subpages the visitor\nenters \u2013 this information is used to optimize the visitor's experience.|\nPersistent| HTML  \nhjViewportId| Hotjar| Saves the user's screen size in order to adjust the size\nof images on the website.| Session| HTML  \ntd| Google| Registers statistical data on users' behaviour on the website.\nUsed for internal analytics by the website operator.| Session| Pixel  \n  \nMarketing cookies are used to track visitors across websites. The intention is\nto display ads that are relevant and engaging for the individual user and\nthereby more valuable for publishers and third party advertisers.\n\nName| Provider| Purpose| Expiry| Type  \n---|---|---|---|---  \nbadges-message| c.disquscdn.com| Collects data on the visitor\u2019s use of the\ncomment system on the website, and what blogs/articles the visitor has read.\nThis can be used for marketing purposes.| Persistent| HTML  \napi/telemetry| Heap Analytics| Collects data on user behaviour and interaction\nin order to optimize the website and make advertisement on the website more\nrelevant.| Session| Pixel  \nh| Heap Analytics| Collects data on user behaviour and interaction in order to\noptimize the website and make advertisement on the website more relevant.|\nSession| Pixel  \n#-#| YouTube| Pending| Session| HTML  \niU5q-!O9@$| YouTube| Registers a unique ID to keep statistics of what videos\nfrom YouTube the user has seen.| Session| HTML  \nLAST_RESULT_ENTRY_KEY| YouTube| Used to track user\u2019s interaction with embedded\ncontent.| Session| HTTP  \nLogsDatabaseV2:V#||LogsRequestsStore| YouTube| Pending| Persistent| IDB  \nnextId| YouTube| Used to track user\u2019s interaction with embedded content.|\nSession| HTTP  \nremote_sid| YouTube| Necessary for the implementation and functionality of\nYouTube video-content on the website.| Session| HTTP  \nrequests| YouTube| Used to track user\u2019s interaction with embedded content.|\nSession| HTTP  \nServiceWorkerLogsDatabase#SWHealthLog| YouTube| Necessary for the\nimplementation and functionality of YouTube video-content on the website.|\nPersistent| IDB  \nTESTCOOKIESENABLED| YouTube| Used to track user\u2019s interaction with embedded\ncontent.| 1 day| HTTP  \nVISITOR_INFO1_LIVE| YouTube| Pending| 180 days| HTTP  \nYSC| YouTube| Pending| Session| HTTP  \nyt.innertube::nextId| YouTube| Registers a unique ID to keep statistics of\nwhat videos from YouTube the user has seen.| Persistent| HTML  \nytidb::LAST_RESULT_ENTRY_KEY| YouTube| Used to track user\u2019s interaction with\nembedded content.| Persistent| HTML  \nYtIdbMeta#databases| YouTube| Used to track user\u2019s interaction with embedded\ncontent.| Persistent| IDB  \nyt-remote-cast-available| YouTube| Stores the user's video player preferences\nusing embedded YouTube video| Session| HTML  \nyt-remote-cast-installed| YouTube| Stores the user's video player preferences\nusing embedded YouTube video| Session| HTML  \nyt-remote-connected-devices| YouTube| Stores the user's video player\npreferences using embedded YouTube video| Persistent| HTML  \nyt-remote-device-id| YouTube| Stores the user's video player preferences using\nembedded YouTube video| Persistent| HTML  \nyt-remote-fast-check-period| YouTube| Stores the user's video player\npreferences using embedded YouTube video| Session| HTML  \nyt-remote-session-app| YouTube| Stores the user's video player preferences\nusing embedded YouTube video| Session| HTML  \nyt-remote-session-name| YouTube| Stores the user's video player preferences\nusing embedded YouTube video| Session| HTML  \n  \nUnclassified cookies are cookies that we are in the process of classifying,\ntogether with the providers of individual cookies.\n\nWe do not use cookies of this type.  \n---  \n  \n[#IABV2_LABEL_PURPOSES#] [#IABV2_LABEL_FEATURES#] [#IABV2_LABEL_PARTNERS#]\n\n[#IABV2_BODY_PURPOSES#]\n\n[#IABV2_BODY_FEATURES#]\n\n[#IABV2_BODY_PARTNERS#]\n\nCookies are small text files that can be used by websites to make a user's\nexperience more efficient. Other than those strictly necessary for the\noperation of the site, we need your permission to store any type of cookies on\nyour device. Learn more about ACM, how you can contact us, and how we process\npersonal data in our Privacy Policy. Also please consult our Cookie Notice.\n\nYou can change or withdraw your consent from the Cookie Declaration on our\nwebsite at any time by visiting the Cookie Declaration page. If contacting us\nregarding your consent, please state your consent ID and date from that page.\n\nYour consent applies to the following domains: dl.acm.org\n\nCookie declaration last updated on 3/31/24 by Cookiebot\n\nskip to main content\n\n  * Advanced Search\n  * Browse\n  * About\n  *     * Sign in\n    * Register\n\nAdvanced Search\n\nACM Computing Surveys\n\nsurvey\n\nOpen Access\n\nShare on\n\n# Efficient Transformers: A Survey\n\n  * Authors:\n  * Yi Tay\n\nGoogle Research, USA\n\nGoogle Research, USA\n\n0000-0001-6896-4496\n\nView Profile\n\n,\n\n  * Mostafa Dehghani\n\nGoogle Research, Amsterdam\n\nGoogle Research, Amsterdam\n\n0000-0002-9772-1095\n\nView Profile\n\n,\n\n  * Dara Bahri\n\nGoogle Research, USA\n\nGoogle Research, USA\n\n0000-0003-0144-2911\n\nView Profile\n\n,\n\n  * Donald Metzler\n\nGoogle Research, USA\n\nGoogle Research, USA\n\n0000-0003-4276-6269\n\nView Profile\n\nAuthors Info & Claims\n\nACM Computing SurveysVolume 55Issue 6Article No.: 109pp\n1\u201328https://doi.org/10.1145/3530811\n\nPublished:07 December 2022Publication History\n\n  * 62citation\n  * 29,219\n  * Downloads\n\nMetrics\n\nTotal Citations62\n\nTotal Downloads29,219\n\nLast 12 Months17,410\n\nLast 6 weeks2,467\n\n  * Get Citation Alerts\n\n## New Citation Alert added!\n\nThis alert has been successfully added and will be sent to:\n\nYou will be notified whenever a record that you have chosen has been cited.\n\nTo manage your alert preferences, click on the button below.\n\nManage my Alerts\n\n## New Citation Alert!\n\nPlease log in to your account\n\n  * Publisher Site\n\n  * PDF\n\n## ACM Computing Surveys\n\nVolume 55, Issue 6\n\nPreviousArticleNextArticle\n\nSkip Abstract Section\n\n## Abstract\n\nTransformer model architectures have garnered immense interest lately due to\ntheir effectiveness across a range of domains like language, vision, and\nreinforcement learning. In the field of natural language processing for\nexample, Transformers have become an indispensable staple in the modern deep\nlearning stack. Recently, a dizzying number of \u201cX-former\u201d models have been\nproposed\u2014Reformer, Linformer, Performer, Longformer, to name a few\u2014which\nimprove upon the original Transformer architecture, many of which make\nimprovements around computational and memory efficiency. With the aim of\nhelping the avid researcher navigate this flurry, this article characterizes a\nlarge and thoughtful selection of recent efficiency-flavored \u201cX-former\u201d\nmodels, providing an organized and comprehensive overview of existing work and\nmodels across multiple domains.\n\nSkip 1INTRODUCTION Section\n\n## 1 INTRODUCTION\n\nTransformers [82] are a formidable force in the modern deep learning stack.\nTransformers are pervasive and have made tremendous impact in many fields such\nas language understanding [6, 12, 20, 60], image processing [7, 55] and\ninformation retrieval [77]. As such, it is only natural that a wealth of\nresearch has been dedicated to making fundamental improvements to the model\nover the past few years [2, 18, 68]. This immense interest has also spurred\nresearch into more efficient variants of the model [5, 11, 14, 15, 36, 37, 59,\n62, 70, 74, 84, 87].\n\nThere has been such a surge of Transformer model variants proposed recently\nthat researchers and practitioners alike may find it challenging to keep pace\nwith the rate of innovation. As of this writing and this manuscript\u2019s first\ndraft (circa August 2020), there have been nearly a dozen new efficiency-\nfocused models proposed in just the past 6 months. Thus, a survey of the\nexisting literature is both beneficial for the community and quite timely.\n\nThe self-attention mechanism is a key defining characteristic of Transformer\nmodels. The mechanism can be viewed as a graph-like inductive bias that\nconnects all tokens in a sequence with a relevance-based pooling operation. A\nwell-known concern with self-attention is the quadratic time and memory\ncomplexity, which can hinder model scalability in many settings. There has\nbeen an overwhelming influx of model variants proposed recently that address\nthis problem. We hereinafter name this class of models \u201cefficient\nTransformers\u201d.\n\nThe efficiency of a model can be interpreted in a variety of ways. It might\nrefer to the memory footprint of the model, which is of importance when the\nmemory of accelerators on which the model is running is limited. Efficiency\nmight also refer to computational costs, e.g., the number of FLOPs, both\nduring training and inference. In particular, for on-device applications,\nmodels often must operate within a highly constrained computational budget.\nThroughout this survey, we refer to the efficiency of Transformers both in\nterms of memory and computation. We are especially interested in how such\nmodels perform when they are applied to large inputs.\n\nEfficient self-attention models are crucial in applications that model long\nsequences. For example, documents, images, and videos are all often composed\nof a relatively large number of pixels or tokens. Efficiency in processing\nlong sequences is therefore paramount for widespread adoption of Transformers.\n\nThis survey sets out to provide a comprehensive overview of the recent\nadvances made in this class of models. We are primarily interested in modeling\nadvances and architectural innovations that improve the general efficiency of\nTransformers, including but not limited to tackling the quadratic complexity\nissue of the self-attention mechanism or reducing the computation costs by\nmeans such as pooling and/or sparsity. We also briefly discuss general\nimprovements and other efficiency improvements such as parameter sharing.\n\nWe propose a taxonomy of efficient Transformer models, characterizing them by\ntheir technical innovation and primary use case. Specifically, we review\nTransformer models that have applications in both language and vision domains,\nattempting to consolidate the literature across the spectrum. We also provide\na detailed walk-through of many of these models and draw connections between\nthem.\n\nSkip 2BACKGROUND ON TRANSFORMERS Section\n\n## 2 BACKGROUND ON TRANSFORMERS\n\nThis section provides an overview of the well-established Transformer\narchitecture [82]. Transformers are multi-layered architectures formed by\nstacking Transformer blocks on top of one another.\n\nTransformer blocks are characterized by a multi-head self-attention mechanism,\na position-wise feed-forward network, layer normalization [4] modules and\nresidual connectors. The input to the Transformer model is often a tensor of\nshape \\\\( \\mathbb {R}^B \\times \\mathbb {R}^N \\\\), where \\\\( B \\\\) is the batch\nsize, \\\\( N \\\\) the sequence length.\n\nThe input first passes through an embedding layer that converts each one-hot\ntoken representation into a \\\\( d_{model} \\\\) dimensional embedding, i.e., \\\\(\n\\mathbb {R}^B \\times \\mathbb {R}^N \\times \\mathbb {R}^{d_{model}} \\\\). The new\ntensor is then additively composed with positional encodings and passed\nthrough a multi-headed self-attention module. Positional encodings can take\nthe form of a sinusoidal input (as per [82]) or be trainable embeddings.\n\nThe inputs and output of the multi-headed self-attention module are connected\nby residual connectors and a layer normalization layer. The output of the\nmulti-headed self-attention module is then passed to a two-layered feed-\nforward network which has its inputs/outputs similarly connected in a residual\nfashion with layer normalization. The sub-layer residual connectors with layer\nnorm is expressed as: \\\\( \\begin{align*} X = \\text{LayerNorm}(F_S(X)) + X,\n\\end{align*} \\\\) where \\\\( F_S \\\\) is the sub-layer module which is either the\nmulti-headed self-attention or the position-wise feed-forward layers.\n\n### 2.1 Multi-Head Self-Attention\n\nThe Transformer model leverages a multi-head self-attention mechanism. The key\nidea behind the mechanism is for each element in the sequence to learn to\ngather from other tokens in the sequence. The operation for a single head is\ndefined as: \\\\( \\begin{align*} A_h = \\text{Softmax}(\\alpha Q_hK_h^\\top)V_h,\n\\end{align*} \\\\) where \\\\( X \\\\) is a matrix in \\\\( \\mathbb {R}^{N \\times d}\n\\\\), \\\\( \\alpha \\\\) is a scaling factor that is typically set to \\\\(\n\\frac{1}{\\sqrt {d}} \\\\), \\\\( Q_h=X\\mathbf {W}_q, K_h=X\\mathbf {W}_k \\\\) and\n\\\\( V_h=X\\mathbf {W}_v \\\\) are linear transformations applied on the temporal\ndimension of the input sequence, and \\\\( \\mathbf {W}_q, \\mathbf {W}_k, \\mathbf\n{W}_v \\in \\mathbb {R}^{d \\times \\frac{d}{H}} \\\\) are the weight matrices\n(parameters) for the query, key, and value projections that project the input\n\\\\( X \\\\) to an output tensor of \\\\( d \\\\) dimensions, and \\\\( N_H \\\\) is the\nnumber of heads. Softmax is applied row-wise.\n\nThe outputs of heads \\\\( A_1 \\cdots A_{H} \\\\) are concatenated together and\npassed into a dense layer. The output \\\\( Y \\\\) can thus be expressed as \\\\( Y\n= \\mathbf {W}_o[A_1 \\cdots A_{H}] \\\\), where \\\\( \\mathbf {W}_{o} \\\\) is an\noutput linear projection. Note that the computation of \\\\( A \\\\) is typically\ndone in a parallel fashion by considering tensors of \\\\( \\mathbb {R}^B \\times\n\\mathbb {R}^N \\times \\mathbb {R}^{H} \\times \\mathbb {R}^{\\frac{d}{H}} \\\\) and\ncomputing the linear transforms for all heads in parallel.\n\nThe attention matrix \\\\( A=QK^\\top \\\\) is chiefly responsible for learning\nalignment scores between tokens in the sequence. In this formulation, the dot\nproduct between each element/token in the query (\\\\( Q \\\\)) and key (\\\\( K\n\\\\)) is taken. This drives the self-alignment process in self-attention\nwhereby tokens learn to gather from each other.\n\n### 2.2 Position-Wise Feed-Forward Layers\n\nThe outputs of the self-attention module are then passed into a two-layered\nfeed-forward network with ReLU activations. This feed-forward layer operates\non each position independently. This is expressed as follows: \\\\(\n\\begin{align*} F_2(ReLU(F_1(X_A))), \\end{align*} \\\\) where \\\\( F_1 \\\\) and \\\\(\nF_2 \\\\) are feed-forward functions of the form \\\\( Wx +b \\\\).\n\n### 2.3 Putting It All Together\n\nEach Transformer block can be expressed as: \\\\( \\begin{align*} X_A &=\n\\text{LayerNorm}(\\text{MultiheadAttention}(X, X)) + X\\\\\\ X_B &=\n\\text{LayerNorm}(\\text{PositionFFN}(X_A)) + X_A \\end{align*} \\\\) where \\\\( X\n\\\\) is the input of the Transformer block and \\\\( X_B \\\\) is the output of the\nTransformer block. Note that the \\\\( \\text{MultiheadAttention}() \\\\) function\naccepts two argument tensors, one for query and the other for key-values. If\nthe first argument and second argument are the same input tensor, this is the\nMultiheadSelfAttention mechanism.\n\n### 2.4 On the Compute Cost of Transformers\n\nThe compute costs of Transformers is derived from multiple factors. First, the\nmemory and computational complexity required to compute the attention matrix\nis quadratic in the input sequence length, i.e., \\\\( N \\times N \\\\). In\nparticular, the \\\\( QK^\\top \\\\) matrix multiplication operation alone consumes\n\\\\( N^2 \\\\) time and memory. This restricts the overall utility of self-\nattentive models in applications which demand the processing of long\nsequences. Memory restrictions tend to be applicable more to training (due to\ngradient updates) and are generally of lesser impact on inference (no gradient\nupdates). The quadratic cost of self-attention impacts speed^1 in both\ntraining and inference. The compute costs of the self-attention mechanism\ncontributes partially to the overall compute cost of the Transformer. A non-\ntrivial amount of compute still stems from the two-layer feed-forward layers\nat every Transformer block (approximately half the compute time and/or FLOPs).\nThe complexity of the FFN is linear with respect to sequence length but is\ngenerally still costly. Hence, a large portion of recent work have explored\nsparsity [23, 45] as a means to scale up the FFN without incurring compute\ncosts. Efficient attention and efficient models are generally\northogonal\u2014although some efficient attention methods explicitly aim to reduce\nthe sequence length [15] and as a result also save computation costs in both\naspects. Efficiency and computational costs is generally a complicated affair\nand we would suggest readers peruse [17] for more details on trade-offs,\nintricacies, and the like.\n\n### 2.5 Transformer Mode\n\nIt is important to note the differences in how the Transformer blocks are\nused. Transformers can primarily be used in three ways, namely: (1) encoder-\nonly (e.g., for classification), (2) decoder-only (e.g., for language\nmodeling), and (3) encoder-decoder (e.g., for machine translation). In\nencoder-decoder mode, there are usually multiple multi-headed self-attention\nmodules, including a standard self-attention in both the encoder and the\ndecoder, along with an encoder-decoder cross-attention that allows the decoder\nto utilize information from the encoder. This influences the design of the\nself-attention mechanism. In the encoder mode, there is no restriction or\nconstraint that the self-attention mechanism has to be causal, i.e., dependent\nsolely on the present and past tokens. In the encoder-decoder setting, self-\nattention used in the decoder (i.e., across decoding positions) must be causal\nsince each auto-regressive decoding step can only depend on previous tokens,\nwhereas the self-attention used in the encoder need not. Fulfilling this\nrequirement can prove challenging for many efficient self-attention designs.\n\nThe mode of usage of a Transformer model generally depends on the target\napplication. Given an input sequence, the sequence is typically passed through\nan encoder stack. At this stage, there might be two options. For multi-class\nclassification, a linear layer with Softmax outputs typically projects the\nsequence representation down to the number of classes. In the case of BERT\n[20], this is a [CLS] token that is appended to the start of the sequence as a\nprefix. Recent work has also explored the usage of Encoder-Decoder\narchitectures for classification, such as T5 [60]. Decoder-only models are\ntypically used for generation and are trained using a language modeling\nobjective (of predicting the next token). Due to the nature of the loss, these\nmodels are often superior for open ended generation [6]. A decoder-only model\nneeds to be causal and a upper triangular mask needs to be applied to prevent\ntokens from peeping into the future. We refer interested readers to [60] for\nmore detailed descriptions of the various Transformer modes.\n\n### 2.6 Applications\n\nTransformers have a wide range of applications ranging from language to\nvision, speech, and reinforcement learning. It was initially introduced within\nthe context of sequence-to-sequence machine translation in NLP. Following\nwhich, most of the applications of Transformers have been within the context\nof language\u2014given the concurrent advance of pretrained models such as BERT\n[20]. Many early improvements to this line of efficient transformers is\ntherefore focused on language processing applications [3, 5]. For historical\nreasons, this article leans slightly towards language. However, it is also\nworth noting that a substantial amount of papers considered in our survey also\nconsiders multimodal applications whereby a sequence processor is required.\nFor example, Roy et al. [62], Choromanski et al. [11], Tay et al. [74], and\nChild et al. [9] consider generative modeling tasks on images or other\nmodalities such as proteins.\n\nSkip 3A SURVEY OF EFFICIENT TRANSFORMER MODELS Section\n\n## 3 A SURVEY OF EFFICIENT TRANSFORMER MODELS\n\nIn this section, we provide a high-level overview of efficient Transformer\nmodels. We begin by presenting a characterization of the different models.\nTable 1 lists the efficient Transformers released to date while Figure 2\npresents a graphical overview of several key efficient Transformer models.\n\nTable 1.\n\nModel/Article| Complexity| Decode| Class  \n---|---|---|---  \nMemory Compressed\\\\( ^\\dagger \\\\) [48]| \\\\( \\mathcal {O}(N_c^2) \\\\)| \u2713| FP+M  \nImage Transformer\\\\( ^\\dagger \\\\) [55]| \\\\( \\mathcal {O}(N.m) \\\\)| \u2713| FP  \nSet Transformer\\\\( ^\\dagger \\\\) [43]| \\\\( \\mathcal {O}(kN) \\\\)| \u2715| M  \nTransformer-XL\\\\( ^\\dagger \\\\) [16]| \\\\( \\mathcal {O}(N^2) \\\\)| \u2713| RC  \nSparse Transformer [9]| \\\\( \\mathcal {O}(N \\sqrt {N}) \\\\)| \u2713| FP  \nReformer\\\\( ^\\dagger \\\\) [37]| \\\\( \\mathcal {O}(N \\log N) \\\\)| \u2713| LP  \nRouting Transformer [62]| \\\\( \\mathcal {O}(N\\sqrt {N)} \\\\)| \u2713| LP  \nAxial Transformer [28]| \\\\( \\mathcal {O}(N \\sqrt {N}) \\\\)| \u2713| FP  \nCompressive Transformer\\\\( ^\\dagger \\\\) [59]| \\\\( \\mathcal {O}(N^2) \\\\)| \u2713| RC  \nSinkhorn Transformer\\\\( ^\\dagger \\\\) [74]| \\\\( \\mathcal {O}(B^2) \\\\)| \u2713| LP  \nLongformer [5]| \\\\( \\mathcal {O}(n(k+m)) \\\\)| \u2713| FP+M  \nETC [3]| \\\\( \\mathcal {O}(N_g^2 + N N_g) \\\\)| \u2715| FP+M  \nSynthesizer [73]| \\\\( \\mathcal {O}(N^2) \\\\)| \u2713| LR+LP  \nPerformer [10]| \\\\( \\mathcal {O}(N) \\\\)| \u2713| KR  \nFunnel Transformer [15]| \\\\( \\mathcal {O}(N^2) \\\\)| \u2713| FP+DS  \nLinformer [87]| \\\\( \\mathcal {O}(N) \\\\)| \u2715| LR  \nLinear Transformers\\\\( ^\\dagger \\\\) [36]| \\\\( \\mathcal {O}(N) \\\\)| \u2713| KR  \nBig Bird [93]| \\\\( \\mathcal {O}(N) \\\\)| \u2715| FP+M  \nRandom Feature Attention\\\\( ^\\dagger \\\\) [56]| \\\\( \\mathcal {O}(N) \\\\)| \u2713| KR  \nLong Short Transformers\\\\( ^\\dagger \\\\) [96]| \\\\( \\mathcal {O}(kN) \\\\)| \u2713| FP\n+ LR  \nPoolingformer\\\\( ^\\dagger \\\\) [95]| \\\\( \\mathcal {O}(N) \\\\)| \u2715| FP+M  \nNystr\u00f6mformer\\\\( ^\\dagger \\\\) [91]| \\\\( \\mathcal {O}(kN) \\\\)| \u2715| M+DS  \nPerceiver [31]| \\\\( \\mathcal {O}(kN) \\\\)| \u2713| M+DS  \nClusterformer\\\\( ^\\dagger \\\\) [88]| \\\\( \\mathcal {O}(N\\log N) \\\\)| \u2715| LP  \nLuna [50]| \\\\( \\mathcal {O}(kN) \\\\)| \u2713| M  \nTokenLearner\\\\( ^\\dagger \\\\) [63]| \\\\( \\mathcal {O}(k^2) \\\\)| \u2715| DS  \nAdaptive Sparse Transformer\\\\( ^\\dagger \\\\) [14]| \\\\( \\mathcal {O}(N^2) \\\\)|\n\u2713| Sparse  \nProduct Key Memory [41]| \\\\( \\mathcal {O}(N^2) \\\\)| \u2713| Sparse  \nSwitch Transformer [23]| \\\\( \\mathcal {O}(N^2) \\\\)| \u2713| Sparse  \nGShard [45]| \\\\( \\mathcal {O}(N^2) \\\\)| \u2713| Sparse  \nScaling Transformers [32]| \\\\( \\mathcal {O}(N^2) \\\\)| \u2713| Sparse  \nGLaM [21]| \\\\( \\mathcal {O}(N^2) \\\\)| \u2713| Sparse  \n  \n  * Models in the first section are mainly efficient attention methods. Models in the subsequent lower section generally refer to sparse models. Articles annotated with a superscript \\\\( \\dagger \\\\) are peer-reviewed articles. Class abbreviations include: FP = Fixed Patterns or Combinations of Fixed Patterns, M = Memory, LP = Learnable Pattern, LR = Low-Rank, KR = Kernel RC = Recurrence, and DS = Downsampling. Furthermore, \\\\( N \\\\) generally refers to the sequence length and \\\\( B \\\\) is the local window (or block) size. \\\\( N_g \\\\) and \\\\( N_c \\\\) denote global model memory length and convolutionally compressed sequence lengths, respectively.\n\nView Table\n\nTable 1. Summary of Efficient Transformer Models\n\n  * Models in the first section are mainly efficient attention methods. Models in the subsequent lower section generally refer to sparse models. Articles annotated with a superscript \\\\( \\dagger \\\\) are peer-reviewed articles. Class abbreviations include: FP = Fixed Patterns or Combinations of Fixed Patterns, M = Memory, LP = Learnable Pattern, LR = Low-Rank, KR = Kernel RC = Recurrence, and DS = Downsampling. Furthermore, \\\\( N \\\\) generally refers to the sequence length and \\\\( B \\\\) is the local window (or block) size. \\\\( N_g \\\\) and \\\\( N_c \\\\) denote global model memory length and convolutionally compressed sequence lengths, respectively.\n\n### 3.1 A Taxonomy of Efficient Transformers\n\nThis section outlines a general taxonomy of efficient Transformer models,\ncharacterized by their core techniques and primary use case. While the primary\ngoal of most of these models is to improve the memory complexity of the self-\nattention mechanism, we also include methods that improve the general\nefficiency of the Transformer architecture.\n\n  * Fixed Patterns (FP). The earliest modifications to self-attention simply sparsifies the attention matrix by limiting the field of view to fixed, predefined patterns such as local windows and block patterns of fixed strides.\n\n\u2013| Blockwise Patterns. The simplest example of this technique in practice is\nthe blockwise (or chunking) paradigm which considers blocks of local receptive\nfields by chunking input sequences into fixed blocks. Examples of models that\ndo this include Blockwise [58] and/or Local Attention [55]. Chunking input\nsequences into blocks reduces the complexity from \\\\( N^2 \\\\) to \\\\( B^2 \\\\)\n(block size) with \\\\( B\\lt \\lt N \\\\), significantly reducing the cost. These\nblockwise or chunking methods serve as a basis for many more complex models.  \n---|---  \n\u2013| Strided Patterns. Another approach is to consider strided attention\npatterns, i.e., only attending at fixed intervals. Models such as Sparse\nTransformer [9] and/or Longformer [5] employ strided or \u201cdilated\u201d windows.  \n\u2013| Compressed Patterns. Another line of attack here is to use some pooling\noperator to down-sample the sequence length to be a form of fixed pattern. For\ninstance, Compressed Attention [48] uses strided convolution to effectively\nreduce the sequence length.  \n  * Combination of Patterns (CP). The key idea of combined^2 approaches is to improve coverage by combining two or more distinct access patterns. For example, the Sparse Transformer [9] combines strided and local attention by assigning half of its heads to each pattern. Similarly, Axial Transformer [28] applies a sequence of self-attention computations given a high-dimensional tensor as input, each along a single axis of the input tensor. In essence, the combination of patterns reduces memory complexity in the same way that fixed patterns does. The difference, however, is that the aggregation and combinaton of multiple patterns improves the overall coverage of the self-attention mechanism.\n\n  * Learnable Patterns (LP). An extension to fixed, pre-determined pattern are learnable ones. Unsurprisingly, models using learnable patterns aim to learn the access pattern in a data-driven fashion. A key characteristic of learning patterns is to determine a notion of token relevance and then assign tokens to buckets or clusters [84, 88]. Notably, Reformer [37] introduces a hash-based similarity measure to efficiently cluster tokens into chunks. In a simlar vein, the Routing Transformer [62] employs online \\\\( k \\\\)-means clustering on the tokens. Meanwhile, the Sinkhorn Sorting Network [74] exposes the sparsity in attention weights by learning to to sort blocks of the input sequence. In all these models, the similarity function is trained end-to-end jointly with the rest of the network. The key idea of learnable patterns is still to exploit fixed patterns (chunked patterns). However, this class of methods learns to sort/cluster the input tokens\u2014enabling a more optimal global view of the sequence while maintaining the efficiency benefits of fixed patterns approaches.\n\n  * Neural Memory. Another prominent method is to leverage a learnable side memory module that can access multiple tokens at once. A common form is global neural^3 memory which is able to access the entire sequence. The global tokens act as a form of model memory that learns to gather from input sequence tokens. This was first introduced in Set Transformers [43] as the inducing points method. These parameters are often interpreted as \u201cmemory\u201d and are used as a form of temporary context for future processing. This can be thought of as a form of parameter attention [71]. Global memory tokens are also used in ETC [3] and Longformer [5]. With a limited amount of neural memory (or inducing points), we are able to perform a preliminary pooling-like operation of the input sequence to compress the input sequence\u2013a neat trick to have at one\u2019s disposal when designing efficient self-attention modules.\n\n  * Low-Rank Methods. Another emerging technique is to improve efficiency by leveraging low-rank approximations of the self-attention matrix. The key idea is to assume low-rank structure in the \\\\( N\\times N \\\\) matrix. The Linformer [87] is a classic example of this technique, as it projects the length dimension of keys and values to a lower-dimensional representation (\\\\( N \\rightarrow k \\\\)). It is easy to see that the low-rank method ameliorates the memory complexity problem of self-attention because the \\\\( N \\times N \\\\) matrix is now decomposed to \\\\( N \\times k \\\\).\n\n  * Kernels. Another recently popular method to improve the efficiency of Transformers is to view the attention mechanism through kernelization. The usage of kernels [10, 36] enable clever mathematical re-writing of the self-attention mechanism to avoid explicitly computing the \\\\( N \\times N \\\\) matrix. Since kernels are a form of approximation of the attention matrix, they can be also viewed as a type of low-rank approach [10]. Examples of recent work in this area include Performers, Linear Transformers, and Random Feature Attention (RFA [56]).\n\n  * Recurrence. A natural extension to the blockwise method is to connect these blocks via recurrence. Transformer-XL [16] proposed a segment-level recurrence mechanism that connects multiple segments and blocks. These models can, in some sense, be viewed as fixed pattern models. However, we decided to create its own category due to its deviation from other block/local approaches.\n\n  * Downsampling. Another popular method of reducing computation cost is to reduce the resolution of the sequence, hence reducing computation costs by a commensurate factor. Examples of this class of models include Perceiver [31], Funnel Transformers [15], Swin Transformer [49], and Charformer [78] models. Notably, there might also be some form of overlap of this class of models with models that leverage memory tokens as models such as Set Transformer can also be viewed as a form of downsampling, albeit within the attention mechanism. The recent Nystr\u00f6mformer [91], on the surface, may seem like a low-rank or kernal-based approach. However, it is actually a downsampling approach where the \u2018landmarks\u2019 are simply strided based pooling\u2014in similar spirit to Set Transformer, Funnel Transformer, or Perceiever.\n\n  * Sparse Models and Conditional Computation. While not targeted specifically at the attention modules, sparse models sparsely activate a subset of the parameters which generally improves the parameter to FLOPs ratio. Examples of this class of model includes Switch Transformers [23], GShard [45], Product-Key Memory Layers [41]. Within the scope of our studied models, sparse models typically operate on an adaptive basis in which the sparsity is typically learned (via mixture-of-experts like mechanism). Within this context, we can also consider sparsification of attention weights to fall under this paradigm. For this reason, we believe there is a close connection to fixed or learned patterns in attention. However, we believe that the emergence of an entire research direction [21, 45, 46, 61] based on sparse efficient should warrant a new category of efficient Transformers.\n\nWe note that these buckets are a broad characterization of the different\nefficient Transformer models. In reality, there is no sharp boundary between\nthe buckets as models may be comprised of multiple technical innovations. For\nexample, the \\\\( k \\\\)-means clustering in Routing Transformer [62] can also\nbe interpreted as a form of global model memory approach, since one can view\nthe centroids as parameterized model memory. In Reformer, however, clustering\nis used to learn the sparsity pattern of the attention weights. Additionally,\npooling [48] can be also interpreted as a form of model memory mechanism. We\nalso note that the recent xformer models (circa December 2021) have started\nadopting some form of two-staged attention mechanism. Many times, these\nattention mechanisms explicitly combine one or more flavors of the above,\ne.g., local windows and then memory in Poolingformer [95], or Long Short\nTransformers [96] that utilize low rank attention with fixed windows (e.g., a\ncombination of local attention with Linformer-like inductive bias).\n\n### 3.2 Detailed Walk-Through of Efficient Transformer Models\n\nThis section delves into the details of several key efficient Transformer\nmodels, discussing their pros, cons, and unique talking points. The goal here\nis not to exhaustively detail all such models, but rather to cover a\nrepresentative sample of models.\n\nStructure of This Section. We begin by discussing local and fixed patterns\nmodels such as the Memory Compressed Transformer [48] and Image Transformer\n[55]. We then discuss the Set Transformers [43], an early approach for\nutilizing global model memory. Following which, we move on to models that\nutilize combinations of patterns such as Sparse Transformers [9], CCNet [30],\nand Axial Transformers [28]. Next, we discuss Longformer [5] and ETC [3], as\nexamples of memory-based Sparse Transformer approaches. Our detailed\nwalkthrough then moves on to models that incorporate learnable patterns (LP)\nsuch as Routing Transformers [62], Reformer [37] and Sinkhorn Transformers\n[74]. After which, we introduce Linformer [87] and Synthesizers [73], models\nthat can be considered low-rank factorization approaches. We then discuss\nmodels based on kernel approaches such as Performer [10] and Linear\nTransformers [36]. Following which, we discuss the models that are based on\nsegment-based recurrence such as Transformer-XL [16] and Compressive\nTransformers [59]. Finally, we discuss the family of Sparse models which\nprimarily leverage Mixture-of-Experts (MoE) type architectures and conditional\ncomputation to achieve computational efficiency. The logical flow of this\nsection is aimed to be loosely chronological instead of categorically\norganized (with the exception of certain buckets like recurrence or sparsity\nthat are more orthogonal approaches). We believe this is pedagogically\nhelpful.\n\n#### 3.2.1 Memory Compressed Transformer.\n\nMemory Compressed Transformer [48] is one of the early attempts at modifying\nTransformers to better handle longer sequences. The modification introduced by\nMemory Compressed Transformers is in two folds: localizing the attention span\nand using memory compressed attention.\n\nLocal Attention Span. A straightforward solution for dealing with long\nsequences in Transformers is to limit the attention span to a local\nneighborhood. Liu et al. [48] proposed dividing the input sequence into blocks\nof similar length so that self-attention can be computed within each block\nindependently. This keeps the cost of attention per block constant, thus the\nnumber of activations scales linearly with the input length.\n\nMemory-Compressed Attention. The idea behind memory compressed attention is to\nreduce the number of keys and values using a strided convolution, while the\nqueries remain unchanged. This leads to a reduction in the size of the\nattention matrix as well as the attention computations based on a compression\nfactor that depends on the kernel size and the strides of the convolution.\nMemory-compressed attention lets the model exchange the information globally\nacross the input sequence as opposed to local attention.\n\nComputation and Memory Complexity. For a block size of \\\\( b \\\\), the\ncomputational and memory cost of self-attention in each block is \\\\( \\mathcal\n{O}(b^2) \\\\). Given there are \\\\( n/b \\\\) blocks, the computational and memory\ncost of local attention is \\\\( \\mathcal {O}(b.n) \\\\). For memory-compressed\nattention, applying a convolution with kernel size and strides of \\\\( k \\\\),\nthe computational and memory cost of the attention mechanism reduces to \\\\(\n\\mathcal {O}(n \\cdot n/k) \\\\).\n\n#### 3.2.2 Image Transformer.\n\nImage Transformer [55], inspired by convolutional neural networks, restricts\nthe receptive field of self-attention to only local neighborhoods. This helps\nthe model scale up to process larger batch sizes while keeping the likelihood\nloss tractable. Besides the efficiency, adapting the notion of locality can be\na desirable inductive bias for processing images. Image Transformer offers the\nencoder-decoder architecture, where the encoder generates a contextualized\nrepresentation for every pixel-channel in the inputs and the decoder\nautoregressively generates one channel per pixel at each time step.\n\nLocalized Attention Span. Limiting the receptive field to a local neighborhood\n[54, 55] addresses the issues with the computational and memory costs of\nrunning global self-attention on large inputs, but changing the neighborhood\nper query position would prohibit packing the computations of the self-\nattention into two matrix multiplications. To avoid that, Image Transformer\nproposes partitioning the inputs into \u201cquery blocks\u201d and their associated\n\u201cmemory blocks\u201d, where for all queries from a single query block, the model\nattends to the same memory block. There are two different schemes for choosing\nquery blocks and their associated memory block neighborhoods: 1-dimensional\nlocal attention and 2-dimensional local attention. Here we briefly explain\nthese schemes in the decoder case.\n\nFor the 1-dimensional local attention, the image is flattened in the raster\norder^4 and partitioned into non-overlapping query blocks \\\\( Q \\\\) of length\n\\\\( l_q \\\\), and for each query block, a memory block \\\\( M \\\\) is built from\nthe same pixels in the \\\\( Q \\\\) as well as a fixed number of pixels, \\\\( l_m\n\\\\), generated before the query pixel. In 2-dimensional local attention,\npixels are generated in raster order. For the 2-dimensional local attention,\nthe image is partitioned into multiple non-overlapping rectangular query\nblocks of length \\\\( l_q = w_q \\times h_q \\\\). The memory block extends the\nquery block to the top, left \\\\( h_m \\\\) and \\\\( w_m \\\\) pixels and to the\nright \\\\( w_m \\\\) pixels, so \\\\( l_m = (w_q \\times q_h) + 2 \\times (h_m + w_m)\n\\\\). The query pixel can attend to all other pixels. In the 2-dimensional\nlocal attention, pixels in the image are generated one query block after\nanother. Generated blocks are in raster order, as well as generated pixels\ninside every block. Figure 1 illustrates an overview of the Transformer\narchitecture.\n\nFig. 1.\n\nView Figure\n\nFig. 1. Architecture of the standard Transformer [82].\n\nFig. 2.\n\nView Figure\n\nFig. 2. Taxonomy of Efficient Transformer Architectures.\n\nComputational and Memory Complexity. In Image Transformer, the attention\nmatrix has the shape of \\\\( l_q \\times m \\\\), where \\\\( l_q \\\\) is the chosen\nlength for the query blocks and \\\\( M \\\\) is the length of the memory block\n(which is in fact \\\\( l_q + l_m \\\\)). Given that memory blocks do not overlap,\nwe have to compute \\\\( n \\times l_q \\\\) attention matrices. Thus, the memory\nand computational complexity of Image Transformer is \\\\( \\mathcal {O}(n\\cdot\nm) \\\\).\n\nRestrictions. Image Transformer, and in general restricting the context in the\nattention mechanism to a local neighborhood, can decrease the cost of memory\nand computation at the price of losing the global receptive field. This can be\nan issue where global information is required to solve the task. Also, local-\nattention has quadratic complexity with respect to the region length, thereby\nintroducing an extra hyper-parameter in the trade-off between performance and\ncomputational complexity.\n\n#### 3.2.3 Set Transformer.\n\nThe Set Transformer [43] adapts the Transformer model for set-input\nproblems\u2014that is, problems wherein the input is a set of features and the\noutput is some function of this set (and is thereby invariant to the\npermutation, or ordering, of the input features). The Set Transformer\nleverages attention to capture interactions between elements of the input set.\nFurthermore, it applies the idea of inducing points from the sparse Gaussian\nprocess literature to reduce the complexity of attention from quadratic to\nlinear in the size of the input set.\n\nProblems involving sets of objects often have a permutation invariance\nproperty: the target value for the set is the same regardless of the order of\nthe objects in the set. Zaheer et al. [94] proved that all permutation-\ninvariant functions can be represented by the following functional form: \\\\(\n\\begin{align*} \\text{network}\\left(\\lbrace x_1,\\dots , x_N\\rbrace \\right) =\n\\rho \\left(\\text{pool}\\left(\\lbrace \\phi (x_1),\\dots ,\\phi (x_N)\\rbrace\n\\right)\\right), \\end{align*} \\\\) where the pooling function \\\\( \\text{pool}\n\\\\) is a simple summation and \\\\( \\phi \\\\) and \\\\( \\rho \\\\) are continuous\nfunctions. This form can be interpreted as the composition of an encoder \\\\(\n\\phi \\\\) and decoder \\\\( \\rho \\left(\\text{pool}(\\cdot)\\right) \\\\). While this\nform is a universal approximator in the space of permutation-invariant\nfunctions, it is unclear how well such models fit tasks in practice. The Set\nTransformer proposes a solution that can be viewed as an encoder and pooled\ndecoder, but where, unlike the form given above, the encoder and decoder can\nattend to input elements individually and the pooling function is\nparameterized. Attention Blocks. The model introduces the following\nconstructs: Multihead Attention Block (MAB), Set Attention Block (SAB),\nInduced Set Attention Block (ISAB), and Pooling by Multihead Attention (PMA).\nThey are defined as follows \\\\( \\begin{align*} \\mathbf {{\\bf MAB}(X, Y)} &:=\n\\text{LayerNorm}\\left(H + \\text{rFF}(H)\\right),\\\\\\ H &:=\n\\text{LayerNorm}\\left(X + \\text{MultiheadAttention}(X, Y)\\right),\\\\\\ \\mathbf\n{{\\bf SAB}(X)} &:= \\text{MAB}(X, X),\\\\\\ \\mathbf {{\\bf ISAB}_m(X)} &:=\n\\text{MAB}\\left(X, \\text{MAB}(I_m, X)\\right)\\\\!.\\\\\\ \\mathbf {{\\bf PMA}_k(X)}\n&:= \\text{MAB}\\left(S_k, \\text{rFF}(X)\\right)\\\\!. \\end{align*} \\\\) Here, \\\\( X\n\\in \\mathbb {R}^{N \\times d} \\\\) represents \\\\( N \\\\) \\\\( d \\\\)-dimensional\ninput/outputs stacked row-wise and \\\\( \\text{rFF} \\\\) is a parameterized feed-\nforward layer that operates on each row of its input matrix separately. \\\\(\nI_m \\in \\mathbb {R}^{m \\times d} \\\\) represents \\\\( m \\\\) trainable \\\\( d\n\\\\)-dimensional \u201cinducing points\u201d while \\\\( S_k \\in \\mathbb {R}^{k \\times d}\n\\\\) represent \\\\( k \\\\) trainable \\\\( d \\\\)-dimensional \u201cseed vectors\u201d (with\n\\\\( k \\\\) set to 1 except when \\\\( k \\gt 1 \\\\) correlated outputs are needed).\nThe Set Transformer\u2019s encoder is just \\\\( N \\\\) layers of either SAB or ISAB\n(with \\\\( N \\\\) often set to 2 in practice) while its decoder is given by: \\\\(\n\\begin{align*} \\mathbf {{\\bf Decoder}(X)} :=\n\\text{rFF}\\left(\\text{SAB}\\left(\\text{PMA}_k(X)\\right)\\right)\\\\!. \\end{align*}\n\\\\) It is straightforward to see that both ISAB and SAB are permutation\nequivariant \u2014in other words, if the input is permuted in some way then the\ncorresponding output of the block is permuted in exactly the same way.\nMeanwhile, the pooling layer PMA is permutation invariant. Since functional\ncomposition, i.e., layering, preserves these properties, the Set Transformer\nencoder-decoder combination is permutation invariant. Efficiency. We can\nunderstand the \\\\( m \\\\) inducing points \\\\( I_m \\\\) learned in each ISAB\nlayer as a form of static model memory. In addition to reducing the \\\\(\n\\mathcal {O}(N n^2) \\\\) complexity of the self-attending SAB layer to \\\\(\n\\mathcal {O}(N m n) \\\\), a reduction particularly valuable when the input set\nis large, the inducing points effectively encode some global structure that\nhelps explain its inputs. For example, in the problem of amortized clustering,\nwhere one attempts to learn to map an input set of points to the centers of\nclusters of points inside the set, the inducing points learned could be\nappropriately distributed so that the encoder can effectively compare query\nelements with each other implicitly via their proximity to the inducing\npoints.\n\nThe trainable \\\\( k \\\\) seeds \\\\( S_k \\\\) used in the pooling layer \\\\(\n\\text{PMA}_k \\\\) can be viewed as static model memory in a similar light,\nreducing the memory and runtime complexity of the architecture.\n\n#### 3.2.4 Sparse Transformer.\n\nThe Sparse Transformer [9] presents a simple initial attempt to reduce the\nquadratic complexity of the standard self-attention mechanism. The key idea is\nto reduce the dense attention matrix to a sparse version by only computing\nattention on a sparse number of \\\\( q_i,k_j \\\\) pairs. Sparse Transformer\nemploys fixed attention patterns which are defined by strides and local\nneighborhoods. Computation is factorized, wherein local and stride patterns\nare split amongst the heads. Local Attention Heads. Half of the heads in the\nSparse Transformer are dedicated to local attention. \\\\( \\begin{align*}\n\\hat{A}_{ij} = {\\left\\lbrace \\begin{array}{ll} Q_{i}(K)_{j}^\\top),& \\text{if}\n\\lfloor {\\,{j}/{N}}\\rfloor = \\lfloor {i/{N}}\\rfloor \\\\\\ 0 & \\text{otherwise}\n\\end{array}\\right.} \\end{align*} \\\\) where \\\\( A_{ij} \\\\) is the attention\nweight of \\\\( q_i,k_j \\\\) and \\\\( \\lfloor \\: \\rfloor \\\\) denote the floor\noperation. In this case, we only compute the attention if \\\\( \\lfloor\n{\\,{j}/{N}}\\rfloor = \\lfloor {i/{N}}\\rfloor \\\\) (within the same block).\nStrided Attention Heads. The other half of the heads are dedicated to fixed\nstrided patterns. Concretely, \\\\( \\begin{align*} \\hat{A}_{ij} = {\\left\\lbrace\n\\begin{array}{ll} Q_{i}(K)_{j}^\\top),& \\text{if} (i-j) \\mod {N} =0 \\\\\\ 0 &\n\\text{otherwise} \\end{array}\\right.} \\end{align*} \\\\) The final result of the\nfactorized sparse attention is visualized in Figure 4. We refer interested\nreaders to [92] for some additional theoretical analysis about the\nexpressiveness of the Sparse attention mechanism. Parameter and Memory\nComplexity. The modification in the self-attention mechanism does not alter\nthe parameter costs of the model since the model still retains the \\\\( Q,K,V\n\\\\) transforms from the original Transformer model. The memory complexity of\nthe attention layer is reduced from \\\\( \\mathcal {O}(n^2) \\\\) to \\\\( \\mathcal\n{O}(n\\log n) \\\\).\n\nRestrictions. The Sparse Transformer implementation requires custom GPU\nkernels to implement a specific block-sparse variant of matrix-matrix-\nmultiplication and cannot be easily implemented on other hardware such as\nTPUs.\n\n#### 3.2.5 Axial Transformer.\n\nAxial Transformer [28, 89] uses factorization in a simple yet effective setup\nfor the self-attention mechanism to process large inputs that are organized as\nmultidimensional tensors. Instead of applying attention to the flattened\nversion of the input, Axial Transformer simply applies multiple attentions,\neach along a single axis of the input tensor. Each attention, in fact, mixes\ninformation along a particular axis, while keeping information along other\naxes independent. Since the length of any single axis is typically much\nsmaller than the total number of elements, Axial Transformer significantly\nsaves computation and memory. Figure 3 provides an illustration of attention\napplied to 2D input in Image Transformer.\n\nFig. 3.\n\nView Figure\n\nFig. 3. Attention span in Image Transformer on a two-dimensional input.\n\nFig. 4.\n\nView Figure\n\nFig. 4. Illustration of patterns of the attention matrix for dense self-\nattention in Transformers and sparse fixed attention in Sparse Transformers.\nBlue in the right diagram represents the local self-attention while green\nrepresents the strided component of the sparse attention.\n\nFig. 5.\n\nView Figure\n\nFig. 5. Attention span in Axial Transformer on a two-dimensional input.\n\nAxial Transformer offers an encoder-decoder architecture. For the decoding, to\nbe able to implement the causal mask, Axial Transformer combines axial\nattentions with shift operations. For instance, for a model on 2-dimensional\ntensors, pixels are generated in raster order and to do that, first, the model\nencodes all pixels through an unmasked row and unmasked column attention.\nThen, for each row, the model applies an unmasked row and masked column\nattention to integrate the previously sampled rows. Finally, the model shifts\nthe encoded representation up to make sure the conditioning information\nsatisfies causality, and runs a masked row-attention to sample a new row in\nthe image.\n\nAn advantage of Axial Transformer over similar methods like Sparse Transformer\nis that while it provides the global receptive field, it is straightforward to\nimplement and does not require a custom kernel for an efficient\nimplementation.\n\nComputational and Memory Complexity. In terms of memory and computational\ncomplexity, on a square image of size \\\\( N \\\\), Axial Transformer performs\nthe attention computation in \\\\( \\mathcal {O}(n \\sqrt {n}) \\\\), which saves\n\\\\( \\mathcal {O}(\\sqrt {n}) \\\\) over normal self-attention. For instance, with\non square image with \\\\( N \\\\) pixels, organized in a \\\\( b\\times b \\\\) grid,\nAxial Transformer runs \\\\( b \\\\) attention sequences of length \\\\( b \\\\),\nwhich is of complexity \\\\( \\mathcal {O}(b.b^2) \\\\). In a more general case,\nfor a \\\\( d \\\\)-dimensional tensor of shape \\\\( N = N^{1/d}\\times \\cdots\n\\times N^{1/d} \\\\), Axial Transformer saves a \\\\( \\mathcal {O}(N^{(d-1)/d})\n\\\\) factor of resources over standard self-attention.\n\n#### 3.2.6 Longformer.\n\nLongformer [5] is a variant of Sparse Transformer. Its key distinction\ncompared to Sparse Transformer is \u201cDilated Sliding Windows\u201d, which can enable\nbetter long-range coverage without sacrificing sparsity. This is achieved by\nincreasing the receptive fields by having gaps in the attention patterns. The\nLongformer also gradually increases the receptive field as the model goes\ndeeper, dedicating lower levels for modeling local patterns and upper levels\nfor modeling global patterns.\n\nGlobal Attention. For classification tasks, Longformer adopts global memory\ntokens that have access to all input sequences.\n\nParameter and Memory Complexity. The complexity of the model is reduced from\n\\\\( \\mathcal {O}(n^2) \\\\) to \\\\( \\mathcal {O}(nk) \\\\) where \\\\( k \\\\) is the\nsize of the window. When using global attention, the Longformer creates\nanother set of query-key-value projections for this global attention, doubling\nthe cost of the parameters at the attention layer.\n\n#### 3.2.7 Extended Transformer Construction (ETC).\n\nThe ETC model [3] is another variation in the Sparse Transformer family. It\nintroduces a new global-local attention mechanism. There are four components\nto this new attention mechanism, namely (1) global-to-global (g2g), global-to-\nlocal (g2l), local-to-global (l2g), and local-to-local (l2l). Aside from the\noriginal input to the model, ETC introduces \\\\( n_g \\\\) auxiliary tokens as a\nprefix to the original input sequence. These tokens are regarded as global\ntokens and take part in global-to-\\\\( * \\\\) and \\\\( * \\\\)-to-global attention.\nThe local-to-local component acts as the local attention with a fixed radius\nof \\\\( k \\\\). Overall, ETC is quite similar to Longformer in the way it\nintroduces global auxiliary tokens. These tokens are trainable parameters and\ncan be interpreted as a form of model memory that pools across the sequence to\ncollect global sequence information.\n\nMemory and Parameter Complexity. The memory complexity of the ETC model is \\\\(\n\\mathcal {O}(n_{g}^2 + n_{g}N) \\\\), where \\\\( n_g \\\\) is the number of global\ntokens and \\\\( N \\\\) is the input sequence length.\n\nRestrictions. Intuitively, it is easy to observe that ETC cannot be used for\nauto-regressive decoding. This is because we are not able to compute causal\nmasks because of the global attention.\n\n#### 3.2.8 Big Bird.\n\nThe Big Bird model [93] is another Transformer for modeling longer sequences\nand is primarily built on top of ETC [3]. The Big Bird model is comprised of\nseveral key components, namely (1) global tokens, (2) random attention\n(queries attend to random keys), and (3) fixed patterns (local sliding\nwindows).\n\nGlobal Attention. Fundamentally, the idea of using global model memory can be\ntraced all the way back to Longformer/ETC and Set Transformer model. Notably,\nthe global model memory in Big Bird is extended to contain tokens within the\nsequence, instead of simply parameterized model memory. The authors call this\nthe \u201cinternal transformer construction (ITC)\u201d in which a subset of indices is\nselected as global tokens. This can be interpreted as a model-memory-based\napproach.\n\nSliding Window Attention. The window-ed attention was first proposed in early\nlocal-based attention models (Image Transformer, Compressed Attention and/or\nSparse Transformer). In Big Bird, each query attends to \\\\( w/2 \\\\) tokens to\nthe left and \\\\( w/2 \\\\) tokens to the right. This corresponds to a fixed\npattern (FP) approach.\n\nRandom Attention. Finally, each query attends to \\\\( r \\\\) random keys. This\npattern is fixed.\n\nMemory and Parameter Complexity. The memory complexity of the self-attention\nis linear, i.e., \\\\( O(n) \\\\). The Big Bird model does not introduce new\nparameters beyond the Transformer model.\n\nRestrictions. Similar to ETC, the Big Bird model cannot be used to\nautoregressively decode. Hence, qualifying it as an encoder-only model.\n\n#### 3.2.9 Routing Transformer.\n\nThe Routing Transformer [62] is a content-based sparse attention mechanism. It\nproposes a clustering-based attention mechanism that learns the attention\nsparsity in a data driven fashion. The first step is to project \\\\( Q \\\\) and\n\\\\( K \\\\) into a routing matrix \\\\( R \\\\) of dimensions \\\\( n \\times d \\\\) (1)\n\\\\( \\begin{align} R = QW_R + KW_R, \\end{align} \\\\) where \\\\( W_R \\\\) is a \\\\(\nd \\times d \\\\) orthonormal projection matrix.\n\n\\\\( k \\\\)-Means Clustering. The \\\\( R \\\\) matrix undergoes \\\\( k \\\\)-means\nclustering with a series of parameterized cluster centroids \\\\( u_1, u_2\n\\cdots c_k \\\\). The \\\\( k \\\\)-means in Routing Transformer is trained in an\nonline fashion. To ensure a similar number of tokens in each cluster, the\nmodel initializes \\\\( \\sqrt {n} \\\\) clusters, computes each token\u2019s distance\nagainst the cluster centroid, and takes an equal top-\\\\( k \\\\) for each\ncentroid. Since the cluster centroids are trainable parameters, this is also\nreminiscent of the all-attention layer proposed by [71].\n\nRouting Strategy. The routing strategy is then defined as: (2) \\\\(\n\\begin{align} X^{\\prime }_i = \\sum _{j \\in C_i, j \\le i} A_{ij} V_j.\n\\end{align} \\\\) where \\\\( C_i \\\\) is the cluster that vector \\\\( R_i \\\\) is\nassigned to. In other words, the token at \\\\( i \\\\) only attends to tokens in\nthe same cluster.\n\nMemory and Parameter Complexity. The Routing Transformer introduces additional\nparameters in the clustering mechanism, namely \\\\( k \\times d \\\\) centroid\nvectors and a \\\\( W_r \\\\) projection matrix. The memory complexity is \\\\(\n\\mathcal {O}(n^{1.5}) \\\\).\n\n#### 3.2.10 Reformer.\n\nReformer [37] is another efficient attention model based on locality sensitive\nhashing (LSH). Reformer also introduces reversible Transformer layers, which\ncontribute to further reducing its memory footprint.\n\nLSH Attention. The LSH attention introduces parameter-sharing between query\nand keys. It hashes the query-keys into buckets using a random-projection\nbased hashing function. The key idea is that nearby vectors should obtain a\nsimilar hash while distant vectors should not, hence being termed as \u201clocality\nsensitive\u201d. To perform hashing, a random matrix \\\\( R \\in \\mathbb {R}^{k\n\\times b/2} \\\\) is first introduced. Next, the hashing function is defined as:\n(3) \\\\( \\begin{align} h(x) = \\text{arg max}([xR;-xR]), \\end{align} \\\\) where\n\\\\( [;] \\\\) is the concatenation of two vectors. For all queries, attention is\ncomputed if and only if the query and key hashes match, i.e., \\\\(\nh(q_i)=h(k_j) \\\\). In other words, attention is computed amongst query and\nkeys if they fall in the same hash bucket. In order to maintain causal\nmasking, Reformer assigns and maintains a position index for every query and\nkey. It is therefore able to compare if each query key comparison is auto-\nregressively valid. Memory Efficiency with LSH Attention. The key idea behind\nLSH attention is to classify tokens into buckets and then process them bucket\nby bucket in a chunked fashion. To this end, queries are first sorted by\nbucket number and then by sequence order within the same bucket. During\ncomputation, tokens only attend to the same bucket in its own chunk and\nprevious chunk. The chunking and sorted bucketing techniques help to improve\nthe overall efficiency of the Reformer model.\n\nParameter and Memory Complexity. The memory complexity of Reformer is \\\\(\n\\mathcal {O}(n \\log n) \\\\). In terms of parameter costs, Reformer shares\nqueries and keys, which reduces the cost of the QKV transforms by a third. The\nrandom projections are not trainable parameters and hence do not incur\nparameter costs. Overall, Reformer has fewer parameters than vanilla\nTransformers. The reversible layers in Reformer also reduce the memory\nconsumption during training by enabling activations to be reconstructed from\nthe next layer\u2019s. This reduces memory cost since this eliminates the need to\nstore activations for all layers during backpropagation.\n\n#### 3.2.11 Sinkhorn Transformers.\n\nThis section introduces the Sparse Sinkhorn Transformer [74]. The Sinkhorn\nTransformer belongs to the family of learned patterns. This model is a\nchunked/ blocked model that learns sparse patterns by re-sorting the input key\nand values in a block-wise fashion and then applying local block-based\nattention. \\\\( \\begin{align*} A_{ij} = {\\left\\lbrace \\begin{array}{ll}\n(Q_{i}\\psi _S(K)_{j}^\\top),& \\text{if} \\lfloor {{j}/{N}}\\rfloor = \\lfloor\n{i/{N}}\\rfloor \\\\\\ 0 & \\text{otherwise} \\end{array}\\right.} \\end{align*} \\\\)\nwhere \\\\( \\psi _S \\\\) applies a sorting operator on the sequence length\ndimension.\n\nSorting Network. The sorting operator is parameterized by a meta sorting\nnetwork. Let \\\\( X \\\\) be the input sequence of dimension \\\\( N \\times d \\\\):\n(4) \\\\( \\begin{equation} \\psi _S(X) = \\phi _S(F_S({\\rm B}{\\rm\\small{LOCK}}{\\rm\nS}{\\rm\\small{UM}}(X)))\\:{\\rm B}{\\rm\\small{LOCK}}{\\rm S}{\\rm\\small{HAPE}}(X),\n\\end{equation} \\\\) where \\\\( F_S(.) \\\\) is a parameterized function such as a\ntwo-layer feed-forward network with ReLU activation. The output of \\\\( F_S(.)\n\\\\) is a tensor of \\\\( n_B \\times n_B \\\\). The BlockSum function learns the\nsum embeddings of local blocks. The BlockShape function reshapes the input\ntensor into \\\\( \\mathbb {R}^{N \\times d} \\rightarrow \\mathbb {R}^{n_B \\times b\n\\times d} \\\\). Here, we note that \\\\( N = n_B \\times b \\\\), where \\\\( b \\\\) is\nthe size of the block and \\\\( n_B \\\\) is the number of total blocks.\n\nSinkhorn Sorting. \\\\( \\phi \\\\) is the Sinkhorn balancing operator [1, 67]\nwhich converts the \\\\( n_B \\times n_B \\\\) matrix into a soft permutation\nmatrix. Specifically, a series of row- and column-wise normalizations are\napplied on the matrix output of \\\\( F_S\\text{BlockSum}(X) \\\\). For the sake of\nbrevity, we do not delve into details of this operation. Further details can\nbe found in Adams and Zemel [1], and Tay et al. [74].\n\nParameter and Memory Complexity. The memory complexity of the Sinkhorn\nTransformer is \\\\( \\mathcal {O}(b^2) \\\\) where \\\\( b \\\\) is the block size and\n\\\\( b=\\frac{N}{N_b} \\\\). Additional parameter costs are incurred from the meta\nsorting network \\\\( F_S(.) \\\\). The number of additional parameters is\ntherefore \\\\( 2d^2 \\\\) when a two-layer ReLU network is used as the sorting\nnetwork.\n\n#### 3.2.12 Linformer.\n\nLinformer [87] is an efficient Transformer based on the idea of low-rank self-\nattention. Low-Rank Projections on Length Dimensions. Linformer projects the\n\\\\( N \\times d \\\\) dimensional keys and values to \\\\( k \\times d \\\\)\ndimensions using additional projection layers. Note that this is a reduction\non the length dimension instead of the key and value dimensions. This can\nGiven the newly projected keys (\\\\( K^{\\prime } \\\\)) and values (\\\\( V^{\\prime\n} \\\\)), the \\\\( QK^{\\prime } \\\\) matrix is now \\\\( (N \\times k) \\\\) dimensions\ninstead of \\\\( (N \\times N) \\\\). The attention matrix \\\\(\n\\text{Softmax}(QK^{\\prime }) \\\\) multiplies with \\\\( V^{\\prime } \\in \\mathbb\n{R}^{k \\times d} \\\\) to result in an output tensor of dimensions \\\\( N \\times\nd \\\\). To some extent, Linformer is reminiscent of depth-wise convolutions\n[35]. A projection on the length dimension causes mixing of sequence\ninformation (dimension-wise) in a single transformation. Hence, it is non-\ntrivial to maintain causal masking and/or prevent mixing of past and future\ninformation when computing attention scores. The formulation of Linformer (for\neach attention head) can be expressed as: (5) \\\\( \\begin{align}\nSoftmax\\left(\\frac{1}{\\sqrt {d_k}}XW^{Q}_{i}\\left(E_i X W_i^K\\right)\\right)\n\\cdot F_iXW_i^V \\end{align} \\\\) where \\\\( W^{Q,K,V} \\\\) are the default linear\ntransformation of \\\\( X \\\\) into queries (as per vanilla Transformer) and \\\\(\nE_{i}, F_i \\\\) are additional \\\\( k \\times N \\\\) projection of the key and\nvalues into \\\\( k \\times d \\\\) tensors.\n\nParameter and Memory Complexity. The memory complexity of Linformer is \\\\(\n\\mathcal {O}(n) \\\\). There is only a minimal parameter costs of the Linformer\ndue to the extra \\\\( N \\times k \\\\) length projections. If \\\\( k \\\\) is\nsufficiently small, there is negligible parameter costs incurred.\n\n#### 3.2.13 Performer.\n\nThe Performer [10, 11] model is characterized by its Generalized Attention\nmechanism and its usage of random Kernels. Generalized Attention. The\ngeneralized attention entangles \\\\( Q_i,K_j \\\\) with a kernel function \\\\( K\n\\\\). The attention matrix in Performer is computed via: (6) \\\\( \\begin{align}\nA = [g(Q_i^\\top)K(Q_i^\\top K_j^\\top) h(K_j^\\top)] \\end{align} \\\\) where \\\\(\nK(.) \\\\) is a kernel function that maps \\\\( d \\times d \\\\) to a scalar value\n\\\\( \\mathbb {R} \\\\) and \\\\( g,h \\\\) are functions that map \\\\( d \\\\) to a\nscalar value \\\\( \\mathbb {R} \\\\). Fast Attention via Orthogonal Random\nFeatures (FAVOR). The above computation is still quadratic in complexity.\nHence, the Performer leverages approximation tricks to avoid storing and\ncomputing the \\\\( N \\times N \\\\) attention matrix. It leverages orthogonal\nrandom features (ORF) for doing so. The final attention output \\\\( Y \\\\) of\nthe Performer is described as follows: (7) \\\\( \\begin{align} Y =\n\\hat{D}^{-1}(Q^{\\prime }((K^{\\prime })^\\top V)) \\end{align} \\\\) where \\\\(\n\\hat{D}=\\text{diag}(Q^{\\prime }((K^{\\prime })^\\top 1_N)) \\\\), \\\\( Q^{\\prime\n}=D_Q\\phi (Q^\\top)^\\top \\\\), and \\\\( K^{\\prime }=D_K\\phi (K^\\top)^\\top \\\\).\nNote that \\\\( D_Q=g(Q_i^\\top),D_K=h(K_i^\\top) \\\\). The function \\\\( \\phi (x)\n\\\\) is defined as: (8) \\\\( \\begin{align} \\phi (X)= \\frac{c}{\\sqrt {M}}f(Wx\n+b)^\\top , \\end{align} \\\\) where \\\\( c \\gt 0 \\\\) is a constant, \\\\( W \\in\n\\mathbb {R}^{M \\times d} \\\\) is a random feature matrix and \\\\( M \\\\) is the\ndimensionality of this matrix that controls the number of random features. We\nare able to see that we do not explicitly compute \\\\( A=QK^\\top \\\\) and hence\navoid paying the \\\\( N^2 \\\\) cost. For rigorous theoretical analysis and\nfurther details, we refer interested readers to [10].\n\nParameter/Memory Complexity and Compute Costs. The complexity of the bi-\ndirectional FAVOR algorithm is \\\\( \\mathcal {O}(Md + N d + MN) \\\\) where \\\\( M\n\\\\) is the dimensionality of the random features. It is worth noting that the\nunidirectional variations cannot be causally masked in an efficient linear-\ntime fashion. As such, during training, running unidirectional (causal)\nimplementation of kernel-based attention on an autoregressive task can be\nseveral times slower than vanilla Transformer during parallelized training due\nto the need to do a left to right pass (i.e., scan operation) in similar\nspirit to Recurrent neural networks. Since many autoregressive tasks trained\nvia parallelization and teacher forcing, this makes training Performer on a\ngenerative task prohibitively slow. In order for KV to be causally masked\nefficiently, one would have to manifest the \\\\( d \\times d \\\\) KV matrix at\nevery time step\u2014recovering a quadratic complexity model. We feel this is one\nof the intricate points that highlight how efficient memory complexity might\nnot equate a faster or more efficient model in practice. We highlight that\nthis only happens during autoregressive training. The inference-time for\nincremental decoding, however, would benefit from a speed-up.\n\n#### 3.2.14 Linear Transformer.\n\nThe Linear Transformer [36] improves the complexity of self-attention from\nquadratic to linear by using a kernel-based formulation of self-attention and\nthe associative property of matrix products. Furthermore, it reduces attention\nwith causal masking (which is used in auto-regressive decoding) to a linear-\ntime, constant memory recurrent neural network (RNN). The model has been shown\nto improve inference speeds up to three orders of magnitude without much loss\nin predictive performance. Linear Transformers are similar to Performers with\nthe exception of the kernel function and therefore also suffer from the same\ndrawbacks (unable to be parallelized across the time dimension during training\nin an autoregressive teacher forced setting).\n\nThe method rests on the simple but powerful observation that the accumulated\nvalue \\\\( V_i^{\\prime } \\\\) for the query \\\\( Q_i \\\\) in position \\\\( i \\\\)\ncan be written as: \\\\( \\begin{align*} V_i^{\\prime } &= \\frac{\\sum _{j=1}^p\n\\text{sim}(Q_i, K_j) V_j}{\\sum _{j=1}^p \\text{sim}(Q_i, K_j)}. \\end{align*}\n\\\\) Here, \\\\( p = N \\\\) in full, unmasked attention and \\\\( p = i \\\\) in the\ncase of causal masking. Now, in usual softmax attention, \\\\( \\text{sim}(q, k)\n= \\exp (\\frac{q^T k}{\\sqrt {d}}) \\\\). Linear Transformer, however, expresses\nthe similarity as a kernel function. That is, \\\\( \\text{sim}(q, k) := \\phi\n(q)^T \\phi (k) \\\\), where \\\\( \\phi \\\\) is a, possibly high-dimensional,\nfeature map. With this choice, we can rewrite \\\\( V_i^{\\prime } \\\\) as: \\\\(\n\\begin{align*} V_i^{\\prime } &= \\frac{\\phi (Q_i)^T S_p}{\\phi (Q_i)^T Z_p},\\\\\\\nS_p &:= \\sum _{j=1}^p \\phi (K_j) V_j^T,\\\\\\ Z_p &:= \\sum _{j=1}^p \\phi (K_j).\n\\end{align*} \\\\) For unmasked attention, since \\\\( p = N \\\\) we only need to\ncompute \\\\( S_N \\\\) and \\\\( Z_N \\\\) once and we reuse them for the computation\nat every position \\\\( 0 \\le i \\le N \\\\). For causal attention, the \\\\( S_i\n\\\\)\u2019s and \\\\( Z_i \\\\)\u2019s can be viewed as states of an RNN that are updated by\nthe following recurrence relations: \\\\( \\begin{align*} S_i &= S_{i-1} + \\phi\n(K_i)V_i^T,\\\\\\ Z_i &= Z_{i-1} + \\phi (K_i) \\end{align*} \\\\) with initial\ncondition \\\\( S_0 = Z_0 = 0 \\\\). If the dimension of the key, query, and\nvalues are all \\\\( d \\\\) and the cost to compute \\\\( \\phi \\\\) is \\\\( \\mathcal\n{O}(c) \\\\), then the overall run-time complexity of Linear Transformer is \\\\(\n\\mathcal {O}{(N c d)} \\\\). The authors choose \\\\( \\begin{align*} \\phi (x) =\n\\text{elu}(x) + 1, \\end{align*} \\\\) where \\\\( \\text{elu}(\\cdot) \\\\) denotes\nthe exponential linear unit [13]. With this choice of feature map, \\\\( c = d\n\\\\) and the end-to-end complexity of the model is \\\\( \\mathcal {O}(N d^2) \\\\).\n\n#### 3.2.15 Synthesizers.\n\nSynthesizer models [73] are an attempt to study and investigate the true\nimportance of conditioning within the self-attention mechanism and are also\nthe first attempts at unconditional token-mixing. In Tay et al. [73], the\nauthors study a synthetic self-attention module in which attention weights are\napproximated instead of being computed by pairwise dot products. Synthesizers\nare only implicitly related to efficient Transformers and can be considered\nmore as a MLP-Mixer [81]. However, the factorized variants can be considered a\nlow-rank efficient Transformer model. Dense Synthesizers. In the Dense\nSynthesizer, each token \\\\( x_i \\\\) is projected to a vector of length \\\\( N\n\\\\) using a two-layered non-linear feed-forward network. The computation of\nthe attention matrix \\\\( A \\\\) is described as: (9) \\\\( \\begin{align} A =\nW_2(\\sigma _{R}(W_1(X)+b))+b, \\end{align} \\\\) where \\\\( X \\in \\mathbb {R}^{N\n\\times d} \\\\) is the input sequence, \\\\( W_2 \\in \\mathbb {R}^{d \\times N}, W_1\n\\in \\mathbb {R}^{d \\times d} \\\\), and \\\\( \\sigma _R \\\\) is the ReLU activation\nfunction. Given \\\\( A \\\\), the output of the Synthetic Dense function is\ncomputed as: (10) \\\\( \\begin{align} Y = \\text{Softmax}(A)G(X), \\end{align} \\\\)\nwhere \\\\( G(X) \\\\) is another parameterized function \\\\( \\mathbb {R}^{N \\times\nd} \\rightarrow \\mathbb {R}^{N \\times d} \\\\). Random Synthesizers. Another\nvariant of the Synthesizer model uses random matrices for \\\\( A \\\\). In this\ncase, the output can be expressed by: (11) \\\\( \\begin{align} Y =\n\\text{Softmax}(R)G(X), \\end{align} \\\\) where \\\\( R \\in \\mathbb {R}^{N \\times\nN} \\\\) is a trainable and/or non-trainable matrix. In Tay et al. [73], the\nauthors show that Random Synthesizers achieve competitive performance.\n\nFactorized Variants. The Dense and Random Synthesizers also come with\nfactorized variants that consider a low-rank structure of the attention\nmatrix. For factorized Random Synthesizer, the output can be written as: (12)\n\\\\( \\begin{align} Y = \\text{Softmax}(R_{1}R_{2}^{\\top })G(X), \\end{align} \\\\)\nwhere \\\\( R_{1},R_{2} \\in \\mathbb {R}^{N \\times k} \\\\). On the other hand, the\nDense Synthesizer can be factorized as follows: (13) \\\\( \\begin{align}\nA=H_B(B)* H_C(C) \\:\\: \\text{where} \\: \\: B, C = F_B(X_i), F_C(X_i),\n\\end{align} \\\\) where \\\\( F_B(.) \\\\) projects onto \\\\( b \\\\) dimensions and\n\\\\( F_C(.) \\\\) projects \\\\( X_i \\\\) onto \\\\( c \\\\) dimensions with \\\\( c\n\\times b=N \\\\). \\\\( H_B,H_C \\\\) are tile and repeat functions, respectively.\nParameter and Memory Complexity. For Random Synthesizers that adopt a non-\ntrainable \\\\( R \\\\), there is no need to store \\\\( N^2 \\\\) activations at this\nlayer. For the trainable Random Synthesizer, the memory complexity and\nparameter complexity remains as \\\\( N^2 \\\\). However, there is no need to\ncompute \\\\( N^2 \\\\) dot products, reducing the computational costs\nsignificantly. The Factorized Random Synthesizers reduce the parameter costs\nto \\\\( 2(N \\times k) \\\\).\n\n#### 3.2.16 Transformer-XL.\n\nThe Transformer-XL model [16] relies on segment-based recurrence. Segment-\nbased recurrence can be considered an orthogonal approach to the other\ntechniques discussed since it does not explicitly sparsify the dense self-\nattention matrix. Instead, it connects adjacent blocks with a recurrent\nmechanism.\n\nSegment Recurrence. The recurrent mechanism in Transformer-XL is described as:\n(14) \\\\( \\begin{align} \\tilde{\\mathbf {h}}^{n-1}_{\\tau +1} &=\n\\left[\\text{SG}\\left(\\mathbf {h}^{n-1}_{\\tau }\\right) \\odot \\mathbf\n{h}^{n-1}_{\\tau +1}\\right] \\end{align} \\\\) (15) \\\\( \\begin{align} q^{n}_{\\tau\n+1}, k^{n}_{\\tau +1}, v^{n}_{\\tau +1} &= \\mathbf {h}^{n-1}_{\\tau +1}\\mathbf\n{W}^\\top _q \\:,\\: \\tilde{\\mathbf {h}}^{n-1}_{\\tau +1}\\mathbf {W}^\\top _k \\:,\\:\n\\tilde{\\mathbf {h}}^{n-1}_{\\tau +1}\\mathbf {W}^\\top _v \\end{align} \\\\) (16)\n\\\\( \\begin{align} \\mathbf {h}^{n}_{\\tau +1} &= \\text{Transformer}\\big\n(q^{n}_{\\tau +1}, k^{n}_{\\tau +1}, v^{n}_{\\tau +1}\\big), \\end{align} \\\\) where\nSG() is the stop gradient function, \\\\( \\odot \\\\) is the concatenation of two\nsequences along the length dimension. Notably, the keys and values are\nconditioned on the previous sequence length \\\\( \\tilde{\\mathbf\n{h}}^{n-1}_{\\tau +1} \\\\) instead of \\\\( \\mathbf {h}^{n-1}_{\\tau +1}.\n\\\\)Relative Positional Encodings. Transformer-XL introduces novel relative\nposition encodings. In this scheme, absolute positional encodings are not\nadded to the content embeddings. Instead, they are only considered while\ncomputing attention weights where they can be replaced with relative position\nencodings. Since the relative position encodings are not directly relevant to\nthe efficiency of the model, we refer interested readers to Dai et al. [16]\nfor more details.\n\n#### 3.2.17 Compressive Transformers.\n\nCompressive Transformers [59] are a natural extension of the Transformer-XL\nmodel. The key idea behind the Compressive Transformer is to maintain a fine-\ngrained memory of past segment activations. This is unlike Transformer-XL,\nwhich discards past activations as it moves across segments. Model Memory. The\nCompressive Transformer is characterized by a dual model memory system\u2014a\nprimary model memory and a secondary compressed model memory. It maintains a\nmodel memory with \\\\( n_m \\\\) memory slots and \\\\( n_{cm} \\\\) compressive\nmemory slots. Whenever the model accepts a new input segment, the oldest \\\\(\nn_s \\\\) activations in the primary model memory are moved to the compressed\nmodel memory where a compression function is applied.\n\nCompression. These memories are compressed with a variety of compression\nfunctions such as (1) mean/max pooling, (2) 1D convolutions, (3) dilated\nconvolutions, and (4) most used (e.g., sorted by usage of attention). Memory\nReconstruction. In order to better retain memories over long sequences, the\nCompressive Transformer implements an auto-encoding loss that learns to\nreconstruct the original memory from its compressed version, i.e., \\\\(\nL^{ae}=|| \\text{old}\\\\_\\text{mem} - g(\\text{new}\\\\_\\text{cm}^{(i)})|| \\\\)\nwhere \\\\( g(.) : \\mathbb {R}^{\\frac{n_s}{c} \\times d} \\rightarrow \\mathbb\n{R}^{n_s \\times d} \\\\) is a parameterized function. A second attention\nreconstruction is a lossy re-construct that attempts to reconstruct the\nattention over model memory instead of the lossless reconstruction of the\nmodel memory itself.\n\n#### 3.2.18 Sparse Models.\n\nIn this section, we describe the family of Sparse models. Sparse models\ntypically achieve a high parameter to FLOP ratio by sparsely activating a\nsubset of parameters or activations. It is good to note that while most of the\nworks within the scope of this survey deals with efficient attention, the\nscope of sparse models goes beyond the attention module and is generally\napplied more frequently to the feed forward layers [23, 45]. In this section,\nwe discuss the prime variant for Sparse models, i.e., the Mixture-of-\nExperts\u2013based Sparse models which includes models such as GShard [45], Switch\nTransformer [23], and GLaM [21]. Mixture-of-Experts. The key idea behind MoE\nis to route token \\\\( x_{i} \\\\) to a set of selected experts determined by a\nrouting function. The routing function typically computed a linear combination\nover experts using the softmax function and can be interpreted as a form of\ngating mechanism. The top-k gate values are then selected for each token \\\\(\nx_{i} \\\\) and the final output of that layer is determined by a linear\ncombination of selected top-k experts. This MoE layer remains foundational and\nfundamental to many MoE architectures, with the exception of certain\nimplementation details. For example, Switch uses a top-1 routing strategy\nwhile GShard uses a group-level top-2 gating.\n\nSkip 4DISCUSSION Section\n\n## 4 DISCUSSION\n\nThis section explores the state of research pertaining to this class of\nefficient models.\n\n### 4.1 On Evaluation\n\nWhile the field is bustling with new Transformer models, there is not an easy\nway to compare these models side by side. Many research papers select their\nown benchmarks to showcase the abilities of the proposed model. This is also\ncoupled with different hyperparameter settings like model sizes and\nconfigurations which can make it difficult to correctly attribute the reason\nfor the performance gains. Moreover, some papers conflate this with\npretraining [20] which makes it even harder to distinguish the relative\nperformance of these different models. It is still a mystery to which\nfundamental efficient Transformer block one should consider using.\n\nOn one hand, there are multiple models that focus on generative modeling,\nshowcasing the ability of the proposed Transformer unit on auto-regressive\nmodeling of sequences. To this end, Sparse Transformers [9], Adaptive\nTransformers [14], Routing Transformers [62], and Reformers [37] are mainly\nfocused on generative modeling tasks. These benchmarks typically involve\nlanguage modeling and/or pixel-wise image generation on datasets such as\nwikitext [51], and/or ImageNet [19]/ CIFAR [38]. Models that use segment-based\nrecurrence such as Transformer-XL and Compressive Transformers are also\nfocused on long-range language modeling tasks such as PG-19.\n\nOn the one hand, a collection of models is mainly focused on encoding-only\ntasks such as question answering, reading comprehension, and/or selections\nfrom the GLUE benchmark. For example, the ETC model [3] only runs experiments\non question-answering benchmarks such as NaturalQuestions [39] or TriviaQA\n[34]. On the other hand, the Linformer [87] focuses on subsets of the GLUE\n[85] benchmark. This split is very natural and intuitive, since models like\nETC and Linformer cannot be used in an auto-regressive fashion. This\nexacerbates the challenges associated with comparing these encoder-only models\nwith the other models.\n\nThere are models that focus on a balance of both. Longformer [5] tries to\nbalance this by running benchmarks on both generative modeling and encoder-\nonly tasks. The Sinkhorn Transformer [74] compares on both generative modeling\ntasks as well as encoding only tasks.\n\nAdditionally, it is also worth noting that, although Seq2Seq machine\ntranslation (MT) was one of the problems that popularized Transformer models,\nnot many of these efficient Transformer models are evaluated on MT tasks. This\nis likely because sequence lengths in MT are not long enough to warrant the\nusage of these models.\n\nWhile generative modeling, GLUE tasks and/or question answering appear to be\nthe common evaluation benchmarks adopted by many of these tasks, there are\nseveral niche benchmarks that a small isolated number of articles choose to\nevaluate on. For starters, the Performer model [10] evaluates on masked\nlanguage modeling on proteins, deviating from serious head-on comparisons with\nother efficient Transformer models. The Linear Transformer [36] also evaluates\non speech recognition, which is a rare benchmark amongst this group of papers.\n\nThere have been recent attempts to unify evaluation on Efficient Transformers,\nnamely Long-Range Arena (LRA), [75] that benchmarked 10 different xformer\nvariants on long-range modeling tasks. It is good to note that LRA was\ndesigned for evaluating Transformers in encoder-only mode and do not consider\ngenerative (or autoregressive tasks) that require causal masking.\n\n### 4.2 On Model Design Trends\n\nWhen matching our broad categorization against the timeline of the\nintroduction of these models, we are able to see the trend that the community\nis taking towards designing efficient Transformer models. Early work in this\narea has primarily been focused on more intuitive and simple approaches such\nas fixed patterns. To this end, most early work in this area is based on\nblock/local patterns such as Image Transformer [55], Compressed Attention\n[48], Blockwise Transformer [58] or the local windows in Sparse Transformer\n[9].\n\nThe paradigm of factorizing various fixed patterns was first introduced in\nChild et al. [9] and CCNet [30]. Around this same time, we start to observe\nearly traces of model-memory-based approaches from both the inducing point\nmethods in the Set Transformer [43] or global nodes in the Star Transformer\n[25] model.\n\nWe observe the next wave of models comes in the form of learnable sparsity\npatterns. Reformer [37] and Routing Transformers [62] are very similar in the\nsense that they are models that learn to cluster/bucket tokens before\nperforming attention. The key difference is the means to the end whereby\nReformer uses a hashing function while the Routing Transformer uses online \\\\(\nk \\\\)-means for cluster assignment. In parallel, Sinkhorn Transformers [74]\nare also based on the idea of sorting, albeit at the block level. These three\nmodels largely follow a similar paradigm of re-arranging sequences for\nefficient computation of attention scores.\n\nNext, we then observe several extensions that are largely built off the Sparse\nTransformer paradigm. The ETC [3] and Longformer [5] models are very similar\nideas that are fundamentally Sparse Transformer extensions. These models\nincorporate the notion of a global model memory, which is reminiscent of the\nSet Transformer\u2019s inducing point method or the global model memory of the Star\nTransformer. Modifications to strides, such as using dilated windows was also\nproposed in the Longformer work.\n\nThe most recent wave of models we\u2019ve been seeing is models that are based on\nlow-rank approximation or kernel methods, e.g., models such as Low-Rank\nTransformer [90], Linformer [87], Performer [10] and/or Linear Transformers\n[36]. Although due to the state of evaluation and the high parallelism of\nresearch, it is quite unclear if this low-rank or kernel paradigm is actually\nbetter than the learnable pattern (LP) or model memory based efficient\nTransformer models.\n\nMore recently, there have been more models that propose a two-pronged or two-\nstep attention mechanism combining models from different techniques. The Long\nShort Transformer [96] is a dynamic form of Linformer combined with Fixed\nPattern attention mechanisms. On the other hand, models like Poolingformer\nalso explicitly construct a two-level attention mechanism with techniques\nreminiscent of memory-based approaches and local attention. Scatter Brain is a\nnew work [8] attempts to unify sparse (fixed pattern) attention with low-rank\nattention. Two-stage attention mechanisms are also proposed by Luna [50].\n\nOn the side, it is important to note that the recurrent based models\n(Transformer-XL and Compressive Transformers) seem to operate orthogonally and\nare not as directly comparable to the other models. We also observe that\nSparse models [23, 45] that are not only applicable to attention modules, are\nalso recently emerging and becoming more popular and have demonstrated\nconsiderable success in the recent months [21].\n\n### 4.3 Brief Discussion on Orthogonal Efficiency Efforts\n\nWhile this article is mainly focused on (1) the computational and memory\ncomplexity of the self-attention module and (2) sparsity and adaptive\ncomputation, we briefly summarize several orthogonal efforts that may also\ncontribute to model efficiency, scalability, and overall usability of\nTransformer models.\n\n  * Weight Sharing. Sharing parameters of the Transformer models would help reduce the overall model size. The Universal Transformers [18] tie attention and transition weights across layers. Similarly, Albert [42] does the same parameter sharing across layers. On the other hand, the Quaternion Transformer [79] proposes a weight sharing scheme inspired by Hamilton products that locally shares the components in the linear transformation layers.\n\n  * Quantization/Mixed Precision. Learning mixed precision models has the potential to improve memory costs. Q-BERT [66] is a model that quantizes Transformer models to ultra-low precision. Meanwhile mixed precision training [53] is a highly popular technique to reduced the memory costs of training Transformers. Fan et al. [22] applies Quantization Aware training to Transformer models.\n\n  * Inference-Time Efficiency and Network Pruning. Multiple research directions explore improving the Transformer efficiency at inference time. One prime example is network model. An example is to prune attention heads during inference [52, 83]. This has shown to have minimal degradation of performance on downstream tasks. On the other hand, Lagunas et al. [40] proposes a \u201cblock\u201d pruning approach which can make a Transformer 2.4\\\\( \\times \\\\) faster with little loss in predictive performance on language tasks. Another line of work involved fast exit during inference which allows us to exit compute if the model is confident of its predictions [65].\n\n  * Knowledge Distillation. Knowledge distillation (KD) [27] has been a useful technique for transfering the knowledge learned from a larger teacher model to a smaller student model. The smaller model can then be efficiently deployed into production. There have been many attempts to distill large Transformer models. For example, DistilBERT [64], task-specific distillation [72] and TinyBERT [33].\n\n  * Neural Architecture Search (NAS). Searching for more efficient Transformer architectures is also a common strategy. Guo et al. [26] proposed Neural Architecture Transformer (NAT), using NAS to search for more compact and efficient Transformers by removing redundant operations. Wang et al. [86] proposed HAT (Hardware-aware Transformers), a method that leverages NAS and uses hardware efficiency feedback as a reward signal.\n\n  * Task Adapters. This line of research has been primarily focused on the problem of fine-tuning large Transformer on \\\\( T \\\\) tasks and aiming to reuse parameters across a variety of tasks. The key idea is that task adapters [29] enable reuse of parameters across tasks and reuse the need of serving \\\\( T \\\\) models in production\u2014resulting in overall parameter savings. A modest number of models have been proposed, such as PALS [69], MAD-X [57], and HyperGrid [80].\n\n  * Alternative Architectures. A considerable amount of effort have gone into designing Transformer alternatives. Amongst the many alternatives considered, a prominent line of emerging research belongs to the family of MLP Mixers [81]. Different mixing operations have been proposed, such as the G-MLP [47], FNet [44]. Synthesizers [73], although commonly referred to as an efficient attention method, is also an early manifestation of the mixer line of work, as the random matrices similarly act as an unconditioned mixing operation. A recent promising line of work, based on Structured State Spaces [24] also demonstrated very promising results on long-range modeling. Last, convolutional models are generally more efficient than Transformers since convolutional kernels operate on a fixed, small local neighborhood around the input token. Tay et al. [76] shows that, when pre-trained, these more efficient convolutional models can sometimes match the predictive performance of Transformer ones.\n\n  * Model-Based Indexing. An emerging novel paradigm is the notion of neural indexing. Differentiable Search Index [77] proposed to index documents within model parameters. This is essentially linking textual content into a memory address within model parameters which could be invoked later by referencing this address token.\n\nSkip 5CONCLUSION Section\n\n## 5 CONCLUSION\n\nIn this article, we surveyed the literature on efficient Transformer models\nespecially pertaining to the quadratic complexity of the self-attention\nmodule. We provided a taxonomy and high-level abstraction of the core\ntechniques employed in these class of new models. We characterized the\nexisting models based on techniques and provided a comprehensive walkthrough\nof several of the efficient Transformer models. Finally, we discussed the\nevaluation landscape of these models along with the design trends of these\nmodels. We ended of with a brief discussion of other parallel orthogonal\nefforts that may improve the efficiency of Transformer models in general.\n\n## Footnotes\n\n  1. ^1 We would like to emphasize that complexity does not always translate to real-world throughput or latency. A model of linear complexity can be slower than a model with quadratic complexity in practice.\n\nFootnote\n\n  2. ^2 We note that this is also often referred to as factorization approaches, e.g., in Child et al. [9]. We decide to refer to this class of models as combination approaches because (1) it is a better fit to what these models are actually doing and (2) to avoid confusion with matrix factorization or low-rank approaches.\n\nFootnote\n\n  3. ^3 We use the term neural here to refer to a representation-like memory that is often manifested in the model.\n\nFootnote\n\n  4. ^4 Given a 2D image as a grid of pixels, the horizontally left-to-right scanning of pixels, line-by-line, creates a raster order.\n\nFootnote\n\n## REFERENCES\n\n  1. [1] Adams Ryan Prescott and Zemel Richard S.. 2011. Ranking via sinkhorn propagation. arXiv preprint arXiv:1106.1925 (2011).Google Scholar\n\nReference 1Reference 2\n\n  2. [2] Ahmed Karim, Keskar Nitish Shirish, and Socher Richard. 2017. Weighted transformer network for machine translation. arXiv preprint arXiv:1711.02132 (2017).Google Scholar\n\nReference\n\n  3. [3] Ainslie Joshua, Ontanon Santiago, Alberti Chris, Pham Philip, Ravula Anirudh, and Sanghai Sumit. 2020. ETC: Encoding long and structured data in transformers. In Proceedings of EMNLP (2020).Google Scholar\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n     * Reference 5\n     * Reference 6\n     * Reference 7\n     * Reference 8\n\n  4. [4] Ba Jimmy Lei, Kiros Jamie Ryan, and Hinton Geoffrey E.. 2016. Layer normalization. arXiv preprint arXiv:1607.06450 (2016).Google Scholar\n\nReference\n\n  5. [5] Beltagy Iz, Peters Matthew E., and Cohan Arman. 2020. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150 (2020).Google Scholar\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n     * Reference 5\n     * Reference 6\n     * Reference 7\n     * Reference 8\n     * Reference 9\n\n  6. [6] Brown Tom B., Mann Benjamin, Ryder Nick, Subbiah Melanie, Kaplan Jared, Dhariwal Prafulla, Neelakantan Arvind, Shyam Pranav, Sastry Girish, Askell Amanda, et al. 2020. Language models are few-shot learners. arXiv preprint arXiv:2005.14165 (2020).Google Scholar\n\nReference 1Reference 2\n\n  7. [7] Carion Nicolas, Massa Francisco, Synnaeve Gabriel, Usunier Nicolas, Kirillov Alexander, and Zagoruyko Sergey. 2020. End-to-end object detection with transformers. arXiv preprint arXiv:2005.12872 (2020).Google Scholar\n\nReference\n\n  8. [8] Chen Beidi, Dao Tri, Winsor Eric, Song Zhao, Rudra Atri, and R\u00e9 Christopher. 2021. Scatterbrain: Unifying sparse and low-rank attention. In Thirty-Fifth Conference on Neural Information Processing Systems.Google Scholar\n\nReference\n\n  9. [9] Child Rewon, Gray Scott, Radford Alec, and Sutskever Ilya. 2019. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509 (2019).Google Scholar\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n     * Reference 5\n     * Reference 6\n     * Reference 7\n     * Reference 8\n     * Reference 9\n\n  10. [10] Choromanski Krzysztof, Likhosherstov Valerii, Dohan David, Song Xingyou, Davis Jared, Sarlos Tamas, Belanger David, Colwell Lucy, and Weller Adrian. 2020. Masked language modeling for proteins via linearly scalable long-context transformers. In Proceedings of ICLR (2020).Google Scholar\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n     * Reference 5\n     * Reference 6\n     * Reference 7\n     * Reference 8\n\n  11. [11] Choromanski Krzysztof, Likhosherstov Valerii, Dohan David, Song Xingyou, Gane Andreea, Sarlos Tamas, Hawkins Peter, Davis Jared, Mohiuddin Afroz, Kaiser Lukasz, et al. 2020. Rethinking attention with performers. In Proceedings of ICLR (2020).Google Scholar\n\nReference 1Reference 2Reference 3\n\n  12. [12] Chowdhery Aakanksha, Narang Sharan, Devlin Jacob, Bosma Maarten, Mishra Gaurav, Roberts Adam, Barham Paul, Chung Hyung Won, Sutton Charles, Gehrmann Sebastian, et al. 2022. PaLM: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311 (2022).Google Scholar\n\nReference\n\n  13. [13] Clevert Djork-Arn\u00e9, Unterthiner Thomas, and Hochreiter Sepp. 2015. Fast and accurate deep network learning by exponential linear units (ELUS). In Proceedings of ICLR 2016 (2015).Google Scholar\n\nReference\n\n  14. [14] Correia Gon\u00e7alo M., Niculae Vlad, and Martins Andr\u00e9 F. T.. 2019. Adaptively sparse transformers. In Proceedings of EMNLP (2019).Google Scholar\n\nReference 1Reference 2Reference 3\n\n  15. [15] Dai Zihang, Lai Guokun, Yang Yiming, and Le Quoc V.. 2020. Funnel-transformer: Filtering out sequential redundancy for efficient language processing. In Proceedings of NeurIPS (2020).Google Scholar\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n\n  16. [16] Dai Zihang, Yang Zhilin, Yang Yiming, Carbonell Jaime, Le Quoc V., and Salakhutdinov Ruslan. 2019. Transformer-XL: Attentive language models beyond a fixed-length context. In Proceedings of ACL (2019).Google Scholar\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n     * Reference 5\n\n  17. [17] Dehghani Mostafa, Arnab Anurag, Beyer Lucas, Vaswani Ashish, and Tay Yi. 2021. The efficiency misnomer. arXiv preprint arXiv:2110.12894 (2021).Google Scholar\n\nReference\n\n  18. [18] Dehghani Mostafa, Gouws Stephan, Vinyals Oriol, Uszkoreit Jakob, and Kaiser \u0141ukasz. 2018. Universal transformers. In Proceedings of ICLR (2018).Google Scholar\n\nReference 1Reference 2\n\n  19. [19] Deng Jia, Dong Wei, Socher Richard, Li Li-Jia, Li Kai, and Fei-Fei Li. 2009. Imagenet: A large-scale hierarchical image database. In Proceedings of the 2009 IEEE Conference on Computer Vision and Pattern Recognition. IEEE, 248\u2013255.Google ScholarCross Ref\n\nReference\n\n  20. [20] Devlin Jacob, Chang Ming-Wei, Lee Kenton, and Toutanova Kristina. 2018. BERT:: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL (2018).Google Scholar\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n\n  21. [21] Du Nan, Huang Yanping, Dai Andrew M., Tong Simon, Lepikhin Dmitry, Xu Yuanzhong, Krikun Maxim, Zhou Yanqi, Adams Wei Yu, Firat Orhan, Zoph Barret, Fedus Liam, Bosma Maarten, Zhou Zongwei, Wang Tao, Wang Yu Emma, Webster Kellie, Pellat Marie, Robinson Kevin, Meier-Hellstern Kathy, Duke Toju, Dixon Lucas, Zhang Kun, Le Quoc V., Wu Yonghui, Chen Zhifeng, and Cui Claire. 2021. GLaM: Efficient scaling of language models with mixture-of-experts. arxiv:2112.06905 [cs.CL].Google Scholar\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n\n  22. [22] Fan Angela, Stock Pierre, Graham Benjamin, Grave Edouard, Gribonval Remi, Jegou Herve, and Joulin Armand. 2020. Training with quantization noise for extreme fixed-point compression. arXiv preprint arXiv:2004.07320 (2020).Google Scholar\n\nReference\n\n  23. [23] Fedus William, Zoph Barret, and Shazeer Noam. 2021. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. arXiv preprint arXiv:2101.03961 (2021).Google Scholar\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n     * Reference 5\n     * Reference 6\n\n  24. [24] Gu Albert, Goel Karan, and R\u00e9 Christopher. 2021. Efficiently modeling long sequences with structured state spaces. In Proceedings of NeurIPS (2021).Google Scholar\n\nReference\n\n  25. [25] Guo Qipeng, Qiu Xipeng, Liu Pengfei, Shao Yunfan, Xue Xiangyang, and Zhang Zheng. 2019. Star-transformer. In Proceedings of NAACL (2019).Google Scholar\n\nReference\n\n  26. [26] Guo Yong, Zheng Yin, Tan Mingkui, Chen Qi, Chen Jian, Zhao Peilin, and Huang Junzhou. 2019. NAT: Neural architecture transformer for accurate and compact architectures. In Advances in Neural Information Processing Systems. 737\u2013748.Google Scholar\n\nReference\n\n  27. [27] Hinton Geoffrey, Vinyals Oriol, and Dean Jeff. 2015. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531 (2015).Google Scholar\n\nReference\n\n  28. [28] Ho Jonathan, Kalchbrenner Nal, Weissenborn Dirk, and Salimans Tim. 2019. Axial attention in multidimensional transformers. arXiv preprint arXiv:1912.12180 (2019).Google Scholar\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n\n  29. [29] Houlsby Neil, Giurgiu Andrei, Jastrzebski Stanislaw, Morrone Bruna, Laroussilhe Quentin De, Gesmundo Andrea, Attariyan Mona, and Gelly Sylvain. 2019. Parameter-efficient transfer learning for NLP. In Proceedings of ICML (2019).Google Scholar\n\nReference\n\n  30. [30] Huang Zilong, Wang Xinggang, Huang Lichao, Huang Chang, Wei Yunchao, and Liu Wenyu. 2019. CCNET: Criss-cross attention for semantic segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 603\u2013612.Google ScholarCross Ref\n\nReference 1Reference 2\n\n  31. [31] Jaegle Andrew, Borgeaud Sebastian, Alayrac Jean-Baptiste, Doersch Carl, Ionescu Catalin, Ding David, Koppula Skanda, Zoran Daniel, Brock Andrew, Shelhamer Evan, et al. 2021. Perceiver IO: A general architecture for structured inputs & outputs. arXiv preprint arXiv:2107.14795 (2021).Google Scholar\n\nReference 1Reference 2\n\n  32. [32] Jaszczur Sebastian, Chowdhery Aakanksha, Mohiuddin Afroz, Kaiser \u0141ukasz, Gajewski Wojciech, Michalewski Henryk, and Kanerva Jonni. 2021. Sparse is enough in scaling transformers. Advances in Neural Information Processing Systems 34 (2021).Google Scholar\n\nReference\n\n  33. [33] Jiao Xiaoqi, Yin Yichun, Shang Lifeng, Jiang Xin, Chen Xiao, Li Linlin, Wang Fang, and Liu Qun. 2019. TINYBERT: Distilling BERT for natural language understanding. arXiv preprint arXiv:1909.10351 (2019).Google Scholar\n\nReference\n\n  34. [34] Joshi Mandar, Choi Eunsol, Weld Daniel S., and Zettlemoyer Luke. 2017. TriviaQA: A large scale distantly supervised challenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Vancouver, Canada.Google ScholarCross Ref\n\nReference\n\n  35. [35] Kaiser Lukasz, Gomez Aidan N., and Chollet Francois. 2017. Depthwise separable convolutions for neural machine translation. In Proceedings of ICLR (2017).Google Scholar\n\nReference\n\n  36. [36] Katharopoulos Angelos, Vyas Apoorv, Pappas Nikolaos, and Fleuret Fran\u00e7ois. 2020. Transformers are RNNs: Fast autoregressive transformers with linear attention. arXiv preprint arXiv:2006.16236 (2020).Google Scholar\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n     * Reference 5\n     * Reference 6\n     * Reference 7\n\n  37. [37] Kitaev Nikita, Kaiser Lukasz, and Levskaya Anselm. 2020. Reformer: The efficient transformer. In Proceedings of the International Conference on Learning Representations. https://openreview.net/forum?id=rkgNKkHtvB.Google Scholar\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n     * Reference 5\n     * Reference 6\n     * Reference 7\n\n  38. [38] Krizhevsky Alex, Hinton Geoffrey, et al. 2009. Learning multiple layers of features from tiny images. Master thesis.Google Scholar\n\nReference\n\n  39. [39] Kwiatkowski Tom, Palomaki Jennimaria, Redfield Olivia, Collins Michael, Parikh Ankur, Alberti Chris, Epstein Danielle, Polosukhin Illia, Kelcey Matthew, Devlin Jacob, Lee Kenton, Toutanova Kristina N., Jones Llion, Chang Ming-Wei, Dai Andrew, Uszkoreit Jakob, Le Quoc, and Petrov Slav. 2019. Natural questions: A benchmark for question answering research. Transactions of the Association of Computational Linguistics (2019).Google ScholarCross Ref\n\nReference\n\n  40. [40] Lagunas Fran\u00e7ois, Charlaix Ella, Sanh Victor, and Rush Alexander M.. 2021. Block pruning for faster transformers. In Proceedings of EMNLP 2021 (2021).Google Scholar\n\nReference\n\n  41. [41] Lample Guillaume, Sablayrolles Alexandre, Ranzato Marc\u2019Aurelio, Denoyer Ludovic, and J\u00e9gou Herv\u00e9. 2019. Large memory layers with product keys. arXiv preprint arXiv:1907.05242 (2019).Google Scholar\n\nReference 1Reference 2\n\n  42. [42] Lan Zhenzhong, Chen Mingda, Goodman Sebastian, Gimpel Kevin, Sharma Piyush, and Soricut Radu. 2019. ALBERT: A lite BERTt for self-supervised learning of language representations. In Proceedings of ICLR (2019).Google Scholar\n\nReference\n\n  43. [43] Lee Juho, Lee Yoonho, Kim Jungtaek, Kosiorek Adam, Choi Seungjin, and Teh Yee Whye. 2019. Set transformer: A framework for attention-based permutation-invariant neural networks. In Proceedings of the International Conference on Machine Learning. 3744\u20133753.Google Scholar\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n     * Reference 5\n\n  44. [44] Lee-Thorp James, Ainslie Joshua, Eckstein Ilya, and Ontanon Santiago. 2021. FNet: Mixing tokens with fourier transforms. arXiv preprint arXiv:2105.03824 (2021).Google Scholar\n\nReference\n\n  45. [45] Lepikhin Dmitry, Lee HyoukJoong, Xu Yuanzhong, Chen Dehao, Firat Orhan, Huang Yanping, Krikun Maxim, Shazeer Noam, and Chen Zhifeng. 2020. GSHARD: Scaling giant models with conditional computation and automatic sharding. arXiv preprint arXiv:2006.16668 (2020).Google Scholar\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n     * Reference 5\n     * Reference 6\n     * Reference 7\n\n  46. [46] Lewis Mike, Bhosale Shruti, Dettmers Tim, Goyal Naman, and Zettlemoyer Luke. 2021. Base layers: Simplifying training of large, sparse models. arXiv preprint arXiv:2103.16716 (2021).Google Scholar\n\nReference\n\n  47. [47] Liu Hanxiao, Dai Zihang, So David R., and Le Quoc V.. 2021. Pay attention to MLPs. In Proceedings of NeurIPS (2021).Google Scholar\n\nReference\n\n  48. [48] Liu Peter J., Saleh Mohammad, Pot Etienne, Goodrich Ben, Sepassi Ryan, Kaiser Lukasz, and Shazeer Noam. 2018. Generating Wikipedia by summarizing long sequences. In Proceedings of ICLR (2018).Google Scholar\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n     * Reference 5\n     * Reference 6\n     * Reference 7\n\n  49. [49] Liu Ze, Lin Yutong, Cao Yue, Hu Han, Wei Yixuan, Zhang Zheng, Lin Stephen, and Guo Baining. 2021. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030 (2021).Google Scholar\n\nReference\n\n  50. [50] Ma Xuezhe, Kong Xiang, Wang Sinong, Zhou Chunting, May Jonathan, Ma Hao, and Zettlemoyer Luke. 2021. LUNA: Linear unified nested attention. In Proceedings of NeurIPS 2021.Google Scholar\n\nReference 1Reference 2\n\n  51. [51] Merity Stephen, Xiong Caiming, Bradbury James, and Socher Richard. 2017. Pointer sentinel mixture models. In Proceedings of ICLR (2017).Google Scholar\n\nReference\n\n  52. [52] Michel Paul, Levy Omer, and Neubig Graham. 2019. Are sixteen heads really better than one? InProceedings of NeurIPS (2019).Google Scholar\n\nReference\n\n  53. [53] Ott Myle, Edunov Sergey, Baevski Alexei, Fan Angela, Gross Sam, Ng Nathan, Grangier David, and Auli Michael. 2019. FAIRSEQ: A fast, extensible toolkit for sequence modeling. arXiv preprint arXiv:1904.01038 (2019).Google Scholar\n\nReference\n\n  54. [54] Parmar Niki, Ramachandran Prajit, Vaswani Ashish, Bello Irwan, Levskaya Anselm, and Shlens Jon. 2019. Stand-alone self-attention in vision models. In Advances in Neural Information Processing Systems. 68\u201380.Google Scholar\n\nReference\n\n  55. [55] Parmar Niki, Vaswani Ashish, Uszkoreit Jakob, Kaiser \u0141ukasz, Shazeer Noam, Ku Alexander, and Tran Dustin. 2018. Image transformer. In Proceedings of ICML 2018 (2018).Google Scholar\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n     * Reference 5\n     * Reference 6\n     * Reference 7\n\n  56. [56] Peng Hao, Pappas Nikolaos, Yogatama Dani, Schwartz Roy, Smith Noah A., and Kong Lingpeng. 2021. Random feature attention. In Proceedings of ICLR (2021).Google Scholar\n\nReference 1Reference 2\n\n  57. [57] Pfeiffer Jonas, Vuli\u0107 Ivan, Gurevych Iryna, and Ruder Sebastian. 2020. MAD-X: An adapter-based framework for multi-task cross-lingual transfer. In Proceedings of EMNLP (2020).Google Scholar\n\nReference\n\n  58. [58] Qiu Jiezhong, Ma Hao, Levy Omer, Yih Scott Wen-tau, Wang Sinong, and Tang Jie. 2019. Blockwise self-attention for long document understanding. arXiv preprint arXiv:1911.02972 (2019).Google Scholar\n\nReference 1Reference 2\n\n  59. [59] Rae Jack W., Potapenko Anna, Jayakumar Siddhant M., Hillier Chloe, and Lillicrap Timothy P.. 2020. Compressive transformers for long-range sequence modelling. In Proceedings of the International Conference on Learning Representations. https://openreview.net/forum?id=SylKikSYDH.Google Scholar\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n\n  60. [60] Raffel Colin, Shazeer Noam, Roberts Adam, Lee Katherine, Narang Sharan, Matena Michael, Zhou Yanqi, Li Wei, and Liu Peter J.. 2019. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 2020 (2019).Google Scholar\n\nReference 1Reference 2Reference 3\n\n  61. [61] Roller Stephen, Sukhbaatar Sainbayar, Szlam Arthur, and Weston Jason. 2021. Hash layers for large sparse models. arXiv preprint arXiv:2106.04426 (2021).Google Scholar\n\nReference\n\n  62. [62] Roy Aurko, Saffar Mohammad, Vaswani Ashish, and Grangier David. 2020. Efficient content-based sparse attention with routing transformers. In Proceedings of TACL (2020).Google Scholar\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n     * Reference 5\n     * Reference 6\n     * Reference 7\n     * Reference 8\n     * Reference 9\n\n  63. [63] Ryoo Michael S., Piergiovanni A. J., Arnab Anurag, Dehghani Mostafa, and Angelova Anelia. 2021. TokenLearner: Adaptive space-time tokenization for videos. Advances in Neural Information Processing Systems (NeurIPS).Google Scholar\n\nReference\n\n  64. [64] Sanh Victor, Debut Lysandre, Chaumond Julien, and Wolf Thomas. 2019. DistilBERT, A distilled version of BERT: Smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108 (2019).Google Scholar\n\nReference\n\n  65. [65] Schuster Tal, Fisch Adam, Jaakkola Tommi, and Barzilay Regina. 2021. Consistent accelerated inference via confident adaptive transformers. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Online and Punta Cana, Dominican Republic, 4962\u20134979. https://aclanthology.org/2021.emnlp-main.406.Google ScholarCross Ref\n\nReference\n\n  66. [66] Shen Sheng, Dong Zhen, Ye Jiayu, Ma Linjian, Yao Zhewei, Gholami Amir, Mahoney Michael W., and Keutzer Kurt. 2020. Q-BERT: Hessian based ultra low precision quantization of BERT. In Proceedings of AAAI.Google Scholar\n\nReference\n\n  67. [67] Sinkhorn Richard. 1964. A relationship between arbitrary positive matrices and doubly stochastic matrices. The Annals of Mathematical Statistics 35, 2 (1964), 876\u2013879.Google ScholarCross Ref\n\nReference\n\n  68. [68] So David R., Liang Chen, and Le Quoc V.. 2019. The evolved transformer. Proceedings of ICML (2019).Google Scholar\n\nReference\n\n  69. [69] Stickland Asa Cooper and Murray Iain. 2019. BERT and pals: Projected attention layers for efficient adaptation in multi-task learning. Proceedings of ICML (2019).Google Scholar\n\nReference\n\n  70. [70] Sukhbaatar Sainbayar, Grave Edouard, Bojanowski Piotr, and Joulin Armand. 2019. Adaptive attention span in transformers. arXiv preprint arXiv:1905.07799 (2019).Google Scholar\n\nReference\n\n  71. [71] Sukhbaatar Sainbayar, Grave Edouard, Lample Guillaume, Jegou Herve, and Joulin Armand. 2019. Augmenting self-attention with persistent memory. arXiv preprint arXiv:1907.01470 (2019).Google Scholar\n\nReference 1Reference 2\n\n  72. [72] Tang Raphael, Lu Yao, Liu Linqing, Mou Lili, Vechtomova Olga, and Lin Jimmy. 2019. Distilling task-specific knowledge from bert into simple neural networks. arXiv preprint arXiv:1903.12136 (2019).Google Scholar\n\nReference\n\n  73. [73] Tay Yi, Bahri Dara, Metzler Donald, Juan Da-Cheng, Zhao Zhe, and Zheng Che. 2020. Synthesizer: Rethinking self-attention in transformer models. In Proceedings of ICML, 2021 (2020).Google Scholar\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n     * Reference 5\n     * Reference 6\n\n  74. [74] Tay Yi, Bahri Dara, Yang Liu, Metzler Donald, and Juan Da-Cheng. 2020. Sparse sinkhorn attention. In Proceedings of ICML (2020).Google Scholar\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n     * Reference 5\n     * Reference 6\n     * Reference 7\n     * Reference 8\n     * Reference 9\n\n  75. [75] Tay Yi, Dehghani Mostafa, Abnar Samira, Shen Yikang, Bahri Dara, Pham Philip, Rao Jinfeng, Yang Liu, Ruder Sebastian, and Metzler Donald. 2021. Long range arena: A benchmark for efficient transformers. Proceedings of ICLR (2021).Google Scholar\n\nReference\n\n  76. [76] Tay Yi, Dehghani Mostafa, Gupta Jai, Bahri Dara, Aribandi Vamsi, Qin Zhen, and Metzler Donald. 2021. Are pre-trained convolutions better than pre-trained transformers?arXiv preprint arXiv:2105.03322 (2021).Google Scholar\n\nReference\n\n  77. [77] Tay Yi, Tran Vinh Q., Dehghani Mostafa, Ni Jianmo, Bahri Dara, Mehta Harsh, Qin Zhen, Hui Kai, Zhao Zhe, Gupta Jai, et al. 2022. Transformer memory as a differentiable search index. arXiv preprint arXiv:2202.06991 (2022).Google Scholar\n\nReference 1Reference 2\n\n  78. [78] Tay Yi, Tran Vinh Q., Ruder Sebastian, Gupta Jai, Chung Hyung Won, Bahri Dara, Qin Zhen, Baumgartner Simon, Yu Cong, and Metzler Donald. 2021. Charformer: Fast character transformers via gradient-based subword tokenization. arXiv preprint arXiv:2106.12672 (2021).Google Scholar\n\nReference\n\n  79. [79] Tay Yi, Zhang Aston, Tuan Luu Anh, Rao Jinfeng, Zhang Shuai, Wang Shuohang, Fu Jie, and Hui Siu Cheung. 2019. Lightweight and efficient neural natural language processing with quaternion networks. In Proceedings of ACL (2019).Google Scholar\n\nReference\n\n  80. [80] Tay Yi, Zhao Zhe, Bahri Dara, Metzler Donald, and Juan Da-Cheng. 2020. HyperGrid: Efficient multi-task transformers with grid-wise decomposable hyper projections. In Proceedings of ICLR (2020).Google Scholar\n\nReference\n\n  81. [81] Tolstikhin Ilya, Houlsby Neil, Kolesnikov Alexander, Beyer Lucas, Zhai Xiaohua, Unterthiner Thomas, Yung Jessica, Steiner Andreas Peter, Keysers Daniel, Uszkoreit Jakob, et al. 2021. Mlp-mixer: An all-mlp architecture for vision. In Thirty-Fifth Conference on Neural Information Processing Systems.Google Scholar\n\nReference 1Reference 2\n\n  82. [82] Vaswani Ashish, Shazeer Noam, Parmar Niki, Uszkoreit Jakob, Jones Llion, Gomez Aidan N, Kaiser \u0141ukasz, and Polosukhin Illia. 2017. Attention is all you need. In Advances in Neural Information Processing Systems. 5998\u20136008.Google Scholar\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n\n  83. [83] Voita Elena, Talbot David, Moiseev Fedor, Sennrich Rico, and Titov Ivan. 2019. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Florence, Italy, 5797\u20135808. Google ScholarCross Ref\n\nReference\n\n  84. [84] Vyas Apoorv, Katharopoulos Angelos, and Fleuret Fran\u00e7ois. 2020. Fast transformers with clustered attention. In Proceedings of NeurIPS (2020).Google Scholar\n\nReference 1Reference 2\n\n  85. [85] Wang Alex, Singh Amanpreet, Michael Julian, Hill Felix, Levy Omer, and Bowman Samuel. 2018. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. Association for Computational Linguistics, Brussels, Belgium, 353\u2013355. Google ScholarCross Ref\n\nReference\n\n  86. [86] Wang Hanrui, Wu Zhanghao, Liu Zhijian, Cai Han, Zhu Ligeng, Gan Chuang, and Han Song. 2020. HAT: Hardware-aware transformers for efficient natural language processing. arXiv preprint arXiv:2005.14187 (2020).Google Scholar\n\nReference\n\n  87. [87] Wang Sinong, Li Belinda, Khabsa Madian, Fang Han, and Ma Hao. 2020. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768 (2020).Google Scholar\n\nNavigate to\n\n     * Reference 1\n     * Reference 2\n     * Reference 3\n     * Reference 4\n     * Reference 5\n     * Reference 6\n     * Reference 7\n\n  88. [88] Wang Shuohang, Zhou Luowei, Gan Zhe, Chen Yen-Chun, Fang Yuwei, Sun Siqi, Cheng Yu, and Liu Jingjing. 2020. Cluster-former: Clustering-based sparse transformer for long-range dependency encoding. In Proceedings of ACL-IJCNLP (Findings) (2020).Google Scholar\n\nReference 1Reference 2\n\n  89. [89] Weissenborn Dirk, T\u00e4ckstr\u00f6m Oscar, and Uszkoreit Jakob. 2019. Scaling autoregressive video models. In Proceedings of ICLR (2019).Google Scholar\n\nReference\n\n  90. [90] Winata Genta Indra, Cahyawijaya Samuel, Lin Zhaojiang, Liu Zihan, and Fung Pascale. 2020. Lightweight and efficient end-to-end speech recognition using low-rank transformer. In Proceedings of the 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP-2020). IEEE, 6144\u20136148.Google ScholarCross Ref\n\nReference\n\n  91. [91] Xiong Yunyang, Zeng Zhanpeng, Chakraborty Rudrasis, Tan Mingxing, Fung Glenn, Li Yin, and Singh Vikas. 2021. Nystr\\\\( \\backslash \\\\)\u201c omformer: A Nystr\\\\( \\backslash \\\\)\u201d om-based algorithm for approximating self-attention. Proceedings of AAAI (2021).Google ScholarCross Ref\n\nReference 1Reference 2\n\n  92. [92] Yun Chulhee, Chang Yin-Wen, Bhojanapalli Srinadh, Rawat Ankit Singh, Reddi Sashank J., and Kumar Sanjiv. 2020. \\\\( O (n) \\\\) Connections are expressive enough: Universal approximability of sparse transformers. In Proceedings of NeurIPS (2020).Google Scholar\n\nReference\n\n  93. [93] Zaheer Manzil, Guruganesh Guru, Dubey Avinava, Ainslie Joshua, Alberti Chris, Ontanon Santiago, Pham Philip, Ravula Anirudh, Wang Qifan, Yang Li, et al. 2020. Big bird: Transformers for longer sequences. In Proceedings of NeurIPS (2020).Google Scholar\n\nReference 1Reference 2\n\n  94. [94] Zaheer Manzil, Kottur Satwik, Ravanbakhsh Siamak, Poczos Barnabas, Salakhutdinov Russ R., and Smola Alexander J.. 2017. Deep sets. In Advances in Neural Information Processing Systems. 3391\u20133401.Google Scholar\n\nReference\n\n  95. [95] Zhang Hang, Gong Yeyun, Shen Yelong, Li Weisheng, Lv Jiancheng, Duan Nan, and Chen Weizhu. 2021. Poolingformer: Long document modeling with pooling attention. In Proceedings of ICML (2021).Google Scholar\n\nReference 1Reference 2\n\n  96. [96] Zhu Chen, Ping Wei, Xiao Chaowei, Shoeybi Mohammad, Goldstein Tom, Anandkumar Anima, and Catanzaro Bryan. 2021. Long-short transformer: Efficient transformers for language and vision. Advances in Neural Information Processing Systems 34 (2021).Google Scholar\n\nReference 1Reference 2Reference 3\n\n## Cited By\n\nView all\n\n## Index Terms\n\n  1. ###### Efficient Transformers: A Survey\n\n    1. Computing methodologies\n\n      1. Machine learning\n\n        1. Machine learning algorithms\n\n## Recommendations\n\n  * ##### Evaluation of the Transformer Architecture for Univariate Time Series Forecasting\n\nAdvances in Artificial Intelligence\n\nAbstract\n\nThe attention-based Transformer architecture is earning increasing popularity\nfor many machine learning tasks. In this study, we aim to explore the\nsuitability of Transformers for time series forecasting, which is a crucial\nproblem in different ...\n\nRead More\n\n  * ##### An Attentive Survey of Attention Models\n\nAttention Model has now become an important concept in neural networks that\nhas been researched within diverse application domains. This survey provides a\nstructured and comprehensive overview of the developments in modeling\nattention. In particular, we ...\n\nRead More\n\n  * ##### Exploring transformers for behavioural biometrics: A case study in gait recognition\n\nHighlights\n\n    * An in-depth analysis of state-of-the-art deep learning approaches for gait recognition on mobile devices.\n\nAbstract\n\nBiometrics on mobile devices has attracted a lot of attention in recent years\nas it is considered a user-friendly authentication method. This interest has\nalso been motivated by the success of Deep Learning (DL). Architectures based\non ...\n\nRead More\n\n## Comments\n\n### Login options\n\nCheck if you have access through your login credentials or your institution to\nget full access on this article.\n\nSign in\n\n### Full Access\n\nGet this Article\n\n  * Information\n  * Contributors\n\n  * ### Published in\n\nACM Computing Surveys Volume 55, Issue 6\n\nJune 2023\n\n781 pages\n\nISSN:0360-0300\n\nEISSN:1557-7341\n\nDOI:10.1145/3567471\n\n    * Editor:\n    * Albert Zomaya\n\nUniversity of Sydney, Australia\n\nIssue\u2019s Table of Contents\n\nCopyright \u00a9 2022 Copyright held by the owner/author(s).\n\nThis work is licensed under a Creative Commons Attribution-NoDerivs\nInternational 4.0 License.\n\n### Sponsors\n\n### In-Cooperation\n\n### Publisher\n\nAssociation for Computing Machinery\n\nNew York, NY, United States\n\n### Publication History\n\n    * Published: 7 December 2022\n    * Online AM: 22 April 2022\n    * Accepted: 6 April 2022\n    * Revised: 15 December 2021\n    * Received: 6 April 2021\n\nPublished in csur Volume 55, Issue 6\n\n### Permissions\n\nRequest permissions about this article.\n\nRequest Permissions\n\n### Check for updates\n\n### Author Tags\n\n    * Transformers\n    * attention\n    * deep learning\n    * neural networks\n\n### Qualifiers\n\n    * survey\n    * Refereed\n\n### Conference\n\n### Funding Sources\n\n  * ### Other Metrics\n\nView Article Metrics\n\n  * Bibliometrics\n  * Citations62\n\n  * ### Article Metrics\n\n    * 62\n\nTotal Citations\n\nView Citations\n\n    * 29,219\n\nTotal Downloads\n\n    * Downloads (Last 12 months)17,410\n    * Downloads (Last 6 weeks)2,467\n\n### Other Metrics\n\nView Author Metrics\n\n  * ### Cited By\n\nView all\n\n### PDF Format\n\nView or Download as a PDF file.\n\nPDF\n\n### eReader\n\nView online with eReader.\n\neReader\n\n### Digital Edition\n\nView this article in digital edition.\n\nView Digital Edition\n\n### HTML Format\n\nView this article in HTML Format .\n\nView HTML Format\n\n  * Figures\n  * Other\n\n### Share this Publication link\n\n### Share on Social Media\n\nShare on\n\nClose Figure Viewer\n\nBrowse AllReturn\n\n### Caption\n\nView Issue\u2019s Table of Contents\n\n## Export Citations\n\n## Footer\n\n### Categories\n\n  * Journals\n  * Magazines\n  * Books\n  * Proceedings\n  * SIGs\n  * Conferences\n  * Collections\n  * People\n\n### About\n\n  * About ACM Digital Library\n  * ACM Digital Library Board\n  * Subscription Information\n  * Author Guidelines\n  * Using ACM Digital Library\n  * All Holdings within the ACM Digital Library\n  * ACM Computing Classification System\n  * Digital Library Accessibility\n\n### Join\n\n  * Join ACM\n  * Join SIGs\n  * Subscribe to Publications\n  * Institutions and Libraries\n\n### Connect\n\n  * Contact\n  * Facebook\n  * Twitter\n  * Linkedin\n  * Feedback\n  * Bug Report\n\nThe ACM Digital Library is published by the Association for Computing\nMachinery. Copyright \u00a9 2024 ACM, Inc.\n\n  * Terms of Usage\n  * Privacy Policy\n  * Code of Ethics\n\nYour Search Results Download Request\n\nWe are preparing your search results for download ...\n\nWe will inform you here when the file is ready.\n\nDownload now!\n\nYour Search Results Download Request\n\nYour file of search results citations is now ready.\n\nDownload now!\n\nYour Search Results Download Request\n\nYour search export query has expired. Please try again.\n\n", "frontpage": false}
