{"aid": "40067486", "title": "Embeddings are a good starting point for the AI curious app developer", "url": "https://bawolf.substack.com/p/embeddings-are-a-good-starting-point", "domain": "bawolf.substack.com", "votes": 7, "user": "bryantwolf", "posted_at": "2024-04-17 17:09:14", "comments": 0, "source_title": "Embeddings are a good starting point for the AI curious app developer", "source_text": "Embeddings are a good starting point for the AI curious app developer\n\n# Bryant\u2019s Newsletter\n\nShare this post\n\n#### Embeddings are a good starting point for the AI curious app developer\n\nbawolf.substack.com\n\n#### Discover more from Bryant\u2019s Newsletter\n\nContinue reading\n\nSign in\n\n# Embeddings are a good starting point for the AI curious app developer\n\nBryant\n\nand\n\nV0.app\n\nApr 17, 2024\n\nShare this post\n\n#### Embeddings are a good starting point for the AI curious app developer\n\nbawolf.substack.com\n\nShare\n\nVector embeddings have been an Overton window shifting experience for me, not\nbecause they\u2019re sufficiently advanced technology indistinguishable from magic,\nbut the opposite. Once I started using them, it felt obvious that this was\nwhat the search experience was always supposed to be: less \u201cHow did you do\nthat?\u201d and more mundanely, \u201cWhy isn\u2019t this everywhere?\u201d\n\nThis feels like the right place to start if you\u2019re an app developer looking\nfor an excuse to dip your toes into this new AI world. Embeddings are just\narrays of numbers, but they contain a compressed form of a considerable amount\nof human knowledge and shrink features that used to be substantial specialized\nprojects into ones that individual product engineers can take on.\n\nThere are a ton of tooling options available to use embeddings. I\u2019ll highlight\nour choices and note where you might want to make different ones for your\nsituation. Here are some points I hope you take away:\n\n  * Vector embeddings work for search and recommendations because they\u2019re good at measuring similarity to arbitrary input. This even works for different spoken languages like French or Japanese.\n\n  * Pgvector is a Postgres extension that stores and queries embeddings without adding a new service. It\u2019s powerful because it can combine standard SQL logic with embedding operations.\n\n  * Unlike LLMs, working with embeddings feels like regular deterministic code.\n\n##\n\nThe project\n\nMy friend Charlie Yuan and I built this mini icon app to help people discover\nicons. It\u2019s pretty short and sweet. We have icon sets you can query, bookmark,\nand add to your project.\n\n###\n\nPgvector\n\nThere are a bunch of specialized vector databases to choose from. Instead, we\nchose Postgres with pgvector to blend embedding search with business logic\nlike filtering and scoring. While it\u2019s not the fastest vector database, we\ndidn\u2019t want to have to compare results across multiple data sources. Pgvector\nprobably already has a library for your favorite database client. Our project\nwas Typescript through and through, and we used drizzle-orm. The docs will be\na more robust place for setup documentation, so I\u2019m leaving out that part to\nfocus on the features you can build.\n\n###\n\nCreating embeddings\n\nOnce set up with pgvector, we created a strategy for encoding our icon data\ninto vector embeddings. Embeddings are points in many-dimensional space, up to\nthousands of dimensions. Unfortunately, the axes of that grid are not humanly\ngrokable ideas like \u201csize\u201d or \u201cbrightness.\u201d They\u2019re a bit of a black box.\nLuckily, like any good abstraction, they\u2019re a black box with a good API.\n\nThe best practice seems to be finding the details that best represent what\npeople want to search for and creating a function that outputs that as a\nstring. Our icons can be a part of many use-case-based \u2018categories\u2019 and have\nmany descriptive \u2018tags\u2019 associated with them. We encoded that information\nalong with the icon name because that best represents what the icon is.\nWhereas the name of the icon set the icon belongs to, or its dimensions aren\u2019t\nrelevant. The strings we generated looked basically like this:\n\n    \n    \n    const createIconEmbeddingsString = (icon) => `icon: \"${icon.name}\", categories: [${categories}] tags: [${tags}]`;\n\nNext, we chose an embedding model. OpenAI\u2019s embedding models will probably\nwork just fine. We\u2019re using their `text-embedding-3-small`. If you want to\ndive in, check out the leaderboard and pick the model that best meets your\nneeds. Whether you use an embeddings API or self-host an open source option,\nthe interface should be text in and embeddings out.\n\n##\n\nImplementing search\n\nMany sites implement search, but most icon sites implement search by text\nmatching or full-text search. If you\u2019re looking for a dog icon, they search\nover the icon metadata for icons that have \u2018dog\u2019 in them. If they want to get\ncraftier, they come up with a bag of words related to \u2018dog,\u2019 like maybe \u2018k9\u2019,\n\u2018puppy,\u2019 and \u2018woof\u2019 to catch near misses. That\u2019s pretty fragile. Someone has\nto choose tags for each icon; if they miss an important one, users won\u2019t find\nwhat they\u2019re looking for.\n\n###\n\nSimilarity search\n\nOur app gets relevant results when searching for \u2018dog.\u2019 We also get solid\nresults for \u2018puppy\u2019 without a bag of words by measuring the cosine similarity\nbetween the embeddings of your search query and each icon. There are multiple\nways to measure how similar embeddings are to each other, but OpenAI\u2019s\nembeddings are designed to work well with cosine distance. Cosine similarity\nis just the opposite of cosine distance. Order by cosine distance wherever you\ncan to take advantage of indexes.\n\n    \n    \n    cosine_similarity(x,y) = 1 - cosine distance(x,y)\n\nYou can even try dog breeds like \u2018hound,\u2019 \u2018poodle,\u2019 or my favorite \u2018samoyed.\u2019\nIt pretty much just works. But that\u2019s not all; it also works for other\nlanguages. Try \u2018chien\u2019 and even \u2018\u72ac\u20191! With pgvector, we can get these results\nwith simple SQL queries.\n\n    \n    \n    SELECT 1 - cosine_distance (search_query.embedding, icon.embedding) as similarity, * FROM icon join search_query on search_query.text = 'dog' ORDER BY cosine_distance (search_query.embedding, icon.embedding) ASC LIMIT 50;\n\nSince we\u2019re ordering by distance, every search returns every result in the\ntable in order of closeness. We cut off results by a fixed number to make this\nmanageable, limiting the query to the top 50. Using an arbitrary distance cut-\noff is tempting, like querying for results with a cosine similarity of less\nthan 0.8. Unfortunately, the absolute distance for one query to produce\ncorrect-feeling results might differ drastically from another. We limit by the\nnumber of results, not by a minimum value, whenever possible.\n\n###\n\nFiltering\n\nIf we only wanted similarity search, any vector database would be fine, but\nPostgres allowed us to layer more features on top. The initial search looks\nacross all icons in all icon sets, but our user\u2019s style might only match some\nof the icon sets. We can filter by values like in any other Postgres query.\n\n    \n    \n    SELECT 1 - cosine_distance (search_query.embedding, icon.embedding) AS similarity, * FROM icon JOIN search_query ON search_query.text = 'dog' JOIN icon_set ON icon_set.slug = icon.icon_set_slug WHERE icon_set.slug in('lucide', 'mdi') ORDER BY cosine_distance (search_query.embedding, icon.embedding) ASC LIMIT 50;\n\nThis is deterministic; everyone searching for a \u2018dog\u2019 will get the same\nresults. However, the inputs are still unbounded, so embedding search doesn\u2019t\nguarantee that the best results will be produced for every input. We can try\ndifferent embedding models or ways of encoding icons into embeddings to\nimprove the system.\n\n###\n\nA more complex algorithm\n\nThe embedding search usually puts the correct icon on the page, but the\ncorrect icon isn\u2019t always the first result. We could make a simple algorithm\nthat adjusts to user feedback and improves over time. To do this, we\u2019d count\nevery time a user clicks on an icon for a particular search query. When\nranking search results, we\u2019d create a score for each icon that combines the\nembedding search with the click data.\n\nHere\u2019s a simple ranking algorithm. The details look messy because there\u2019s some\nnull checking and unit conversions2, but here are the basics.\n\n  1. Get the cosine similarity of the icon for the search query. It will be a number between 0 and 1. Multiply it by 0.5.\n\n  2. Divide the number of clicks for each icon by the icon with the most clicks for that query. This normalizes the most clicked icon to 1, and the least clicked to 0. Multiply by 0.5.\n\n  3. The final score is these two values added together for a range between 0-1.3\n\n    \n    \n    SELECT ( 0.5 * COALESCE( -- so nulls are turned into 0 1 - cosine_distance (search_query.embedding, icon.embedding), 0 ) + 0.5 * -- so clicks matter less than embeddings. COALESCE( search_query_selection. \"count\"::decimal / max(search_query_selection. \"count\") OVER (), 0) ) AS score, icon.* FROM icon LEFT JOIN search_query_selection ON icon.id = search_query_selection.icon_id LEFT JOIN search_query ON search_query.text = 'dog' AND search_query.id = search_query_selection.search_query_id ORDER BY score DESC LIMIT 50;\n\nShould vector embedding distance and clicks be equally weighted? Should the\norder of magnitude of clicks matter more than the raw number? The algorithm\nmight need tuning, but this is just an example of how the database handles the\ncalculation nicely.\n\nWith a separate vector database, we might have to get values for all icons\nfrom both databases before comparing them in application code or making\ntradeoffs like pulling 100 results from the vector db and filtering the\nPostgres query for click score to those results or vice versa. Instead, we\nsimply query for results and display them.\n\n##\n\nSimilar recommendations\n\nAdditionally, we include a content forward section of each icon page with\nother icons in the same icon set and category. That way, you can see other\n\u2018navigation\u2019 icons when looking at `arrow-up` in case you need those.\nUnfortunately, not all of our icon sets have categories. In these cases, we\nmake a similar icons section using embeddings. Instead of getting an embedding\nfrom user input, we can compare the same cosine similarity measure against the\nselected icon\u2019s embedding.\n\n    \n    \n    WITH current_icon AS ( SELECT embedding, slug, icon_set_slug FROM icon WHERE icon_set_slug = 'lucide' AND slug = 'activity' ) SELECT * FROM icon INNER JOIN current_icon ON icon.icon_set_slug = current_icon.icon_set_slug AND icon.slug != current_icon.slug ORDER BY 1 - cosine_distance ( icon.embedding, current_icon.embedding ) LIMIT 50;\n\n#\n\nConclusion\n\nI hope you have a good experience playing with our app and vector embeddings!\nWe owe a big thanks to all the engineers who pushed the state of the art so\nfar that we can stand on their shoulders. I hope this modest contribution\nhelps make adding embedding features to your app approachable!\n\nHere is a summary of our implementation decisions and some sources for other\noptions.\n\n###\n\nVector database\n\nWe chose pgvector/Postgres, but there are plenty of other choices, including\nsome for other standard databases like MongoDB.\n\n###\n\nPgvector client\n\nWe worked in Typescript and chose drizzle-orm. I\u2019ve also worked in the Phoenix\nelixir ecosystem using the elixir library with Ecto. You can find a library\nfor your client of choice here.\n\n###\n\nDatabase host\n\nOur app is hosted on Neon. I\u2019ve also used fly.io, though their option wasn\u2019t\nreally a managed instance at the time. Now, they have a managed solution with\nSupabase. If you want to look up someone else, Pgvector has a clumsy list of\nhosts that support it in this git issue.\n\n###\n\nEmbedding model\n\nWe chose OpenAI\u2019s `text-embedding-3-small`. If you want to try something else,\ncheck out Huggingface\u2019s leaderboard.\n\n###\n\nEmbedding string\n\nWe embedded strings of key and value pairs for the attributes we thought best\ndescribed our icons. It doesn\u2019t seem like any of the major players are doing\nanything crazy here; the significant knobs appear to be whether or not to\nembed keys or just values and which attributes of your records are relevant.\nOther examples in OpenAI\u2019s cookbook show other choices.\n\n###\n\nDistance metric\n\nWe used cosine similarity as our distance function because that\u2019s what OpenAI\nrecommends for their embeddings. Other embeddings may be optimized for\ndifferent strategies. Pgvector supports l2 distance, inner product, and cosine\ndistance.\n\n###\n\nSearch Size\n\nIn these examples, we limited our queries to the top 50 results. You can also\nlimit your search to be above or below a certain distance threshold. That\ndoesn\u2019t seem super reliable. Relative distances seem more meaningful than\ndiscrete amounts. If you\u2019re set on using a threshold, I\u2019d recommend keeping it\npretty wide, like 0.1 or 0.05, and using it with a limit. You may get some\nmileage using a wide range to avoid returning irrelevant long-tail results.\n\n1\n\nEven when the results aren\u2019t icons of dogs, they still feel relevant to the\nquery, like how the \u2018snowshoeing\u2019 or \u2018snowman\u2019 icons rank highly for\n\u2018samoyed,\u2019 a type of dog bred for sled pulling.\n\n2\n\nCOALESCE is just there, so null values become 0. `over ()` is a window\nfunction, so we can get the `max` of our desired subset of clicks without\nusing `group by.` If you don\u2019t cast the count to a decimal, the division will\ntruncate, and 0.45 will be rounded to 0.\n\n3\n\nThis algorithm is similar to the one used to rank relevant memories in the\nSimulacra paper, which describes a 0-player game in which AI agents take on\nthe role of 8-bit characters in a simulated town.\n\n### Subscribe to Bryant\u2019s Newsletter\n\nLaunched 13 minutes ago\n\nShare this post\n\n#### Embeddings are a good starting point for the AI curious app developer\n\nbawolf.substack.com\n\nShare\n\nA guest post by| V0.appModern tools for frontend developers| Subscribe to\nV0.app  \n---|---  \n  \nComments\n\nReady for more?\n\n\u00a9 2024 Bryant Wolf\n\nPrivacy \u2219 Terms \u2219 Collection notice\n\nStart WritingGet the app\n\nSubstack is the home for great culture\n\nShare\n\n## Create your profile\n\n## Only paid subscribers can comment on this post\n\nAlready a paid subscriber? Sign in\n\n#### Check your email\n\nFor your security, we need to re-authenticate you.\n\nClick the link we sent to , or click here to sign in.\n\n", "frontpage": true}
