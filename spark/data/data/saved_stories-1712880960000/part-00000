{"aid": "40005104", "title": "Nvidia Blackwell Perf TCO Analysis \u2013 B100 vs. B200 vs. GB200NVL72", "url": "https://www.semianalysis.com/p/nvidia-blackwell-perf-tco-analysis", "domain": "semianalysis.com", "votes": 1, "user": "sshroot", "posted_at": "2024-04-11 18:14:36", "comments": 0, "source_title": "Nvidia Blackwell Perf TCO Analysis - B100 vs B200 vs GB200NVL72", "source_text": "Nvidia Blackwell Perf TCO Analysis - B100 vs B200 vs GB200NVL72\n\nShare this post\n\n#### Nvidia Blackwell Perf TCO Analysis - B100 vs B200 vs GB200NVL72\n\nwww.semianalysis.com\n\n# Nvidia Blackwell Perf TCO Analysis - B100 vs B200 vs GB200NVL72\n\n### GPT-4 Profitability, Cost, Inference Simulator, Parallelism Explained,\nPerformance TCO Modeling In Large & Small Model Inference and Training\n\nDylan Patel\n\nand\n\nDaniel Nishball\n\nApr 10, 2024\n\n\u2219 Paid\n\n74\n\nShare this post\n\n#### Nvidia Blackwell Perf TCO Analysis - B100 vs B200 vs GB200NVL72\n\nwww.semianalysis.com\n\n14\n\nShare\n\nNvidia\u2019s announcement of the B100, B200, and GB200 has garnered more attention\nthan even iPhone launches, at least among the nerds of the world. The real\nquestion that everyone is asking is, what is the real performance increase?\nNvidia\u2019s claimed 30x, but is that true? Moreover, the question is really, what\nis the performance/TCO?\n\nIn the last generation, with the H100, the performance/TCO uplift over the\nA100 was poor due to the huge increase in pricing, with the A100 actually\nhaving better TCO than the H100 in inference because of the H100\u2019s anemic\nmemory bandwidth gains and massive price increase from the A100\u2019s trough\npricing in Q3 of 2022. This didn\u2019t matter much though because the massive\nrequirements in the AI industry for training as opposed to inference benefited\nmore from H100\u2019s greater FLOPS performance, and most the price increase was\ndriven by opportunistically high margins from Nvidia.\n\nWith Blackwell, that all changes as pricing is not up nearly as much\ngenerationally. This is due to competition from existing huge volumes of H100s\nand H200s and emerging challengers such as hyperscale custom silicon, AMD\u2019s\nMI300X and Intel\u2019s Gaudi 2/3 entering the market and making their own\nperformance/TCO arguments. As such, Nvidia must make a compelling argument to\nsell their new generation. Of course, they aren\u2019t letting competition encroach\nin, with very aggressive, perhaps even benevolent pricing.\n\nSource: Nvidia\n\nNvidia has claimed as much as 30x higher performance for Blackwell over\nHopper. The problem is that the 30x figure is based on a very specific best-\ncase scenario. To be clear, this scenario is certainly realistic and possible\nto achieve (outside of unfair quantization differences) but is not exactly a\nscenario that is representative of the market. Today, let\u2019s walk through\nNvidia\u2019s performance claims, and home in on the actual performance uplift for\na variety of applications including inference and training on a variety of\nmodel sizes using the LLM model performance simulator that we have been\nbuilding for over the last 1.5 years. We will also dissect whether the\ncompetition has a shot when selling their merchant silicon and if hyperscale\nsilicon is competitive with Nvidia\u2019s new offerings despite a massive cost\ndifference.\n\nThere are a handful of primary workloads that should be tracked, and each has\nvarying characteristics. Inference versus training is quite different\nobviously due to the backwards pass in training and difference in batch sizes.\nWorking with large size models can lead to very different performance\ncharacteristics as well as one breaks GPU and node boundaries \u2013 for instance\nextending parallelism beyond the 8 GPUs in a typical HGX H100 server.\n\nMany folks tend to focus in on small model inference today (<100 billion\nparameters) when discussing performance, but with Blackwell pushing the cost\nof inference down so dramatically, and with workload needs continuing to be\nmet poorly by small models, combined with the release of open models such\nDatabricks DBRX 132B, xAI Grok-1 314B Cohere Command R+ 104B, Mistral 8x22B,\nas well as the upcoming Meta LLAMA 3 release, it is clear that the focus will\nshift back towards inference performance for large models.\n\nLarger than 100 billion parameter models will be the new norm for \u201csmall\nmodel\u201d fine tuning and inference, and larger than 1 trillion parameter sparse\nmodels will be the norm among large models. To be clear, these large models\nalready take up most of the inference and training compute today. The bar for\nlarge models will only continue to move higher with future model releases.\nRemember that to make economic sense, hardware has to stick around and be\neffective for 4-6 years, not just until the next model release.\n\nLet\u2019s start off by looking at specifications before diving into our LLM\nperformance simulator and what it tells us about large and small model\ninference versus training.\n\n##\n\nSpecifications \u2013 More Than Meets The Eye\n\nThe performance gains showcased at the keynote speech were achieved through\nimprovements across multiple dimensions \u2013 with the foundation and easiest to\nunderstand factor being simply the improvements in memory bandwidth and\nfloating-point operation (FLOPS) capacity.\n\nThe air-cooled 700W B100 will be the first to ship and will deliver 1,750\nTFLOPS of FP16/BF16 compute. The B100\u2019s baseboard is made to slot into the\nsame design used in today\u2019s HGX H100 systems \u2013 forcing the B100 to run at\nlower power and clock speeds to remain within the thermal envelope of existing\nsystems. Very soon after the B100 ships, the B200 will come to market at a\nhigher power and faster clock speed, delivering 2,250 TFLOPS of FP16/BF16\ncompute. Furthermore, the use of liquid cooling in the GB200 NVL72 will allow\nthe Blackwell GPU to run at even higher power levels, unlocking further\nperformance upside \u2013 delivering 2,500 TFLOPS of FP16/BF16 compute \u2013 a 153%\nimprovement over the H100 and H200. There is also a 1200W B200 that is not\nincluded in the table.\n\nSource: Nvidia, SemiAnalysis\n\nFLOPS are only up 77% on FP16 and TF32 with B100, but as power increases, and\ncombining further quantization, total FLOPS can scale to as much as 4x. Memory\nBandwidth, arguably the most important specification upgrade, grows from 3.4\nTB/s in the H100 and 4.8 TB/s in the H200 to up to 8.0 TB/s in the Blackwell\nfamily - this most directly improves inference throughput and interactivity\n(tokens/second/user) because inference is often memory bandwidth constrained.\n\nWe should note that, even in the worst-case scenario, FP16 vs FP16, FLOPS are\nup 153% gen on gen, but memory bandwidth gains are smaller. The bandwidth\ngains from A100 to H100 were larger than that of this generation. The memory\nwall is one of the greatest challenges facing the AI industry for future\nscaling.\n\nSource: Nvidia, SemiAnalysis\n\nMore important is to look at FLOPS times number of bits divided by bandwidth;\nthis metric shows the real story. At most number formats, the ratio is about\nthe same, i.e. the arithmetic intensity required to fully utilize the FLOPS\nstays stable. Putting aside the advent of a new number format, most code\nshould port and achieve similar utilization given the arithmetic intensity.\nOnce all the new tricks of the Blackwell tensor core are fully utilized\nthough, there should be better MFU on Blackwell versus Hopper generally.\n\nHowever \u2013 these performance gains should be viewed in the context of the fact\nthat the silicon area of Blackwell (~1600mm^2 with 208B transistors) is double\nthat of Hopper (~800mm^2 with 80B transistors). Nvidia, given the slowdown of\nMoore\u2019s law and 3nm issues, had to deliver generational performance without a\nreal process node shrink. Through the use of DTCO and a mild 6% optical\nprocess shrink Blackwell was still able to deliver twice the performance of\nHopper.\n\nWhen looking at raw TFLOPS/mm^2 of silicon, i.e. benching against logic\nmanufacturing cost, the B100 actually delivers less performance, a 77%\nimprovement in FLOPS, versus a ~100% growth in silicon area. This is due to\nthe underclocking to hit the existing 700W platforms for quick time to market,\nand it is only in the B200 and GB200 NVL72 where we see per silicon area\nimprovements.\n\nNormalizing by silicon area gain, the air-cooled B200 only delivers a 14% FP16\nFLOPS improvement per silicon area \u2013 hardly what one would expect from a\nbrand-new architecture. Because most of the performance gain was simply\nthrough more silicon area and quantization. People need to understand how\nmicroscaling works and solve FP8, FP6, and FP4 training with the Blackwell\narchitecture.\n\n#### Neural Network Quantization & Number Formats From First Principles\n\nDylan Patel\n\n\u00b7\n\nJan 11\n\nRead full story\n\n###### Also note throughout this piece we show FP4 and FP6 FLOPS for H100 and\nH200 as the same as FP8 FLOPS. While there is a slight overhead casting up the\nformat to FP8 after loading from memory as FP4, in compute bound scenarios,\nthe memory bandwidth savings reduce power consumption enough and give more\nheadroom for achieving max FLOPS, which effectively wipes away overhead in\nreal world use cases.\n\nGiven the premise that twice the silicon area should require twice the power,\nit is important to analyze the isopower performance gains \u2013 that is \u2013 the\nFLOPS achieved per GPU watt.\n\nSource: Nvidia, SemiAnalysis\n\nWhile the B100 does deliver 77% more FLOPS of FP16/BF16 with the same 700W of\npower, the B200 and GB200 both deliver diminishing improvement in FLOPS for\nevery incremental power delivered to the chip. The GB200\u2019s 47% improvement in\nTFLOPS per GPU Watt vs the H100 is helpful \u2013 but again hardly anything to\nwrite home about without further quantization of models, and as such is\ncertainly not enough to deliver the 30x inference performance showcased in the\nkeynote address.\n\nThe FLOPS cost similarly is unremarkable. The TFLOPS per $ for the GB200 NVL\nand B200 are not a meaningfully different story.\n\nSource: Nvidia, SemiAnalysis\n\nAs the above simple analysis makes clear, specs alone are only a small part of\nthe story and the vast majority of the claimed 30x inference performance gains\nare from quantization as well as architectural improvements along other game\nchanging dimensions.\n\n##\n\nModel Performance Investigated\n\nNvidia claims 30x performance gains with GB200 versus H200, but as the above\nanalysis demonstrates, no single specification comes anywhere close to that\nuplift.\n\nHow is that possible? Well, it\u2019s because systems matter more than just\nindividual chip specifications. FabricatedKnowledge had a fantastic think\npiece on the Jensen\u2019s \u201cDatacenter is the unit of compute\u201d line that he\u2019s been\nsaying for years but has finally come to fruition with the GB200 NVL72. We\nshould note that the NVLink backplane and rack level product is not new in the\ncontext of machine learning hardware.\n\nGoogle has been shipping 64 TPU passive copper connected subslices that are\noptically connected beyond that with fully water-cooled racks since 2018. The\nmajor distinction between TPUv5p (Viperfish) and TPUv5e (Viperlite) besides\nchip level specs is that the v5e (Viperlite) connects 256 TPUs with copper\nwhile not scaling out further and the v5p (Viperfish) connects 64 with copper\nand connects to the rest of the pod of 8960 through the Optical Circuit Switch\n(OCS).\n\n#### Google AI Infrastructure Supremacy: Systems Matter More Than\nMicroarchitecture\n\nDylan Patel, George Cozma, and Gerald Wong\n\n\u00b7\n\nApril 12, 2023\n\nRead full story\n\nBelow is probably the coolest chart we\u2019ve ever seen regarding machine learning\nperformance modeling and the search space of optimal total cost of ownership\n(TCO). It reveals a lot about LLM performance modeling and trends. It also\nshows a variety of different parallelism strategies, batch sizes.\n\nSource: Nvidia\n\nWhen running inference on smaller models on a single GPU, you generally end up\nwith a curve like the below which shows very high interactivity\n(tokens/second/user) at low batch sizes, and as you increase batch size,\nthroughput grows, but interactivity also comes down. There is a tradeoff curve\nbetween system throughput for all users (cost per token) versus single user\ninteractivity (user experience).\n\nSource: SemiAnalysis\n\nUnfortunately, the simplicity of a single tradeoff curve is wiped away with\nmassive models such as GPT-4. Massive models must be split amoung many GPUs,\nwhich introduces many complications.\n\n#### GPT-4 Architecture, Infrastructure, Training Dataset, Costs, Vision, MoE\n\nDylan Patel and Gerald Wong\n\n\u00b7\n\nJuly 10, 2023\n\nRead full story\n\nFor example, with GPT-4 MoE\u2019s 55B attention parameters and 16 experts of 111B\nparameters each across 120 layers, that\u2019s 1.831 trillion parameters at 8 bits\nper parameter, in total requiring 1,831 GB of memory.\n\nSource: SemiAnalysis\n\nIt is impossible for this model to fit on a single GPU or even an 8 GPU\nserver. As such, it must be split across dozens of GPUs. How the model is\nsplit is very important as each different configuration means very different\nperformance characteristics.\n\nLet\u2019s start simple and work our way up.\n\n##\n\nInference Parallelism Techniques \u2013 Pipeline Parallelism, Tensor Parallelism,\nExpert Parallelism and Data Parallelism\n\nParallelism, splitting tasks across multiple GPUs, is necessary just to fit\nthe model into a system. But as we will see \u2013 parallelism can do much more\nthan just alleviate this capacity constraint. Let\u2019s explain the most important\nforms of parallelism and how the use of varying configurations of parallelism\nis at the core of the performance gains.\n\n##\n\nPipeline Parallelism\n\nIn pipeline parallelism, the simplest form of parallelism, the model\u2019s layers\nare split up across multiple GPUs \u2013 in the example below using GPT-4 MoE,\nsplitting the 120 layers across 16 GPUs.\n\nEach token in each user\u2019s query is sent sequentially through each GPU during\nthe forward pass across all layers until it runs through the entire model.\nBecause there are fewer layers per GPU, the model can now be held across all\n16 GPUs.\n\nSource: SemiAnalysis\n\nHowever \u2013 since the tokens in the query still have to pass sequentially\nthrough all the GPUs in the pipeline parallelism setup \u2013 there is no net\nimprovement in interactivity (tokens/second/user).\n\nComparing the PP16 setup above to a hypothetical one GPU deployment, we see\nthat the interactivity is the same (ignoring the FLOPS and memory capacity\nconstraint of the 1 GPU system \u2013 which is triggered in the case of inference\non GPT-4 MoE with 100 users).\n\nThe main benefit of pipeline parallelism is that memory capacity pressures are\nalleviated \u2013 making the model fit, and not about making it fast.\n\n##\n\nTensor Parallelism\n\nBoth Pipeline Parallelism and Tensor Parallelism have the benefit of\novercoming memory capacity constraints (i.e. fitting the model into the\nsystem), but in tensor parallelism, every layer has its work distributed\nacross multiple GPUs generally across the hidden dimension. Intermediate work\nis exchanged via all-reductions across devices multiple times across self-\nattention, feed forward network, and layer normalizations for each layer. This\nrequires high bandwidth and especially needs very low latency.\n\nSource: Accelerating PyTorch Model Training\n\nIn effect, every GPU in the domain works together on every layer with every\nother GPU as if there were all one giant GPU. The below diagram demonstrates\nthe two networks within a DGX H100 \u2013 namely the NVLink Scale up network\n(multicolored lanes connecting to the NVSwitch) and the InfiniBand/Ethernet\nscale-out network accessed via the ConnectX-7 network interface cards in\norange.\n\nScale up networks like NVLink and Google\u2019s ICI enable tensor parallelism to be\nmuch faster than scale out networks.\n\nSource: Nvidia\n\nTensor Parallelism allows memory bandwidth to be pooled and shared across all\nGPUs. This means that instead of only 8,000 GB/s of memory bandwidth available\nto load model parameters for each layer during the forward pass, 128,000 GB/s\nis now available.\n\nIn the below example, interactivity (tokens/second/user) is 16 times better\nthan the PP16 example, at 69.9 tokens/second/user vs the 4.4 achieved in the\nPP16 system. Total system throughput is accordingly 16 times greater as well,\nin this simplistic example.\n\nSource: SemiAnalysis\n\nThe example presented above, however, is the perfect scenario, without taking\ninto account various factors that lead to the lower memory bandwidth\nutilization (MBU) that are observed in reality. The most important effect to\nnote here is the communications penalty created by the need for all-reduce and\nall-to-all operations among GPUs. The greater the number of GPUs in the tensor\nparallelism, the more this effect acts to diminish interactivity and\nthroughput.\n\nIn reality, pipeline parallelism achieves higher throughput than tensor\nparallelism due to less GPU-to-GPU communication bottlenecks. It also\ngenerally achieve higher batch sizes / MFU.\n\n##\n\nExpert Parallelism\n\nWhile in Tensor Parallelism, all of the GPUs work together to host all layers,\nin Expert Parallelism, the experts are split amongst different GPUs, but\nattention is replicated.\n\nIn the below example of an EP16 system, each GPU hosts one expert. This lowers\nthe total parameters loaded to only 166 B per expert domain, that is 55B for\nthe replicated attention and 111B for each expert.\n\nSource: SemiAnalysis\n\nThe fact that each expert domain also has to load the attention imposes\nadditional overhead in the form of memory bandwidth requirements per token.\nThus, with greater memory bandwidth needs in EP vs TP, the ratio of memory\nbandwidth available to bandwidth required to load the model is lower for the\nEP16 example vs the TP16 example, resulting in lower interactivity of 48.2 for\nEP16 vs 69.9 for TP16, and commensurately lower throughput.\n\nRecall that the use of Tensor Parallelism imposes a major communications\npenalty on memory bandwidth utilization, slowing down throughput. This effect\nis in opposition to the additional overhead from the attention layer in expert\nparallelism \u2013 and the degree of this communications penalty determines which\neffect dominates.\n\nSource: A Hybrid Tensor-Expert-Data Parallelism Approach to Optimize Mixture-\nof-Experts Training. Singh et al.\n\nExpert parallelism also has communication overhead as shown by the graphic\nabove, but splitting expert domains and replicating attention means\nsignificantly less overhead. Basically, the All Reduce and All to All\noperations on the right side of the graphic do not need to be executed in\nexpert parallelism. We should also note parallel transformers have a huge\nimprovement in inference and training costs, especially as model sizes scale,\nprecisely due to requiring less communication within each layer. There are\nchallenges implementing them at scale though, at least in the open model\nworld.\n\n##\n\nData Parallelism\n\nData parallelism is perhaps the simplest of all the parallelisms \u2013 essentially\nit replicates everything about the system without sharing or consolidating any\nsystem resources. It\u2019s like having a web server in the US and Asia. Different\nusers are hitting each server and they are running the same stuff, but\ncompletely independent of each other.\n\nIn the example below, there is no interactivity speed up because each Data\nParallel System of TP16 has already hit the memory wall. Keeping the same\nnumber of total users at 100, total throughput is also the same as in the TP16\nexample. Data parallelism increases the headroom at which you can increase the\nnumber of users before hitting the FLOPS constraint due to having 32 total\nGPUs worth of FLOPS available (vs 16 for TP16), but if we do not introduce\nmore users to the system, hence generating more throughput, going from TP16 to\nTP16 DP2 is a waste of resources.\n\nSource: SemiAnalysis\n\nIf we were to keep the 100 users on our prior TP16 system example but\nimplement two data parallel systems \u2013 that is 200 users in total, then we\nwould have twice the throughput. The most important benefit of data\nparallelism is that there is no overhead given each data parallel system\noperates completely independently. We should note that while for the other\nsystems we handwaved away all the overheads for sake of simplicity in the\nexplanation, with data parallel, there aren\u2019t any overheads!\n\n##\n\nStacking up Parallelism\n\nWe can stack up the various parallelism schemes as well to suit a given set of\nmodels, users, and interactivity as well as throughput objectives.\n\nIn the below example of TP2 EP8 Parallelism, we implement 8 expert domains,\nwith two GPUs in each domain operating in TP2 Tensor Parallelism. Compared to\nEP16 Parallelism, each expert domain now loads two experts\u2019 worth of\nparameters plus attention as opposed to one expert parameters plus attention \u2013\nthus reducing memory capacity needed overall and bandwidth overhead. The total\nsystem parameters memory requirement is therefore lower at 2,216 GB in TP2 EP8\nvs EP16 with 2,656 GB of memory requirements.\n\nThis enables higher interactivity at 57.8 tokens/s/user in TP2 EP8 vs 48.2\ntokens/s/user in EP16. However, while there is less memory capacity/bandwidth\noverhead \u2013 moving to TP2 EP8 will introduce a communications penalty which\nagain is omitted from this analysis.\n\nSource: SemiAnalysis\n\n##\n\n72-Way Parallelism Stacking\n\nThe unveiling of the GB200 NVL72 made waves \u2013 partially for the wrong reasons\n\u2013 and led to quite a bit of salivation over increased liquid cooling content\nand much higher data center power density. NVL72 enabled a non-blocking all to\nall network among 72 GPUs running at 900 GB/s unidirectional bandwidth, far\nfaster than the 50 GB/s (400G) currently delivered by InfiniBand/Ethernet\nscale out networks. More importantly than the bandwidth increases, the NVL72\nalso attains lower latency.\n\nPutting everything together - the main innovation of NVL72 is that it vastly\nexpands the set of parallelisms that are enabled by the NVLink network. The\nH100 and H200\u2019s 8 GPU NVLink network only allowed a small set of\nconfigurations \u2013 for instance, the list of practical permutations of Tensor\nParallelism and Expert Parallelism is not long: (TP1 EP8), (TP2 EP4), (TP4\nEP2) and (TP8, EP1). We say practical because you can run tensor parallelism\noutside of a single server, but doing this murders your performance.\n\n##\n\nBenchmarking the H200 on its Bad Hair Day\n\nWhen looking at the H200 running GPT-4 at FP8, the pareto optimal frontier of\nparallelism options cliffs off quite badly as the number of GPUs employed for\ntensor parallelism grows.\n\nAt TP8, the GPUs are all communicating via NVLink, and so the penalties aren\u2019t\nthat bad as interactivity (tokens per second per user) is scaled higher, but\nthen, all of a sudden, the throughput tanks. This is due to TP being extended\nbeyond 8 GPUs. To go from interactivity of ~6.3 to ~6.5 drives a ~23% decrease\nin total throughput per GPU. This is a very gnarly impact that is all because\ncommunications for tensor parallelism must now cross the boundary out of the\nNVLink network and onto InfiniBand/Ethernet.\n\nSource: Nvidia, SemiAnalysis\n\nThe primary cause of this is because the latency to get from one GPU to\nanother is relatively high when going through a ConnectX-7 NIC in addition to\na network switch. Furthermore, there are often DSPs and Optics involved, or\nAECs to pass through to cross server to server. In contrast, NVLink networks\nonly require passing through the NVLink Switch, and otherwise purely short\nreach copper.\n\nFurthermore, this impact occurs yet again when hitting >16 and >32 GPUs for\ntensor parallelism.\n\nWith this in mind - turning to the slide from the keynote speech \u2013 notice that\nthe slide Nvidia used has chosen to benchmark off of TP64, the worst possible\nparallelism scheme to run on the H200.\n\nSource: Nvidia\n\nNot only that, but Nvidia also intentionally handicapped the H200 and B200\nsystems with FP8 despite using FP4 on the GB200. All of these systems could\nbenefit from a memory perspective with using FP4, which would push all of\nthese curves to the right with regards to the interactivity metric.\nFurthermore, B200 was limited to FP8 despite also having 2x the FLOPs at FP4.\n\nThe most obvious factor explaining the 30x performance gain is comparing GB200\nNVL performance at FP4 vs the H200 and B200 using FP8 quantization. Peeling\nthis in our simulator shows we have only an ~18x performance gain from the\nH200 to the GB200. Not nearly as shocking as Nvidia made it out to be with 30x\ngains, but still an incredibly impressive feat.\n\nThe next most impactful factor is somewhat more subtle \u2013 the benchmark\nscenario which imposes a 32k input, 1k output for GPT-4 with a 5 second time\nto first token (TTFT) generation constraint on all benchmarks. Prefill is\nextremely FLOPS bound, and as such any limitations here are very hard on lower\nFLOPS systems like the H200. Furthermore, by maxing out the prefill tokens per\nuser, and minimizing decode, these constraints get even tighter.\n\nThis scenario games the benchmark by effectively eliminating all large batch\nsize system setups using an H200 system, as running a large batch size on an\nH200 system would blow way past the 5 seconds time to first token constraint\ndue the lower FLOPS. Without large batch sizes, there is no way the H200\nsystem can deliver high overall system throughput \u2013 meaning the H200 curves\nare lower than they otherwise would be if not for the 5 second constraint.\n\nTo be clear, it is very desirable to have lower time to first token, but that\nis a tradeoff for the user to consider and make. This benchmark diminishes the\nperformance envelope in terms of throughput that could be achieved by an H200\nsystem albeit at some tradeoff against time to first token.\n\nIf this was a 512 input 2k output scenario, with the same 5 second time to\nfirst token (TTFT) and 20 interactivity requirements, performance gains are\nless than 8x. We aren\u2019t sure of at scale deployment ratios of input vs output\ntokens though, and it\u2019s possible many people require extremely high input and\nlow output ratios. Potentially agent and other emerging workloads will have\neven higher ratios of prefill to decode than the 32:1 ratio that Nvidia\npresented.\n\nThe performance gains are still remarkable in this cherry-picked scenario due\nto architectural and networking gains, even after peeling away the impact of\npure specs and marketing gimmicks.\n\nShare\n\nNow instead let\u2019s look at real performance and TCO improvements across what we\nconsider more realistic scenarios of various model sizes and training versus\ninference. Furthermore, lets dive into what these performance gains drive for\nprofitability of inference systems.\n\nGet 20% off a group subscription\n\n##\n\nActual Performance and TCO Improvements\n\nWhen we run our model simulator, we get very different performance uplift per\nGPU SKU. Next let\u2019s dive into the performance and TCO improvements across from\nH100/H200 to B100, B200, and GB200 large models and small models in both\ntraining and inference. Furthermore, we will show figures for revenue, cost,\nand profitability per inference system for GPT-4 at scale.\n\n## This post is for paid subscribers\n\nAlready a paid subscriber? Sign in\n\n\u00a9 2024 SemiAnalysis LLC\n\nPrivacy \u2219 Terms \u2219 Collection notice\n\nStart WritingGet the app\n\nSubstack is the home for great writing\n\nShare\n\n", "frontpage": false}
