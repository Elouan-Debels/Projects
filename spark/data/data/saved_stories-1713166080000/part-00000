{"aid": "40036172", "title": "What Nvidia Didn't Say \u2013 Groq", "url": "https://wow.groq.com/what-nvidia-didnt-say/", "domain": "groq.com", "votes": 1, "user": "throwoutway", "posted_at": "2024-04-15 01:26:30", "comments": 0, "source_title": "What NVIDIA Didn\u2019t Say - Groq", "source_text": "What NVIDIA Didn\u2019t Say - Groq\n\n# What NVIDIA Didn\u2019t Say\n\nWritten by:\n\nGroq\n\nHi everyone,\n\nWe were captivated by Jensen Huang\u2019s opening keynote last week at the NVIDIA\nGTC. He did a masterful job, mixing in humor, a sly Taylor Swift reference,\nMichael Dell flyovers, and sci-fi references (Star Trek\u2019s opening sequence and\na callback to Silent Running\u2019s droids).\n\nAnd, of course, the Blackwell announcement. Let\u2019s dig into that. What did\nJensen say, and what didn\u2019t he say? Here\u2019s what we heard.\n\n###### What He Said\n\nJensen spent the first part of his talk describing how creating simulations\nand \u201cdigital twins\u201d in the NVIDIA-hosted omniverse can create wonderful new\nsolutions. Many of these solutions, though, require ever-larger models and\nreal-time performance in inference. Which leads to a problem: compute. General\ncomputing has \u201crun out of steam.\u201d We need a new approach.\n\nWhat is this new approach? Big. Blackwell big: 20 petaflops (up to 40\npetaflops at FP4), 208 billion transistors, multi-trillion parameter large\nlanguage models (LLMs). To paraphrase Tiny Elvis, that chip is huge.\n\nJensen went on to talk about all the different things big compute will enable,\nincluding a whole host of \u201cco-pilot\u201d, \u201cconversational\u201d, and \u201cdigital twin\u201d\nsolutions. These solutions demand superior inference performance, which has\nbeen hard to achieve when the model is too big to fit on a single chip. Hence,\nthere is a tradeoff between throughput and speed, so businesses need to choose\nwhich to prioritize.\n\nJensen\u2019s answer, once again, was big.\n\n###### What He Didn\u2019t Say\n\nWhile Jensen talked about getting bigger, he didn\u2019t say anything about getting\nsmarter. Nothing about changing the underlying approach to running models.\nNothing about how to improve performance or efficiency through redesigned\nsystem architecture. Nothing about how there is another way to achieve the\ninference performance, cost, and energy objectives of the industry.\n\nWe call this other way the Groq LPUTM Inference Engine, which we architected\nfrom the ground up to run solutions like LLMs and other generative AI models\nwith 10X better interactive performance and 10X better energy efficiency.\n\nHow does the LPU achieve such superior specs? When a model is compiled to run\non the Groq LPU, the compiler partitions the model into smaller chunks which\nare spatially mapped onto multiple LPU chips. The result is like a compute\nassembly line. Each cluster of LPUs on this line are set up to run a\nparticular compute stage, and they store all of the data needed to perform\nthat task in their local on-chip SRAM memory. The only data they need to\nretrieve from other chips is the intermediate output that has been generated\nby either the previous compute stage or the current compute stage. This data\ntransfer is entirely LPU to LPU, requiring no external HBM chips and no\nexternal router.\n\nThis efficient, assembly line architecture is only feasible because the Groq\nLPU Inference Engine is entirely deterministic. That means that from the\nmoment a new workload is compiled to run on Groq, the system knows exactly\nwhat is happening at each stage on each chip at each moment. This perfect\ndeterminism enables the assembly line to work at peak efficiency.\n\nContrast this to how GPUs work. They operate in small chip clusters, and each\ncluster executes every sequential compute stage required to generate a token.\nFor each stage, the GPUs retrieve all data required to execute the stage\n(model parameters, cumulative output of previous stages) from high bandwidth\nmemory on another chip, and after they complete their task data goes back to\nthe off-chip HBM. The architecture is non-deterministic, so all the data being\nshuttled around requires direction from a router, yet another external chip.\nThis is inefficient and expensive.\n\n###### The Model T & the LPU\n\nAnother thing Jensen didn\u2019t talk about: the Ford Model T. In 1913 Henry Ford\ntransformed the automotive industry, and all of manufacturing, by adopting the\nassembly line method for building cars. Before then, a car under construction\nwas stationary, while the workers who were building it moved back and forth,\noften retrieving required parts from nearby storage.\n\nUnder Ford\u2019s new assembly line system, the emerging car moved on a conveyor\nbelt and the workers stayed in place, each performing just one or two tasks\nwith all the parts they needed right at hand. It took 84 stations to assemble\n3,000 parts into a Model T. Sound familiar? It should, because this is\nanalogous to how LPUs create tokens versus how GPUs do it.\n\nThe best way to improve an outdated process, such as the GPU\u2019s inference\narchitecture, is to supercharge it. For example, two of the highlights of the\nBlackwell announcement are its bigger HBM storage and bandwidth and its more\npowerful routers. They didn\u2019t eliminate the need for separate HBM storage and\nrouter chips; they made bigger ones.\n\nThese improvements are part of how Blackwell can achieve, according to one of\nthe graphs displayed during the keynote, a substantial inference performance\nimprovement over current GPUs. That same graph also highlights the limitations\nof GPU architecture. It featured throughput (tokens / GPU / second) on the Y\naxis and interactivity (tokens / user / second) on the X axis, to demonstrate\nhow there is always going to be a tradeoff between the two. This tradeoff\ndoesn\u2019t exist for LPUs. If you maximize efficiency of token generation via an\nLPU\u2019s assembly line architecture, you can optimize both interactivity and\nthroughput!\n\nFurthermore, the X axis in Jensen\u2019s graph \u2013 interactivity \u2013 maxed out at 50\ntokens / second. This may seem fast for a 1.8 trillion parameter model running\non a GPU, which is what the graph represents, but it\u2019s far too slow for the\nnext generation of real-time AI solutions. To succeed, interactive co-pilots\nand conversational AI assistants will need to achieve human levels of fluidity\nin their responses and reasoning. 50 tokens / second won\u2019t cut it.\n\nThen there\u2019s energy consumption. All that data moving back and forth between\nGPUs and HBM chews up a lot of joules, and supercharging the various\ncomponents doesn\u2019t change that. Scaling up usually yields economies, and\nBlackwell is indeed much more efficient. But it\u2019s still shuttling data between\nchips for every single compute task. Economies of scale can\u2019t fix that. By\ncomparison, the Groq LPU Inference Engine is already at least 10X more energy\nefficient than GPUs, because its assembly line approach minimizes off-chip\ndata flow.\n\n###### Faster Horses\n\nHenry Ford is often quoted as allegedly saying, \u201cIf I would have asked people\nwhat they wanted, they would have said faster horses.\u201d\n\nNVIDIA\u2019s Blackwell isn\u2019t just faster horses, it\u2019s more of them, tied to more\nbuggies, yoked together by an expanding network of harnesses. The scale is\nstupendous, the engineering remarkable, and, it\u2019s still a horse and buggy\narchitecture. That\u2019s another thing Jensen didn\u2019t say.\n\nNever miss a Groq update! Sign up below for our latest news.\n\nSign up for Groq updates\n\nThis site uses cookies to operate our website and analyze website traffic.\nRead MoreAcceptReject\n\nPrivacy & Cookies Policy\n\n#### Privacy Overview\n\nThis website uses cookies to improve your experience while you navigate\nthrough the website. Out of these, the cookies that are categorized as\nnecessary are stored on your browser as they are essential for the working of\nbasic functionalities of the ...\n\nNecessary\n\nAlways Enabled\n\nNecessary cookies are absolutely essential for the website to function\nproperly. This category only includes cookies that ensures basic\nfunctionalities and security features of the website. These cookies do not\nstore any personal information.\n\nNon-necessary\n\nAny cookies that may not be particularly necessary for the website to function\nand is used specifically to collect user personal data via analytics, ads,\nother embedded contents are termed as non-necessary cookies. It is mandatory\nto procure user consent prior to running these cookies on your website.\n\nSAVE & ACCEPT\n\n", "frontpage": false}
