{"aid": "40043995", "title": "How to build your own LLM evaluation framework", "url": "https://www.confident-ai.com/blog/how-to-build-an-llm-evaluation-framework-from-scratch", "domain": "confident-ai.com", "votes": 1, "user": "3d27", "posted_at": "2024-04-15 18:25:30", "comments": 0, "source_title": "How to Build an LLM Evaluation Framework, from Scratch - Confident AI", "source_text": "How to Build an LLM Evaluation Framework, from Scratch - Confident AI\n\nConfident AI\n\nBlog\n\nGithub\n\nDocumentation\n\nPricing\n\nSign Up\n\nIn this story\n\nJeffrey Ip\n\nCofounder @ Confident AI, creating companies that tell stories. Ex-Googler\n(YouTube), Microsoft AI (Office365). Working overtime to enforce responsible\nAI.\n\n# How to Build an LLM Evaluation Framework, from Scratch\n\nApril 5, 2024\n\n\u00b7\n\n9 min read\n\nPresenting...\n\nThe open-source LLM evaluation framework.\n\n\u2b50 Star on Github\n\nLet\u2019s set the stage: I\u2019m about to change my prompt template for the 44th time\nwhen I get a message from my manager: \u201cHey Jeff, I hope you\u2019re doing well\ntoday. Have you seen the newly open-sourced Mistral model? I\u2019d like you to try\nit out since I heard gives better results than the LLaMA-2 you\u2019re using.\u201d\n\nOh no, I think to myself, not again.\n\nThis frustrating interruption (and by this I mean the releasing of new models)\nis why I, as the creator of DeepEval, am here today to teach you how to build\nan LLM evaluation framework to systematically identify the best\nhyperparameters for your LLM systems.\n\nWant to be one terminal command away from knowing whether you should be using\nthe newly release Claude-3 Opus model, or which prompt template you should be\nusing? Let\u2019s begin.\n\n## What is an LLM Evaluation Framework?\n\nAn LLM evaluation framework is a software package that is designed to evaluate\nand test outputs of LLM systems on a range of different criteria. The the\nperformance of an LLM system (which can just be the LLM itself) on different\ncriteria is quantified by LLM evaluation metrics, which uses different scoring\nmethods depending on the task at hand.\n\nA typical LLM evaluation framework architecture\n\nThese evaluation metrics scores, will ultimately make up your evaluation\nresults, which you can use to identify regressions (which some people call\nregression testing) in LLM systems over time, or even be used to compare LLM\nsystems with one another.\n\nFor example, let's say I\u2019m building a RAG-based, news article summarization\nchatbot for users to quickly skim through today\u2019s morning news. My LLM\nevaluation framework would need to have:\n\n  1. A list of LLM test cases (or an evaluation dataset), which is a set of LLM inputs-outputs pairs, the \u201cevaluatee\u201d.\n  2. LLM evaluation metrics, to quantify my chatbot assessment criteria I hope it to perform well on, the \u201cevaluator\u201d.\n\nFor this particular example, two appropriate metrics could be the\nsummarization and contextual relevancy metric. The summarization metric will\nmeasure whether the summaries are relevant and non-hallucinating, while the\ncontextual relevancy metric will determine whether the my RAG pipeline\u2019s\nretriever is able to retrieve relevant text chunks for the summary.\n\nOnce you have the test cases and necessary LLM evaluation metrics in place,\nyou can easily quantify how different combinations of hyperparameters (such as\nmodels and prompt templates) in LLM systems affect your evaluation results.\n\nSeems straightforward, right?\n\n## Challenges in Building an LLM Evaluation Framework\n\nBuilding a robust LLM evaluation framework is tough, which is why it took me a\nbit more than 5 months to build DeepEval, the open-source LLM evaluation\nframework.\n\nFrom working closely with hundreds of open-source users, here were the main\ntwo challenges:\n\n  1. LLM evaluation metrics are hard to make accurate and reliable. In fact, it is so tough that I previously dedicated an entire article talking about everything you need to know about LLM evaluation metrics. Most LLM evaluation metrics nowadays are LLM-Evals, which means using LLMs to evaluate LLM outputs. Although LLMs have superior reasoning capabilities that makes them great candidates for evaluating LLM outputs, they can be unreliable at times and must be carefully prompt engineered to provide a reliable score\n  2. Evaluation datasets/test cases are hard to make comprehensive. Preparing an evaluation dataset that covers all the edge cases that might appear in a production setting is a difficult and hard to get right task. Unfortunately, it is also a very time-consuming problem.\n\nSo with this in mind, lets walk through how to build your own LLM evaluation\nframework from scratch.\n\n# Confident AI: Everything You Need for LLM Evaluation\n\nAn all-in-one platform to evaluate and test LLM applications, fully integrated\nwith DeepEval.\n\nLLM evaluation metrics for ANY use case.\n\nReal-time evaluations and monitoring.\n\nHyperparameters discovery.\n\nManage evaluation datasets on the cloud.\n\nTry Now for Free\n\nCheckout DeepEval\n\n## Step-By-Step Guide: Building an LLM Evaluation Framework\n\n#### 1\\. Framework Setup\n\nThe first step involves setting up the infrastructure needed to make a\nmediocre LLM evaluation framework great. There are two components to any\nevaluation or testing framework: the thing to be evaluated (the \u201cevaluatee\u201d),\nand the thing we\u2019re using to evaluate the evaluatee (the \u201cevaluator\u201d).\n\nIn this case, the \u201cevaluatee\u201d is an LLM test case, which contains the\ninformation for the LLM evaluation metrics, the \u201cevaluator\u201d, to score your LLM\nsystem.\n\nTo be more concrete, an LLM test case should contain parameters such as:\n\n  1. Input: The input to your LLM system. Note that this does NOT include the prompt template, but literally the user input (we\u2019ll see why later).\n  2. Actual Output: The actual output of your LLM system for a given input. We call it \u201cactual\u201d output because...\n  3. Expected Output: The expected output of your LLM system for a given input. This is where data labelers for example would give target answers for a given input.\n  4. Context: The undisputed ground truth for a given input. Context and expected output is often time confused with one another since they are both factually similar. A good example to make things clear, is that while context could be a raw PDF page that contains everything you need to answer a question in the input, expected output is the way in which you would want the answer answered.\n  5. Retrieval Context: The retrieved text chunks in a RAG system. As the description suggests, this is only applicable for RAG applications.\n\nNote that only the input and actual output parameters are mandatory for an LLM\ntest case. This is because some LLM systems might just be an LLM itself, while\nothers can be RAG pipelines that require parameters such as retrieval context\nfor evaluation.\n\nThe point is, different LLM architectures require different needs, but in\ngeneral they are pretty much similar. Here\u2019s how you can setup an LLM test\ncase in code:\n\n    \n    \n    from typing import Optional, List class LLMTestCase: input: str actual_output: str expected_output: Optional[str] = None context: Optional[List[str]] = None retrieval_context: Optional[List[str]] = None\n\nRemember to check for type errors at runtime!\n\n#### 2\\. Implement LLM Evaluation Metrics\n\nProbably the toughest part of building an LLM evaluation framework, which is\nalso why I\u2019ve dedicated an entire article talking about everything you need to\nknow about LLM evaluation metrics.\n\nLet\u2019s imagine we\u2019re building a RAG-based customer support assistant, and we\nwant to test whether the retriever is able to retrieve relevant text chunks\n(aka. contextual relevancy) for a range of different user queries. In this\nscenario, the contextual relevancy metric is what we will be implementing, and\nto use it to test a wide range of user queries we\u2019ll need a wide range of test\ncases with different inputs.\n\nThis is an example LLM test case:\n\n    \n    \n    # ...previous implementation of LLMTestCase class test_case = LLMTestCase( input=\"How much did I spend this month?\" actual_output=\"You spent too much this month.\", retrieval_context=[\"...\"] )\n\nWe need a metric that gives us a contextual relevancy score based on the input\nand retrieval context. Note that the actual output doesn\u2019t matter here, as\ngenerating the actual output concerns the generator, not the retriever.\n\nAs explain in a previous article, we\u2019ll make all metrics output scores in the\n0\u20131 range, along with a passing threshold so that you can be aware whenever a\nparticular metric is failing on a test case. We\u2019ll be using an QAG-based, LLM\nscorer to calculate the contextual relevancy score, and here\u2019s the algorithm:\n\n  1. For each node in retrieval context, use an LLM to judge whether it is relevant to the given input.\n  2. Count all the relevant and irrelevant nodes. The contextual relevancy score will be the number of relevant nodes / total number of nodes.\n  3. Check whether the contextual relevancy score is \u2265 threshold. If it is, mark the metric as passing, and move on to the next test case.\n\n    \n    \n    # ...previous implementation of LLMTestCase class class ContextualPrecisionMetric: def __init__(self, threshold: float): self.threshold = threshold def measure(self, test_case: LLMTestCase): irrelevant_count = 0 relevant_count = 0 for node in test_case.retrieval_context: # is_relevant() will be determined by an LLM if is_relevant(node, test_case.input): relevant_count += 1 else: irrelevant_count += 1 self.score = relevant_count / (relevant_count + irrelevant_count) self.success = self.score >= self.threshold return self.score ################### ## Example Usage ## ################### metric = ContextualPrecisionMetric(threshold=0.6) metric.measure(test_case) print(metric.score) print(metric.success)\n\nContextual Relevancy is probably the simplest RAG retrieval metric, and you\u2019ll\nnotice it unsurpringly overlooks important factors such as the\npositioning/ranking of nodes. This is important because nodes that are more\nrelevant should be ranked higher in the retrieval context, as it greatly\naffects the quality of the final output. (In fact, this is calculated by\nanother metric called Contextual Precision.)\n\nI\u2019ve left the is_relevant function for you to implement, but if you\u2019re\ninterested in a real example here is DeepEval\u2019s implementation of contextual\nrelevancy.\n\n#### 3\\. Implement Synthetic Data Generator\n\nAlthough this step is optional, you\u2019ll likely find generating synthetic data\nmore accessible than creating your own set of LLM test cases/evaluation\ndataset. That\u2019s not to say generating synthetic data is a straightforward\nthing to do \u2014 LLM generated data can sound and look repetitive, and might not\nrepresent the underlying data distribution accurately, which is why in\nDeepEval we had to evolve or complicate the generated synthetic data multiple\ntimes.\n\nGenerating synthetic data is the process of generating input-(expected)output\npairs based on some given context. However, I would recommend avoid using\n\u201cmediocre\u201d (ie. non-OpenAI or Anthropic) LLMs to generate expected outputs,\nsince it may introduce hallucinated expected outputs in your dataset.\n\nHere is how we can create an EvaluationDataset class:\n\n    \n    \n    class EvaluationDataset: def __init__(self, test_cases: List[LLMTestCase]): self.test_cases = test_cases def generate_synthetic_test_cases(self, contexts: List[List[str]]): for context in contexts: # generate_input_output_pair() will a function that uses # an LLM to generate input and output based on the given context input, expected_output = generate_input_output_pair(context) test_case = LLMTestCase( input=input, expected_output=expected_output, context=context ) self.test_cases.append(test_case) def evaluate(self, metric): for test_case in self.test_cases: metric.measure(test_case) print(test_case, metric.score) ################### ## Example Usage ## ################### metric = ContextualRelevancyMetric() dataset = EvaluationDataset() dataset.generate_synthetic_test_cases([[\"...\"], [\"...\"]]) dataset.evaluate(metric)\n\nAgain, I\u2019ve left the LLM generation part out since you\u2019ll likely have unique\nprompt templates depending on the LLM you\u2019re using, but if you\u2019re looking for\nsomething quick you can borrow DeepEval\u2019s synthetic data generator, which you\ncan pass in entire documents instead of lists of strings that you have to\nprocess yourself:\n\n    \n    \n    pip install deepeval\n    \n    \n    from deepeval.dataset import EvaluationDataset dataset = EvaluationDataset() dataset.generate_goldens_from_docs( document_paths=['example_1.txt', 'example_2.docx', 'example_3.pdf'], max_goldens_per_document=2 )\n\nFor the sake of simplicity, \u201cgoldens\u201d and \u201ctest cases\u201d can be interpreted as\nthe same thing here, but the only difference being goldens are not instantly\nready for evaluation (since they don't have actual outputs).\n\n#### 4\\. Optimize for Speed\n\nYou\u2019ll notice that in the evaluate() method, we used a for loop to evaluate\neach test case. This can get very slow as it is not uncommon for there to be\nthousands of test cases in your evaluation dataset. What you\u2019ll need to do, is\nto make each metric run asynchronously, so the for loop can execute\nconcurrently on all test cases, at the same time. But beware, asynchronous\nprogramming can get very messy especially in environments where an event loop\nis already running (eg. colab/jupyter notebook environments), so it is vital\nto handle asynchronous errors gracefully.\n\nGoing back to the contextual relevancy metric implementation to make it\nasynchronous:\n\n    \n    \n    import asyncio class ContextualPrecisionMetric: # ...previous methods ######################## ### New Async Method ### ######################## async def a_measure(self, test_case: LLMTestCase): irrelevant_count = 0 relevant_count = 0 # Prepare tasks for asynchronous execution for node in test_case.retrieval_context: # Here, is_relevant is assumed to be async task = asyncio.create_task(is_relevant(node, test_case.input)) tasks.append(task) # Await the tasks and process results as they come in for task in asyncio.as_completed(tasks): is_relevant_result = await task if is_relevant_result: relevant_count += 1 else: irrelevant_count += 1 self.score = relevant_count / (relevant_count + irrelevant_count) self.success = self.score >= self.threshold return self.score\n\nThen, change the evaluate function to make it use the asynchronous a_measure\nmethod instead:\n\n    \n    \n    import asyncio class EvaluationDataset: # ...previous methods def evaluate(self, metric): # Define an asynchronous inner function to handle concurrency async def evaluate_async(): tasks = [] # Schedule a_measure for each test case to run concurrently for test_case in self.test_cases: task = asyncio.create_task(self.a_measure(test_case, metric)) tasks.append(task) # Wait for all scheduled tasks to complete results = await asyncio.gather(*tasks) # Process results for test_case, result in zip(self.test_cases, results): print(test_case, result) asyncio.run(evaluate_async())\n\nUsers of DeepEval have reported that this decreases evaluation time from hours\nto minutes. If you\u2019re looking to build a scalable evaluation framework, speed\noptimization is definitely something that you shouldn\u2019t overlook.\n\n#### 5\\. Caching Results and Error Handling\n\n(From this point onwards, we\u2019ll be discussing each concept rather than diving\ninto the specific implementation)\n\nHere\u2019s another common scenario \u2014 you evaluate a dataset with 1000 test cases,\nit fails on the 999th test case, and now you\u2019ve to rerun 999 test cases just\nto finish evaluating the remaining one. Not ideal, right?\n\nGiven how costly each metric run can get, you\u2019ll want an automated way to\ncache test case results so that you can use it when you need to. For example,\nyou can design your LLM evaluation framework to cache successfully ran test\ncases, and optionally use it whenever you run into the scenario described\nabove.\n\nCaching is a bit too complicated of an implementation to include in this\narticle, and I\u2019ve personally spent more than a week on this feature when\nbuilding on DeepEval.\n\nAnother option, is to ignore errors raised. This is much more straightforward,\nsince you can simply wrap each metric execution in a try-catch block. But,\nwhat good is ignoring errors if you have to rerun every single test case to\nexecute previously errored metrics?\n\nSeriously, if you want automated, memory efficient caching for LLM evaluation\nresults, just use DeepEval.\n\n#### 6\\. Logging Hyperparameters\n\nThe ultimate goal of LLM evaluation, is to figure out the optimal\nhyperparameters to use for your LLM systems. And by hyperparameters, I mean\nmodels, prompt templates, etc.\n\nTo achieve this, you must associate hyperparameters with evaluation results.\nThis is fairly straightforward, but the difficult part actually lies in\ngaining metrics results based on different filters of hyperparameter\ncombinations.\n\nThis is a UI problem, which DeepEval also solves through its integration with\nConfident AI. Confident AI is an evaluation platform for LLMs, and you can\nsign up here and try it for free.\n\n#### 7\\. CI/CD integration\n\nLastly, what good is an LLM evaluation framework if LLM evaluation isn\u2019t\nautomated? (Here is another great read on how to unit test RAG applications in\nCI/CD.)\n\nYou\u2019ll need to restructure your LLM evaluation framework so that it not only\nworks in a notebook or python script, but also in a CI/CD pipeline where unit\ntesting is the norm. Fortunately, in the previous implementation for\ncontextual relevancy we already included a threshold value that can act as a\n\u201cpassing\u201d criteria, which you can include in CI/CD testing frameworks like\nPytest.\n\nBut what about caching, ignoring errors, repeating metric executions, and\nparallelizing evaluation in CI/CD? DeepEval has support for all of these\nfeatures, along with a Pytest integration.\n\n## Conclusion\n\nIn this article, we\u2019ve learnt why LLM evaluation is important and how to build\nyour own LLM evaluation framework to optimize on the optimal set of\nhyperparameters. However, we\u2019ve also learnt how difficult it is to build an\nLLM evaluation framework from scratch, such challenges stemming from synthetic\ndata generation, accuracy and robustness of LLM evaluation metrics, efficiency\nof framework, etc.\n\nIf you\u2019re looking to learn how LLM evaluation works, building your own LLM\nevaluation framework is a great choice. However, if you want something robust\nand working, use DeepEval, we\u2019ve done all the hard work for you already.\nDeepEval is free, open-source, offers 14+ research backed metrics, integration\nwith Pytest for CI/CD, has all the optimization features discussed in this\narticle, and comes with a free platform with a pretty UI for evaluation\nresults.\n\nIf you\u2019ve found this article useful, give \u2b50 DeepEval a star on GitHub \u2b50, and\nas always, till next time.\n\n* * * * *\n\nDo you want to brainstorm how to evaluate your LLM (application)? Schedule a\ncall with me here (it\u2019s free), or ask us anything in our discord. I might give\nyou an \u201caha!\u201d moment, who knows?\n\n# Confident AI: Everything You Need for LLM Evaluation\n\nAn all-in-one platform to evaluate and test LLM applications, fully integrated\nwith DeepEval.\n\nLLM evaluation metrics for ANY use case.\n\nReal-time evaluations and monitoring.\n\nHyperparameters discovery.\n\nManage evaluation datasets on the cloud.\n\nTry Now for Free\n\nCheckout DeepEval\n\nIn this story\n\n  * What is an LLM Evaluation Framework?\n  * Challenges in Building an LLM Evaluation Framework\n  * Step-By-Step Guide: Building an LLM Evaluation Framework\n  * 1\\. Framework Setup\n  * 2\\. Implement LLM Evaluation Metrics\n  * 3\\. Implement Synthetic Data Generator\n  * 4\\. Optimize for Speed\n  * 5\\. Caching Results and Error Handling\n  * 6\\. Logging Hyperparameters\n  * 7\\. CI/CD integration\n  * Conclusion\n\nJeffrey Ip\n\nCofounder @ Confident AI, creating companies that tell stories. Ex-Googler\n(YouTube), Microsoft AI (Office365). Working overtime to enforce responsible\nAI.\n\n# Stay Confident\n\nSubscribe to our weekly newsletter to stay confident in the AI systems you\nbuild.\n\nThank you! You're now subscribed to Confident AI's weekly newsletter.\n\nOops! Something went wrong while submitting the form.\n\nMore stories from us...\n\nLLM Benchmarks: MMLU, HellaSwag, BBH, and Beyond\n\nIn this article, I'm going to go through all the top LLM benchmarks currently\nused and why they matter.\n\nKritin Vongthongsri\n\nMarch 16, 2024\n\n\u00b7\n\n12 min read\n\nLLM Testing in 2024: Top Methods and Strategies\n\nIn this article, we'll learn everything there is to LLM testing, including\nbest practices and methods to test LLMs.\n\nJeffrey Ip\n\nFebruary 25, 2024\n\n\u00b7\n\n8 min read\n\nThe Ultimate Guide to Fine-Tune LLaMA 2, With LLM Evaluations\n\nIn this article, we'll walkthrough how to fine-tune and evaluate a LLaMA-2\nmodel using Hugging Face and DeepEval\n\nJeffrey Ip\n\nFebruary 20, 2024\n\n\u00b7\n\n12 min read\n\nNext\n\n## Subscribe to receive articles right in your inbox\n\nThanks for joining our newsletter.\n\nOops! Something went wrong.\n\nConfident AI\n\nCopyright @ 2024 Confident AI Inc. All rights reserved.\n\n", "frontpage": false}
