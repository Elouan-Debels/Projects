{"aid": "40004551", "title": "Profiling Memory Usage in MySQL", "url": "https://planetscale.com/blog/profiling-memory-usage-in-mysql", "domain": "planetscale.com", "votes": 4, "user": "bddicken", "posted_at": "2024-04-11 17:19:55", "comments": 0, "source_title": "Profiling memory usage in MySQL", "source_text": "Profiling memory usage in MySQL\n\nThis website stores cookies on your computer. We use this information to\nimprove your browsing experience and for analytics about our visitors both on\nthis website and other media. To learn more about the cookies used, see our\nPrivacy Policy.\n\nCookies settings\n\nAccept All Decline All\n\nSkip to content\n\nBlogEngineering\n\n# Profiling memory usage in MySQL\n\nLearn how to visualize the memory usage of a MySQL connection\n\nWhen considering the performance of any software, there's a classic trade-off\nbetween time and space. In the process of assessing the performance of a MySQL\nquery, we often focus on execution time (or query latency) and use it as the\nprimary metric for query performance. This is a good metric to use, as\nultimately, we want to get query results as quickly as possible.\n\nI recently released a blog post about how to identify and profile problematic\nMySQL queries, with a discussion centered around measuring poor performance in\nterms of execution time and row reads. However, in this discussion, memory\nconsumption was largely ignored.\n\nThough it may not be needed as often, MySQL also has built-in mechanisms for\ngaining a deep understanding of both how much memory a query is using and also\nwhat that memory is being used for. Let's take a deep dive through this\nfunctionality and see how we can perform live monitoring of the memory usage\nof a MySQL connection.\n\n## Memory statistics\n\nIn MySQL, there are many components of the system that can be individually\ninstrumented. The performance_schema.setup_instruments table lists each of\nthese components, and there are quite a few:\n\n    \n    \n    SELECT count(*) FROM performance_schema.setup_instruments;\n    \n    +----------+\n    \n    | count(*) |\n    \n    +----------+\n    \n    | 1255 |\n    \n    +----------+\n\nIncluded in this table are a number of instruments that can be used for memory\nprofiling. To see what is available, try selecting from the table and\nfiltering by memory/.\n\n    \n    \n    SELECT name, documentation\n    \n    FROM performance_schema.setup_instruments\n    \n    WHERE name LIKE 'memory/%';\n\nYou should see several-hundred results. Each of these represent a different\ncategory of memory that can be individually instrumented in MySQL. Some of\nthese categories contain a short bit of documentation describing what this\nmemory category represents or is used for. If you'd like to see only memory\ntypes that have a non-null documentation value, you can run:\n\n    \n    \n    SELECT name, documentation\n    \n    FROM performance_schema.setup_instruments\n    \n    WHERE name LIKE 'memory/%'\n    \n    AND documentation IS NOT NULL;\n\nEach of these memory categories can be sampled at several different\ngranularities. The various levels of granularity are stored across several\ntables:\n\n    \n    \n    SELECT table_name\n    \n    FROM information_schema.tables\n    \n    WHERE table_name LIKE '%memory_summary%'\n    \n    AND table_schema = 'performance_schema';\n    \n    +-----------------------------------------+\n    \n    | TABLE_NAME |\n    \n    +-----------------------------------------+\n    \n    | memory_summary_by_account_by_event_name |\n    \n    | memory_summary_by_host_by_event_name |\n    \n    | memory_summary_by_thread_by_event_name |\n    \n    | memory_summary_by_user_by_event_name |\n    \n    | memory_summary_global_by_event_name |\n    \n    +-----------------------------------------+\n\n  * memory_summary_by_account_by_event_name Summarizes memory events based on accounts (An account is a combination of a user and host)\n  * memory_summary_by_host_by_event_name Summarizes memory events at a host granularity\n  * memory_summary_by_thread_by_event_name Summarizes memory events at a MySQL thread granularity\n  * memory_summary_by_user_by_event_name Summarizes memory events at a user granularity\n  * memory_summary_global_by_event_name A global summary of memory statistics\n\nNotice that there is no specific tracking for memory usage at a per-query\nlevel. However, this does not mean we cannot profile the memory usage of a\nquery! To accomplish this, we can monitor the usage of memory on whatever\nconnection the query of interest is being executed on. Because of this, we'll\nfocus our use on the memory_summary_by_thread_by_event_name table, as there is\na convenient mapping between a MySQL connection and a thread.\n\n## Finding usage for a connection\n\nAt this point, you should set up two separate connections to your MySQL server\non the command line. The first is the one that will execute the query you want\nto monitor memory usage for. The second will be used for monitoring purposes.\n\nOn the first connection, run these queries to get your connection ID and\nthread ID.\n\n    \n    \n    SET @cid = (SELECT CONNECTION_ID());\n    \n    SET @tid = (SELECT thread_id\n    \n    FROM performance_schema.threads\n    \n    WHERE PROCESSLIST_ID=@cid);\n\nThen grab these values. Of course, yours will likely look different than what\nyou see here.\n\n    \n    \n    SELECT @cid, @tid;\n    \n    +------+------+\n    \n    | @cid | @tid |\n    \n    +------+------+\n    \n    | 49 | 89 |\n    \n    +------+------+\n\nNext up, execute some long-running query you'd like to profile the memory\nusage for. For this example, I'll do a large SELECT from a table that has 100\nmillion rows in it, which should take awhile since there is no index on the\nalias column:\n\n    \n    \n    SELECT alias FROM chat.message ORDER BY alias DESC LIMIT 100000;\n\nNow, while this is executing, switch over to your other console connection and\nrun the following, replacing the thread ID with the one from your connection:\n\n    \n    \n    SELECT\n    \n    event_name,\n    \n    current_number_of_bytes_used\n    \n    FROM performance_schema.memory_summary_by_thread_by_event_name\n    \n    WHERE thread_id = YOUR_THREAD_ID\n    \n    ORDER BY current_number_of_bytes_used DESC\n\nYou should see results along the lines of this, though the details will depend\nhighly on your query and data:\n\n    \n    \n    +---------------------------------------+------------------------------+\n    \n    | event_name | current_number_of_bytes_used |\n    \n    +---------------------------------------+------------------------------+\n    \n    | memory/sql/Filesort_buffer::sort_keys | 203488 |\n    \n    | memory/innodb/memory | 169800 |\n    \n    | memory/sql/THD::main_mem_root | 46176 |\n    \n    | memory/innodb/ha_innodb | 35936 |\n    \n    ...\n\nThis indicates the amount of memory for each category being used at the exact\nmoment this query was executed. If you run this query several times while the\nother SELECT alias... query is executing, you may see differences in the\nresults, as memory usage for a query is not necessarily constant over its\nwhole execution. Each execution of this query represents a sample at a moment\nof time. Thus, if we want to see how the usage changes over time, we'll need\nto take many samples.\n\nThe documentation for memory/sql/Filesort_buffer::sort_keys is missing from\nthe performance_schema.setup_instruments table.\n\n    \n    \n    SELECT name, documentation\n    \n    FROM performance_schema.setup_instruments\n    \n    WHERE name LIKE 'memory%sort_keys';\n    \n    +---------------------------------------+---------------+\n    \n    | name | documentation |\n    \n    +---------------------------------------+---------------+\n    \n    | memory/sql/Filesort_buffer::sort_keys | <null> |\n    \n    +---------------------------------------+---------------+\n\nHowever, the name indicates that it is memory being used for sorting data from\na file. This makes sense as a large part of the expense of this query would be\nsorting the data so that is can be displayed in descending order.\n\n## Collecting usage over time\n\nAs a next step, we need to be able to sample this memory usage over time. For\nshort queries this will not be as useful, as we'll only be able to execute\nthis query once, or a small number of times while the profiled query is\nexecuting. This will be more useful for longer-running queries, ones that take\nmultiple seconds or minutes. These, would be the types of queries we'd want to\nprofile anyways, as these are the ones likely to use a large portion of\nmemory.\n\nThis could be implemented fully in SQL and invoked via a stored procedure.\nHowever, in this case, let's use a separate script in Python to provide\nmonitoring.\n\nPython\n\n    \n    \n    #!/usr/bin/env python3\n    \n    import time\n    \n    import MySQLdb\n    \n    import argparse\n    \n    MEM_QUERY='''\n    \n    SELECT event_name, current_number_of_bytes_used\n    \n    FROM performance_schema.memory_summary_by_thread_by_event_name\n    \n    WHERE thread_id = %s\n    \n    ORDER BY current_number_of_bytes_used DESC LIMIT 4\n    \n    '''\n    \n    parser = argparse.ArgumentParser()\n    \n    parser.add_argument('--thread-id', type=int, required=True)\n    \n    args = parser.parse_args()\n    \n    dbc = MySQLdb.connect(host='127.0.0.1', user='root', password='password')\n    \n    c = dbc.cursor()\n    \n    ms = 0\n    \n    while(True):\n    \n    c.execute(MEM_QUERY, (args.thread_id,))\n    \n    results = c.fetchall()\n    \n    print(f'\\n## Memory usage at time {ms} ##')\n    \n    for r in results:\n    \n    print(f'{r[0][7:]} -> {round(r[1]/1024,2)}Kb')\n    \n    ms+=250\n    \n    time.sleep(0.25)\n\nThis is a simple, first stab at such a monitoring script. In summary, this\ncode does the following:\n\n  * Get the provided thread ID to monitor via command line\n  * Set up a connection to a MySQL database\n  * Every 250 milliseconds, execute a query to get the top 4 used memory categories and print a readout\n\nThis could be adjusted in many ways depending on your profiling needs. For\nexample, tweaking the frequency of the ping to the server or changing how many\nmemory categories are listed per iteration. Running this while a query is\nexecuting provides results like this:\n\n    \n    \n    ...\n    \n    ## Memory usage at time 4250 ##\n    \n    innodb/row0sel -> 25.22Kb\n    \n    sql/String::value -> 16.07Kb\n    \n    sql/user_var_entry -> 0.41Kb\n    \n    innodb/memory -> 0.23Kb\n    \n    ## Memory usage at time 4500 ##\n    \n    innodb/row0sel -> 25.22Kb\n    \n    sql/String::value -> 16.07Kb\n    \n    sql/user_var_entry -> 0.41Kb\n    \n    innodb/memory -> 0.23Kb\n    \n    ## Memory usage at time 4750 ##\n    \n    innodb/row0sel -> 25.22Kb\n    \n    sql/String::value -> 16.07Kb\n    \n    sql/user_var_entry -> 0.41Kb\n    \n    innodb/memory -> 0.23Kb\n    \n    ## Memory usage at time 5000 ##\n    \n    innodb/row0sel -> 25.22Kb\n    \n    sql/String::value -> 16.07Kb\n    \n    sql/user_var_entry -> 0.41Kb\n    \n    innodb/memory -> 0.23Kb\n    \n    ...\n\nThis is great, but there's a few weaknesses. It would be nice to see more than\nthe top 4 memory usage categories, but increasing that numbers increases the\nsize of this already-large output dump. It would also be nice to have an\neasier way to get a picture of the memory usage at-a-glance via some\nvisualizations. This could be done by having the script dump the results to a\nCSV or JSON, and then loading them up later in a visualization tool. Even\nbetter, we could plot the results we are getting live, as the data is\nstreaming in. This provides a more up-to-date view, and allows us to observe\nthe memory usage live as it is happening, all in one tool.\n\n## Plotting memory usage\n\nIn order make this tool even more useful and provide visualizations, a few\nchanges are going to be made.\n\n  * The user will provide connection ID on the command line, and the script will be responsible for finding the underlying thread.\n  * The frequency at which the script requests memory data will be configurable, also via the command line.\n  * The matplotlib library will be used to generate a visualization of the memory usage. This will consist of a stack plot with a legend showing the top memory usage categories, and will retain the past 50 samples.\n\nIt's quite a bit of code, but is included here for the sake of completeness.\n\nPython\n\n    \n    \n    #!/usr/bin/env python3\n    \n    import matplotlib.pyplot as plt\n    \n    import numpy as np\n    \n    import MySQLdb\n    \n    import argparse\n    \n    MEM_QUERY='''\n    \n    SELECT event_name, current_number_of_bytes_used\n    \n    FROM performance_schema.memory_summary_by_thread_by_event_name\n    \n    WHERE thread_id = %s\n    \n    ORDER BY event_name DESC'''\n    \n    TID_QUERY='''\n    \n    SELECT thread_id\n    \n    FROM performance_schema.threads\n    \n    WHERE PROCESSLIST_ID=%s'''\n    \n    class MemoryProfiler:\n    \n    def __init__(self):\n    \n    self.x = []\n    \n    self.y = []\n    \n    self.mem_labels = ['XXXXXXXXXXXXXXXXXXXXXXX']\n    \n    self.ms = 0\n    \n    self.color_sequence = ['#ffc59b', '#d4c9fe', '#a9dffe', '#a9ecb8',\n    \n    '#fff1a8', '#fbbfc7', '#fd812d', '#a18bf5',\n    \n    '#47b7f8', '#40d763', '#f2b600', '#ff7082']\n    \n    plt.rcParams['axes.xmargin'] = 0\n    \n    plt.rcParams['axes.ymargin'] = 0\n    \n    plt.rcParams[\"font.family\"] = \"inter\"\n    \n    def update_xy_axis(self, results, frequency):\n    \n    self.ms += frequency\n    \n    self.x.append(self.ms)\n    \n    if (len(self.y) == 0):\n    \n    self.y = [[] for x in range(len(results))]\n    \n    for i in range(len(results)-1, -1, -1):\n    \n    usage = float(results[i][1]) / 1024\n    \n    self.y[i].append(usage)\n    \n    if (len(self.x) > 50):\n    \n    self.x.pop(0)\n    \n    for i in range(len(self.y)):\n    \n    self.y[i].pop(0)\n    \n    def update_labels(self, results):\n    \n    total_mem = sum(map(lambda e: e[1], results))\n    \n    self.mem_labels.clear()\n    \n    for i in range(len(results)-1, -1, -1):\n    \n    usage = float(results[i][1]) / 1024\n    \n    mem_type = results[i][0]\n    \n    # Remove 'memory/' from beginning of name for brevity\n    \n    mem_type = mem_type[7:]\n    \n    # Only show top memory users in legend\n    \n    if (usage < total_mem / 1024 / 50):\n    \n    mem_type = '_' + mem_type\n    \n    self.mem_labels.insert(0, mem_type)\n    \n    def draw_plot(self, plt):\n    \n    plt.clf()\n    \n    plt.stackplot(self.x, self.y, colors = self.color_sequence)\n    \n    plt.legend(labels=self.mem_labels, bbox_to_anchor=(1.04, 1), loc=\"upper left\", borderaxespad=0)\n    \n    plt.xlabel(\"milliseconds since monitor began\")\n    \n    plt.ylabel(\"Kilobytes of memory\")\n    \n    def configure_plot(self, plt):\n    \n    plt.ion()\n    \n    fig = plt.figure(figsize=(12,5))\n    \n    plt.stackplot(self.x, self.y, colors=self.color_sequence)\n    \n    plt.legend(labels=self.mem_labels, bbox_to_anchor=(1.04, 1), loc=\"upper left\", borderaxespad=0)\n    \n    plt.tight_layout(pad=4)\n    \n    return fig\n    \n    def start_visualization(self, database_connection, connection_id, frequency):\n    \n    c = database_connection.cursor();\n    \n    fig = self.configure_plot(plt)\n    \n    while(True):\n    \n    c.execute(MEM_QUERY, (connection_id,))\n    \n    results = c.fetchall()\n    \n    self.update_xy_axis(results, frequency)\n    \n    self.update_labels(results)\n    \n    self.draw_plot(plt)\n    \n    fig.canvas.draw_idle()\n    \n    fig.canvas.start_event_loop(frequency / 1000)\n    \n    def get_command_line_args():\n    \n    '''\n    \n    Process arguments and return argparse object to caller.\n    \n    '''\n    \n    parser = argparse.ArgumentParser(description='Monitor MySQL query memory for a particular connection.')\n    \n    parser.add_argument('--connection-id', type=int, required=True,\n    \n    help='The MySQL connection to monitor memory usage of')\n    \n    parser.add_argument('--frequency', type=float, default=500,\n    \n    help='The frequency at which to ping for memory usage update in milliseconds')\n    \n    return parser.parse_args()\n    \n    def get_thread_for_connection_id(database_connection, cid):\n    \n    '''\n    \n    Get a thread ID corresponding to the connection ID\n    \n    PARAMS\n    \n    database_connection - Database connection object\n    \n    cid - The connection ID to find the thread for\n    \n    '''\n    \n    c = database_connection.cursor()\n    \n    c.execute(TID_QUERY, (cid,))\n    \n    result = c.fetchone()\n    \n    return int(result[0])\n    \n    def main():\n    \n    args = get_command_line_args()\n    \n    database_connection = MySQLdb.connect(host='127.0.0.1', user='root', password='password')\n    \n    connection_id = get_thread_for_connection_id(database_connection, args.connection_id)\n    \n    m = MemoryProfiler()\n    \n    m.start_visualization(database_connection, connection_id, args.frequency)\n    \n    connection.close()\n    \n    if __name__ == \"__main__\":\n    \n    main()\n\nWith this, we can do detailed monitoring of executing MySQL queries. To use\nit, first get the connection ID for the connection you want to profile:\n\n    \n    \n    SELECT CONNECTION_ID();\n\nThen, executing the following will begin a monitoring session:\n\nTerminal\n\n    \n    \n    ./monitor.py --connection-id YOUR_CONNECTION_ID --frequency 250\n\nWhen executing a query on the database, we can observe the increase in memory\nusage, and see what categories of memory are the largest contributors.\n\nThis visualization can also help us to clearly see what kinds of operations\nare memory hogs. For example, here is a snippet of a memory profile for\ncreating a FULLTEXT index on a large table:\n\nThe memory usage is significant, and continues to grow into using hundreds of\nmegabytes as it executes.\n\nNote\n\nFor another example of how you can use MySQL to profile memory usage, see\ncheck out this DBAMA presentation and the corresponding GitHub repository.\n\n## Conclusion\n\nThough it may not be needed as often, having the ability to get detailed\nmemory usage information can be extremely valuable when the need for detailed\nquery optimization arises. Doing this can reveal when and why MySQL may be\ncause memory pressure on the system, or if a memory upgrade for your database\nserver may be needed. MySQL provides a number of primitives that you can build\nupon to develop profiling tooling for your queries and workload.\n\n## Want to use the worlds most advanced MySQL and Vitess platform?\n\nSign up\n\n## Share\n\n## Next post\n\nSummer 2023: Fuzzing Vitess at PlanetScale\n\nWritten by\n\nBenjamin DickenDeveloper Educator\n\nLast updated April 11, 2024\n\n## Database scaling course\n\nThis 22 lesson video course covers partitioning, caching, replication,\nsharding, and more.\n\nTable of contents\n\n  * Memory statistics\n  * Finding usage for a connection\n  * Collecting usage over time\n  * Plotting memory usage\n  * Conclusion\n\n## Related posts\n\nEngineering\n\n### How PlanetScale makes schema changes\n\nEngineering\n\n### Identifying and profiling problematic MySQL queries\n\nEngineering\n\n### The problem with using a UUID primary key in MySQL\n\n## Database scaling course\n\nThis 22 lesson video course covers partitioning, caching, replication,\nsharding, and more.\n\n\u00a9 2024 PlanetScale, Inc. All rights reserved.\n\nPrivacyTermsCookiesDo Not Share My Personal Information\n\n", "frontpage": false}
