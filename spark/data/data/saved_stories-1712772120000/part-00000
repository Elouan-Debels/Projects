{"aid": "39989708", "title": "LLM Colosseum", "url": "https://github.com/OpenGenerativeAI/llm-colosseum", "domain": "github.com/opengenerativeai", "votes": 1, "user": "wanderingmind", "posted_at": "2024-04-10 11:59:51", "comments": 0, "source_title": "GitHub - OpenGenerativeAI/llm-colosseum: Benchmark LLMs by fighting in Street Fighter 3! The new way to evaluate the quality of an LLM", "source_text": "GitHub - OpenGenerativeAI/llm-colosseum: Benchmark LLMs by fighting in Street\nFighter 3! The new way to evaluate the quality of an LLM\n\nSkip to content\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nOpenGenerativeAI / llm-colosseum Public\n\n  * Notifications\n  * Fork 88\n  * Star 818\n\nBenchmark LLMs by fighting in Street Fighter 3! The new way to evaluate the\nquality of an LLM\n\nhuggingface.co/spaces/junior-labs/llm-colosseum\n\n### License\n\nMIT license\n\n818 stars 88 forks Branches Tags Activity\n\nStar\n\nNotifications\n\n# OpenGenerativeAI/llm-colosseum\n\nThis commit does not belong to any branch on this repository, and may belong\nto a fork outside of the repository.\n\n17 Branches\n\n1 Tags\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\noulianovAdds licence2206c4e \u00b7\n\n## History\n\n241 Commits  \n  \n### agent\n\n|\n\n### agent\n\n| Edit read me  \n  \n### eval\n\n|\n\n### eval\n\n| Refacto LLM calls  \n  \n### notebooks\n\n|\n\n### notebooks\n\n| Refacto LLM calls  \n  \n### .env.example\n\n|\n\n### .env.example\n\n| Fix doc  \n  \n### .gitignore\n\n|\n\n### .gitignore\n\n| WIP dashboard for ELO  \n  \n### LICENCE\n\n|\n\n### LICENCE\n\n| Adds licence  \n  \n### Makefile\n\n|\n\n### Makefile\n\n| Adds demo script with local models  \n  \n### README.md\n\n|\n\n### README.md\n\n| Adds demo script with local models  \n  \n### demo.py\n\n|\n\n### demo.py\n\n| Adds demo script with local models  \n  \n### logo.png\n\n|\n\n### logo.png\n\n| logo  \n  \n### multi_agents.png\n\n|\n\n### multi_agents.png\n\n| Edit readme  \n  \n### observation.npy\n\n|\n\n### observation.npy\n\n| Remove obs  \n  \n### ollama.py\n\n|\n\n### ollama.py\n\n| Make qwen default  \n  \n### requirements.txt\n\n|\n\n### requirements.txt\n\n| Updates req for qwen install  \n  \n### result.py\n\n|\n\n### result.py\n\n| Fix  \n  \n### script.py\n\n|\n\n### script.py\n\n| Fixes  \n  \n### winscreen.jpg\n\n|\n\n### winscreen.jpg\n\n| Put winscreen somewhere else  \n  \n## Repository files navigation\n\n# Evaluate LLMs in real time with Street Fighter III\n\nMake LLM fight each other in real time in Street Fighter III.\n\nWhich LLM will be the best fighter ?\n\n## Our criterias \ud83d\udd25\n\nThey need to be:\n\n  * Fast: It is a real time game, fast decisions are key\n  * Smart: A good fighter thinks 50 moves ahead\n  * Out of the box thinking: Outsmart your opponent with unexpected moves\n  * Adaptable: Learn from your mistakes and adapt your strategy\n  * Resilient: Keep your RPS high for an entire game\n\n## Let the fight begin \ud83e\udd77\n\n### 1 VS 1: Mistral 7B vs Mistral 7B\n\n### 1 VS 1 X 6 : Mistral 7B vs Mistral 7B\n\n## A new kind of benchmark ?\n\nStreet Fighter III assesses the ability of LLMs to understand their\nenvironment and take actions based on a specific context. As opposed to RL\nmodels, which blindly take actions based on the reward function, LLMs are\nfully aware of the context and act accordingly.\n\n# Results\n\nOur experimentations (342 fights so far) led to the following leader board.\nEach LLM has an ELO score based on its results\n\n## Ranking\n\n### ELO ranking\n\nModel| Rating  \n---|---  \n\ud83e\udd47openai:gpt-3.5-turbo-0125| 1776.11  \n\ud83e\udd48mistral:mistral-small-latest| 1586.16  \n\ud83e\udd49openai:gpt-4-1106-preview| 1584.78  \nopenai:gpt-4| 1517.2  \nopenai:gpt-4-turbo-preview| 1509.28  \nopenai:gpt-4-0125-preview| 1438.92  \nmistral:mistral-medium-latest| 1356.19  \nmistral:mistral-large-latest| 1231.36  \n  \n### Win rate matrix\n\n# Explanation\n\nEach player is controlled by an LLM. We send to the LLM a text description of\nthe screen. The LLM decide on the next moves its character will make. The next\nmoves depends on its previous moves, the moves of its opponents, its power and\nhealth bars.\n\n  * Agent based\n\n  * Multithreading\n\n  * Real time\n\n# Installation\n\n  * Follow instructions in https://docs.diambra.ai/#installation\n  * Download the ROM and put it in ~/.diambra/roms\n  * (Optional) Create and activate a new python venv\n  * Install dependencies with make install or pip install -r requirements.txt\n  * Create a .env file and fill it with the content like in the .env.example file\n  * Run with make run\n\n## Test mode\n\nTo disable the LLM calls, set DISABLE_LLM to True in the .env file. It will\nchoose the actions randomly.\n\n## Logging\n\nChange the logging level in the script.py file.\n\n## Local model\n\nYou can run the arena with local models using Ollama.\n\n  1. Make sure you have ollama installed, running, and with a model downloaded (run ollama serve mistral in the terminal for example)\n\n  2. Run make local to start the fight.\n\nBy default, it runs mistral against mistral. To use other models, you need to\nchange the parameter model in ollama.py.\n\n    \n    \n    from eval.game import Game, Player1, Player2 def main(): game = Game( render=True, save_game=True, player_1=Player1( nickname=\"Baby\", model=\"ollama:mistral\", # change this ), player_2=Player2( nickname=\"Daddy\", model=\"ollama:mistral\", # change this ), ) game.run() return 0\n\nThe convention we use is model_provider:model_name. If you want to use another\nlocal model than Mistral, you can do ollama:some_other_model\n\n## How to make my own LLM model play? Can I improve the prompts?\n\nThe LLM is called in Robot.call_llm() method of the agent/robot.py file.\n\n    \n    \n    def call_llm( self, temperature: float = 0.7, max_tokens: int = 50, top_p: float = 1.0, ) -> str: \"\"\" Make an API call to the language model. Edit this method to change the behavior of the robot! \"\"\" # self.model is a slug like mistral:mistral-small-latest or ollama:mistral provider_name, model_name = get_provider_and_model(self.model) client = get_sync_client(provider_name) # OpenAI client # Generate the prompts move_list = \"- \" + \"\\n - \".join([move for move in META_INSTRUCTIONS]) system_prompt = f\"\"\"You are the best and most aggressive Street Fighter III 3rd strike player in the world. Your character is {self.character}. Your goal is to beat the other opponent. You respond with a bullet point list of moves. {self.context_prompt()} The moves you can use are: {move_list} ---- Reply with a bullet point list of moves. The format should be: `- <name of the move>` separated by a new line. Example if the opponent is close: - Move closer - Medium Punch Example if the opponent is far: - Fireball - Move closer\"\"\" # Call the LLM completion = client.chat.completions.create( model=model_name, messages=[ {\"role\": \"system\", \"content\": system_prompt}, {\"role\": \"user\", \"content\": \"Your next moves are:\"}, ], temperature=temperature, max_tokens=max_tokens, top_p=top_p, ) # Return the string to be parsed with regex llm_response = completion.choices[0].message.content.strip() return llm_response\n\nTo use another model or other prompts, make a call to another client in this\nfunction, change the system prompt, or make any fancy stuff.\n\n### Submit your model\n\nCreate a new class herited from Robot that has the changes you want to make\nand open a PR.\n\nWe'll do our best to add it to the ranking!\n\n# Credits\n\nMade with \u2764\ufe0f by the OpenGenerativeAI team from phospho (@oulianov @Pierre-\nLouisBJT @Platinn) and Quivr (@StanGirard) during Mistral Hackathon 2024 in\nSan Francisco\n\n## About\n\nBenchmark LLMs by fighting in Street Fighter 3! The new way to evaluate the\nquality of an LLM\n\nhuggingface.co/spaces/junior-labs/llm-colosseum\n\n### Topics\n\nbenchmark llm streetfighterai genai\n\n### Resources\n\nReadme\n\n### License\n\nMIT license\n\nActivity\n\nCustom properties\n\n### Stars\n\n818 stars\n\n### Watchers\n\n12 watching\n\n### Forks\n\n88 forks\n\nReport repository\n\n## Releases 1\n\nv0.0.1 Latest\n\nMar 24, 2024\n\n## Packages 0\n\nNo packages published\n\n## Contributors 4\n\n  * oulianov Nicolas Oulianov\n  * Pierre-LouisBJT Pierre-Louis Biojout\n  * Platinn PL Venard\n  * StanGirard Stan Girard\n\n## Languages\n\n  * Jupyter Notebook 93.1%\n  * Python 6.8%\n  * Makefile 0.1%\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": false}
