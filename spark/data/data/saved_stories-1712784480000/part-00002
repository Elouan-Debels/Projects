{"aid": "39991801", "title": "How can OpenAI defend itself against LLM competition?", "url": "https://eshanagarwal.substack.com/p/how-can-openai-defend-itself-against", "domain": "eshanagarwal.substack.com", "votes": 3, "user": "eagrwl", "posted_at": "2024-04-10 15:27:29", "comments": 0, "source_title": "How can OpenAI defend itself against LLM competition?", "source_text": "How can OpenAI defend itself against LLM competition?\n\n# Rabbit Holes\n\nShare this post\n\n#### How can OpenAI defend itself against LLM competition?\n\neshanagarwal.substack.com\n\n#### Discover more from Rabbit Holes\n\nA space to share learnings, brainstorm fresh ideas, and create new stories.\n\nContinue reading\n\nSign in\n\n# How can OpenAI defend itself against LLM competition?\n\n### Developer whiplash is bad for companies selling access to language models.\nSo how should a company like OpenAI defend itself against this?\n\nEshan Agarwal\n\nApr 10, 2024\n\nShare this post\n\n#### How can OpenAI defend itself against LLM competition?\n\neshanagarwal.substack.com\n\nShare\n\nIt\u2019s common to hear the word \u201ccommoditization\u201d when discussing large language\nmodels.\n\nLanguage models have become plug-in-play and easily interchangeable. When a\nnew model boasts better performance, swathes of developers rush over to\nextract the extra margin. And when their original model provider ships an\nupdate to recapture the lead, they rush back.\n\nThis is particularly relevant this week, which featured launches of Gemini 1.5\nPro, improved GPT-4 turbo, and news from Llama and Mistral. And that\u2019s after\nClaude 3 went viral for state of the art performance less than one month ago.\n\nThis kind of developer whiplash is obviously bad for companies selling access\nto language models. Spending money to train models and acquiring new users\n(e.g. through free credits) is a leaky bucket if developers can bolt any time\nfor a suddenly more attractive competitor.\n\nSo how can an LLM provider like OpenAI defend itself against this?\n\nThe long term success of OpenAI\u2019s API business may not come from the quality\nof the models but rather from developer tooling.\n\nDevelopers using LLMs in their products face very unique challenges. Language\noutputs are very hard to monitor for quality assurance. How do you as a\ndeveloper know when your product is hallucinating or outputting unhelpful\nresults in production for certain user queries? Offline analytics are\nnecessary for measuring real time model performance.\n\nIt\u2019s also hard to evaluate product changes. Even small prompt changes can lead\nto drastically different outputs. Being able to experiment prior to launch is\ncrucial for ensuring a smooth product experience. Experimentation requires a\ntoolset for evaluating and monitoring LLM outputs.\n\nThis has become the most obvious pain point for LLM developers. As a result,\nswathes of startups have already flooded the space of LLM developer tooling.\n\nBut to build long term customer retention, it makes a whole lot of sense of\nOpenAI and other LLM providers to build their own in house solution.\n\nA good LLM tooling solution needs a few things\n\n  1. A code integration to allow logging\n\n  2. A method of running recurring \u2018eval\u2019 scripts on the resulting logs\n\n  3. An experimentation playground to back test new prompts\n\n  4. A dashboard to show results\n\nOpenAI can build a solution that\u2019s inherently better than its competitors\u2019. On\neach of the above dimensions, OpenAI is well suited to offer a complete\ndeveloper experience.\n\n(1) Developers are already calling OpenAI\u2019s API, and so logs are already being\nsent to OpenAI and can used for analytics without any additional work from the\ndeveloper. This means OpenAI users could easily release an out of the box\nmonitoring and analytics solution built in to their current API calls. That\u2019s\nvery attractive, especially to startups who would love to avoid additional\ncomplexity.\n\n(2) While in the past, OpenAI has struggled to scale infrastructure to keep up\nwith demand, developers supply their own API keys to run eval scrips with\nthird party vendors. Meaning, these workflows are already running on OpenAI\ninfrastructure. Making such a solution out of the box may entice many\ndevelopers who weren\u2019t previously running any analytics to start doing so, but\ninfrastructure problems can\u2019t be a long term concern.\n\n(3) OpenAI already has a prompt playground which many people still use for\ntesting new prompts. In fact, Tyler Cowen likes using this playground instead\nof the actual ChatGPT interface!\n\n(4) Similarly, OpenAI already has a dashboard for showing usage metrics. While\nit\u2019s currently geared towards showing pricing information, it could easily be\nextended to show monitoring results. Having a source of truth dashboard that\nincludes pricing with monitoring in the same dashboard would be a terrific\nexperience for developers!\n\nGiven this, it\u2019s quite feasible for OpenAI to build a great monitoring and\nanalytics solution for LLM developers.\n\nBut why focus on this over a countless list of other priorities?\n\nBuilding an out of the box monitoring solution for LLM developers will create\nan entire developer experience around their API. Development teams will rely\non OpenAI not just for LLM access but also logging, evaluation,\nexperimentation, QA and more.\n\nSwitching to a different monitoring and analytics tool requires moving over\nhistorical data, reaching parity on configurations, and training teams to re-\nlearn workflows that they\u2019ve already become accustomed to. That\u2019s an expensive\nprocess for teams.\n\nThis will make it much less attractive for teams to switch between models for\nshort term quality gains. Instead, many would opt to wait for their current\nmodel provider to release their own update that will likely surpass the\nquality of any intermediate competitor model. Therefore, models will need to\nhave significantly higher performance to convince their competitors\u2019 customers\nto switch.\n\nBy prioritizing LLM developer tooling, OpenAI can make the leap from commodity\nto ecosystem.\n\nIf you like this article, consider subscribing or following me on Twitter!\n\n### Subscribe to Rabbit Holes\n\nBy Eshan Agarwal \u00b7 Launched a year ago\n\nA space to share learnings, brainstorm fresh ideas, and create new stories.\n\nShare this post\n\n#### How can OpenAI defend itself against LLM competition?\n\neshanagarwal.substack.com\n\nShare\n\nComments\n\nYC W23 Week 1: Go smaller, not bigger\n\nYou will often be asked \u201cHow big can this get\u201d. But to go big, it may be wise\nto first get really small.\n\nJan 19, 2023 \u2022\n\nEshan Agarwal\n\n4\n\nShare this post\n\n#### YC W23 Week 1: Go smaller, not bigger\n\neshanagarwal.substack.com\n\n1\n\nVision is about market not product\n\nInvestors want to fund companies that will be worth 10 billion dollars 10\nyears from now.\n\nApr 9, 2023 \u2022\n\nEshan Agarwal\n\n4\n\nShare this post\n\n#### Vision is about market not product\n\neshanagarwal.substack.com\n\n2\n\nWhy we pivoted to Epsilon and what we learned doing it\n\nIn April of last year, we pitched our old product to over 1,500 investors. In\nJuly, we shut it down and started on something new. Here's why.\n\nFeb 7 \u2022\n\nEshan Agarwal\n\n1\n\nShare this post\n\n#### Why we pivoted to Epsilon and what we learned doing it\n\neshanagarwal.substack.com\n\nReady for more?\n\n\u00a9 2024 Eshan Agarwal\n\nPrivacy \u2219 Terms \u2219 Collection notice\n\nStart WritingGet the app\n\nSubstack is the home for great writing\n\nShare\n\n## Create your profile\n\n## Only paid subscribers can comment on this post\n\nAlready a paid subscriber? Sign in\n\n#### Check your email\n\nFor your security, we need to re-authenticate you.\n\nClick the link we sent to , or click here to sign in.\n\n", "frontpage": false}
