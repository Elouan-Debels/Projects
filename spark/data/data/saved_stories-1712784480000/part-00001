{"aid": "39991785", "title": "Understanding Loss.backward()", "url": "https://akash5100.github.io/blog/2024/04/02/understanding_loss.backward().html", "domain": "akash5100.github.io", "votes": 1, "user": "okar1n", "posted_at": "2024-04-10 15:25:55", "comments": 0, "source_title": "Understanding loss.backward()", "source_text": "Understanding loss.backward() | AmreV\n\nLoading [MathJax]/jax/output/CommonHTML/fonts/TeX/fontdata.js\n\nAmreV\n\n# Understanding loss.backward()\n\nApr 2, 2024\n\nRelying on any library is not good; therefore, this time, removing this black\nbox and understand what\u2019s happening inside. Writing a backward pass manually\nwould be helpful. Backpropagation is not something that works magically or\nautomatically if you are hoping to debug. In other words, it is easy to fall\ninto the trap of abstracting away the learning process, believing that you can\nsimply stack arbitrary layers together and backprop will magically make them\nwork on your data. So let\u2019s look at a few explicit examples where this is not\nthe case in quite unintuitive ways.\n\nThis blog post is clearly set of personal notes for the notes by Andrej\nKarpathy\u2019s cs231n lectures.^1^2^3\n\n# Table of contents\n\n  * Simple expressions and interpretation of the gradient\n  * Compound Expressions with Chain Rule\n  * Intutive understanding of backpropogation\n  * Backprop in practice: Staged computation\n  * Gradients for vectorized operations\n  * Gradient checking\n  * Sources\n\n### Simple expressions and interpretation of the gradient\n\nLet's start with simple expressions to develop the notation and conventions\nfor more complex ones. Consider a straightforward multiplication function of\ntwo numbers f(x,y)=xy. It's a matter of simple calculus to derive the partial\nderivative for either input: \u2202f\u2202x=yand\u2202f\u2202y=x Interpretation: Derivatives\nindicate the rate of change of a function with respect to that variable around\nan infinitesimally small region near a particular point:\ndf(x)dx=limh\u21920f(x+h)\u2212f(x)h A technical note is that the division sign on the\nleft-hand side is not a division; it indicates that the operator ddx is being\napplied to the function f, returning a different function (the derivative).\nWhen h is very small, the function is well-approximated by a straight line,\nand the derivative is its slope. In other words, the derivative on each\nvariable tells you the sensitivity of the whole expression to its value. For\nexample, if x=4,y=\u22123, then f(x,y)=\u221212 and the derivative on x, \u2202f\u2202x=\u22123,\nindicates that increasing the value of x by a tiny amount would decrease the\nwhole expression by three times that amount. Similarly, since \u2202f\u2202y=4,\nincreasing y by a small amount would increase the output of the function by\nfour times that amount.\n\nThe Gradient: The gradient \u2207f is the vector of partial derivatives, so we have\n\u2207f=[\u2202f\u2202x,\u2202f\u2202y]=[y,x]. Though technically a vector, we'll often use terms like\n\"the gradient on x\" instead of the technically correct phrase \"the partial\nderivative on x\" for simplicity. We can also derive the derivatives for the\naddition operation: f(x,y)=x+y\u2192\u2202f\u2202x=1and\u2202f\u2202y=1 That is, the derivative on both\nx and y is one, regardless of their values. This makes sense since increasing\neither x or y would increase the output of f, and the rate of increase would\nbe independent of their actual values. Let's consider the max operation:\nf(x,y)=max(x,y)\u2192\u2202f\u2202x={1if x\u2265y0otherwiseand\u2202f\u2202y={0if x\u2265y1otherwise Here, the\n(sub)gradient is 1 on the input that was larger and 0 on the other input.\nIntuitively, if x=4 and y=2, then the max is 4, and the function is not\nsensitive to the setting of y. That is, if we were to increase it by a tiny\namount h, the function would keep outputting 4, and therefore the gradient is\nzero\u2014there's no effect. Of course, if we were to change y by a large amount\n(e.g., larger than 2), then the value of f would change, but the derivatives\ntell us nothing about the effect of such large changes on the inputs of a\nfunction; they are only informative for tiny, infinitesimally small changes,\nas indicated by the limh\u21920 in their definition.\n\n### Compound Expressions with Chain Rule\n\nLet's now consider more complicated expressions that involve multiple composed\nfunctions, such as f(x,y,z)=(x+y)z. While this expression is simple enough to\ndifferentiate directly, we'll take a particular approach that will be helpful\nfor understanding the intuition behind backpropagation. Specifically, note\nthat this expression can be broken down into two expressions: q=x+y and f=qz.\nMoreover, we know how to compute the derivatives of both expressions\nseparately, as seen in the previous section. f is just the multiplication of q\nand z, so \u2202f\u2202q=z, \u2202f\u2202z=q, and q is the addition of x and y so \u2202q\u2202x=1, \u2202q\u2202y=1.\nHowever, we don\u2019t necessarily care about the gradient on the intermediate\nvalue q\u2014the value of \u2202f\u2202q is not useful. Instead, we are ultimately interested\nin the gradient of f with respect to its inputs x,y,z. The chain rule tells us\nthat the correct way to \"chain\" these gradient expressions together is through\nmultiplication. For example, \u2202f\u2202x=\u2202f\u2202q\u2202q\u2202x. In practice, this is simply a\nmultiplication of the two numbers that hold the two gradients. Let's see this\nwith an example:\n\n    \n    \n    # inputs x = -2; y = 5; z = -4 # forward pass q = x + y # q becomes 3 f = q * z # f becomes -12 # backward pass (backpropagation) in reverse order: # First backprop through f = q * z dfdz = q # df/dz = q, so gradient on z becomes 3 dfdq = z # df/dq = z, so gradient on q becomes -4 dqdx = 1.0 dqdy = 1.0 # Now backprop through q = x + y dfdx = dfdq * dqdx # The multiplication here is the chain rule! dfdy = dfdq * dqdy\n\nWe are left with the gradient in the variables [\u2202f\u2202x,\u2202f\u2202y,\u2202f\u2202z], which tell us\nthe sensitivity of the variables x,y,z on f. This is the simplest example of\nbackpropagation. Going forward, we will use a more concise notation that omits\nthe df prefix. For example, we will simply write dq instead of \u2202f\u2202q, and\nalways assume that the gradient is computed on the final output.\n\nImage is from cs231n notes by Karpathy. Visualization of above example as\ncircuit. The forward pass, left to right, (shown in green) computes values\nfrom inputs to output. The backward pass, right to left, then performs\nbackpropogation which starts at the end and recursively applies the chain rule\nto compute the gradients (shown in red) all the way to the inputs. Notice, in\nthe addition node, the gradient (-4) flowed to both prior node equally, this\nis the basic idea of skip connections in ResNet.\n\n### Intutive understanding of backpropogation\n\nBackpropogation is a local process. Every gate in a circuit diagram gets some\ninputs and can right away compute two things: 1. its output value and 2. the\nlocal gradient of its output with respect to its inputs. The gates can do this\ncompletely independentlty without being aware of any of the details full\ncircuit. Once the forward pass is over, during backprop the gate will\neventually learn about the gradients of its output value on the final output\nof the entire circuit. And with the chain rule you take the gradient and\nmultiply it by the gradient of its outputwith respect to its inputs.\n\n> This extra multiplication (for each input) due to the chain rule can turn a\n> single and relatively useless gate into a cog of a complex circuit such as\n> an entire neural network.\n\nIn the above example image, the intuition behind how backpropagation works\nusing an example of a simple circuit with an add gate and a multiply gate.\n\n  1. The add gate receives inputs [-2, 5] and computes an output of 3. Since it\u2019s performing addition, its local gradient for both inputs is +1.\n\n  2. The rest of the circuit computes the final value, which is -12.\n\n  3. During the backward pass, the add gate learns that the gradient for its output was -4. If we imagine the circuit as wanting to output a higher value, then the add gate \u201cwants\u201d its output to be lower due to the negative sign, with a force of 4.\n\n  4. To continue the chain rule and propagate gradients, the add gate multiplies the gradient (-4) to all of the local gradients for its inputs. This results in the gradient on both inputs (x and y) being -4.\n\n  5. This process has the desired effect: If the inputs were to decrease (in response to their negative gradients), the add gate\u2019s output would decrease, causing the multiply gate\u2019s output to increase.\n\nBackpropogation can thus be thought of as gates communicating to each other\n(through the gradient signal) whether they want their outputs to increase or\ndecrease (and how strongly), so as to make the final output value higher.\n\nThe derivative of the sigmoid function and its application in backpropagation\nfor a neuron in a neural network.\n\nThe sigmoid function is defined as:\n\n\u03c3(x)=11+e\u2212x\n\nThe derivative of the sigmoid function with respect to its input x is derived\nas follows:\n\n\u03c3(x)=11+e\u2212xd\u03c3(x)dx=e\u2212x(1+e\u2212x)2=11+e\u2212x\u22c5e\u2212x1+e\u2212x=(1\u2212\u03c3(x))\u22c5\u03c3(x) This\nsimplification of the gradient of the sigmoid function yields a\nstraightforward expression: (1\u2212\u03c3(x))\u22c5\u03c3(x).\n\nIn practical applications, this simplified expression is advantageous as it\nallows for efficient computation and reduces numerical issues.\n\nThe provided Python code demonstrates the backpropagation process for a\nneuron:\n\n    \n    \n    w = [2, -3, -3] # assume some random weights and data x = [-1, -2] # Forward pass dot = w[0]*x[0] + w[1]*x[1] + w[2] f = 1.0 / (1 + math.exp(-dot)) # Sigmoid function # Backward pass through the neuron # starting from end ddot = (1 - f) * f # Gradient on dot variable, using the sigmoid gradient derivation dx = [w[0] * ddot, w[1] * ddot] # Backprop into x dw = [x[0] * ddot, x[1] * ddot, 1.0 * ddot] # Backprop into w # We're done! We have the gradients on the inputs to the circuit\n\n### Backprop in practice: Staged computation\n\nSuppose that we have a function of the form:\n\nf(x,y)=x+\u03c3(y)\u03c3(x)+(x+y)2\n\nTo be clear, this function is completely useless and it\u2019s not clear why you\nwould ever want to compute its gradient, except for the fact that it is a good\nexample of backpropagation in practice\n\nHere is the forward pass:\n\n    \n    \n    x = 3 y = -4 # forward pass sigy = 1 / (1 + math.exp(-y)) # (1) num = x + sigy # (2) sigx = 1 / (1 + math.exp(-x)) # (3) xy = x + y # (4) xy2 = xy**2 # (5) den = sigx + xy2 # (6) invden = 1/den # (7) f = num * invden # (8)\n\nWe have to compute gradients for sigy, num, sigx, xy, xy2, den, invden.\n\n    \n    \n    # backprop f = num * invden dnum = invden # df/dnum = 1*invden + num*0 = invden dinvden = num # df/dinvden # backprop invden = 1/den dden = dinvden * (-1/(den**2)) # self-learned-rule-of-thumb! # for chain rule, instead of getting confused on # what to multiply because there are branches # instead multiply the d of LHS, in above the example # the function was # invdev = 1/den # we mul the dinvdev, which is dLHS. # backprop den = sigx + xy2 dsigx = dden * 1 dxy2 = dden * 1 # backprop xy2 = xy**2 dxy = dxy2 * (2 * xy) # backprop xy = x + y dx = dxy * 1 dy = dxy * 1 # backprop sigx = 1 / (1 + math.exp(-x)) dx += dsigx * ((1-sigx) * sigx) # Notice, +=, see notes below # backprop num = x + sigy dx += dnum * 1 dsigy = dnum * 1 # backprop sigy = 1 / (1 + math.exp(-y)) dy += dsigy * ((1-sigy) * sigy) # Done!\n\nCache forward pass variables. To compute the backward pass it is very helpful\nto have some of the variables that were used in the forward pass. In practice\nyou want to structure your code so that you cache these variables, and so that\nthey are available during backpropagation. If this is too difficult, it is\npossible (but wasteful) to recompute them.\n\nGradients add up at forks. The forward expression involves the variables x,y\nmultiple times, so when we perform backpropagation we must be careful to use\n+= instead of = to accumulate the gradient on these variables (otherwise we\nwould overwrite it). This follows the multivariable chain rule in Calculus,\nwhich states that if a variable branches out to different parts of the\ncircuit, then the gradients that flow back to it will add.\n\nImage is from cs231n notes by Karpathy. An example circuit demonstrating the\nintuition behind the operations that backpropagation performs during the\nbackward pass in order to compute the gradients on the inputs. Sum operation\ndistributes gradients equally to all its inputs. Max operation routes the\ngradient to the higher input. Multiply gate takes the input activations, swaps\nthem and multiplies by its gradient\n\nThe add gate always takes the gradient on its output and distributes it\nequally to all of its inputs, regardless of what their values were during the\nforward pass. This follows from the fact that the local gradient for the add\noperation is simply +1.0, so the gradients on all inputs will exactly equal\nthe gradients on the output because it will be multiplied by x1.0 (and remain\nunchanged). In the example circuit above, note that the + gate routed the\ngradient of 2.00 to both of its inputs, equally and unchanged. For this reason\nthe add gate is also called gradient highway.^1\n\nThe max gate routes the gradient. Unlike the add gate which distributed the\ngradient unchanged to all its inputs, the max gate distributes the gradient\n(unchanged) to exactly one of its inputs (the input that had the highest value\nduring the forward pass). This is because the local gradient for a max gate is\n1.0 for the highest value, and 0.0 for all other values. In the example\ncircuit above, the max operation routed the gradient of 2.00 to the z\nvariable, which had a higher value than w, and the gradient on w remains\nzero.^1\n\nThe multiply gate is a little less easy to interpret. Its local gradients are\nthe input values (except switched), and this is multiplied by the gradient on\nits output during the chain rule. In the example above, the gradient on x is\n-8.00, which is -4.00 x 2.00.^1\n\nUnintuitive effects and their consequences.\n\nNotice that if one of the inputs to the multiply gate is very small and the\nother is very big, then the multiply gate will do something slightly\nunintuitive: it will assign a relatively huge gradient to the small input and\na tiny gradient to the large input. Note that in linear classifiers where the\nweights are dot producted wTxi (multiplied) with the inputs, this implies that\nthe scale of the data has an effect on the magnitude of the gradient for the\nweights. For example, if you multiplied all input data examples xi by 1000\nduring preprocessing, then the gradient on the weights will be 1000 times\nlarger, and you\u2019d have to lower the learning rate by that factor to\ncompensate. This is why preprocessing matters a lot, sometimes in subtle ways!\nAnd having intuitive understanding for how the gradients flow can help you\ndebug some of these cases.\n\n### Gradients for vectorized operations\n\nGradients for vectorized operations extend concepts to matrix and vector\noperations, requiring attention to dimensions and transpose operations.\n\nMatrix-matrix multiplication presents a challenge: Forward pass:\n\n    \n    \n    W = np.random.randn(5, 10) X = np.random.randn(10, 3) D = W.dot(X)\n\nSuppose we have the gradient on D:\n\n    \n    \n    dD = np.random.randn(*D.shape) # same shape as D dW = dD.dot(X.T) # transpose of X dX = W.T.dot(dD)\n\nTip: Utilize dimension analysis to derive gradient expressions. The resulting\ngradients must match the respective variable sizes. For instance, dW must\nmatch the size of W and depends on the matrix multiplication of X and dD.\n\nStart with small, explicit examples to derive gradients manually, then\ngeneralize to efficient, vectorized forms. This approach aids understanding\nand application of vectorized expressions.\n\n### Gradient checking\n\nTLDwrote; cs231n notes on gradient checking\n\n### Sources\n\n  1. Inspired by CS231n notes \u21a9 \u21a9^2 \u21a9^3 \u21a9^4\n\n  2. This blog is written as notes to this youtube lecture. \u21a9\n\n  3. CS231n Lecture on backprop can be found here. \u21a9\n\n## AmreV\n\n  * AmreV\n  * akashzsh08@gmail.com\n\n  * akash5100\n  * akzsh5100\n\nDocumenting my deeplearning journey and a home for my poorly researched ideas\nthat I find myself repeating often anyway\n\n", "frontpage": false}
