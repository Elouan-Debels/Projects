{"aid": "40041069", "title": "Efficient knowledge distillation without the teacher model using CLIP", "url": "https://github.com/lnairGT/CLIP-Distillation", "domain": "github.com/lnairgt", "votes": 2, "user": "Illustrious_sir", "posted_at": "2024-04-15 14:33:53", "comments": 0, "source_title": "GitHub - lnairGT/CLIP-Distillation: Knowledge Distillation using Contrastive Language-Image Pretraining (CLIP) without a teacher model.", "source_text": "GitHub - lnairGT/CLIP-Distillation: Knowledge Distillation using Contrastive\nLanguage-Image Pretraining (CLIP) without a teacher model.\n\nSkip to content\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nlnairGT / CLIP-Distillation Public\n\n  * Notifications\n  * Fork 0\n  * Star 3\n\nKnowledge Distillation using Contrastive Language-Image Pretraining (CLIP)\nwithout a teacher model.\n\n### License\n\nMIT license\n\n3 stars 0 forks Branches Tags Activity\n\nStar\n\nNotifications\n\n# lnairGT/CLIP-Distillation\n\nThis commit does not belong to any branch on this repository, and may belong\nto a fork outside of the repository.\n\n1 Branch\n\n0 Tags\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\nlnairGTUpdate README.md with arxiv link4ae704f \u00b7\n\n## History\n\n11 Commits  \n  \n### LICENSE\n\n|\n\n### LICENSE\n\n| Initial commit  \n  \n### README.md\n\n|\n\n### README.md\n\n| Update README.md with arxiv link  \n  \n### citation.cff\n\n|\n\n### citation.cff\n\n| citation file  \n  \n### cluster.py\n\n|\n\n### cluster.py\n\n| Initial files  \n  \n### comp_resources.png\n\n|\n\n### comp_resources.png\n\n| Results  \n  \n### config.py\n\n|\n\n### config.py\n\n| Initial files  \n  \n### dataloaders.py\n\n|\n\n### dataloaders.py\n\n| Initial files  \n  \n### train.py\n\n|\n\n### train.py\n\n| Initial files  \n  \n### train.sh\n\n|\n\n### train.sh\n\n| Initial files  \n  \n### utils.py\n\n|\n\n### utils.py\n\n| Initial files  \n  \n### vit_model.py\n\n|\n\n### vit_model.py\n\n| Initial files  \n  \n## Repository files navigation\n\n# CLIP-Embed-KD: Computationally Efficient Knowledge Distillation Using\nEmbeddings as Teachers\n\nPAPER: arxiv link\n\nCan pre-computed embeddings obtained from the teacher model be used to train\nthe student model in knowledge distillation?\n\nThis project extends CLIP for efficient knowledge distillation, by utilizing\nembeddings as teachers. Typical knowledge distillation frameworks require\nrunning forward passes through a teacher model, which is often prohibitive in\nthe case of billion or trillion parameter teachers. Using only the embeddings\nof the teacher models to guide the distillation can yield significant\ncomputational savings.\n\n## RUNNING THE SCRIPT\n\nRun the following command with appropriate arguments:\n\n    \n    \n    python train.py \\ --dataset-name <CIFAR10; CIFAR100; ImageNet> \\ --teacher-model <Huggingface-ckpt-name> \\ --log-folder <folder-to-save-logs> \\ --ckpt-save-name <name-of-ckpt-to-save-trained-model> \\ --train-type <embed-KD; teacher-KD; vanilla>\n\nAn example command for running the script is available in train.sh. The\nargument train-type vanilla refers to regular knowledge distillation (without\nusing the CLIP distillation loss).\n\nThe argument teacher-model is the name of the HuggingFace checkpoint. Teacher\nmodels used in the paper include: google/vit-large-patch16-224-in21k,\ngoogle/vit-large-patch32-224-in21k, google/vit-base-patch16-224-in21k and\ngoogle/vit-base-patch32-224-in21k. The configuration of the student model, and\nother training parameters are in config.py.\n\n### CLIP-Embed-KD: computational efficiency\n\n## CITATION\n\nIf you find this work useful, please consider citing the paper:\n\n    \n    \n    @misc{nair2024clipembedkd, title={CLIP-Embed-KD: Computationally Efficient Knowledge Distillation Using Embeddings as Teachers}, author={Lakshmi Nair}, year={2024}, eprint={2404.06170}, archivePrefix={arXiv}, primaryClass={cs.LG} }\n\n## About\n\nKnowledge Distillation using Contrastive Language-Image Pretraining (CLIP)\nwithout a teacher model.\n\n### Resources\n\nReadme\n\n### License\n\nMIT license\n\n### Citation\n\nActivity\n\n### Stars\n\n3 stars\n\n### Watchers\n\n2 watching\n\n### Forks\n\n0 forks\n\nReport repository\n\n## Releases\n\nNo releases published\n\n## Packages 0\n\nNo packages published\n\n## Languages\n\n  * Python 99.2%\n  * Shell 0.8%\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": false}
