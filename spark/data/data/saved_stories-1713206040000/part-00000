{"aid": "40039731", "title": "Dealing with Unconventional Facial Expressions in Neural Synthesis", "url": "https://blog.metaphysic.ai/dealing-with-unconventional-facial-expressions-in-neural-synthesis/", "domain": "metaphysic.ai", "votes": 1, "user": "Hard_Space", "posted_at": "2024-04-15 12:32:12", "comments": 0, "source_title": "Dealing with Unconventional Facial Expressions in Neural Synthesis", "source_text": "Dealing with Unconventional Facial Expressions in Neural Synthesis -\nMetaphysic.ai\n\nSkip to content\n\n## Home\n\n# Dealing with Unconventional Facial Expressions in Neural Synthesis\n\n  * April 15, 2024\n  * 12:29 pm\n\n### About the author\n\n#### Martin Anderson\n\nI'm Martin Anderson, a writer occupied exclusively with machine learning,\nartificial intelligence, big data, and closely-related topics, with an\nemphasis on image synthesis, computer vision, and NLP.\n\n  * Author Website\n  * Author Archive\n\n## Share This Post\n\nConsidering the tremendous surge of interest in human synthesis over the last\n18 months, relatively little attention is being paid, either in the research\nscene or in the public\u2019s understanding of the implications of generative AI,\nto the challenge of recognizing, altering and synthesizing human facial\nexpressions.\n\nAnyone who has ever worked with a demanding director, on set or in post-\nproduction, will be aware of the assiduous attention to detail that can cause\n127 takes of a scene, or cause such production delays that the movie needs to\nbe shot a second time, overtaken by emerging technology in the interim.\n\nSo as it becomes increasingly possible to alter facial expressions in post-\nproduction, via neural techniques such as expression editing, directors will\nexpect a full toolkit of techniques in this regard \u2013 and that currently does\nnot exist.\n\nPart of the problem is that the underlying principles of facial affect\nrecognition itself are dominated by the mere 6-7 \u2018core\u2019 universal expressions\nof the Facial Action Coding System (FACS).\n\nFacial Action Units, divided between different parts of the face, codified by\nthe 1970s-born Facial Action Coding System (FACS), at work in AffectNet.\nSource: https://arxiv.org/pdf/2103.15792.pdf\n\nThe nascent study and classification of facial expressions is not evolving in\nservice of human synthesis, but is motivated by the psychological and security\nresearch sectors, which have their own agendas and their own timetables.\n\nArguably, if we compare the current conceptual state-of-the-art in the study\nof human facial expressions, it would appear to be nearer the stage that\npsychiatry and psychology was at sometime in the 1930s \u2013 the future is bright,\nbut distant.\n\n## Face Value\n\nAs it stands, whether a director wishes to create or amend an expression in a\npurely generative video (and here the advent of Sora has convinced some\nindustry luminaries that generative video technologies are encroaching on\ntraditional film and TV production at lightning speed), or to use neural\nmethods to fine-tune an actor\u2019s performance months after the wrap, six\navailable expressions are clearly not going to cut it.\n\nWhat we\u2019re currently witnessing, instead, is the use of AI techniques to\nchange recorded faces into a kind of \u2018neural clay\u2019, so that by pushing around\nsections of a model\u2019s latent space, it\u2019s possible to raise an eyebrow, furrow\na brow, dial down a smile, or otherwise tweak the expressions that were\nrecorded on the day.\n\nSome examples of expression editing from the recent ChatFace offering. Source:\nhttps://arxiv.org/pdf/2305.14742.pdf\n\nThis kind of plasticity is useful but artisanal in nature \u2013 no more automated\nthan any of the other CGI-based techniques that have been developed over the\nlast 30-40 years, in that a professional or end-user is required to play\naround with possibilities until the face \u2018looks right\u2019, rather than choosing\nand applying an apposite expression from a general toolkit.\n\nSo this kind of neural plasticity is a useful tool, but it doesn\u2019t enhance or\nadvance the limited semantic understanding that prevents us imposing a wider\nrange of more subtle emotions, i.e., by defining any required emotion in terms\nof facial expression, and imposing it neurally.\n\nIn this sense, both FACS, and the current methodologies of Facial Expression\nRecognition (FER) datasets are arguably the choke-points; if a system settles\non just six core expressions, the multitude of other possible and more subtle\npossible facial expressions will end up shunted into some \u2018ghetto\u2019 at the\nperiphery of one of these severely limited categories.\n\nWorse, from the point of view of developing better systems, there are no\nupstream libraries that can bring a wider gamut of expressiveness into new\nsystems \u2013 because the dominant systems (FACS, etc.) tends to define them, and\nfiner-grained facial expressions are therefore hard to either capture or\nsynthesize.\n\nA further obstacle to improving on the current state-of-the-art is the\ndifficulty in capturing expressions which are meaningful and interpretable to\npeople, yet which are outside of the standard intent and accepted semantics of\nfacial expressions.\n\nPartly this is because identity and facial emotion are very entwined; the\nlineaments of a person\u2019s neutral face may accord with a known emotion, which\ncan confuse an FER system, for instance.\n\nSecondly, where does a smile (as a neutral concept) end and a particular\nidentity begin? This is an impossible separation, since the medium is also the\nmessage, and facial topology and expressed emotion are difficult to prise\napart, so that we may obtain \u2018neutral\u2019 domain models for a broad range of\nexpressions.\n\n## SMIRK\n\nOne recent collaboration between Greece and Germany, however, has at least\napparently advanced the state-of-the-art in being able to distinguish\n\u2018outlier\u2019 expressions. Titled Spatial Modeling for Image-based Reconstruction\nof Kinesics (aka the tortuously shoe-horned acronym SMIRK), the system uses a\nneural rendering module in concert with extracted 3D facial meshes (via the\n3DMM-style FLAME framework) to separate topology from secondary aspects, such\nas albedo, which are less explanatory of emotion than facial identity.\n\nExamples of mesh extrapolation via the improved processes of SMIRK. Source:\nhttps://arxiv.org/pdf/2404.04104.pdf\n\nBy focusing solely on the inferred geometry, the authors claim, it is easier\nto isolate facial manipulations which may indicate non-mainstream expressions\n(such as a \u2018smirk\u2019, which has no place in the FACS canon, for instance).\n\nAdditionally, this method allows the researchers to generate versions of the\ninput source identity that have different expressions than the one depicted in\nthe original image, and these amended expressions can be used as further input\nto generalize the reconstruction model.\n\nSMIRK's expression perturbation pipeline aides in overall reconstruction,\nfacilitating a better ability to extract unusual expressions.\n\nThe authors state:\n\n\u2018Our extensive experimental results show that SMIRK outperforms previous\nmethods and can faithfully reconstruct expressive 3D faces, including\nchallenging complex expressions such as asymmetries, and subtle expressions\nsuch as smirking.\u2019\n\nThe paper is titled 3D Facial Expressions through Analysis-by-Neural-\nSynthesis, and comes from seven researchers across Greece\u2019s Institute of\nRobotics at the Athena Research Center, National Technical University of\nAthens, and the Institute of Computer Science at Hellas (FORTH), as well as\nthe Max Planck Institute for Intelligent Systems in Tubingen, Germany.\n\n## Method\n\nThe new system is inspired by a strand of recent projects that use facial\nreconstruction methodologies, including the pivotal EMOCA project, also from\nthe Max Planck Institute; SPECTRE, a prior work featuring many of the same\nauthors as for the new paper; and, among others, the Chinese offering 3D Face\nReconstruction Using A Spectral-Based Graph Convolution Encoder.\n\nThe through-line of most of the prior works informing SMIRK is the use of CGI-\nbased parametric models \u2013 default heads and/or bodies in base canonical\npositions and configurations, which are then conformed to features extracted\nfrom real images of people.\n\nOne of the oldest technologies still actively used in current neural synthesis\ndevelopment, 3DMM (and later off-shoots) projects real characteristics onto a\nCGI head. Source: https://arxiv.org/pdf/1909.01815.pdf\n\nOnce a connection has been made between the keypoints on the parametric head\nand the landmarks and other tokens obtained from the original images, the user\nhas an optimal degree of instrumentality in controlling several key neural\nprocesses.\n\nThe difference with SMIRK, the authors claim, is that the new image>image\nmethod employed in the system bridges the domain gap between the real source\ninput images and the synthesized output.\n\nA domain gap is common when synthetic imagery is used, since the quality of\nsynthesized imagery in training datasets is not usually of the same standard\nas the novel data that the system will be asked to create at inference time.\nBy creating an interstitial altered image based directly on the individual\nsource image in each case, ground truth can be obtained on a per-instance\nbasis improving accuracy.\n\nIn the initial reconstruction pass, the source image is passed to an encoder\nthat regress camera parameters and other data from the FLAME CGI model. Thus a\n3D model is conformed to the source image and rendered with a differentiable\nrasterizer, before being reconstructed with the image-image translation\nnetwork. After this, self-supervised photometric, landmark and perceptual\nlosses are estimated.\n\nThe researchers state*:\n\n\u2018SMIRK contributes with a novel neural rendering module that bridges the\ndomain gap between the input and the synthesized output. By minimizing this\ndiscrepancy, SMIRK enables a stronger supervision signal within an analysis-\nby-synthesis framework.\n\n\u2018Notably, this means that neural-network based losses such as perceptual,\nidentity, or emotion can be used to compare the reconstructed and input images\nwithout the typical domain-gap problem that is present in most works.\u2019\n\nThe FLAME model used by SMIRK appears in a number of synthesis projects that\nwe have covered, including Gaussian avatar deepfakes, an improvement on the\nControlnet Stable Diffusion ancillary framework, and the ManVatar NeRF-based\navatar framework, among others.\n\nThe generated mesh has 5023 vertices, which is considered modest in facial CGI\nmodeling, and is capable of eye closure and jaw rotation.\n\nThe encoder used for processing through to the expression variation is a deep\nneural network that regresses the FLAME parameters into three distinct\nbranches, each of which is a MobileNet V3 backbone.\n\nThe neural renderer component is intended to replace conventional graphics-\nbased networks in similar systems with an image-to-image model. During the\ntransliteration process, random pixels are blocked out from the generated face\nimages as the system iterates, so that the ultimate representation does not\noverfit to the source image, but relies instead on the mesh.\n\nRandom dropout of parts of the processing image is a common technique during\ntraining, designed to help the model generalize and not obsess about being\nfaithful to the exact configuration of the original photo.\n\nThe masking of the image itself is accomplished, effectively, by joining\ntogether the facial landmark dots that are inferred from the source image. The\nsystem used for this is the Facial Alignment Network (FAN) library that also\npowers pose and topology recognition for the 2017-era deepfake packages\nDeepFaceLab, DeepFaceLive, and FaceSwap.\n\nAs for the reconstruction loss functions that ensure the system improves with\ntraining, the functions used include photometric loss, which is a simple L1\nloss that compares the output to the source image; and VGG Loss, a perceptual\nencoder that can converge faster in the earlier phases of training.\n\nAdditional loss functions are landmark loss, an L2 comparison between\nestimated landmarks in source and generated images; expression regularization,\nan L2 loss that penalizes extreme or unlikely expressions; and emotion loss,\nwhich derives a loss based on the aforementioned EMOCA FER network.\n\nIn this regard, the authors state:\n\n\u2018To prevent the image translator from adversarially optimizing the emotion\nloss by perturbing a few pixels, for this loss we keep the image translator T\n\u201cfrozen\u201d, optimizing only the expression encoder E_\u03c8 . Note that unlike EMOCA,\nour framework ensures that the emotion loss does not suffer from domain gap\nproblems, as the compared images reside in the same space.\u2019\n\nHowever, the improved supervision signal obtained by these methods remains at\nthe mercy of the poor diversity in the underlying training datasets, which,\nthe authors note, is also a choke-point for all previous analogous systems.\n\nThe authors illustrate the issue:\n\n\u2018This means for example that if a more complex lip structure, scarcely seen in\nthe training data, cannot be reproduced fast enough by the encoder, the\ntranslator T could learn to correlate miss-aligned lip 3D structures and\nimages and thus multiple similar, but distinct, facial expressions will be\ncollapsed to a single reconstructed representation.\n\n\u2018Further, this may lead to the translator compensating for the encoder\u2019s\nfailures during the joint optimization.\u2019\n\nThis issue is addressed in the new system by what the authors term an\naugmented expression cycle consistency path.\n\nPermutations, including an attempt at evaluating a 'neutral' expression, using\nthe augmented expression cycle consistency path part of the SMIRK workflow.\n\nHere the original expressions are replaced with new variations, and\nphotorealistic images generated, as seen in the image above. Thus novel\ntraining pairs are generated dynamically.\n\nThis cycle consistency loss is therefore implemented directly in the\nexpression parameter space of the FLAME model, which allows the predicted\nexpression to resemble the captured one far more closely than in similar\nsystems.\n\nThis approach stops over-compensation errors (\u2018bad guesses\u2019 on the part of the\ninternal mechanisms iterating through the data), and encourages a broader\nrange of facial expressions.\n\nThe various methods used, seen in the image above, in this module, are\npermutation, perturbation, template injection and zero expression (neutral\nexpressions).\n\nThough the others are reasonably self-explanatory, template injection involves\nfeeding the FLAME facial CGI model with parameters obtained from the FaMoS\ndataset, which features multiple subjects performing asymmetric and extreme\nexpressions.\n\nExamples from the FaMoS dataset, which features atypical expressions. Source:\nhttps://tempeh.is.tue.mpg.de/#dataset\n\nTwo losses are used during this phase \u2013 the self-explanatory identity\nconsistency and expression consistency. The system alternates between these\nvarious passes, alternately freezing the encoder and the translator, to\nmaximize a random effect that will aid flexibility.\n\n## Data and Tests\n\nThe researchers conducted quantitative and qualitative tests for SMIRK. Where\nvideo examples are indicated, some of these can be seen in the official\naccompanying video for the system, embedded at the end of this article.\n\nDatasets used in the tests were FFHQ, CelebA, LRS3, and MEAD. Competing rival\nsystems were DECA and EMOCA V2, both also reliant on the FLAME model, and\nadditionally Deep3DFace and FOCUS, which instead use the BFM model.\n\nThe three encoders used for tests were all pre-trained and supervised by two\nlosses: landmark loss, for pose and expression, and shape predictions based on\nthe MICA framework.\n\n(Though it is not referred to otherwise in the paper, the results \u2013 see\nfurther below \u2013 feature MGCnet as well)\n\nExamples from the MICA framework, which contributes to losses used in tests.\nSource: https://arxiv.org/pdf/2204.06607.pdf\n\nThe researchers point out the frequent assertions in the literature that\ncontend that evaluating facial expression reconstruction is an ill-posed\nproblem in itself*:\n\n\u2018The geometric errors tend to be dominated by the identity face shape and do\nnot correlate well with human perception of facial expressions. Accordingly,\nwe compare our method in a quantitative manner with three experiments: 1)\nemotion recognition accuracy, ability of a model to guide a UNet to faithfully\nreconstruct an input image, and 3) a perceptual user study.\u2019\n\nIn accordance with the EMOCA protocol, the authors trained a Multi-Layer\nPerceptron (MLP) to classify eight basic expressions, and to run regression on\nvalence and arousal values with the use of AffectNet.\n\nMetrics reported for the emotion recognition test were Concordance Correlation\nCoefficient (CCC), Root Mean Squared Error (RMSE) for arousal and valence, and\nexpression classification accuracy (E-ACC).\n\nEmotion recognition performance on AffectNet.\n\nThe authors report:\n\n\u2018[SMIRK] achieves a higher emotion recognition score compared to most other\nmethods, although falling behind EMOCAv1/2 and Deep3DFace. It is worth noting\nthat, although EMOCA v1 achieves the highest emotion accuracy, it often\noverexaggerates expressions which helps with emotion recognition.\n\n\u2018EMOCA v2, arguably a more accurate reconstruction model, performs slightly\nworse. Our main model is comparable with Deep3DFace and outperforms DECA and\nFOCUS. We can also train a model that scores better on emotion recognition, by\nincreasing the emotion loss weight.\u2019\n\nThough it is possible to train a model that would score better for emotion\nrecognition, the authors observe that previous work for EMOCA has indicated\nthis leads to increased artifacts.\n\nNext, the researchers tested for reconstruction loss. For this, a Unet image-\nto-image translator was trained with the encoder frozen, thus training only\nthe translation functionality. The authors explain:\n\n\u2018[If] the 3D mesh is accurate enough, the reconstruction will be more\nfaithful, due to a one-to-one appearance correspondence. For each method\n(including ours for fairness), we train a UNet for 5 epochs, using the masked\nimage and the rendered 3D geometry as input.\u2019\n\nThe L1 reconstruction loss and the VGG loss were reported in these tests,\nwhich were powered by the AffectNet database.\n\nResults for image reconstruction performance.\n\nHere the authors comment:\n\n\u2018We observe here that using the information for the rendered shape geometry of\nSMIRK, the trained UNet achieves a more faithful reconstruction of the input\nimage when compared to DECA and EMOCAv2.\n\n\u2018Particularly for EMOCAv2, we observe that although it can capture\nexpressions, the results in many cases do not faithfully represent the input\nimage, leading to an overall worse image reconstruction error.\n\n\u2018In terms of L1 loss, SMIRK is on par with Deep3DFace and FOCUS and has a\nsmall improvement in terms of VGG loss.\u2019\n\nFor the user study, 80 images each from AffectNet and MEAD were used to\nperform 3D face reconstruction with SMIRK and the prior frameworks, The 85\npeople participating were shown a face image together with two 3D\nreconstructions from one or other of the methods, and were requested to choose\nthe optimal image in terms of facial expression representation.\n\nResults from the user study.\n\nOf the user study results, the researchers comment:\n\n\u2018[Our] method was significantly preferred over all competitors, confirming the\nperformance of SMIRK in terms of faithful expressive 3D reconstruction.\n\n\u2018The results were statistically significant (for all pairs, p < 0.01 with\nbinomial test, adjusted using the Bonferroni method). EMOCAv2, which also uses\nan emotion loss for expressive 3D reconstruction, was the closest competitor\nto our method, followed by FOCUS and Deep3D, while DECA was the least\nselected.\u2019\n\nLastly (as we do not cover ablation studies except in extraordinary\ncircumstance \u2013 please refer to the source paper for these), the authors\nproduced general qualitative examples, both in the form of static images for\nthe paper, and in video form (refer to embedded video at end of article).\n\nVisual comparisons. For some reason the paper has not included text legends\nfor the compared frameworks \u2013 from left to right, they are Deep3DFaceRecon,\nFOCUS, DECA, EMOCA V2 and, finally, SMIRK. Please refer to the source paper\nfor better resolution.\n\nHere the authors comment:\n\n\u2018[Our] method can more accurately capture the facial expressions across\nmultiple diverse subjects and conditions.\n\n\u2018Furthermore, the presented methodology can also capture expressions that\nother methods fail to capture, such as non-symmetric mouth movements, eye\nclosures, and exaggerated expressions.\u2019\n\n## Conclusion\n\nAny new innovation that advances the state-of-the-art in fine-grained\nexpression recognition and synthesis is welcome, and SMIRK certainly qualifies\nhere. However, it is arguably only supplying better recognition methods (in\nsome cases) for later and better semantic architectures around FER than we\nmust currently contend with.\n\nFor the moment, a great deal of the challenge that SMIRK and previous systems\nare addressing is organizational, even philosophical, rather than technical in\nnature.\n\n* My substitution of hyperlinks for the authors\u2019 inline citations.\n\nRETURN TO METAPHYSIC BLOG HOME\n\nPrevPreviousHigh Resolution (And High Accuracy) Stable Diffusion With a\nRelatively Simple Hack\n\n## More To Explore\n\nAI ML DL\n\n### Dealing with Unconventional Facial Expressions in Neural Synthesis\n\nThe much-used Facial Action Coding System (FACS) has a very narrow range of\ncore expressions \u2013 6-8, depending on which version you\u2019re using. However, film\nand TV directors interested in using neural tools to adjust facial\nperformances in post are going to need a more precise toolkit, which falls\noutside the FACS paradigm. Now, researchers from Greece and Germany are at\nleast offering an expression recognition and synthesis system, titled SMIRK,\nwhich can address a wider range of facial expressions, and hopefully advance\nthe state of the art in this under-served aspect of human synthesis.\n\nMartin Anderson April 15, 2024\n\nAI ML DL\n\n### High Resolution (And High Accuracy) Stable Diffusion With a Relatively\nSimple Hack\n\nMost new innovations in upscaling and text-prompt accuracy emerging from the\nresearch sector tend to be tortuous affairs involving the gathering of new\ndatasets, expensive and extensive training, and/or the use of heavyweight\nadjunct systems. By contrast, this new approach to improving the resolution\nand fidelity of Stable Diffusion involves only a small change to the internal\nprocess \u2013 yet offers native upscaling without the traditional Lovecraft-style\nhorrors when one tries to generate an image above the native resolution of\nimages on which Stable Diffusion was trained.\n\nMartin Anderson April 8, 2024\n\n\u201c\n\n## It is the mark of an educated mind to be able to entertain a thought\nwithout accepting it.\n\nAristotle\n\nCopyright \u00a9 2023. All rights reserved. Privacy Policy\n\n### Quick Links\n\n  * Home\n  * Every Anyone\n  * Synthetic Futures\n\n### Connect with us\n\n  * Discord\n  * Tiktok\n  * Twitter\n  * Youtube\n  * Instagram\n  * Github\n  * Linkedin\n\n### Contact Info\n\n  * info@metaphysic.ai\n  * press@metaphysic.ai\n\n", "frontpage": false}
