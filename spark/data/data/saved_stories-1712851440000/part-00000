{"aid": "40000421", "title": "Gelsinger: With Gaudi 3 and Xeon 6, AI Workloads Will Come Our Way", "url": "https://www.nextplatform.com/2024/04/10/gelsinger-with-gaudi-3-and-xeon-6-ai-workloads-will-come-our-way/", "domain": "nextplatform.com", "votes": 1, "user": "rbanffy", "posted_at": "2024-04-11 10:02:47", "comments": 0, "source_title": "Gelsinger: With Gaudi 3 and Xeon 6, AI Workloads Will Come Our Way", "source_text": "Gelsinger: With Gaudi 3 and Xeon 6, AI Workloads Will Come Our Way\n\nLatest\n\n  * [ April 10, 2024 ] With MTIA v2 Chip, Meta Can Do AI Training As Well As Inference AI\n  * [ April 10, 2024 ] Gelsinger: With Gaudi 3 and Xeon 6, AI Workloads Will Come Our Way AI\n  * [ April 9, 2024 ] Google Joins The Homegrown Arm Server CPU Club Compute\n  * [ April 9, 2024 ] With Gaudi 3, Intel Can Sell AI Accelerators To The PyTorch Masses AI\n  * [ April 8, 2024 ] Mixed Results For The Datacenter Thundering Thirteen In Q4 Compute\n  * [ April 4, 2024 ] Intel\u2019s Chips No Longer Pay More Than Their Fair Share Of Foundry Costs Compute\n  * [ April 4, 2024 ] Celestial AI Wants To Break The Memory Wall, Fuse HBM With DDR5 Compute\n  * [ April 4, 2024 ] Creating A Greener Edge Edge\n\n# Gelsinger: With Gaudi 3 and Xeon 6, AI Workloads Will Come Our Way\n\nApril 10, 2024 Jeffrey Burt AI, Compute 0\n\nThe steady rise of AI over the past several years \u2013 and the accelerated growth\nwith the introduction generative AI since OpenAI\u2019s launch of ChatGPT in\nNovember 2022 \u2013 has shifted Intel\u2019s status as a challenger in a chip market\nthat it long had dominated.\n\nFor sure, Intel still commands a good percentage of the consumer and\nenterprise CPU space, but the focus is now \u2013 will be into the future \u2013 AI\nworkloads. Also, AI isn\u2019t the only reason Intel doesn\u2019t own the industry like\nit once did \u2013 internal design and manufacturing challenges that led to costly\nand embarrassing product delays didn\u2019t help \u2013 and trends like cloud computing\nand the edge opened up avenues for traditional rivals and startups alike. But\nNvidia more than a decade ago put all of its money on AI, using it as the\nNorth Star for its innovations going forward.\n\nAnd it\u2019s paid off and \u2013 as we noted in February \u2013 paid off handsomely, with\nthe GPU maker poised on the verge of becoming a $100 billion company and\nposting Q4 2024 revenue of $22.1 billion, a year-over-year increase of 265.3\npercent fueled by its combination of hardware and software. Meanwhile, Intel\nearlier this month saw is important foundry business get hit with a $7 billion\nloss, illustrating the hurdles facing CEO Pat Gelsinger as he stares at a\nyears-long effort to get the company going in the right direction.\n\nFar from chipping away at his trademark enthusiasm, Gelsinger during his\nkeynote at the company\u2019s Intel Vision 2024 event in Arizona seemed to relish\nthe role of the underdog as he put forth a roadmap for fueling Intel\u2019s vision\nof driving AI everywhere that includes embracing an open-source approach,\npromising silicon that will outperform what Nvidia has on the market and\noffering a portfolio that will address AI workloads in the datacenter and\ncloud, on the PC, and at the edge.\n\n### Open Architectures and Intel\n\nThe CEO also said the AI market will evolve in a way that bends it toward\nIntel, noting that demand will turn to open platforms and the strong support\nIntel is getting from major AI and IT companies and arguing that enterprises\nwill want to stick with an architecture they\u2019ve been familiar with for decades\nif it can offer AI performance and efficiency equal to or better than Nvidia.\n\nWhether it plays out like that is unclear, but Gelsinger, as he introduced the\nupcoming latest generation of Xeon server chips \u2013 the Xeon 6 family \u2013 and\nGaudi 3 AI accelerators and spoke about an open systems strategy, was taking\nhis swings.\n\n\u201cOne of the key questions that our AI customers are asking us is, \u2018How do I\ndeploy?\u2019\u201d he said. \u201cAnd nearly all of the GenAI deployed developments today\nare moving to higher level environments, PyTorch frameworks and other\ncommunity models from Hugging Face. Industry is quickly moving away from\nproprietary [Nvidia] CUDA models. Literally, a few lines of code and you\u2019re\nable to be up and running with industry-standard frameworks on power-\nperformant, efficient cloud infrastructure.\u201d\n\nIntel\u2019s got a mountain to climb. Gaudi 3 is coming in the wake of Nvidia\u2019s\nintroduction last month of its Blackwell datacenter GPUs, including its GB200\nGrace Blackwell Superchip, which combines two B200 GPUs with a single Grace\nCPU linked by its next-generation NVLink connectivity. To prove the\nadvancements in Blackwell, Nvidia founder and CEO Jensen Huang said during the\ncompany\u2019 GTC 2024 show that it takes 8,000 previous Hopper GPUs and 15\nmegawatts of power to train a 1.8 trillion-parameter AI model; with Blackwell,\nonly 2,000 accelerators are needed and will consume 4 megawatts.\n\n### Gaudi 3: Good Performance At A Lower Cost\n\nGelsinger pushed back, boasting of Gaudi 3\u2019s performance and TCO. The\naccelerators is 50 percent bette with inference and 40 percent more power\nefficient than Nvidia\u2019s current \u2013 and at times hard-to-find H100, given the\nskyrocketing demand for compute power for AI workloads \u2013 and will come in at a\n\u201cfraction\u201d of the cost.\n\nIn speaking with journalists and analysts after the keynote, Gelsinger said he\nwasn\u2019t ready to discuss pricing for Gaudi 3, but added that \u201cwe are very\nconfident it will be well below the number for the H100 or Blackwell. Not a\nlittle below, but a lot below, and that can be a factor in TCO.\u201d\n\nThe new Xeon 6 branding for the latest CPUs for datacenters, the cloud, and\nthe edge. \u201cSierra Forrest,\u201d with E-cores, will launch later this quarter,\nwhile \u201cGranite Rapids\u201d and their performance cores will come soon after. Intel\nis boasting of a 2.4-times performance-per-watt jump and 2.7X better rack\ndensity over second-gen Xeons with Sierra Forest.\n\nAgain, Gelsinger positioned the CPUs as equally performant and more open\nalternatives to Nvidia\u2019s high-powered GPUs and CUDA framework. The new Xeons\nsupport the MXFP4 data format, which he described as a narrower format that\nallows for memory compression, efficient AI training, and AI inferencing \u201cwith\nminimal accuracy loss.\u201d With MXFP4, Granite Rapids will be able to reduce next\ntoken latency by up to 6.5 times better over 4^th Gen Xeons that use FP16,\nallowing it to run a 70-billion parameter Llama-2 model.\n\n### Love The Architecture You\u2019re With\n\nThis will be attractive for enterprises that have long used Xeons in their\ndatacenters, are comfortable with the Intel Architecture, and tend to\ngravitate to open environments.\n\n\u201cYour data, your data stack, and almost every one of your data stacks runs on\nXeon,\u201d he said. \u201cIt\u2019s where the databases run on and, amazingly, here we are\nin year 22 or so of the cloud journey and the majority \u2013 over 60 percent \u2013 of\nthe computing is done in the cloud or some cloud embodiment, but the vast\nmajority of data is still on-prem. Sixty-six percent of the data is unused and\n90 percent of unstructured data unused.\u201d\n\nEnterprises can address this unused and on-prem data for AI by adopting the\nRetrieval-Augmented Generation (RAG) technique to include corporate\ninformation in the data being used to train large language model.\n\n\u201cXeon is obviously a tremendous machine to run these RAG environments and\nmaking LLMs more effective and efficient on your data,\u201d Gelsinger said. \u201cXeon\nis not only able to be your database frontend, but increasingly it\u2019s able to\nrun the LLMs as well. No new management, no new networking, no new security\nmodels, no new IT things to learn, no proprietary networking. It just works on\nthe industry-standard Xeons you know and love.\u201d\n\nOpen architectures and hardware and software systems will be the key,\naccording to the CEO.\n\n\u201cWe\u2019ve clearly seen there\u2019s interest in how we can move these standards\nforward \u2013 standard management, no networking lock-in, no security issues,\nworking with LLMs that run natively on Xeon but also with Gaudi 2 and 3 and\nNvidia,\u201d he said. \u201cNow\u2019s the time to build open platforms for enterprise AI.\nWe\u2019re working to address key requirements that include how you efficiently\ndeploy using existing infrastructure, seamless integrated with hardened\nenterprise software stacks that you have today, a high degree of reliability,\navailability, security, support issues.\u201d\n\n#### Sign up to our Newsletter\n\nFeaturing highlights, analysis, and stories from the week directly from us to\nyour inbox with nothing in between. Subscribe now\n\n### Related Articles\n\nHPC\n\n### Argonne Aurora A21: All\u2019s Well That Ends Better\n\nJune 27, 2023 Timothy Prickett Morgan HPC 19\n\nWhen it comes to a lot of high performance computing systems we have seen over\nthe decades, we are fond of saying that the hardware is the easy part. This is\nnot universally true, and it certainly has not been true for the \u201cAurora\u201d\nsupercomputer at Argonne National Laboratory, the ...\n\nConnect\n\n### Intel Networking: Not Just A Bag Of Parts\n\nOctober 15, 2020 Timothy Prickett Morgan Connect 0\n\nWhat is the hardest job at Intel, excepting whoever is in charge of the\ndevelopment of chip etching processes and the foundries that implement it? We\nthink it is running its disparate networking business. The competition is\ntough, and getting tougher with both incumbent and upstart players taking on\nIntel ...\n\nConnect\n\n### The Tech Tricks That Make PCI-Express 6.0 And Beyond Possible\n\nAugust 6, 2020 Timothy Prickett Morgan Connect 4\n\nMultiplying things by two and putting them on a roadmap is easy, even if it\ndoes take a lot of courage to do that. Actually making that 2X performance\nboost happen is, in a lot of cases in the technology trade, very hard work.\nSometimes, it is seemingly impossible. This ...\n\n#### Be the first to comment\n\n### Leave a Reply Cancel reply\n\nThis site uses Akismet to reduce spam. Learn how your comment data is\nprocessed.\n\n###### About\n\nThe Next Platform is published by Stackhouse Publishing Inc in partnership\nwith the UK\u2019s top technology publication, The Register.\n\nIt offers in-depth coverage of high-end computing at large enterprises,\nsupercomputing centers, hyperscale data centers, and public clouds. Read\nmore...\n\n###### Newsletter\n\nFeaturing highlights, analysis, and stories from the week directly from us to\nyour inbox with nothing in between. Subscribe now\n\nAll Content Copyright The Next Platform\n\n", "frontpage": false}
