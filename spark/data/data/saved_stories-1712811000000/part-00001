{"aid": "39996497", "title": "Responsible optimism for ML, which is still Programming's Asbestos (2023)", "url": "https://morepablo.com/2023/07/ml-is-still-programming-s-asbestos.html", "domain": "morepablo.com", "votes": 2, "user": "mooreds", "posted_at": "2024-04-10 22:48:45", "comments": 0, "source_title": "Responsible optimism for ML, which is still Programming's Asbestos", "source_text": "Responsible optimism for ML, which is still Programming's Asbestos\n\n# Responsible optimism for ML, which is still Programming's Asbestos\n\nWednesday, July 19, 2023 :: Tagged under: engineering essay. \u23f0 11 minutes.\n\n\ud83c\udfb5 The song for this post is Hardware Store, by Weird Al. Truly one of his\nbest. \ud83c\udfb5\n\nFour years ago, I wrote that ML/AI is programming's asbestos: amazing for the\napplications we're finding, but the long-term effects aren't known and I\nsuspect going to bring a lot more complications than peddlers are willing to\nadmit (this was hot on the heels of reading a horribly credulous and\ninaccurate book on it). In the last year, a LOT happened: we had Midjourney\nand DALL-E and generative images which, freaky hands aside, produced stunning\nwork (and they've largely fixed the hands). Now we've got ChatGPT and LLMs.\n\nI think the original article holds up, but I firmly believe these were game\nchangers, and as a technologist I'm pretty blown away. I didn't think I'd see\nresults like this until way later in my lifetime. I feel pretty confident\nwe'll find at least a few great uses for this.\n\nSo I'm optimistic, but I still get treated like a hater for bringing up what I\nthink are very reasonable observations about the limits. Here are some notes\non shortcomings: I'd consider basic respect for these a prerequisite for\n\"responsible optimism\" for the tech. We've got a lot of companies AI-washing\ntheir product offerings (a bit like when a Long Island Ice Tea company added\n\"Blockchain\" to their name, and their stock soared). As someone who actually,\nyou know, builds things and not just talks about them? Here are some\nweaknesses you've got to keep in mind before you start running your mouth on\nhow this is going to Change Everything Forever And Ever and if you don't,\nyou're not gonna make it.\n\nNote: I gathered most of these links and notes ~6 months ago, and I really\nwish I'd published this then, when people were REALLY losing their minds,\nbecause I would have seemed so much smarter \ud83d\ude1b. A lot of the wilder hype seems\nto have died down. Also, a lot of my outbound links point to Twitter threads,\nand Elon made the change that you have to be logged-in to see tweets, which\nabsolutely sucks, so, sorry about that, hope you have an account.\n\n## On Grander visions: LLM progress does not carry over to other AI domains\n\n\"AI\" means \"Artificial Intelligence,\" but the most recent things to get all\nthe attention (LLMs, or \"Large Language Models\") do not carry over into other\ndomains. So just because ChatGPT is really cool, this doesn't mean anything\nnew for, say, robotics, or playing chess.\n\nLLMs are just a button you press to say \"give me the next word.\" It happens to\nbe really, really good at it. But other fields demand other skills. Robotics\nis hideously hard because of challenges with interactions in the real world\nand sensors: it's very hard for a robot to know, for example, how to hold an\norange without squeezing it too hard. No amount of \"what's the next word in\nthe sequence\" is going to help the robot with that task.\n\nAnother great example is self-driving cars: they've been \"around the corner\"\nsince 2012. But they're still not widely deployed because that last 10% is\nreally, really hard to do safely, and LLMs or Stable Diffusion aren't going to\nhelp them with that.\n\nThis is Stockfish (a chess program) playing as White, and ChatGPT playing as\nBlack. If you don't understand chess, you should understand that at a certain\npoint, ChatGPT is making up the rules.\n\nSo celebrate the wins! But if someone is talking about The Future and how AI\nwill solve so much, bear in mind that each of these cases is quite limited,\nand don't cross-pollinate easily.\n\n## It still fails. A lot. And that's fundamental to what they are\n\nAI is often still dumb as rocks. Here's a military simulation that was\ndefeated with cardboard boxes. The strategy game Go was considered \"solved by\nAI\" because it was beating grandmasters, but it turns out you can do the\nequivalent of \"cardboard boxes\" and win in a way no human would ever allow.\nTranslation was considered \"solved\" but if you type random Latin-sounding\nwords in Google Translate it will make up new meanings for your gibberish. The\nfirst paragraph of this article has a lot of examples of how all this auto-\ngenerated crap is making all the most popular products way worse.\n\nAI is still pretty powered by humans. Not in the loop (e.g. the old \"AI\"\ncompanies like x.ai said they were AI-powered but run by a person); but these\nmodels are trained on $2/hour international labor (stories here and here).\nPeople think AIs will free us up to live fuller lives, but it turns out the\nfuture we built was one where humans do all the drudgery and the computers get\nto make all the art. Technology making humans do more drudgery and spend less\ntime doing life-fulfilling things is the historical precedent, by the way.\n\nEveryone knows this already and if you don't care already, you're never gonna\ncare, but: the results are as biased as the data, and locks us into those\nbiases harder. If you ask it to \"professionalize\" something, it makes it\nWhiter and Male-er.\n\nvia\n\nThe models, which were already incredibly opaque, are becoming moreso,\nespecially after OpenAI removed any semblance of \"Open\" from their mission\nwith the release of GPT4. This sucks, in part, because now you can't audit\ntheir work: there's evidence of contamination in the training and we just now\nhave to take on faith they built these things correctly and ethically (it's\nnot like someone would ship software with bugs, right?). We're increasingly\nrelying on proprietary, opaque models that lock in biases as Microsoft fires\ntheir entire AI Ethics team and Google had a scandal with firing their\nresearchers the year prior.\n\nThe same models, when running at different times, perform with wildly\ndifferent accuracies on the same task. This is a new finding! But it turns out\nif you asked GPT4 in March 2023 to identify prime numbers, it was correct ~98%\nof the time. If you asked it in June with the same questions it was right 2.4%\nof the time. This is likely a regression with updates (again, we ship bugs all\nthe time), but again: how good do you feel betting anything significant on an\nopaque product behind a company wall that can regress that quickly? How safe\ndo you feel betting your company on it?\n\n## Complications when incorporating into a product\n\nOkay, functionally, all those previous things, but: if you're a company and\nyou want to put AI in your product, what risks should you be aware of?\n\nFirst, there seems to be no way to secure these things particularly well from\ninjection attacks. (here, here). If you're giving user inputs to an LLM (which\nis what most people are doing, because it's cool and the part that definitely\nworks and feels like magic), you have to be very, very, very careful.\n\nCOGS. This article speculates it cost $100m to train GPT4. Even if they're\nexaggerating (IMO press about OpenAI isn't skeptical enough, other figures put\nit at $40m), it's still extremely expensive to do in-house and many people\ndon't have the core competencies yet. Also, good luck getting your hands on\nthose GPUs.\n\nOkay, so you use a third-party model. Congrats, a core feature of your product\nis now handled by someone else. It's okay to outsource parts of your business\nfor sure, but if you want to use AI for an actual product category, it's\nalways dangerous to outsource a core competency. Additionally, you're locked\ninto their pricing.\n\nWhich brings us back to COGS. If you have a button that costs entire dollars\nto click (vs., say, an ElasticSearch query that you could hit millions of\ntimes before it costs you pennies) you've suddenly made your software much\nmore expensive to run, especially if it's users who press that button. And how\nwell are VC-backed companies doing right now on cost controls? I'll die on\nthis hill but I think software organizations should care about what things\ncost.\n\nLastly, data privacy. If you're using a third-party, you're now submitting\nyour data and maybe your customer's data to someone you don't know, who's\nprobably going to train on it, may log sensitive data by accident, could get\nhacked (this is distinct from the data privacy of from injection attacks,\nmentioned above).\n\n\"Oh, but their contract says they won't retain or train on it!\" lol, okay, but\nlisten to boosters when an artist or a writer wants to opt-out of their work\nbeing used to train these models. The peddlers become entitled and selfish,\naccusing the artists of being afraid of or holding back progress. The contempt\nfor the desires of the artists are plain and naked. The companies are also\ndesperate and afraid of falling behind (because they have no moat) to someone\nless ethical. If you trust these companies with data privacy or data security\nafter the last two decades of tech in addition to the ways they talk about the\ndata they've already stolen, you're a chump.\n\n## Long term outlook (Ouroborous effect)\n\nThis one is harder to explain succinctly, but: the current AI results were\nfrom training on a human-made corpus, the greatest one to ever exist. We're\nnow putting AI output into that corpus, which does 2 things:\n\n  * Subsequent AI models will be trained on the output of previous models, so it'll be less \"human\" in all the ways human output is different (creativity, cadence, tone...). Additionally, it'll get harder to make improvements because we've poisoned the corpus with substandard training material. Quality will go down.\n\n  * We'll slow the advancement of culture. If a large portion of 2023's Internet is built using models trained on 2021's Internet, 2023 will smell more like 2021. And 2024 will smell even more like that, and so on, and so on.\n\nThis \"culture eating its tail\" has happened before with the Internet and\nGoogle Search: it used to be a map of a very human internet full of websites.\nBut over the course of decades, the Internet made choices to satisfy the map\nmakers. Now that human internet got replaced with mountains and mountains of\nSEO-laden crap meant to please the ad robots instead of people. It's\nimpossible to find decent consumer advice on the Internet anymore, pages\nbarely load with all the trackers we put on, and most of us are resorting to\nsilly hacks to reach human output again.\n\nDo you like innovation? Do you think there are treasures in the past that\ndidn't get enough attention? Well with LLMs, we don't have enough training\ndata to use anything that wasn't popular from the past, and it'll be that much\nharder to innovate on core abstractions in the future, a bit like the fact\nthat our terminal emulators are condemned to forever pretend they're a machine\ndesigned in the 70's. (via)\n\nThis will also solidify incumbants, and we're likely to lose a ton of\nknowledge that isn't easily trainable. It'll make it even harder to innovate\non certain layers of technology because of how much training data there is.\n\n## The weird religion\n\nvia\n\nFinally... please ignore the weird religion that's coming out of this. People\nare saying things like \"AGI [Artificial General Intelligence] will become\nsuperintelligent at a rate we can't keep up with and destroy us all! We must\nappease Roko's Basilisk! It's an existential threat to humanity!!\"\n\nLook... I loved Deus Ex. It was a really cool game. But we've been \"on the\nbrink\" of creating AGI several times and never have, and most people who rant\nabout this demonstrate within 5 minutes of talking to them that they've given\nvery little thought to the definitions of cognition and consciousness or how\nto differentiate the two. Running electricity through a box of NAND switches\nto run operations to generate another word from the last one isn't\nconsciousness. There's a lot of study behind this kind of thing, and while I\nknow tech people like to rub their temples while saying \"first principles\"\nover and over about fields of study they've thought about for a few hours,\nthey end up being idiots stuck on the second floor since they were too smart\nto read or understand prior scholarship (look at the crypto people\nspeedrunning why we have financial regulation).\n\nI call it \"the weird religion\" because it so strongly echoes \"our actions will\nsummon God into this world and he will smite us for our sins unless we repent\nnow.\" It's not even worth engaging with the arguments, but if you want a good\nexplainer for why \"the computer will make itself smarter\" is unlikely, besides\nthe clear limitations in the sections above and/or knowing what LLMs are, I\nlove Ted Chiang.\n\n## So, it's all shit?\n\nNot at all! Again, this stuff is awesome too.\n\nBut I worry. Self-driving got to this point and it was never fully deployed\nbecause roads and cars are well-regulated, we recognized cars are lethal, and\nwe waited for them to get a little more reliable before releasing them into\nthe world. It turns out that last very important step never materialized, and\nmy friends in industry say it'll be another decade at least, if it does at\nall. In the meantime, cars are as lethal as they ever were, but not more\n(though Tesla released something they call \"Full Self-Driving\" anyway, and\nhours later there was nine-car crash).\n\nIn the information game, there are no regulating bodies. This isn't recognized\nas potentially lethal. So we're going full-steam ahead and putting these\nthings everywhere. I think there are risks to this, materially and culturally.\nI'm excited for the tech, but if you're a grown-ass adult you can keep two\nthings in your head at the same time: optimism for what this allows but also a\nrealistic and grounded sense of where they're weak.\n\nAdditionally, one should exercise prudence before stuffing this tech into your\nproduct. There are decent use cases for it, but in the presence of injection\nattacks and leaking confidential data, on top of the rampant costs associated\nwith it and the inability of most companies to make their own models... I'd\nconsider it a risky bet. If you absolutely must, consider using a leaked big\nmodel or an open source one.\n\n\ud83d\udc69\ud83c\udfa8 Being \"rockstars\": when software was a talents/creatives industry \ud83c\udfad \u2192\n\n\u2190 Burning Man 2023\n\nThanks for the read! Disagreed? Violent agreement!? Feel free to join my\nmailing list, drop me a line at pablo@morepablo.com, or leave a comment below!\nI'd love to hear from you \ud83d\ude04\n\n", "frontpage": false}
