{"aid": "39988925", "title": "DS-Moe: Making Moe Models More Efficient and Less Memory-Intensive", "url": "https://huggingface.co/blog/bpan/ds-moe", "domain": "huggingface.co", "votes": 1, "user": "victormustar", "posted_at": "2024-04-10 10:03:36", "comments": 0, "source_title": "DS-MoE: Making MoE Models More Efficient and Less Memory-Intensive", "source_text": "DS-MoE: Making MoE Models More Efficient and Less Memory-Intensive\n\nHugging Face\n\nBack to Articles\n\n# DS-MoE: Making MoE Models More Efficient and Less Memory-Intensive\n\nCommunity Article Published April 9, 2024\n\nUpvote\n\n11\n\nbpan Bowen Pan\n\nEstimated reading time: 4 minutes\n\nMixture-of-Experts (MoE) language models are known for their ability to reduce\ncomputing needs by 2 to 4 times compared to traditional dense models, without\nsacrificing performance. This makes them especially useful in situations where\ncomputing resources are limited. However, MoE models typically need 2 to 4\ntimes more parameters to perform as well as a dense model. For example, models\nlike DeepSeekMoE-16B and Qwen1.5-MoE-A2.7B which has 16B parameters were\ndesigned to match the performance of a 7B model. The large number of\nparameters in MoE models incurs larger GPU memory requirements which makes\nthem less efficient in I/O-bounded scenarios like autoregressive generation.\n\nFigure 1: Decoding Throughput of Dense Models and the SMoE Models with Similar\nPerformance. We test the performance under the setup where only the input\nlength is 1 and output length is 512. This study reveals that traditional\nSparse Mixture of Experts (SMoE) models exhibit reduced output throughput in\nI/O-bounded situations despite their lower computational demands. The models\nare tested with HuggingFace Transformers.\n\nIs it necessary for MoE models to be so large to achieve high performance? Can\nwe create an MoE model that maintains performance but uses fewer parameters\nand less computational power? Enter DS-MoE. This model achieves similar\nperformance to dense models but uses about one-third of the computational\nresources and only half as many parameters as other MoE models.\n\nFigure 2: Number of Parameters for Performance-Matched Models. We plot the\nsize and computational profiles of the Dense-3B, SMoE-5B, and DS-MoE-3B models\ntrained with 100B tokens, each achieving a comparable averaged task\nperformance. DS-MoE demonstrates both computational efficiency and parameter\nefficiency, where the computational cost is quantified by counting the number\nof active parameters engaged during inference.\n\nThe concept of DS-MoE involves densely training the experts and forcing the\nmodel's routers to gradually ignore unnecessary experts for a given token. We\nemploy the Mutual Information (MI) loss to the training process, which\nbalances the load of each expert across the entire batch, but also encourages\neach input token to concentrate their gating probability to fewer experts.\n\nFigure 3: Subfigure (a) illustrates the conventional sparse training method in\nMoE models, characterized by sparse gradient propagation in both the router\nand the experts. Subfigure (b) details the dense training strategy in DS-MoE,\nwhich involves dense propagation of gradients for both routers and experts.\n\nThe MI loss is defined as LMI=\u2212H(e)+\u2223X\u22231x\u2208X\u2211H(e\u2223x),H(e)=\u2212i=1\u2211Np(e)log(p(e)),\nwhere X denotes the tokens in a minibatch, and e denotes the experts.\nIntuitively, maximizing H(e) balances the load of each expert across the\nentire batch, and minimizing H(e|x) encourages each input x to concentrate\ntheir gating probability to fewer experts.\n\nDuring inference, DS-MoE chooses only the top K experts based on their scores.\nThe determination of the number of K is based on either a predefined value or\nan adaptive method, contingent upon the count of experts with scores\nsurpassing a certain threshold. As a result, DS-MoE can perform as well as\nsimilarly sized dense models while using far fewer active parameters, as\ndemonstrated in the table.\n\nModel| HellaSwag| PIQA| WinoGrande| SciQ| Arc-e| Arc-c| Avg. Perf.| Active\nParams  \n---|---|---|---|---|---|---|---|---  \nDense-3B| 40.4| 71.4| 58.7| 86.0| 59.6| 26.1| 57.0| 2705M  \nSMoE-5B| 40.1| 70.7| 56.5| 85.6| 58.4| 24.8| 56.0| 1212M  \nDS-MoE-3B| 39.3| 71.6| 57.9| 85.6| 57.7| 24.9| 56.2| 934M  \nDense-6B| 44.3| 72.2| 59.9| 88.0| 62.9| 27.9| 59.2| 6186M  \nDS-MoE-6B| 43.5| 73.0| 57.9| 86.9| 61.9| 27.9| 58.5| 1813M  \n  \nWe also tested DS-MoE with vLLM to see how it compares to other models in\nterms of processing speed and memory usage at the 7B performance tier. We\nlooked at how many requests and tokens it could handle per second, using a\nsetup where each input and output consisted of 1,000 tokens and the GPU memory\nusage was capped at 90%.\n\nModel| Total Params| Active Params| Model Memory| A100 Throughput| A100 TPS|\nH100 Throughput| H100 TPS  \n---|---|---|---|---|---|---|---  \nDense-6B| 6.4B| 6.4B| 12.3 GiB| 1.04| 2079.8| 1.40| 2808.7  \nMistral-7B| 7.2B| 7.2B| 13.5 GiB| 1.07| 2140.8| 1.52| 3047.4  \nDeepSeekMoE| 17.3B| 2.8B| 30.5 GiB| 1.17| 2330.1| 1.57| 3144.1  \nQwen1.5-MoE| 16.4B| 2.7B| 26.7 GiB| 1.33| 2665.7| 1.81| 3616.9  \nDS-MoE-6B| 6.5B| 2.2B| 12.6 GiB| 2.00| 3992.8| 2.30| 4603.9  \n  \nThe test shows that DS-MoE outperforms both dense models in terms of\ncomputational cost and sparsely trained MoEs in model memory, leading to\nfaster processing in the computation-bounded scenarios as well the I/O bounded\nscenarios. Note that DS-MoE-6B is not yet comparable with other models\nregarding downstream performance, due to its training on merely 100 billion\ntokens (versus the trillions for other models). Nevertheless, DS-MoE has\ndemonstrated significant promise in achieving the performance levels of dense\nmodels with a comparable volume of training data.\n\nRead More in the Paper\n\nUpvote\n\n11\n\n", "frontpage": false}
