{"aid": "39995946", "title": "JetStream \u2013 high throughput inference on TPUs", "url": "https://cloud.google.com/blog/products/compute/accelerating-ai-inference-with-google-cloud-tpus-and-gpus", "domain": "cloud.google.com", "votes": 8, "user": "kiratp", "posted_at": "2024-04-10 21:31:55", "comments": 0, "source_title": "Accelerating AI Inference with Google Cloud TPUs and GPUs | Google Cloud Blog", "source_text": "Accelerating AI Inference with Google Cloud TPUs and GPUs | Google Cloud Blog\n\ncloud.google.com uses cookies from Google to deliver and enhance the quality\nof its services and to analyze traffic. Learn more.\n\nJump to Content\n\nCloud\n\nBlog\n\nContact sales Get started for free\n\nCloud\n\nBlog\n\nSolutions & technology\n\nSecurity\n\nEcosystem\n\nIndustries\n\n  * Solutions & technology\n  * Ecosystem\n  * Developers & Practitioners\n  * Transform with Google Cloud\n\n  * AI & Machine Learning\n  * API Management\n  * Application Development\n  * Application Modernization\n  * Chrome Enterprise\n  * Compute\n  * Containers & Kubernetes\n  * Data Analytics\n  * Databases\n  * DevOps & SRE\n  * Maps & Geospatial\n  * Security\n  * Infrastructure\n  * Infrastructure Modernization\n  * Networking\n  * Productivity & Collaboration\n  * SAP on Google Cloud\n  * Storage & Data Transfer\n  * Sustainability\n\n  * Security & Identity\n  * Threat Intelligence\n\n  * IT Leaders\n  * Industries\n  * Partners\n  * Startups & SMB\n  * Training & Certifications\n  * Inside Google Cloud\n  * Google Cloud Next & Events\n  * Google Maps Platform\n  * Google Workspace\n\n  * Financial Services\n  * Healthcare & Life Sciences\n  * Manufacturing\n  * Media & Entertainment\n  * Public Sector\n  * Retail\n  * Supply Chain\n  * Telecommunications\n\nContact sales Get started for free\n\nCompute\n\n#\n\nAccelerate AI Inference with Google Cloud TPUs and GPUs\n\nApril 11, 2024\n\n##### Alex Spiridonov\n\nGroup Product Manager\n\n##### Google Cloud Next is live!\n\nSee the latest announcements from Next '24.\n\nJoin us\n\nIn the rapidly evolving landscape of artificial intelligence, the demand for\nhigh-performance, cost-efficient AI inference (serving) has never been\ngreater. This week we announced two new open source software offerings:\nJetStream and MaxDiffusion.\n\nJetStream is a new inference engine for XLA devices, starting with Cloud TPUs.\nJetStream is specifically designed for large language models (LLMs) and\nrepresents a significant leap forward in both performance and cost efficiency,\noffering up to 3x more inferences per dollar for LLMs than previous Cloud TPU\ninference engines. JetStream supports PyTorch models through PyTorch/XLA, and\nJAX models through MaxText \u2013 our highly scalable, high-performance reference\nimplementation for LLMs that customers can fork to accelerate their\ndevelopment.\n\nMaxDiffusion is the analog of MaxText for latent diffusion models, and makes\nit easier to train and serve diffusion models optimized for high performance\non XLA devices, starting with Cloud TPUs.\n\nIn addition, we are proud to share the latest performance results from\nMLPerfTM Inference v4.0, showcasing the power and versatility of Google\nCloud\u2019s A3 virtual machines (VMs) powered by NVIDIA H100 GPUs.\n\n### JetStream: High-performance, cost-efficient LLM inference\n\nLLMs are at the forefront of the AI revolution, powering a wide range of\napplications such as natural language understanding, text generation, and\nlanguage translation. To reduce our customers\u2019 LLM inference costs, we built\nJetStream: an inference engine that provides up to 3x more inferences per\ndollar than previous Cloud TPU inference engines.\n\nFigure 1: The JetStream stack.\n\nJetStream includes advanced performance optimizations such as continuous\nbatching, sliding window attention, and int8 quantization for weights,\nactivations, and key-value (KV) cache. And whether you're working with JAX or\nPyTorch, JetStream supports your preferred framework. To further streamline\nyour LLM inference workflows, we provide MaxText and PyTorch/XLA\nimplementations of popular open models such as Gemma and Llama, optimized for\npeak cost-efficiency and performance.\n\nOn Cloud TPU v5e-8, JetStream delivers up to 4783 tokens/second for open\nmodels including Gemma in MaxText and Llama 2 in PyTorch/XLA:\n\nFigure 2: JetStream throughput (output tokens / second). Google internal data.\nMeasured using Gemma 7B (MaxText), Llama 2 7B (PyTorch/XLA), and Llama 2 13B\n(PyTorch/XLA) on Cloud TPU v5e-8. Maximum input length: 1024, maximum output\nlength: 1024. Continuous batching, int8 quantization for weights, activations,\nKV cache. PyTorch/XLA uses sliding window attention. As of April, 2024.\n\nJetStream\u2019s high performance and efficiency mean lower inference costs for\nGoogle Cloud customers, making LLM inference more accessible and affordable:\n\nFigure 3: JetStream cost to generate 1 million output tokens. Google internal\ndata. Measured using Gemma 7B (MaxText), Llama 2 7B (PyTorch/XLA), and Llama 2\n13B (PyTorch/XLA) on Cloud TPU v5e-8. Maximum input length: 1024, maximum\noutput length: 1024. Continuous batching, int8 quantization for weights,\nactivations, KV cache. PyTorch/XLA uses sliding window attention. JetStream\n($0.30 per 1M tokens) achieves up to 3x more inferences per dollar on Gemma 7B\ncompared to the previous Cloud TPU LLM inference stack ($1.10 per 1M tokens).\nCost is based on the 3Y CUD price for Cloud TPU v5e-8 in the US. As of April,\n2024.\n\nCustomers such as Osmos are using JetStream to accelerate their LLM inference\nworkloads:\n\n\u201cAt Osmos, we've developed an AI-powered data transformation engine to help\ncompanies scale their business relationships through the automation of data\nprocessing. The incoming data from customers and business partners is often\nmessy and non-standard, and needs intelligence applied to every row of data to\nmap, validate, and transform it into good, usable data. To achieve this we\nneed high-performance, scalable, cost-efficient AI infrastructure for\ntraining, fine-tuning, and inference. That\u2019s why we chose Cloud TPU v5e with\nMaxText, JAX, and JetStream for our end-to-end AI workflows. With Google\nCloud, we were able to quickly and easily fine-tune Google\u2019s latest Gemma open\nmodel on billions of tokens using MaxText and deploy it for inference using\nJetStream, all on Cloud TPU v5e. Google\u2019s optimized AI hardware and software\nstack enabled us to achieve results within hours, not days.\u201d \u2013 Kirat Pandya,\nCEO, Osmos\n\nBy providing researchers and developers with a powerful, cost-efficient, open-\nsource foundation for LLM inference, we're powering the next generation of AI\napplications. Whether you're a seasoned AI practitioner or just getting\nstarted with LLMs, JetStream is here to accelerate your journey and unlock new\npossibilities in natural language processing.\n\nExperience the future of LLM inference with JetStream today. Visit our GitHub\nrepository to learn more about JetStream and get started on your next LLM\nproject. We are committed to developing and supporting JetStream over the long\nterm on GitHub and through Google Cloud Customer Care. We are inviting the\ncommunity to build with us and contribute improvements to further advance the\nstate of the art.\n\n### MaxDiffusion: High-performance diffusion model inference\n\nJust as LLMs have revolutionized natural language processing, diffusion models\nare transforming the field of computer vision. To reduce our customers\u2019 costs\nof deploying these models, we created MaxDiffusion: a collection of open-\nsource diffusion-model reference implementations. These implementations are\nwritten in JAX and are highly performant, scalable, and customizable \u2013 think\nMaxText for computer vision.\n\nMaxDiffusion provides high-performance implementations of core components of\ndiffusion models such as cross attention, convolutions, and high-throughput\nimage data loading. MaxDiffusion is designed to be highly adaptable and\ncustomizable: whether you're a researcher pushing the boundaries of image\ngeneration or a developer seeking to integrate cutting-edge gen AI\ncapabilities into your applications, MaxDiffusion provides the foundation you\nneed to succeed.\n\nThe MaxDiffusion implementation of the new SDXL-Lightning model achieves 6\nimages/s on Cloud TPU v5e-4, and throughput scales linearly to 12 images/s on\nCloud TPU v5e-8, taking full advantage of the high performance and scalability\nof Cloud TPUs\n\nFigure 4: MaxDiffusion throughput (images per second). Google internal data.\nMeasured using the SDXL-Lightning model on Cloud TPU v5e-4 and Cloud TPU\nv5e-8. Resolution: 1024x1024, batch size per device: 2, decode steps: 4. As of\nApril, 2024.\n\nAnd like MaxText and JetStream, MaxDiffusion is cost-efficient: generating\n1000 images on Cloud TPU v5e-4 or Cloud TPU v5e-8 costs just $0.10.\n\nFigure 5: MaxDiffusion cost to generate 1000 images. Google internal data.\nMeasured using the SDXL-Lightning model on Cloud TPU v5e-4 and Cloud TPU\nv5e-8. Resolution: 1024x1024, batch size per device: 2, decode steps: 4. Cost\nis based on the 3Y CUD prices for Cloud TPU v5e-4 and Cloud TPU v5e-8 in the\nUS. As of April, 2024.\n\nCustomers such as Codeway are using Google Cloud to maximize cost-efficiency\nfor diffusion model inference at scale:\n\n\u201cAt Codeway, we create chart-topping apps and games used by more than 115\nmillion people in 160 countries around the world. \"Wonder,\" for example, is an\nAI-powered app that turns words into digital artworks, while \"Facedance\" makes\nfaces dance with a range of fun animations. Putting AI in the hands of\nmillions of users requires a highly scalable and cost-efficient inference\ninfrastructure. With Cloud TPU v5e, we achieved 45% faster serving time for\nserving diffusion models compared to other inference solutions, and can serve\n3.6 times more requests per hour. At our scale, this translates into\nsignificant infrastructure cost savings, and makes it possible for us to bring\nAI-powered applications to even more users in a cost-efficient manner.\u201d \u2013 U\u011fur\nArpac\u0131, Head of DevOps, Codeway\n\nMaxDiffusion provides a high-performance, scalable, flexible foundation for\nimage generation. Whether you're a seasoned computer vision expert or just\ndipping your toes into the world of image generation, MaxDiffusion is here to\nsupport you on your journey.\n\nVisit our GitHub repository to learn more about MaxDiffusion and start\nbuilding your next creative project today.\n\n### A3 VMs: Strong results in MLPerfTM 4.0 Inference\n\nIn August 2023 we announced the general availability of A3 VMs. Powered by 8\nNVIDIA H100 Tensor Core GPUs in a single VM, A3s are purpose-built to train\nand serve demanding gen AI workloads and LLMs. A3 Mega, powered by NVIDIA H100\nGPUs, will be generally available next month and offers double the GPU-to-GPU\nnetworking bandwidth of A3.\n\nFor the MLPerfTM Inference v4.0 benchmark testing, Google submitted 20 results\nacross seven models, including the new Stable Diffusion XL and Llama 2 (70B)\nbenchmarks, using A3 VMs:\n\n  * RetinaNet (Server and Offline)\n\n  * 3D U-Net: 99% and 99.9% accuracy (Offline)\n\n  * BERT: 99 and 99% accuracy (Server and Offline)\n\n  * DLRM v2: 99.9% accuracy (Server and Offline)\n\n  * GPT-J: 99% and 99% accuracy (Server and Offline)\n\n  * Stable Diffusion XL (Server and Offline)\n\n  * Llama 2: 99% and 99% accuracy (Server and Offline)\n\nAll results were within 0-5% of the peak performance demonstrated by NVIDIA\u2019s\nsubmissions. These results are a testament to Google Cloud\u2019s close partnership\nwith NVIDIA to build workload-optimized end-to-end solutions specifically for\nLLMs and gen AI.\n\n### Powering the future of AI with Google Cloud TPUs and NVIDIA GPUs\n\nGoogle's innovation in AI inference, powered by hardware advancements in\nGoogle Cloud TPUs and NVIDIA GPUs, plus software innovations such as\nJetStream, MaxText, and MaxDiffusion, empower our customers to build and scale\nAI applications. With JetStream, developers can achieve new levels of\nperformance and cost efficiency in LLM inference, unlocking new opportunities\nfor natural language processing applications. MaxDiffusion provides a\nfoundation that empowers researchers and developers to explore the full\npotential of diffusion models to accelerate image generation. Our robust\nMLPerfTM 4.0 inference results on A3 VMs powered by NVIDIA H100 Tensor Core\nGPUs showcase the power and versatility of Cloud GPUs.\n\nVisit our website to learn more and get started with Google Cloud TPU and GPU\ninference today.\n\nPosted in\n\n  * Compute\n  * AI & Machine Learning\n  * Google Cloud Next\n\n##### Related articles\n\nAI & Machine Learning\n\n### Introducing ML Productivity Goodput: a metric to measure AI system\nefficiency\n\nBy Vaibhav Singh \u2022 8-minute read\n\nCompute\n\n### What\u2019s new with Google Cloud\u2019s AI Hypercomputer architecture\n\nBy Mark Lohmeyer \u2022 12-minute read\n\nCompute\n\n### Introducing Google Axion Processors, our new Arm-based CPUs\n\nBy Amin Vahdat \u2022 7-minute read\n\nCompute\n\n### What\u2019s new in Google Cloud\u2019s workload-optimized infrastructure\n\nBy Salil Suri \u2022 11-minute read\n\n### Footer Links\n\n#### Follow us\n\n  * Google Cloud\n  * Google Cloud Products\n  * Privacy\n  * Terms\n  * Cookies management controls\n\n  * Help\n\n", "frontpage": false}
