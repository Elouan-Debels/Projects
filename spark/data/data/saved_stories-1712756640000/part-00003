{"aid": "39988047", "title": "Duckplyr: Dplyr Powered by DuckDB", "url": "https://duckdb.org/2024/04/02/duckplyr.html", "domain": "duckdb.org", "votes": 1, "user": "hackandthink", "posted_at": "2024-04-10 07:42:40", "comments": 0, "source_title": "duckplyr: dplyr powered by DuckDB", "source_text": "duckplyr: dplyr powered by DuckDB - DuckDB\n\nSupport\n\n# duckplyr: dplyr powered by DuckDB\n\nHannes M\u00fchleisen2024-04-02\n\nTL;DR: The new R package duckplyr translates the dplyr API to DuckDB\u2019s\nexecution engine.\n\n## Background\n\nWrangling tabular data into a form suitable for analysis can be a challenging\ntask. Somehow, every data set is created differently. Differences between\ndatasets exist in their logical organization of information into rows and\ncolumns or in more specific choices like the representation of dates,\ncurrency, categorical values, missing data and so on. The task is not\nsimplified by the lack of global consensus on trivial issues like which\ncharacter to use as a decimal separator. To gain new insights, we also\ncommonly need to combine information from multiple sources, for example by\njoining two data sets using a common identifier. There are some common\nrecurring operations however, that have been found to be universally useful in\nreshaping data for analysis. For example, the Structured (English) Query\nLanguage, or SQL (\u201cSee-Quel\u201d) for short describes a set of common operations\nthat can be applied to tabular data like selection, projections, joins,\naggregation, sorting, windowing, and more. SQL proved to be a huge success,\ndespite its many warts and many attempts to replace it, it is still the de-\nfacto language for data transformation with a gigantic industry behind it.\n\n    \n    \n    library(\"DBI\") con <- dbConnect(...) df <- dbGetQuery(con, \"SELECT something, very, complicated FROM some_table JOIN another_table BY (some_shared_attribute) GROUP BY group_one, group_two ORDER BY some_column, and_another_column;\")\n\nA not very ergonomic way of pulling data into R\n\nFor data analysts in interactive programming environments like R or Python\npossibly from within IDEs such as RStudio or Jupyter Notebooks, using SQL to\nreshape data was never really a natural choice. Sure, sometimes it was\nrequired to use SQL to pull data from operational systems as shown above, but\nwhen given a choice, analysts much preferred to use the more ergonomic data\nreshaping facilities provided by those languages. R had built-in data\nwrangling from the start as part of the language with the data.frame class to\nrepresent tabular data. Later on, in 2014, Hadley Wickham defined the logical\nstructure of tabular data for so-called \u201ctidy\u201d data and published the first\nversion of the dplyr package designed to unify and simplify the previously\nunwieldy R commands to reshape data into a singular, unified and consistent\nAPI. In Python-land, the widely popular Pandas project extended Python with a\nde-facto tabular data representation along with relational-style operators\nalbeit without any attempt at \u201ctidiness\u201d.\n\nAt some point however, the R and Python data processing facilities started to\ncreak under the ever-increasing weight of datasets that people wished to\nanalyze. Datasets quickly grew into millions of rows. For example, one of the\nearly datasets that required special handling was the American Community\nSurvey dataset, because there are just so many Americans. But tools like\nPandas and dplyr had been designed for convenience, not necessarily\nefficiency. For example, they lack the ability to parallelize data reshaping\njobs on the now-common multicore processors.\n\nAnd while there was a whole set of emerging \u201cBig Data\u201d tools, using those from\nan interactive data analysis environment proved to be a poor developer\nexperience, for example due to multi-second job startup times and very complex\nsetup procedures far beyond the skill set of most data analysts. However, the\nworld of relational data management systems had not stood still in the\nmeantime. Great progress had been made to improve the efficiency of analytical\ndata analysis from SQL: Innovations around columnar data representation,\nefficient query interpretation or even compilation, and automatic efficient\nparallelization increased query processing efficiency by several orders of\nmagnitude. Regrettably, those innovations did not find their way into the data\nanalysts toolkit \u2013 even as decades passed \u2013 due to lack of communication\nbetween communities and siloing of innovations into corporate, commercial, and\nclose-source products.\n\nThere are two possible ways out of this unfortunate scenario:\n\n  1. improve the data analysis capabilities of R and Python to be able to handle larger datasets through general efficiency improvements, optimization, and parallelization;\n  2. somehow integrate existing state-of-the-art technology into interactive data analysis environments.\n\nThe main issue with approach one is that building a competitive analytical\nquery engine from scratch is a multi-million dollar effort requiring a team of\nhighly specialized experts on query engine construction. There are many moving\nhighly complex parts that all have to play together nicely. There are\nseemingly-obvious questions in query engines that one can get a PhD in data\nmanagement systems for a solution. Recouping such a massive investment in a\nspace where it is common that tools are built by volunteers in their spare\ntime and released for free is challenging. That being said, there are a few\ncommendable projects in this space like data.table or more recently pola.rs\nthat offer greatly improved performance over older tools.\n\nApproach two is also not without its challenges: State of the art query engine\ntechnology is often hidden behind incompatible architectures. For example, the\ntwo-tier architecture where a data management system runs on a dedicated\ndatabase server and client applications use a client protocol to interact with\nsaid server is rather incompatible with interactive analysis. Setting up and\nmaintaining a separate database \u201cserver\u201d \u2013 even on the same computer \u2013 is\nstill painful. Moving data back and forth between the analysis environment and\nthe database server has been shown to be quite expensive. Unfortunately, those\narchitectural decisions deeply influence the query engine trade-offs and are\ntherefore difficult to change afterwards.\n\nThere has been movement in this space however: One of the stated goals of\nDuckDB is to unshackle state-of-the-art analytical data management technology\nfrom system architecture with its in-process architecture. Simply put, this\nmeans there is no separate database server and DuckDB instead runs within a\n\u201chost\u201d process. This host can be any application that requires data management\ncapabilities or just an interactive data analysis environment like Python or\nR. Running within the host environment has another massive advantage: Moving\ndata back and forth between the host and DuckDB is very cheap. For R and\nPython, DuckDB can directly run complex queries on data frames within the\nanalysis environment without any import or conversion steps. Conversely,\nDuckDB\u2019s query results can directly be converted to data frames, greatly\nreducing the overhead of integrating with downstream libraries for plotting,\nfurther analysis or Machine Learning. DuckDB is able to efficiently execute\narbitrarily complex relational queries including recursive and correlated\nqueries. DuckDB is able to handle larger-than-memory datasets both in reading\nand writing but also when dealing with large intermediate results, for example\nresulting from aggregations with millions of groups. DuckDB has a\nsophisticated full query optimizer that removes the previously common manual\noptimization steps. DuckDB also offers persistence, tabular data being stored\nin files on disk. The tables in those files can be changed, too \u2013 while\nkeeping transactional integrity. Those are unheard-of features in interactive\ndata analysis, they are the result of decades of research and engineering in\nanalytical data systems.\n\nOne issue remains however, DuckDB speaks SQL. While SQL is a popular language,\nnot all analysts want to express their data transformations in SQL. One of the\nmain issues here is that typically, queries are expressed as strings in R or\nPython scripts, which are sent to a database system in an opaque way. This\nmeans that those queries carry all-or-nothing semantics and it can be\nchallenging to debug problems (\u201cYou have an error in your SQL syntax; check\nthe manual...\u201d). APIs like dplyr are often more convenient for the user, they\nallow an IDE to support things like auto-completion on functions, variable\nnames etc. In addition, the additive nature of the dplyr API allows to build a\nsequence of data transformation in small steps, which reduces the cognitive\nload of the analyst considerably compared to writing a hundred-line SQL query.\nThere have been some early experimental attempts to overload R\u2019s native data\nframe API in order to map to SQL databases, but those approaches have been\nfound to be too limited in generality, surprising to users and generally too\nbrittle. A better approach is needed.\n\n## The duckplyr R package\n\nTo address those issues, we have partnered up with the dplyr project team at\nPosit (formerly RStudio) and cynkra to develop duckplyr. duckplyr is a drop-in\nreplacement for dplyr, powered by DuckDB for performance. Duckplyr implements\nseveral innovations in the interactive analysis space. First of all,\ninstalling duckplyr is just as easy as installing dplyr. DuckDB has been\npackaged for R as a stand-alone R package that contains the entire data\nmanagement system code as well as wrappers for R. Both the DuckDB R package as\nwell as duckplyr are available on CRAN, making installation on all major\nplatforms a straightforward:\n\n    \n    \n    install.packages(\"duckplyr\")\n\n### Verbs\n\nUnder the hood, duckplyr translates the sort-of-relational dplyr operations\n(\u201cverbs\u201d) to DuckDB\u2019s relational query processing engine. Apart from some\nnaming confusion, there is a mostly straightforward mapping between dplyr\u2019s\nverbs such as select, filter, summarise. etc. and DuckDB\u2019s project, filter and\naggregate operators. A crucial difference from previous approaches is that\nduckplyr does not go through DuckDB\u2019s SQL interface to create query plans.\nInstead, duckplyr uses DuckDB\u2019s so-called \u201crelational\u201d API to directly\nconstruct logical query plans. This API allows to bypass the SQL parser\nentirely, greatly reducing the difficulty in operator, identifier, constant,\nand table name escaping that plagues other approaches such as dbplyr.\n\nWe have exposed the C++-Level relational API to R, so that it is possible to\ndirectly construct DuckDB query plans from R. This low-level API is not meant\nto be used directly, but it is used by duckplyr to transform the dplyr verbs\nto the DuckDB relational API and thus to query plans. Here is an example:\n\n    \n    \n    library(\"duckplyr\") as_duckplyr_df(data.frame(n=1:10)) |> mutate(m=n+1) |> filter (m > 5) |> count() |> explain()\n    \n    \n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 PROJECTION \u2502 \u2502 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2502 \u2502 n \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 UNGROUPED_AGGREGATE \u2502 \u2502 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2502 \u2502 count_star() \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 FILTER \u2502 \u2502 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2502 \u2502(+(CAST(n AS DOUBLE), 1.0) \u2502 \u2502 > 5.0) \u2502 \u2502 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2502 \u2502 EC: 10 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 R_DATAFRAME_SCAN \u2502 \u2502 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2502 \u2502 data.frame \u2502 \u2502 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2502 \u2502 n \u2502 \u2502 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2502 \u2502 EC: 10 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nWe can see how a sequence of dplyr verbs mutate, filter, and count is\n\u201cmagically\u201d transformed into a DuckDB query plan consisting of a scan, a\nfilter, projections and an aggregate. We can see at the very bottom an\nR_DATAFRAME_SCAN operator is added. This operator directly reads an R data\nframe as if it were a table in DuckDB, without requiring actual data import.\nThe new verb explain() causes DuckDB\u2019s logical query plan to be printed so\nthat we can expect what DuckDB intends to execute based on the duckplyr\nsequence of verbs.\n\n### Expressions\n\nAn often overlooked yet crucial component of data transformations are so-\ncalled expressions. Expressions are (conceptually) scalar transformations of\nconstants and columns from the data that can be used to for example produce\nderived columns or to transform actual column values to boolean values to be\nused in filters. For example, one might write an expression like (amount -\ndiscount) * tax to compute the actual invoiced amount without that amount\nactually being stored in a column or use an expression like value > 42 in a\nfilter expression to remove all rows where the value is less than or equal to\n42. Dplyr relies on the base R engine to evaluate expressions with some minor\nmodifications to resolve variable names to columns in the input data. When\nmoving evaluation of expressions over to DuckDB, the process becomes a little\nbit more involved. DuckDB has its own and independent expression system\nconsisting of a built-in set of functions (e.g. min), scalar values and types.\nTo transform R expressions into DuckDB expressions, we use an interesting R\nfeature to capture un-evaluated abstract syntax trees from function arguments.\nBy traversing the tree, we can transform R scalar values into DuckDB scalar\nvalues, R function calls into DuckDB function calls, and R-level variable\nreferences into DuckDB column references. It should be clear that this\ntransformation cannot be perfect: There are functions in R that DuckDB simply\ndoes not support, for example those coming from the myriad of contributed\npackages. While we are working on expanding the set of supported expressions,\nthere will always be some that cannot be translated. However, in the case of\nnon-translatable expressions, we would still be able to return a result to the\nuser. To achieve this, we have implemented a transparent fall-back mechanism\nthat uses the existing R-level expression evaluation method in the case that\nan expression cannot be translated to DuckDB\u2019s expression language. For\nexample, the following transformation m = n + 1 can be translated:\n\n    \n    \n    as_duckplyr_df(data.frame(n=1:10)) |> mutate(m=n+1) |> explain()\n    \n    \n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 PROJECTION \u2502 \u2502 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2502 \u2502 n \u2502 \u2502 m \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 R_DATAFRAME_SCAN \u2502 \u2502 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2502 \u2502 data.frame \u2502 \u2502 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2502 \u2502 n \u2502 \u2502 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2502 \u2502 EC: 10 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nWhile the following transformation using a inline lambda function cannot\n(yet):\n\n    \n    \n    as_duckplyr_df(data.frame(n=1:10)) |> mutate(m=(\\(x) x+1)(n)) |> explain()\n    \n    \n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 R_DATAFRAME_SCAN \u2502 \u2502 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2502 \u2502 data.frame \u2502 \u2502 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2502 \u2502 n \u2502 \u2502 m \u2502 \u2502 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2500 \u2502 \u2502 EC: 10 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\nIt is a little hard to see (and we are working on improving this), the\nexplain() output clearly differs between the two mutate expressions. In the\nfirst case, DuckDB computes the + 1 as part of the projection operator, in the\nsecond case, the translation failed and a fallback was used, leading to the\ncomputation happening in the R engine. The upside of automatic fallback is\nthat things \u201cjust work\u201d. The downside is that there will usually be a\nperformance hit from the fallback due to \u2013 for example \u2013 the lack of automatic\nparallelization. We are planning to add a debug mode where users can inspect\nthe translation process and get insight into why translations fail.\n\n### Eager vs. lazy materialization\n\nDplyr and Pandas follow an execution strategy known as \u201ceager\nmaterialization\u201d. Every time an operation is invoked on a data frame, this\noperation is immediately executed and the result created in memory. This can\nbe problematic. Consider the following example, a ten million row dataset is\nmodified by adding 1 to a column. Then, the top_n operation is invoked to\nretrieve the first ten rows only. Because of eager materialization, the\naddition operation is executed on ten million rows, the result is created in\nmemory, only for almost all of it to be thrown away immediately because only\nthe first ten rows were requested. Duckplyr solves this problem by using a so-\ncalled \u201clazy materialization\u201d strategy where no action is performed initially\nbut instead the users\u2019 intent is being captured. This means that the addition\nof one to ten million rows will not be performed immediately. The system is\ninstead able to optimize the requested computation and will only perform the\naddition on the first few rows. Also importantly, the intermediate result of\nthe addition is never actually created in memory, greatly reducing the memory\npressure.\n\nHowever, lazy computation presents a possible integration issue: The result of\nlazy computation has to be some sort of lazy computation placeholder object,\nthat can be passed to another lazy operation or forced to be evaluated, e.g.\nvia a special print method. However, this would break backwards compatibility\nwith dplyr, where the result of each dplyr operation is a fully materialized\ndata frame itself. This means that those results can be directly passed on to\ndownstream operations like plotting without the plotting package having to be\naware of the \u201clazyness\u201d of the duckplyr result object. To address this, we\nhave creatively used a R feature known as ALTREP. ALTREP allows R objects to\nhave different in-memory representations, and for custom code to be executed\nwhenever those objects are accessed. Duckplyr results are lazy placeholder\nobjects, yes, but they appear to be bog-standard R data frames at the same\ntime. R data frames are essentially named lists of typed vectors with a\nspecial row.names attribute. Because DuckDB\u2019s lazy query planning already\nknows the names and types of the resulting table, we can export the names into\nthe lazy data frame. We do not however know the number of rows nor their\ncontents yet. We therefore make both the actual data vectors and the row names\nvector that contains the data frame length lazy vectors. Those vectors carry a\ncallback that the R engine will invoke whenever downstream code \u2013 e.g.\nplotting code \u2013 touches those vectors. The callback will actually trigger\ncomputation of the entire pipeline and transformation of the result ot a R\ndata frame. Duckplyr\u2019s own operations will refrain from touching those\nvectors, they instead continue lazily using a special lazy computation object\nthat is also stored in the lazy data frame. This method allows duckplyr to be\nboth lazy and not at the same time, which allows full drop-in replacement with\nthe eagerly evaluated dplyr while keeping the lazy evaluation that is crucial\nfor DuckDB to be able to do a full-query optimization of the various\ntransformation steps.\n\nHere is an example of the duality of the result of duckplyr operations using\nR\u2019s inspect() method:\n\n    \n    \n    dd <- as_duckplyr_df(data.frame(n=1:10)) |> mutate(m=n+1) .Internal(inspect(dd))\n    \n    \n    @12daad988 19 VECSXP g0c2 [OBJ,REF(2),ATT] (len=2, tl=0) @13e0c9d60 13 INTSXP g0c0 [REF(4)] DUCKDB_ALTREP_REL_VECTOR n (INTEGER) @13e0ca1c0 14 REALSXP g0c0 [REF(4)] DUCKDB_ALTREP_REL_VECTOR m (DOUBLE) ATTRIB: @12817a838 02 LISTSXP g0c0 [REF(1)] TAG: @13d80d420 01 SYMSXP g1c0 [MARK,REF(65535),LCK,gp=0x4000] \"names\" (has value) @12daada08 16 STRSXP g0c2 [REF(65535)] (len=2, tl=0) @13d852ef0 09 CHARSXP g1c1 [MARK,REF(553),gp=0x61] [ASCII] [cached] \"n\" @13e086338 09 CHARSXP g1c1 [MARK,REF(150),gp=0x61] [ASCII] [cached] \"m\" TAG: @13d80d9d0 01 SYMSXP g1c0 [MARK,REF(56009),LCK,gp=0x4000] \"class\" (has value) @12da9e208 16 STRSXP g0c2 [REF(65535)] (len=2, tl=0) @11ff15708 09 CHARSXP g0c2 [MARK,REF(423),gp=0x60] [ASCII] [cached] \"duckplyr_df\" @13d892308 09 CHARSXP g1c2 [MARK,REF(1513),gp=0x61,ATT] [ASCII] [cached] \"data.frame\" TAG: @13d80d1f0 01 SYMSXP g1c0 [MARK,REF(65535),LCK,gp=0x4000] \"row.names\" (has value) @13e0c9970 13 INTSXP g0c0 [REF(65535)] DUCKDB_ALTREP_REL_ROWNAMES\n\nWe can see that the internal structure of the data frame indeed reflects a\ndata frame, but we can also see the special vectors DUCKDB_ALTREP_REL_VECTOR\nthat hide the un-evaluated data vectors as well as DUCKDB_ALTREP_REL_ROWNAMES\nthat hide the fact that the true dimensions of the data frame are not yet\nknown.\n\n## Benchmark: TPC-H Q1\n\nLet\u2019s finish with a quick demonstration of duckplyr\u2019s performance\nimprovements. We use the data generator from the well known TPC-H benchmark,\nwhich is helpfully available as a DuckDB extension. With the \u201cscale factor\u201d of\n1, the following DuckDB/R one-liner will generate a data set with a little\nover 6 million rows and store it in the R data frame named \u201clineitem\u201d:\n\n    \n    \n    lineitem <- duckdb:::sql(\"INSTALL tpch; LOAD tpch; CALL dbgen(sf=1); FROM lineitem;\")\n\nWe have transformed the TPC-H benchmark query 1 from its original SQL\nformulation to dplyr syntax:\n\n    \n    \n    tpch_01 <- function() { lineitem |> select(l_shipdate, l_returnflag, l_linestatus, l_quantity, l_extendedprice, l_discount, l_tax) |> filter(l_shipdate <= !!as.Date(\"1998-09-02\")) |> select(l_returnflag, l_linestatus, l_quantity, l_extendedprice, l_discount, l_tax) |> summarise( sum_qty = sum(l_quantity), sum_base_price = sum(l_extendedprice), sum_disc_price = sum(l_extendedprice * (1 - l_discount)), sum_charge = sum(l_extendedprice * (1 - l_discount) * (1 + l_tax)), avg_qty = mean(l_quantity), avg_price = mean(l_extendedprice), avg_disc = mean(l_discount), count_order = n(), .by = c(l_returnflag, l_linestatus) ) |> arrange(l_returnflag, l_linestatus) }\n\nWe can now execute this function with both dplyr and duckplyr and observe the\ntime required to compute the result. \u201cStock\u201d dplyr takes ca. 400 milliseconds\non my MacBook for this query, duckplyr requires only ca 70 milliseconds.\nAgain, this time includes all the magic transforming the sequence of dplyr\nverbs into a relational operator tree, optimizing said tree, converting the\ninput R data frame into a DuckDB intermediate on-the-fly, and transforming the\n(admittedly small) result back to a R data frame. Of course, the data set used\nhere is still relatively small and the query is not that complex either,\nessentially a single grouped aggregation. The differences will be much more\npronounced for more complex transformations on larger data sets. duckplyr can\nalso directly access large collections of e.g. Parquet files on storage, and\npush down filters into those scans, which can also greatly improve\nperformance.\n\n## Conclusion\n\nThe duckplyr package for R wraps DuckDB\u2019s state-of-the-art analytical query\nprocessing techniques in a dplyr-compatible API. We have gone to great lengths\nto ensure compatibility despite switching execution paradigms from eager to\nlazy and having to translate expressions to a different environment. We\ncontinue to work to expand duckplyr\u2019s capabilites but would love to hear your\nexperiences trying it out.\n\nHere are two recordings from last year\u2019s posit::conf where we present DuckDB\nfor R and duckplyr:\n\n  * In-Process Analytical Data Management with DuckDB \u2013 posit::conf(2023)\n  * duckplyr: Tight Integration of duckdb with R and the tidyverse \u2013 posit::conf(2023)\n\n# Recent posts\n\n2024-08-15\n\n### DuckCon #5 in Seattle\n\n2024-04-02\n\n### duckplyr: dplyr powered by DuckDB\n\n2024-03-29\n\n### No Memory? No Problem. External Aggregation in DuckDB\n\nAll blog posts\n\n###### Documentation\n\nInstallation SQL Introduction Why DuckDB\n\n###### Internals\n\nFAQs Code of Conduct Foundation Trademark Use\n\n###### Commercial Support\n\nDuckDB Labs Support Options\n\n###### Community\n\nX (Twitter) LinkedIn Discord GitHub Stack Overflow\n\n\u00a9 2024 DuckDB Foundation, Amsterdam NL\n\n", "frontpage": false}
