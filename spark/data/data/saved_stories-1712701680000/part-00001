{"aid": "39981067", "title": "AIME API Server is a scalable open source AI model inference API Server", "url": "https://api.aime.info/", "domain": "aime.info", "votes": 1, "user": "headkit", "posted_at": "2024-04-09 16:25:44", "comments": 0, "source_title": "AIME API Server - The Scalable Model Inference API Server", "source_text": "AIME API Server - The The Scalable Model Inference API Server\n\n# AIME API Server\n\n## The Scalable Model Inference API Server\n\nWith AIME API one deploys deep learning models (Pytorch, TensorFlow) through a\njob queue as scalable API endpoint capable of serving millions of model\ninference requests. Turn a console Python script to a secure and robust web\nAPI acting as your interface to the mobile, browser and desktop world.\n\n## A modern, powerful and scalable Inference API framework - and more!\n\nDeliver outstanding services fast - meet your and your customers requirements\nand grow with your needs. Simplify and accelerate your development work and\nscale your AI model inference performance according to demand.\n\n  * Fast - asynchronous and multi process API server\n  * Scalable & Robust- distributed cluster ready architecture\n  * Secure - type safe interface and input validation\n  * Aggregates API requests to GPU batch jobs for maximum throughput\n  * Easy integratable into exisiting Python and TensorFlow projects\n  * High performance image and audio input/ouput conversion for common media formats\n  * Pythonic - easily extendable in your favourite programming language\n\n## Overview of the AIME API Architecture\n\nThe AIME API server solution implements a distributed server architecture with\na central API Server communicating through a job queue with a scalable GPU\ncompute cluster. The GPU compute cluster can be heterogeneous and distributed\nat different locations without requiring an interconnect.\n\n### AIME API Server\n\nThe central part is the API Server, an efficient asynchronous HTTP/HTTPS web\nserver which can be used as a stand-alone web server or integrated into\nApache, NGINX or similar web servers. It receives the client requests, load\nbalances the requests and distributes them to the API compute workers.\n\n### Compute Workers\n\nThe model compute jobs are processed through so called compute workers which\nconnect to the API server through a secure HTTPS interface. You can easily\nturn your existing Pytorch and TensorFlow script into an API compute worker by\nintegrating the AIME API Worker Interface.\n\n### Clients\n\nClients, like web browsers, smartphones and desktop apps can easily\nintegrating model inference API calls with the AIME API Client Interfaces.\n\n## Example Endpoints\n\nTo illustrate the usage and capabilities of AIME API we currently run\nfollowing demo api services. The source code of these applications is\navailable for review and use in the respective GitHub repository.\n\n### LlaMa2 Chat\n\nChat with 'Dave', our LLaMa2 based chat-bot.\n\n  * AIME Demo Server\n  * GitHub Source Code\n\n### Stable Diffusion XL\n\nCreate photo realistic images from text prompts.\n\n  * AIME Demo Server\n  * GitHub Source Code\n\n### Seamless Communication\n\nTranslate between 36 languages in near realtime: Text-to-Text, Speech-to-Text,\nText-to-Speech and Speech-to-Speech!\n\n  * AIME Demo Server\n  * GitHub Source Code\n\n## How to setup and start the AIME API Server\n\n### Setup the environment\n\nWe recommend creating a virtual environment for local development. Create and\nactivate a virtual environment, like 'venv' with:\n\n    \n    \n    python3 -m venv venv source ./venv/bin/activate\n\nDownload or clone the AIME API server:\n\n    \n    \n    git clone --recurse-submodules https://github.com/aime-team/aime-api-server.git\n\nAlternatively, for excluding Worker interface and Client interfaces\nsubmodules, which are not needed to run the API server itself, use:\n\n    \n    \n    git clone https://github.com/aime-team/aime-api-server.git\n\nThen install required pip packages:\n\n    \n    \n    pip install -r requirements.txt\n\n### Optional: install ffmpeg (required for image and audio conversion)\n\nUbuntu/Debian:\n\n    \n    \n    sudo apt install ffmpeg\n\n### Starting the server\n\nTo start the API server run:\n\n    \n    \n    python3 run api_server.py [-H HOST] [-p PORT] [-c EP_CONFIG] [--dev]\n\nThe server is booting and loading the example endpoints configurations defined\nin the \"/endpoints\" directory.\n\nWhen started it is reachable at http://localhost:7777 (or the port given). As\ndefault the README.md file is served. The example endpoints are available and\nare taking requests.\n\nThe server is now ready to connect corresponding compute workers.\n\n## Compute Workers\n\nYou can easily turn your existing PyTorch and TensorFlow script into an API\ncompute worker by integrating the AIME API Worker Interface.\n\nFollowing example workers implementations are available as open source, which\neasily can be adapted to similar use cases:\n\n#### How to run a LLaMa2 Chat Worker (Large Language Model Chat)\n\nhttps://github.com/aime-labs/llama2_chat\n\n#### How to run a Stable Diffusion Worker (Image Generation)\n\nhttps://github.com/aime-labs/stable_diffusion_xl\n\n#### How to run a Seamless Communication Worker (Text2Text, SpeechText,\nText2Speech, Speech2Speech)\n\nhttps://github.com/aime-labs/seamless_communication\n\n## Available Client Interfaces\n\n### JavaScript\n\nSimple single call example for an AIME API Server request on endpoint LLaMa 2\nwith JavaScript:\n\n    \n    \n    function onResultCallback(data) { console.log(data.text) // print generated text to console } const endpointName = 'llama2_chat'; const params = { text: 'Your text prompt' }; doAPIRequest(endpointName, params, onResultCallback);\n\n### Python\n\nSimple synchronous single call example for an AIME API Server request on\nendpoint LLaMa 2 with Python:\n\n    \n    \n    aime_api_client_interface import do_api_request api_server_address = 'https://api.aime.info' endpoint_name = 'llama2_chat' params = {'text': 'Your text prompt'} result = do_api_request(api_server_address, endpoint_name, params) print(result.get('text')) # print generated text to console\n\n### More to come...\n\nWe are currently working on sample interfaces for: iOS, Android, Java, PHP,\nRuby, C/C++ etc.\n\n## Documentation\n\nFor more information about the AIME API read our blog article AIME API - The\nScalable AI Model Inference Solution.\n\nOr consult the AIME API documentation.\n\n## AIME API is Open Source\n\nThe AIME API is free of charge for non-commercial use. Details of licensing can be found in the LICENSE file. We look forward to hearing from you regarding collaboration or licensing on other devices: hello@aime.info | +49 30 459 54 381.\n\nInstall & try out AIME API on your AIME server or instance!\n\nGet your AIME API server\n\n### Legal\n\n  * Privacy Policy\n  * Licensing\n  * Imprint\n  * Terms\n\n### Social Media\n\n  * Mastodon\n  * Blue Sky\n  * LinkedIn\n  * GitHub\n\n### Company\n\n  * AIME HPC Servers & Clusters\n  * AIME GPU Cloud\n  * AIME Blog\n\n### AIME API\n\n  * LlaMa2-Chat Demo\n  * Stable Diffusion XL Demo\n  * Seamless Communication Demo\n  * GitHub-Code\n  * API Docs\n\n\u00a9 2024 AIME GmbH. All Rights Reserved.\n\n|\n\n", "frontpage": false}
