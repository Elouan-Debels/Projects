{"aid": "40060728", "title": "Caching secrets of the HTTP elders, part 1", "url": "https://csvbase.com/blog/8", "domain": "csvbase.com", "votes": 11, "user": "calpaterson", "posted_at": "2024-04-17 05:19:55", "comments": 0, "source_title": "Caching secrets of the HTTP elders, part 1", "source_text": "Caching secrets of the HTTP elders, part 1\n\ncsvbase is a simple web database. Learn more on the about page.\n\n# Caching secrets of the HTTP elders, part 1\n\nETags - an elegant weapon, for a more civilised age\n\n2024-04-16\n\nby Cal Paterson\n\nThere is no hard limit on table size in csvbase. Within reason, they can be as\nbig as you like. And if you go off and run your own instance you can do\nwhatever you want. Big Data isn't always Better Data, but it's nice to have\nthe option.\n\nThat said, wouldn't it be nice to have those tables cached, so that if you\nread it twice you don't have to fully download it again the second time?\n\n## Cache-control header - with max-age\n\nPerhaps this HTTP server header?:\n\n    \n    \n    Cache-Control: max-age=86400\n\nThe above header says \"keep using this file for a day [86400 seconds] before\nre-downloading it\". Great!\n\nHang on though. If someone changes that table you'd be stuck looking at the\nold version for up to 24 hours. Being eventually consistent was once very\ntrendy. But it's no longer hip - even Amazon S3 no longer returns stale data.\nSo serving stale data just won't do for csvbase.\n\n## HTTP/1.1 has a lot of caching features\n\nThe elden ones thought deeply about caching. DNS is a cache. Virtual memory is\na cache. And if you've ever wondered why HTTP/1.0 got a patch release\n(HTTP/1.1) less than a year after it came out: more caching.\n\nHTTP/1.0 had almost nothing in the way of caching. In HTTP/1.1, a whole new\ntop-level section on the subject was added to the standard. This was essential\nbecause, back in the day, the repeated re-downloading of unchanged files was a\nreal headache.\n\nThis needless re-downloading was especially bad for popular sites, whose pages\nwould be massively re-downloaded over and over - even though they hadn't\nchanged - by many, many people. Enough to start cause problems for the\ninternet as a whole.\n\nOne of the cool new features in HTTP/1.1 (implemented in Netscape Navigator\n4.0, available from all reputable software stockists) is the ETag header. It\nworks like this:\n\nThe HTTP server generates a unique id for each version of a file. 'Version' in\nthis context means not just revision but also format, so CSV files get\ndifferent ids from Parquet files even if served from the same URL (csvbase\ndoes this). The HTTP server returns this id in the ETag header.\n\nWhen an HTTP client requests the file again, it provides that same id in the\nIf-None-Match header.\n\nIf that ETag is up-to-date, the server just sets the status to 304 Not\nModified and returns absolutely nothing. The client then knows that it should\nuse the file it already has.\n\nThis is a way to both keep a cache and avoid stale data.\n\n## Generating the ETag\n\nHow should the ETag be generated? That is up to the server - up to csvbase.\ncsvbase could just md5sum the file and put that in the header. That would be\nvalid and it would work.\n\nWell, except. Except, csvbase is not actually a database of CSV files. The\nname is a bit of a trick. csvbase doesn't keep the CSV files on some\nfilesystem (or in an object store for that matter). Instead, uploaded CSV\nfiles are fully parsed and then put into a SQL database. When csvbase returns\nyou a CSV file it's being generated on the fly. That's how csvbase returns\nother formats like Parquet, JSONlines, JSON, etc.\n\nIt would be quicker for csvbase not to have to regenerate the CSV file (or\nParquet file) each time, just in order to check whether the md5sum of that\nfile matched the If-None-Match header.\n\nSo instead of hashing the contents of the file, we'll create the ETag by\nhashing a key:\n\n(table id, last change time, file format, page number [if any])\n\nThat key represents both the version of the file plus what page is being\nlooked it (the JSON and HTML formats are paginated). And if your table is\nchanged, last change time is different and the ETag changes too. Fab.\n\n## How long to cache for?\n\nAt this point we have a caching system that:\n\n  1. Spares the client downloading the file\n\n     * though they must make a tiny request\n  2. Also avoids the server regenerating the file\n  3. Prevents any stale results (ie: avoids \"TTL hell\")\n\nThe only question now is how long we tell clients to cache stuff. How long?\n\nThe answer is forever. csvbase sets no max-age. Clients can cache stuff as\nlong as they like, providing that they revalidate each time the use that data.\n\nHere's what csvbase's response looks like\n\n    \n    \n    HTTP/1.1 content-type: text/csv etag: <some gobbledegook string> cache-control: no-cache, must-revalidate [ then loads of CSV data ]\n\nThe operative bit here is the no-cache pragma. Despite the misleading name,\nthis tells clients that they can in fact cache this file, but that they must\nrevalidate each time they use it. Here's how MDN describes it (emphasis is\nmine):\n\n> \"The no-cache response directive indicates that the response can be stored\n> in caches, but the response must be validated with the origin server before\n> each reuse, even when the cache is disconnected from the origin server.\"\n>\n> \"If you want caches to always check for content updates while reusing stored\n> content, no-cache is the directive to use. It does this by requiring caches\n> to revalidate each request with the origin server.\"\n\nmust-revalidate is probably not necessary but the original RFC suggests that\nmaybe caches might choose, in exceptional circumstances, yadda yadda yadda, to\nactually serve stale data anyway. must-revalidate bans that. must-revalidate's\nname actually makes sense so I probably don't need to quote chapter and verse,\nbut here, anyway, again from MDN:\n\n> HTTP allows caches to reuse stale responses when they are disconnected from\n> the origin server. must-revalidate is a way to prevent this from happening -\n> either the stored response is revalidated with the origin server or a 504\n> (Gateway Timeout) response is generated.\n\nNo TTL necessary. And browsers really support this stuff. It all works.\n\ncsvbase-client also supports this. You can test it out this way:\n\n    \n    \n    import pandas as pd # read the table df1 = pd.read_csv(\"csvbase://calpaterson/opcodes-6502\") # read it again (this came from your local cache) df2 = pd.read_csv(\"csvbase://calpaterson/opcodes-6502\")\n\nAnd if you prefer just to use curl, here's how to test it:\n\n    \n    \n    curl \\ --etag-compare opcodes-6502-etag.txt \\ --etag-save opcodes-6502-etag.txt \\ https://csvbase.com/calpaterson/opcodes-6502.parquet \\-O\n\n## What else is possible with ETags?\n\nA bit more. Join me next time when I use ETags as an effective form of\nconcurrency control. Until then:\n\n  * Try out csvbase and, really, really, importantly: send me your feedback\n  * Join the underground conspiracy on the csvbase discord server\n\n    * memes to be kept to a socially acceptable level of dankness\n  * Leaf through the source code\n  * Or come and meet me in person at Helsinki Python's May meetup\n\n## See also\n\nI wrote another article on caching a couple of years ago. The subject bit\ndifferent but the aim is the same: caching but without correctness problems.\n\nI loved Eric's Brewer's 2004 lecture on Inktomi. Inktomi was effectively a\nprimordial Google who tried white-labelling instead of running the search\nengine themselves. They later changed to being a content distribution network\nand at around 57 minutes Brewer relays the story of the release of the Starr\nreport which vindicated much of the effort put into web caching since HTTP/1.1\nwas released. He is very emotional throughout as Inktomi had gone\nspectacularly bust a couple of years before.\n\nOne wrinkle I have not discussed is that many caching reverse proxies don't\nhave exactly have spectacular support for cache revalidation. The going advice\non Varnish is to use some quite complicated config. Cloudflare do the wrong\nthing with no-cache but it's possible to work around it. What can't be worked\naround, for csvbase, is their lack of support for Vary. This is all HTTP/1.1\nstuff - from 1997 - and browsers overwhelmingly support it. But reverse\nproxies don't, for some reason.\n\n  * Source code\n  * Privacy policy\n  * Terms\n\n", "frontpage": true}
