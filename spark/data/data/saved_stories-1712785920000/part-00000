{"aid": "39992067", "title": "Apple Silicon GPUs, Docker and Ollama: Pick Two", "url": "https://chariotsolutions.com/blog/post/apple-silicon-gpus-docker-and-ollama-pick-two/", "domain": "chariotsolutions.com", "votes": 2, "user": "amai", "posted_at": "2024-04-10 15:50:15", "comments": 0, "source_title": "Apple Silicon GPUs, Docker and Ollama: Pick two.", "source_text": "Apple Silicon GPUs, Docker and Ollama: Pick two. \u2014 Chariot Solutions\n\nFind us at the upcoming SBC Summit! More Info\n\nSkip to content\n\n# Apple Silicon GPUs, Docker and Ollama: Pick two.\n\nApril 3, 2024February 26, 2024 by Ken Rimple Tags: apple, docker, gpu, llm,\nollama Category: Field Reports, tutorial\n\nAs part of our research on LLMs, we started working on a chatbot project using\nRAG, Ollama and Mistral. Our developer hardware varied between Macbook Pros\n(M1 chip, our developer machines) and one Windows machine with a \"Superbad\"\nGPU running WSL2 and Docker on WSL. All hail the desktop with the big GPU.\n\nWe planned on deploying to an Amazon EC2 instance as a quick test (running\nDocker on a g4dn.xlarge instance), and I thought initially that we could use\nDocker locally to start up the application stack. Then we could deploy it on\nWindows, Mac and Linux, and everything would run in one quick startup script\nvia docker-compose up.\n\nI built up the docker-compose file to start the whole stack, and hoped\neverything would be great.\n\nThe important parts for our discussion:\n\n?\n\nversion: \"3.7\"services:...frontend:env_file:\\- ./.env.localcontainer_name:\nfrontendbuild:context: ./clientdockerfile: Dockerfileports:\\-\n\"8080:80\"appserver:env_file:\\- ./.env.localcontainer_name:\nchariot_chatbotbuild:context: ./appserverdockerfile: Dockerfileports:\\-\n\"8000:8000\"ollama:container_name: ollamaimage: ollama/ollamacommand:\nserveports:\\- \"11434:11434\"volumes:\\- ./ollama:/root/.ollama  \n---  \n  \nAnyone who has been through the process of discovering Apple's differences\nfrom Linux/Mac NVIDIA can stop here and say \"I told you so.\" But for the rest\nof us, I submit my learning process.\n\n## Everything was not great\n\nAs you can see above, the ollama service is a Docker container that was\nreleased in October of 2023. Yay!\n\nI quickly skimmed the blog post announcing it. Do you see what I didn't?\n\n> We recommend running Ollama alongside Docker Desktop for MacOS in order for\n> Ollama to enable GPU acceleration for models\n\n\u2014 From https://ollama.com/blog/ollama-is-now-available-as-an-official-docker-\nimage\n\nYeah, so you'll see my folly now. Alongside basically means \"DO NOT USE THIS\nDOCKER CONTAINER\".\n\nAnyway, I started pulling together a stack that included an Ollama service.\nThe Ollama Docker container supported GPUs, but the author of this blog post\n(erm, me) will tell you he didn't RTFM properly. It turns out, Docker does NOT\nsupport M1 Mac GPUs.\n\nWhat was wrong? I assumed it could have been:\n\n  * The fact that I was running Docker\n  * Maybe a problem with the Docker service configuration?\n  * The M1 GPUs and API?\n\nMacs with their Apple GPUs which use the Metal Performance Shaders API aren't\nsupported as widely as CUDA, NVIDIA's GPU API for machine learning use. There\nis a reason which I'll get to later in the article.\n\nTo prove that, let me show you what the output of the Ollama log looks like\nwhen it doesn't detect an Apple Silicon GPU.\n\nStarting Docker Desktop: it does not detect a GPU.\n\n?\n\n$ docker run ollama/ollamatime=2024-02-21T21:09:40.438Z level=INFO\nsource=payload_common.go:146 msg=\"Dynamic LLM libraries [cuda_v11\ncpu]\"time=2024-02-21T21:09:40.438Z level=INFO source=gpu.go:94 msg=\"Detecting\nGPU type\"time=2024-02-21T21:09:40.438Z level=INFO source=gpu.go:265\nmsg=\"Searching for GPU management library libnvidia-\nml.so\"time=2024-02-21T21:09:40.438Z level=INFO source=gpu.go:311\nmsg=\"Discovered GPU libraries: []\"time=2024-02-21T21:09:40.438Z level=INFO\nsource=gpu.go:265 msg=\"Searching for GPU management library\nlibrocm_smi64.so\"time=2024-02-21T21:09:40.438Z level=INFO source=gpu.go:311\nmsg=\"Discovered GPU libraries: []\"time=2024-02-21T21:09:40.438Z level=INFO\nsource=cpu_common.go:18 msg=\"CPU does not have vector\nextensions\"time=2024-02-21T21:09:40.438Z level=INFO source=routes.go:1042\nmsg=\"no GPU detected\"  \n---  \n  \nSo, right there, the Docker image isn't being exposed to a GPU, and the only\nGPU library supported by Docker and viewable as hardware to the image is the\nNVidia GPU library.\n\n## What about other Docker engines like Colima?\n\nColima is a docker engine that runs as a backend to the Docker CLI if you set\nit up properly. I had thought I could use this engine and maybe it would\nexpose the Apple Silicon Metal GPU shaders and use them unlike Docker.\n\nAssuming you\u2019ve installed and configured Colima, here\u2019s how you switch to it\nas a Docker provider context and run Ollama:\n\n?\n\n$ colima start$ docker context colima$ docker run\nollama/ollamatime=2024-02-21T20:58:55.265Z level=INFO\nsource=payload_common.go:146 msg=\"Dynamic LLM libraries [cpu\ncuda_v11]\"time=2024-02-21T20:58:55.265Z level=INFO source=gpu.go:94\nmsg=\"Detecting GPU type\"time=2024-02-21T20:58:55.265Z level=INFO\nsource=gpu.go:262 msg=\"Searching for GPU management library libnvidia-\nml.so\"time=2024-02-21T20:58:55.267Z level=INFO source=gpu.go:308\nmsg=\"Discovered GPU libraries: []\"time=2024-02-21T20:58:55.267Z level=INFO\nsource=gpu.go:262 msg=\"Searching for GPU management library\nlibrocm_smi64.so\"time=2024-02-21T20:58:55.267Z level=INFO source=gpu.go:308\nmsg=\"Discovered GPU libraries: []\"time=2024-02-21T20:58:55.267Z level=INFO\nsource=cpu_common.go:18 msg=\"CPU does not have vector\nextensions\"time=2024-02-21T20:58:55.267Z level=INFO source=routes.go:1037\nmsg=\"no GPU detected\"  \n---  \n  \nNo way.\n\n## No GPU for Apple in Docker engines\n\nI really dug in and tried to make this work, thinking \"this MUST be\nsupported.\"\n\nI spent a bit of time trying out various settings in both Colima and Docker,\nnone of which did anything more than make the Docker memory footprint bigger\n(good), increase the number of CPUs (good), yet did not recognize the GPU\n(bad).\n\nTurns out, even Docker informs us that Docker Desktop on Windows with WSL\nsupports GPU acceleration, not Docker Desktop on Mac:\n\nI could have saved myself a lot of time by googling beforehand, but this is\nhow I learn. As my mother used to say \"Ken, you always have to find out the\nhard way. Good luck at school.\" But hey, the facts are cemented in my brain\nnow: currently Docker on Mac does not see the GPU.\n\nBut why?\n\n## Virtualization on Macs is the issue\n\nFrom a really good article on Apple Silicon performance for LLMs from Andreas\nKunar (sorry, this blog is a Medium link):\n\n> Apple\u2019s mandatory \u201cApple Virtualization Framework\u201d seems mostly to blame for\n> this, the silicon should technically support it from M2 onwards. Parallels,\n> Docker,... all have to use it \u2014 Parallels did their own, better virtualizer\n> for Intel-Macs. There might be hope for future MacOS versions, but I won\u2019t\n> hold my breath for it.\n\n\u2014 Andreas Kunar, Thoughts on Apple Silicon Performance for Local LLMs\n\nIt turns out that Docker and NVIDIA have a tight relationship when it comes to\naccelerated graphics. Docker only supports native Linux GPUs (with a supported\nrelease of CUDA and an NVIDIA card driver), and on Windows with WSL 2 and and\nsupported video card. Macs are not included because Apple hasn't provided an\nopen GPU API for their mandatory Virtualization engine.\n\nThat $5k Mac with a lot of memory is not currently going to support\nacceleration in Docker.\n\n## But Native Ollama Does Support Apple Silicon\n\nComing back to the beginning of this saga, that vaguely worded sentence\nbasically said \"run Ollama locally!\"\n\nSo I ran Ollama in the terminal to test it out. Note: if you\u2019re running Ollama\nvia the icon at the top of the Mac screen, you can kill it and do this, or\n`tail -f ~/.ollama/logs/server.log`.\n\nRunning Ollama in the terminal:\n\n?\n\n$ ollama servetime=2024-02-22T10:12:42.723-05:00 level=INFO\nsource=images.go:706 msg=\"total blobs: 5\"time=2024-02-22T10:12:42.724-05:00\nlevel=INFO source=images.go:713 msg=\"total unused blobs removed:\n0\"time=2024-02-22T10:12:42.725-05:00 level=INFO source=routes.go:1014\nmsg=\"Listening on 127.0.0.1:11434 (version\n0.1.25)\"time=2024-02-22T10:12:42.725-05:00 level=INFO\nsource=payload_common.go:107 msg=\"Extracting dynamic\nlibraries...\"time=2024-02-22T10:12:42.743-05:00 level=INFO\nsource=payload_common.go:146 msg=\"Dynamic LLM libraries [metal]\"  \n---  \n  \nDo you see that? [metal] is the Dynamic LLM library exposed and available to\nthe engine. So I fired up a mistral engine.\n\nRunning my llm chatbot in another terminal:\n\n?\n\n$ ollama run mistral  \n---  \n  \nThe output in the first terminal (there is a lot of it, but only when you\nactivate a model):\n\n?\n\nllm_load_tensors: offloading 32 repeating layers to GPUllm_load_tensors:\noffloading non-repeating layers to GPUllm_load_tensors: offloaded 33/33 layers\nto GPUllm_load_tensors: CPU buffer size = 70.31 MiBllm_load_tensors: Metal\nbuffer size = 3847.56\nMiB...................................................................................................llama_new_context_with_model:\nn_ctx = 2048llama_new_context_with_model: freq_base =\n1000000.0llama_new_context_with_model: freq_scale = 1ggml_metal_init:\nallocatingggml_metal_init: found device: Apple M1 Proggml_metal_init: picking\ndefault device: Apple M1 Proggml_metal_init: default.metallib not found,\nloading from sourceggml_metal_init: GGML_METAL_PATH_RESOURCES =\n/var/folders/r0/ww1scvgj7wz4dm_l3spst1gw0000gn/T/ollama1338032702ggml_metal_init:\nloading\n'/var/folders/r0/ww1scvgj7wz4dm_l3spst1gw0000gn/T/ollama1338032702/ggml-\nmetal.metal'ggml_metal_init: GPU name: Apple M1 Proggml_metal_init: GPU\nfamily: MTLGPUFamilyApple7 (1007)ggml_metal_init: GPU family:\nMTLGPUFamilyCommon3 (3003)ggml_metal_init: GPU family: MTLGPUFamilyMetal3\n(5001)ggml_metal_init: simdgroup reduction support = trueggml_metal_init:\nsimdgroup matrix mul. support = trueggml_metal_init: hasUnifiedMemory =\ntrueggml_metal_init: recommendedMaxWorkingSetSize = 22906.50 MB  \n---  \n  \nAs they'd say in Renaissance Faire, \"Huzzah!\" [cue clinking of swords]\n\n## Should we use Apple Macs for LLM work?\n\nI still don't really know enough to say, personally, that an Apple Silicon Mac\nwould be a fast and productive if you were on the scientific end of LLMs, for\nexample training models with fine tuning, LoRA, etc. There were some comments\non bugs in PyTorch by one blogger, but I'm thinking that over time this stuff\nwill get slowly corrected. That said, you can't run virtual machines on a Mac\n(same issue with the Mac Virtualization platform) and get accelerated GPU APIs\nexpose to your programs, so if your project requires a virtual machine, you're\nstuck with a very expensive paperweight.\n\nAnd before you think of doing this, the Apple Silicon Macs do NOT support\nThunderbolt external GPUs. You have been warned.\n\n## Alternatives?\n\nMaybe you could use a shared Linux workstation with a nice beefy card you'd\naccess as a team via a VPN or something, so you can run training jobs etc. so\nyou save some cost by sharing. I'm unsure whether it's worth it. That would be\nanother up-front cost, and you'd have to maintain it.\n\nOf course, as I mentioned in my last article you could you use a cloud\nprovider (I happened to mention AWS because it's my goto) for your training\nand LLM work, but those costs vary and you will have to be very careful not to\nrun up a big bill.\n\nThere are managed services / serverless engines that are beginning to surface,\nsuch as Amazon's suite of offerings around Bedrock and Kendra too, but these\nare going to be hard or impossible to develop on locally, you're tied to the\ncloud and the costs are incurred as you develop.\n\n## The Mac is fine for learning / basic development\n\nBut try to use the GPU if at all possible \u2013 the speed of CPU-only LLM\nprocessing is much slower than using Metal.\n\nFor writing code that runs queries on an LLM, against an already pre-trained\nmodel, a Mac with 32GB has 32GB of unified memory (that the GPU and CPU both\nshare). If most of your work is integrating a RAG LLM solution, it could be\ngood enough (albeit maybe slower than a really expensive GPU) to get work done\nas a developer, then push to a few engineers with beefier hardware or the\ncloud to shake it down.\n\nIn the end, we decided to have a native-mostly startup engine (running vite\nfor the React front-end, Uvicorn in Python for our RESTful BFF server, Ollama\nnatively, and virtualizing our Postgres engine with pgvector, as a developer\nlaunch option.\n\nIn the cloud, the docker-compose file works great, as long as you add in the\nproper NVIDIA detection settings (checkout the deploy config for the ollama\nengine:)\n\n?\n\nollama:container_name: ollamaimage:\nollama/ollamadeploy:resources:reservations:devices:\\- driver: nvidiacount:\n1capabilities: [gpu]  \n---  \n  \nYep, with NVIDIA on Linux (or Windows WSL2 and Docker Desktop) you even have\nto activate the GPU in the Docker container with configuration! So configurer\nbeware.\n\n## It's a moving landscape\n\nThings are changing constantly. New APIs are coming out all the time, and if\nyou wait 6 months, this article may be completely irrelevant. Just consider\nthat, as of Feb 22, 2024, this is the way it is: don't virtualize Ollama in\nDocker, or any (supported) Apple Silicon-enabled processes on a Mac. For other\nGPU-based workloads, make sure whether there is a way to run under Apple\nSilicon (for example, there is support for PyTorch on Apple Silicon GPUs, but\nyou have to set it up properly.\n\nAbout Services Our Work Blog Podcasts Videos Contact Us\n\n515 Pennsylvania Avenue Suite 202 Fort Washington, PA 19034 T: 215-358-1780\n\n\u00a9 Copyright 2024\n\nPrivacy Policy\n\n", "frontpage": false}
