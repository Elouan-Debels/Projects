{"aid": "39991675", "title": "Meta MTIA v2", "url": "https://ai.meta.com/blog/next-generation-meta-training-inference-accelerator-AI-MTIA/", "domain": "meta.com", "votes": 26, "user": "_yo2u", "posted_at": "2024-04-10 15:16:22", "comments": 0, "source_title": "Our next generation Meta Training and Inference Accelerator", "source_text": "Our next generation Meta Training and Inference Accelerator\n\nResearch\n\nBlog\n\nResources\n\nAbout\n\nResearch\n\nBlog\n\nResources\n\nAbout\n\nOur next-generation Meta Training and Inference Accelerator\n\nApril 10, 2024 \u00b7 8 min read\n\n##### Takeaways\n\n  * We\u2019re sharing details about the next generation of the Meta Training and Inference Accelerator (MTIA), our family of custom-made chips designed for Meta\u2019s AI workloads.\n  * This latest version shows significant performance improvements over MTIA v1 and helps power our ranking and recommendation ads models.\n  * MTIA is part of our growing investment in our AI infrastructure and will complement our existing and future AI infrastructure to deliver new and better experiences across our products and services.\n\nThe next generation of Meta\u2019s large-scale infrastructure is being built with\nAI in mind, including supporting new generative AI (GenAI) products and\nservices, recommendation systems, and advanced AI research. It\u2019s an investment\nwe expect will grow in the years ahead as the compute requirements to support\nAI models increase alongside the models\u2019 sophistication.\n\nLast year, we unveiled the Meta Training and Inference Accelerator (MTIA) v1,\nour first-generation AI inference accelerator that we designed in-house with\nMeta\u2019s AI workloads in mind \u2013 specifically our deep learning recommendation\nmodels that are improving a variety of experiences across our products.\n\nMTIA is a long-term venture to provide the most efficient architecture for\nMeta\u2019s unique workloads. As AI workloads become increasingly important to our\nproducts and services, this efficiency will improve our ability to provide the\nbest experiences for our users around the world. MTIA v1 was an important step\nin improving the compute efficiency of our infrastructure and better\nsupporting our software developers as they build AI models that will\nfacilitate new and better user experiences.\n\nNow, we\u2019re sharing details about the next generation of MTIA.\n\nNext generation MTIA chip model. Drag to rotate.\n\nThis inference accelerator is part of our broader full-stack development\nprogram for custom, domain-specific silicon that addresses our unique\nworkloads and systems. This new version of MTIA more than doubles the compute\nand memory bandwidth of our previous solution while maintaining our close tie-\nin to our workloads. It is designed to efficiently serve the ranking and\nrecommendation models that provide high-quality recommendations to users.\n\nGET INVOLVED\n\nJoin us on our journey to build infrastructure for AI\n\nVisit the Meta careers page\n\nMeta Training & Inference Accelerator\n\nUTH\n\nHardware\n\nSoftware\n\nPerformance\n\nOngoing Investment\n\n## Under the hood\n\nThis chip\u2019s architecture is fundamentally focused on providing the right\nbalance of compute, memory bandwidth, and memory capacity for serving ranking\nand recommendation models. In inference we need to be able to provide\nrelatively high utilization, even when our batch sizes are relatively low. By\nfocusing on providing outsized SRAM capacity, relative to typical GPUs, we can\nprovide high utilization in cases where batch sizes are limited and provide\nenough compute when we experience larger amounts of potential concurrent work.\n\nThis accelerator consists of an 8x8 grid of processing elements (PEs). These\nPEs provide significantly increased dense compute performance (3.5x over MTIA\nv1) and sparse compute performance (7x improvement). This comes partly from\nimprovements in the architecture associated with pipelining of sparse compute.\nIt also comes from how we feed the PE grid: We have tripled the size of the\nlocal PE storage, doubled the on-chip SRAM and increased its bandwidth by\n3.5X, and doubled the capacity of LPDDR5.\n\nOur new MTIA design also features an improved network on chip (NoC)\narchitecture that doubles the bandwidth and allows us to coordinate between\ndifferent PEs at low latency. These and other new functions in the PEs form\nthe key technologies that are vital to our long-term roadmap to scale MTIA to\na wider variety of more challenging workloads.\n\n##### First Gen MTIA\n\nTechnology\n\nTSMC 7nm\n\nFrequency\n\n800MHz\n\nInstances\n\n1.12B gates, 65M flops\n\nArea\n\n19.34mm x 19.1mm, 373mm2\n\nPackage\n\n43mm x 43mm\n\nVoltage\n\n0.67V logic, 0.75V memory\n\nTDP\n\n25W\n\nHost Connection\n\n8x PCIe Gen4 (16 GB/s)\n\nGEMM TOPS\n\n102.4 TFLOPS/s (INT8) 51.2 TFLOPS/s (FP16/BF16)\n\nSIMD TOPS\n\nVector core: 3.2 TFLOPS/s (INT8), 1.6 TFLOPS/s (FP16/BF16), 0.8 TFLOPS/s\n(FP32) SIMD: 3.2 TFLOPS/s (INT8/FP16/BF16), 1.6 TFLOPS/s (FP32)\n\nMemory Capacity\n\nLocal memory: 128 KB per PE On-chip memory: 128 MB Off-chip LPDDR5: 64 GB\n\nMemory Bandwidth\n\nLocal memory: 400 GB/s per PE On-chip memory: 800 GB/s Off-chip LPDDR5: 176\nGB/s\n\n##### Next Gen MTIA\n\nTechnology\n\nTSMC 5nm\n\nFrequency\n\n1.35GHz\n\nInstances\n\n2.35B gates, 103M flops\n\nArea\n\n25.6mm x 16.4mm, 421mm2\n\nPackage\n\n50mm x 40mm\n\nVoltage\n\n0.85V\n\nTDP\n\n90W\n\nHost Connection\n\n8x PCIe Gen5 (32 GB/s)\n\nGEMM TOPS\n\n708 TFLOPS/s (INT8) (sparsity) 354 TFLOPS/s (INT8) 354 TFLOPS/s (FP16/BF16)\n(sparsity) 177 TFLOPS/s (FP16/BF16)\n\nSIMD TOPS\n\nVector core: 11.06 TFLOPS/s (INT8), 5.53 TFLOPS/s (FP16/BF16), 2.76 TFLOPS/s\n(FP32) SIMD: 5.53 TFLOPS/s (INT8/FP16/BF16), 2.76 TFLOPS/s (FP32)\n\nMemory Capacity\n\nLocal memory: 384 KB per PE On-chip memory: 256 MB Off-chip LPDDR5: 128 GB\n\nMemory Bandwidth\n\nLocal memory: 1 TB/s per PE On-chip memory: 2.7 TB/s Off-chip LPDDR5: 204.8\nGB/s\n\n## The hardware\n\nServing our workloads effectively is not simply a silicon challenge. Co-\ndesigning the hardware system and the software stack along with the silicon is\nessential for the success of the overall inference solution.\n\nTo support the next-generation silicon we have developed a large, rack-based\nsystem that holds up to 72 accelerators. This consists of three chassis, each\ncontaining 12 boards that house two accelerators each. We specifically\ndesigned the system so that we could clock the chip at 1.35GHz (up from 800\nMHz) and run it at 90 watts compared to 25 watts for our first-generation\ndesign. Our design ensures we provide denser capabilities with higher compute,\nmemory bandwidth, and memory capacity. This density allows us to more easily\naccommodate a broad range of model complexities and sizes.\n\nBeyond this, we have upgraded the fabric between the accelerators and between\nthe host and accelerators to PCIe Gen5 to increase the bandwidth and\nscalability of our system. There is also the option to add an RDMA NIC if we\nchoose to scale out beyond the rack.\n\n## The software stack\n\nSoftware has been one of our key areas of focus from the start of our\ninvestment in MTIA. As the initial developers of PyTorch, we value\nprogrammability and developer efficiency. Our MTIA stack is designed to fully\nintegrate with PyTorch 2.0 and features like TorchDynamo and TorchInductor.\nFrontend graph-level capturing, analysis, transformation, and extraction\nmechanisms (such as TorchDynamo, torch.export, etc.) are agnostic to MTIA and\nare being reused The lower level compiler for MTIA takes the outputs from the\nfrontend and produces highly efficient and device-specific code. This lower\nlevel compiler itself consists of a few components that are responsible for\ngenerating executable code for models and kernels.\n\nBelow this sits the runtime stack responsible for interfacing with the\ndriver/firmware. The MTIA Streaming interface abstraction provides the basic\nand essential operations that both inference and (in the future) training\nsoftware require to manage the device memory, as well as run operators and\nexecute compiled graphs on the device. Finally, the runtime interacts with the\ndriver, which sits in user space \u2013 a decision we made to enable us to iterate\nfaster on the driver and firmware within our production stack.\n\nIn many ways this new chip system runs the software stack similarly to MTIA\nv1, which made it much faster for the team to deploy since we had already done\nmuch of the necessary integration and development work needed to be able to\nrun our applications on this architecture. The new MTIA is designed to be\ncompatible with code developed for MTIA v1. Since we had already integrated\nthe full software stack to the silicon, we were up and running our traffic\nwith this new chip in a matter of days. This allowed us to land this next-\ngeneration MTIA silicon rapidly, going from first silicon to production models\nrunning in 16 regions in less than nine months.\n\n### Triton-MTIA\n\nWe\u2019ve further optimized the software stack by creating the Triton-MTIA\ncompiler backend to generate high-performance code for the MTIA hardware.\nTriton is an open source language and compiler for writing highly efficient ML\ncompute kernels. It improves developer productivity for writing GPU code and\nwe have found that the Triton language is sufficiently hardware-agnostic to be\napplicable to non-GPU hardware architectures like MTIA.\n\nThe Triton-MTIA backend performs optimizations to maximize hardware\nutilization and support high-performance kernels. It also exposes key knobs to\nleverage Triton and MTIA auto-tuning infrastructures to explore the kernel\nconfiguration and optimization space.\n\nWe have implemented support for the features of the Triton language and\nintegration into PyTorch 2, providing extensive coverage for PyTorch\noperators. Thanks to TorchInductor, for example, our developers can leverage\nTriton-MTIA in both ahead-of-time (AOT) and just-in-time (JIT) workflows.\n\nWe observed dramatically improved developer efficiency with Triton-MTIA, which\nallowed us to scale up compute kernel authoring and significantly expand the\nsupport of PyTorch operators.\n\n## Performance Results\n\nThe results so far show that this MTIA chip can handle both the low complexity\n(LC) and high complexity (HC) ranking and recommendation models that are\ncomponents of Meta\u2019s products. Across these models, there can be a ~10x-100x\ndifference in model size and the amount of compute per input sample. Because\nwe control the whole stack, we can achieve greater efficiency compared to\ncommercially available GPUs. Realizing these gains is an ongoing effort and we\ncontinue to improve performance per watt as we build up and deploy MTIA chips\nin our systems.\n\nEarly results show that this next generation silicon has already improved\nperformance by 3x over our first generation chip across four key models we\nevaluated. At the platform level, with 2x the number of devices and a powerful\n2-socket CPU, we are able to achieve 6x model serving throughput and a 1.5x\nperformance per watt improvement over the first generation MTIA system. To\nachieve this, we have made significant progress optimizing kernels, compiler,\nruntime, and host serving stack. The time to optimize models is going down as\nthe developer ecosystem matures, yet there is more headroom to improve\nefficiency in the future.\n\n3x\n\nimproved performance over our first gen chip\n\nMTIA has been deployed in the data center and is now serving models in\nproduction. We are already seeing the positive results of this program as it's\nallowing us to dedicate and invest in more compute power for our more\nintensive AI workloads. It is proving to be highly complementary to\ncommercially available GPUs in delivering the optimal mix of performance and\nefficiency on Meta-specific workloads.\n\n## Meta\u2019s ongoing investment in custom silicon\n\nMTIA will be an important piece of our long-term roadmap to build and scale\nthe most powerful and efficient infrastructure possible for Meta\u2019s unique AI\nworkloads.\n\nWe\u2019re designing our custom silicon to work in cooperation with our existing\ninfrastructure as well as with new, more advanced hardware (including next-\ngeneration GPUs) that we may leverage in the future. Meeting our ambitions for\nour custom silicon means investing not only in compute silicon but also in\nmemory bandwidth, networking and capacity as well as other next-generation\nhardware systems.\n\nWe currently have several programs underway aimed at expanding the scope of\nMTIA, including support for GenAI workloads.\n\nWe\u2019re only at the beginning of this journey, and we\u2019re inviting people who\nwant to be a part of it to visit Meta Careers to learn about our open\npositions.\n\n#### Written by:\n\n#### Eran Tal, Nicolaas Viljoen and Joel Coburn\n\nWho We Are\n\nAbout\n\nPeople\n\nCareers\n\nEvents\n\nLatest Work\n\nResearch\n\nInfrastructure\n\nBlog\n\nResources\n\nOur Actions\n\nResponsibilities\n\nNewsletter\n\nSign Up\n\nWho We Are\n\nWho We AreAboutPeopleCareersEvents\n\nLatest Work\n\nLatest WorkResearchInfrastructureBlogResources\n\nOur Actions\n\nOur ActionsResponsibilities\n\nNewsletter\n\nNewsletterSign Up\n\nPrivacy Policy\n\nTerms\n\nCookies\n\nMeta \u00a9 2024\n\nAllow the use of cookies from Meta on this browser?\n\nWe use essential cookies and similar technologies to help:\n\nProvide and improve content on Meta Products\n\nProvide a safer experience by using information we receive from cookies on and\noff Meta Products\n\nProvide and improve Meta Company Products for people using a Meta or Oculus\naccount\n\nWe use tools on Meta from other companies that also use cookies. These tools\nare used for things like:\n\n  * Advertising and measurement services off of Meta Products\n  * Analytics\n  * Providing certain features\n  * Improving our services\n\nYou can allow the use of all cookies, just essential cookies or you can choose\nmore options below. You can learn more about cookies and how we use them, and\nreview or change your choice at any time in our Cookie Policy.\n\nEssential cookies\n\nThese cookies are required to use Meta Company Products. They\u2019re necessary for\nMeta websites to work as intended.\n\nOptional cookies\n\nOptional cookies from other companies\n\nWe use tools from other companies for advertising and measurement services off\nof Meta Company Products, analytics, and to provide certain features and\nimprove our services for you. These companies also use cookies.\n\nIf you allow these cookies:\n\n  * We\u2019ll be able to better personalize ads for you off of Meta Products, and measure their performance\n  * Features on our products will not be affected\n  * Other companies will receive information about you when you use cookies\n\nIf you don\u2019t allow these cookies:\n\n  * We won\u2019t use cookies from other companies to help personalize ads for you off of Meta Products or measure ads performance\n  * Some features on our products may not work\n\nOther ways you can control tracking\n\nAd settings\n\nIf you have added your Meta or Oculus account to the same Accounts Center as\nyour Facebook or Instagram account, you can manage how different data is used\nto personalize ads in ad settings. To show you better ads, we use data that\nadvertisers and other partners provide us about your activity off Meta Company\nProducts, including websites and apps. You can control whether we use this\ndata to show you ads in your ad settings.\n\nThe Facebook Audience Network is a way for advertisers to show you ads in apps\nand websites off the Meta Company Products. One of the ways Audience Network\nshows relevant ads is by using your ad preferences to determine which ads you\nmay be interested in seeing.\n\nAd preferences\n\nIn Ad preferences, you can choose whether we show you ads and make choices\nabout the information used to show you ads.\n\nYou can opt out of seeing online interest-based ads from Meta and other\nparticipating companies through the Digital Advertising Alliance in the US,\nthe Digital Advertising Alliance of Canada in Canada or the European\nInteractive Digital Advertising Alliance in Europe, or through your mobile\ndevice settings, if you are using Android, iOS 13 or an earlier version of\niOS. Please note that ad blockers and tools that restrict our cookie use may\ninterfere with these controls.The advertising companies we work with generally\nuse cookies and similar technologies as part of their services. To learn more\nabout how advertisers generally use cookies and the choices they offer, you\ncan review the following resources:\n\n  * Digital Advertising Alliance\n  * Digital Advertising Alliance of Canada\n  * European Interactive Digital Advertising Alliance\n\nYour browser or device may offer settings that allow you to choose whether\nbrowser cookies are set and to delete them. These controls vary by browser,\nand manufacturers may change both the settings they make available and how\nthey work at any time. As of 5 October 2020, you may find additional\ninformation about the controls offered by popular browsers at the links below.\nCertain parts of Meta Products may not work properly if you have disabled\nbrowser cookies. Please be aware these controls are distinct from the controls\nthat Instagram and Facebook offer.\n\n  * Google Chrome\n  * Internet Explorer\n  * Firefox\n  * Safari\n  * Safari Mobile\n  * Opera\n\n", "frontpage": true}
