{"aid": "40000159", "title": "Intel's Gaudi 3 Goes After Nvidia", "url": "https://spectrum.ieee.org/intel-gaudi-3", "domain": "ieee.org", "votes": 1, "user": "cannibalXxx", "posted_at": "2024-04-11 09:22:29", "comments": 0, "source_title": "Intel\u2019s Gaudi 3 Goes After Nvidia", "source_text": "Intel\u2019s Gaudi 3 Goes After Nvidia - IEEE Spectrum\n\nOpens in a new window Opens an external website Opens an external website in a\nnew window\n\nThis website utilizes technologies such as cookies to enable essential site\nfunctionality, as well as for analytics, personalization, and targeted\nadvertising purposes. You may change your settings at any time or accept the\ndefault settings. You may close this banner to continue with only essential\ncookies. Privacy Policy\n\nStorage Preferences\n\nIEEE.orgIEEE Xplore Digital LibraryIEEE StandardsMore Sites\n\nSign InJoin IEEE\n\nIntel\u2019s Gaudi 3 Goes After Nvidia\n\nShare\n\nFOR THE TECHNOLOGY INSIDER\n\nExplore by topic\n\nAerospaceArtificial IntelligenceBiomedicalClimate TechComputingConsumer\nElectronicsEnergyHistory of\nTechnologyRoboticsSemiconductorsTelecommunicationsTransportation\n\nIEEE Spectrum\n\nFOR THE TECHNOLOGY INSIDER\n\n### Topics\n\nAerospaceArtificial IntelligenceBiomedicalClimate TechComputingConsumer\nElectronicsEnergyHistory of\nTechnologyRoboticsSemiconductorsTelecommunicationsTransportation\n\n### Sections\n\nFeaturesNewsOpinionCareersDIYEngineering Resources\n\n### More\n\nNewslettersPodcastsSpecial ReportsCollectionsExplainersTop Programming\nLanguagesRobots Guide \u2197IEEE Job Site \u2197\n\n### For IEEE Members\n\nCurrent IssueMagazine ArchiveThe InstituteThe Institute Archive\n\n### For IEEE Members\n\nCurrent IssueMagazine ArchiveThe InstituteThe Institute Archive\n\n### IEEE Spectrum\n\nAbout UsContact UsReprints & Permissions \u2197Advertising \u2197\n\n### Follow IEEE Spectrum\n\n### Support IEEE Spectrum\n\nIEEE Spectrum is the flagship publication of the IEEE \u2014 the world\u2019s largest\nprofessional organization devoted to engineering and applied sciences. Our\narticles, podcasts, and infographics inform our readers about developments in\ntechnology, engineering, and science.\n\nJoin IEEE\n\nSubscribe\n\nAbout IEEEContact & SupportAccessibilityNondiscrimination PolicyTermsIEEE\nPrivacy PolicyCookie PreferencesAd Privacy Options\n\n\u00a9 Copyright 2024 IEEE \u2014 All rights reserved. A not-for-profit organization,\nIEEE is the world's largest technical professional organization dedicated to\nadvancing technology for the benefit of humanity.\n\n## Enjoy more free content and benefits by creating an account\n\n## Saving articles to read later requires an IEEE Spectrum account\n\n## The Institute content is only available for members\n\n## Downloading full PDF issues is exclusive for IEEE Members\n\n## Downloading this e-book is exclusive for IEEE Members\n\n## Access to Spectrum 's Digital Edition is exclusive for IEEE Members\n\n## Following topics is a feature exclusive for IEEE Members\n\n## Adding your response to an article requires an IEEE Spectrum account\n\n## Create an account to access more content and features on IEEE Spectrum ,\nincluding the ability to save articles to read later, download Spectrum\nCollections, and participate in conversations with readers and editors. For\nmore exclusive content and features, consider Joining IEEE .\n\n## Join the world\u2019s largest professional organization devoted to engineering\nand applied sciences and get access to all of Spectrum\u2019s articles, archives,\nPDF downloads, and other benefits. Learn more \u2192\n\n## Join the world\u2019s largest professional organization devoted to engineering\nand applied sciences and get access to this e-book plus all of IEEE Spectrum\u2019s\narticles, archives, PDF downloads, and other benefits. Learn more \u2192\n\nCREATE AN ACCOUNTSIGN IN\n\nJOIN IEEESIGN IN\n\nClose\n\n## Access Thousands of Articles \u2014 Completely Free\n\n## Create an account and get exclusive content and features: Save articles,\ndownload collections, and talk to tech insiders \u2014 all free! For full access\nand benefits, join IEEE as a paying member.\n\nCREATE AN ACCOUNTSIGN IN\n\nSemiconductorsArtificial IntelligenceNewsComputing\n\n# Intel\u2019s Gaudi 3 Goes After Nvidia\n\n##\n\nThe company predicts victory over H100 in LLMs\n\nSamuel K. Moore\n\n09 Apr 2024\n\n3 min read\n\n1\n\nSamuel K. Moore is IEEE Spectrum\u2019s semiconductor editor.\n\nIntel's Gaudi 3 is packaged with eight high-bandwidth memory chips.\n\nIntel\n\nintel nvidia blackwell hopper gaudi amd artificial intelligence llms ai chips\nAI chips\n\nAlthough the race to power the massive ambitions of AI companies might seem\nlike it\u2019s all about Nvidia, there is a real competition going in AI\naccelerator chips. The latest example: At Intel\u2019s Vision 2024 event this week\nin Phoenix, Ariz., the company gave the first architectural details of its\nthird-generation AI accelerator, Gaudi 3.\n\nWith the predecessor chip, the company had touted how close to parity its\nperformance was to Nvidia\u2019s top chip of the time, H100, and claimed a superior\nratio of price versus performance. With Gaudi 3, it\u2019s pointing to large-\nlanguage-model (LLM) performance where it can claim outright superiority. But,\nlooming in the background is Nvidia\u2019s next GPU, the Blackwell B200, expected\nto arrive later this year.\n\n## Gaudi Architecture Evolution\n\nGaudi 3 doubles down on its predecessor Gaudi 2\u2019s architecture, literally in\nsome cases. Instead of Gaudi 2\u2019s single chip, Gaudi 3 is made up of two\nidentical silicon dies joined by a high-bandwidth connection. Each has a\ncentral region of 48 megabytes of cache memory. Surrounding that are the\nchip\u2019s AI workforce\u2014four engines for matrix multiplication and 32 programmable\nunits called tensor processor cores. All that is surrounded by connections to\nmemory and capped with media processing and network infrastructure at one end.\n\nIntel says that all that combines to produce double the AI compute of Gaudi 2\nusing 8-bit floating-point infrastructure that has emerged as key to training\ntransformer models. It also provides a fourfold boost for computations using\nthe BFloat 16 number format.\n\n## Gaudi 3 LLM Performance\n\nIntel projects a 40 percent faster training time for the GPT-3 175B large\nlanguage model versus the H100 and even better results for the 7-billion and\n8-billion parameter versions of Llama2.\n\nFor inferencing, the contest was much closer, according to Intel, where the\nnew chip delivered 95 to 170 percent of the performance of H100 for two\nversions of Llama. Though for the Falcon 180B model, Gaudi 3 achieved as much\nas a fourfold advantage. Unsurprisingly, the advantage was smaller against the\nNvidia H200\u201480 to 110 percent for Llama and 3.8x for Falcon.\n\nIntel claims more dramatic results when measuring power efficiency, where it\nprojects as much as 220 percent H100\u2019s value on Llama and 230 percent on\nFalcon.\n\n\u201cOur customers are telling us that what they find limiting is getting enough\npower to the data center,\u201d says Intel\u2019s Habana Labs chief operating officer\nEitan Medina.\n\nThe energy-efficiency results were best when the LLMs were tasked with\ndelivering a longer output. Medina puts that advantage down to the Gaudi\narchitecture\u2019s large-matrix math engines. These are 512 bits across. Other\narchitectures use many smaller engines to perform the same calculation, but\nGaudi\u2019s supersize version \u201cneeds almost an order of magnitude less memory\nbandwidth to feed it,\u201d he says.\n\n## Gaudi 3 Versus Blackwell\n\nIt\u2019s speculation to compare accelerators before they\u2019re in hand, but there are\na couple of data points to compare, particular in memory and memory bandwidth.\nMemory has always been important in AI, and as generative AI has taken hold\nand popular models reach the tens of billions of parameters in size it\u2019s\nbecome even more critical.\n\nBoth make use of high-bandwidth memory (HBM), which is a stack of DRAM memory\ndies atop a control chip. In high-end accelerators, it sits inside the same\npackage as the logic silicon, surrounding it on at least two sides. Chipmakers\nuse advanced packaging, such as Intel\u2019s EMIB silicon bridges or TSMC\u2019s chip-\non-wafer-on-silicon (CoWoS), to provide a high-bandwidth path between the\nlogic and memory.\n\nAs the chart shows, Gaudi 3 has more HBM than H100, but less than H200, B200,\nor AMD\u2019s MI300. It\u2019s memory bandwidth is also superior to H100\u2019s. Possibly of\nimportance to Gaudi\u2019s price competitiveness, it uses the less expensive HBM2e\nversus the others\u2019 HBM3 or HBM3e, which are thought to be a significant\nfraction of the tens of thousands of dollars the accelerators reportedly sell\nfor.\n\nOne more point of comparison is that Gaudi 3 is made using TSMC\u2019s N5\n(sometimes called 5-nanometer) process technology. Intel has basically been a\nprocess node behind Nvidia for generations of Gaudi, so it\u2019s been stuck\ncomparing its latest chip to one that was at least one rung higher on the\nMoore\u2019s Law ladder. With Gaudi 3, that part of the race is narrowing slightly.\nThe new chip uses the same process as H100 and H200. What\u2019s more, instead of\nmoving to 3-nm technology, the coming competitor Blackwell is done on a\nprocess called N4P. TSMC describes N4P as being in the same 5-nm family as N5\nbut delivering an 11 percent performance boost, 22 percent better efficiency,\nand 6 percent higher density.\n\nIn terms of Moore\u2019s Law, the big question is what technology the next\ngeneration of Gaudi, currently code-named Falcon Shores, will use. So far the\nproduct has relied on TSMC technology while Intel gets its foundry business up\nand running. But next year Intel will begin offering its 18A technology to\nfoundry customers and will already be using 20A internally. These two nodes\nbring the next generation of transistor technology, nanosheets, with backside\npower delivery, a combination TSMC is not planning until 2026.\n\nFrom Your Site Articles\n\n  * How We\u2019ll Reach a 1 Trillion Transistor GPU \u203a\n  * Google, Intel, Nvidia Battle in Generative AI Training \u203a\n\nintelnvidiablackwellhoppergaudiamdartificial intelligencellmsai chipsAI chips\n\nSamuel K. Moore\n\nis the senior editor at IEEE Spectrum in charge of semiconductors coverage.\nSee full bio \u2192\n\nThe Conversation (0)\n\nSort by\n\nAerospaceRoboticsGuest ArticleConsumer ElectronicsDIYTelecommunications\n\n## Ukraine Is the First \u201cHackers\u2019 War\u201d\n\n18 hours ago\n\n8 min read\n\nAerospaceNews\n\n## ESA Satellites to Test Razor-Sharp Formation Flying\n\n19 hours ago\n\n3 min read\n\n1\n\nThe InstituteCareersArticle\n\n## Enhance Your Tech and Business Skills During IEEE Education Week\n\n09 Apr 2024\n\n3 min read\n\n## Related Stories\n\nArtificial IntelligenceComputingSemiconductorsNews\n\n## SambaNova\u2019s New Chip Means GPTs for Everyone\n\nComputingArtificial IntelligenceNews\n\n## AI Coding Is Going From Copilot to Autopilot\n\nAerospaceArtificial IntelligenceNews\n\n## AI Generates 3D City Maps From Single Radar Images\n\n", "frontpage": false}
