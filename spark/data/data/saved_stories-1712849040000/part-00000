{"aid": "40000157", "title": "With Gaudi 3, Intel Can Sell AI Accelerators to the PyTorch Masses", "url": "https://www.nextplatform.com/2024/04/09/with-gaudi-3-intel-can-sell-ai-accelerators-to-the-pytorch-masses/", "domain": "nextplatform.com", "votes": 1, "user": "rbanffy", "posted_at": "2024-04-11 09:22:14", "comments": 0, "source_title": "With Gaudi 3, Intel Can Sell AI Accelerators To The PyTorch Masses", "source_text": "With Gaudi 3, Intel Can Sell AI Accelerators To The PyTorch Masses\n\nLatest\n\n  * [ April 10, 2024 ] With MTIA v2 Chip, Meta Can Do AI Training As Well As Inference AI\n  * [ April 10, 2024 ] Gelsinger: With Gaudi 3 and Xeon 6, AI Workloads Will Come Our Way AI\n  * [ April 9, 2024 ] Google Joins The Homegrown Arm Server CPU Club Compute\n  * [ April 9, 2024 ] With Gaudi 3, Intel Can Sell AI Accelerators To The PyTorch Masses AI\n  * [ April 8, 2024 ] Mixed Results For The Datacenter Thundering Thirteen In Q4 Compute\n  * [ April 4, 2024 ] Intel\u2019s Chips No Longer Pay More Than Their Fair Share Of Foundry Costs Compute\n  * [ April 4, 2024 ] Celestial AI Wants To Break The Memory Wall, Fuse HBM With DDR5 Compute\n  * [ April 4, 2024 ] Creating A Greener Edge Edge\n\n# With Gaudi 3, Intel Can Sell AI Accelerators To The PyTorch Masses\n\nApril 9, 2024 Timothy Prickett Morgan AI, Compute 1\n\nWe have said it before, and we will say it again right here: If you can make a\nmatrix math engine that runs the PyTorch framework and the Llama large\nlanguage model, both of which are open source and both of which come out of\nMeta Platforms and both of which will be widely adopted by enterprises, then\nyou can sell that matrix math engine.\n\nWith Intel unveiling its third generation Gaudi AI accelerators today at its\nVision 2024 event in Phoenix, where the company has big foundry operations\nnearby Chandler, the only questions, then, are how many Gaudi 3 accelerators\ncan Intel make, what will it charge for them, and when can it ship them?\n\nThe first question is something that only Intel, which is working with Taiwan\nSemiconductor Manufacturing Co to etch and package the Gaudi 3 accelerators,\nknows. Habana Labs, which was acquired by Intel in December 2019 for $2\nbillion, launched the Gaudi 1 accelerator back in July 2019, which was when\nNvidia\u2019s \u201cVolta\u201d V100 was the competition in a ridiculously smaller AI space.\nHopefully Intel will be more aggressive in making and selling Gaudi 3 chips\nthan it has been with either Gaudi 1 or Gaudi 2, even if it will soon be\nsignificantly behind Nvidia when it comes to raw accelerator performance.\nThere is a chance for Intel to be aggressive and attract the PyTorch crowd,\nwith Llama LLMs and a seemingly zillion other AI models available on Hugging\nFace.\n\nThis is enough to build the foundation of an AI accelerator business \u2013\nprovided that customers trust there will be enough architectural similarities\nbetween Gaudi 3 and the hybrid CPU/NNP design that is coming with \u201cFalcon\nShores\u201d out in late 2024 or early 2025 or so. That launch date for Falcon\nShores, which will sport a variant of HBM3 memory, depends on how you want to\nread the vague roadmap Intel has published for Gaudi line. What we do know is\nthat Falcon Shores merges the Gaudi line with the Max Series GPU line to\ncreate a GPU that has the Ethernet interconnect and matrix math tensor cores\nof Gaudi combined with the X^e vector engines of a \u201cPonte Vecchio\u201d GPU.\n\nAs for the cost of the Gaudi 3 accelerators, the answer is simple. Intel\u2019s\nprice will be directly proportional to the performance of the Nvidia \u201cHopper\u201d\nH100 GPU accelerator that it completes against and the street price that\nHopper H100 GPUs have with 96 GB of HBM3 memory capacity and 3.9 TB/sec of\nbandwidth. As the Hopper H200 variants, which have 141 GB of HBM3e memory\ncapacity and 4.8 TB/sec of bandwidth, start shipping in a few months, the math\nwill shift to a comparison with the H200s. And when the future \u201cBlackwell\u201d\nB100 and B200 GPU accelerators start shipping from Nvidia later this year and\ninto 2025, the Gaudi 3 price will have to adjust accordingly.\n\n### Is The Third Time The Charm?\n\nIt would have been better for Intel, obviously, if the Gaudi 3 accelerator was\nlaunched in the spring of 2022 alongside the Hopper GPU, and in appreciable\nvolumes at that. Here we are two years later, and demand for Nvidia GPUs is so\nstrong that there is never a better time to sell a technology that should\nalready be two years old. Better to launch it now than not launch it at all,\nand at this point, it would be better to get Falcon Shores into the field\nsooner rather than later.\n\nIntel can sell all of Gaudi 3 and Falcon Shores that it can make in the short\nterm. Now is the time to hustle. The timing for Gaudi 3 and Falcon Shores\ncould have been better, but any delays in ramps means billions of dollars in\nsystems sales being deferred and possibly lost as Nvidia ramps mightily and\nAMD starts getting traction, too, with its \u201cAntares\u201d Instinct MI300 series of\nGPU accelerators.\n\nThe Gaudi 3 accelerator is an big evolutionary jump from the Gaudi 2. But the\nbig architectural change is coming with Falcon Shores, but one that will\nlargely be insulated from customers using PyTorch and higher models upon that\nframework. It would have been good for Habana Labs and then Intel if Meta\nPlatforms had chosen the Gaudi accelerators as its AI engines, but clearly\nthat did not happen and the social network is creating its own line of MTIA\naccelerators for AI training and inference and in the interim is buying\nhundreds of thousands of Nvidia GPUs.\n\nLet\u2019s walk through the Gaudi 1 and Gaudi 2 architectures and then take a look\nat the Gaudi 3 accelerator, the system designs using it, and the various\npackagings for Gaudi 3 that Intel will bring to market. We will do a separate\nstory comparing the price/performance of current Nvidia and AMD GPUs to the\nGaudi 3 line.\n\n### Made To Do Math\n\nAs is the case with other AI accelerators that include matrix math units and\ntensor cores (which are a special kind of matrix math unit), the original\nGaudi 1 accelerator could in theory be used to accelerator other kinds of\nworkloads, including HPC simulation and modeling and even data analytics. It\nis just a matter of creating the software stack for it. But in this case, as\nis also the case with other mixed precision AI accelerators, the availability\nof mixed \u2013 and importantly, low precision \u2013 floating point and integer math\nmakes is most suited to AI training and inference.\n\nThe Gaudi 1 accelerator and software stack that Habana Labs put together,\nwhich compelled Intel to buy the company even though it had previously bought\nAI accelerator maker Nervana Systems for $350 million back in August 2016.\n(Intel didn\u2019t productize the Nervana NNPs until November 2019, and a month\nlater it bought Habana Labs. Go figure.)\n\nHere is the block diagram of the Gaudi 1:\n\nThe architecture includes a generic GEMM Engine matrix math engine, plus eight\ntensor processing cores (TPCs) with their own local memory. The GEMM Engine\ndid the math operations for fully connected layers, convolutions, and batched\nGEMM processing in integer format at 16-bit precision, while the TPC is a\nspecial SIMD processor made to handle other machine learning operations. The\nTPCs process data in FP32, BF16, INT32, INT16, INT8, UINT32, UINT16, and UINT8\nformats.\n\nThe SRAM memory shared by the TPCs (and presumably also the GEMM unit) weighed\nin at 24 MB and had 1 TB/sec of bandwidth; the size and the bandwidth of the\nlocal memories on the TPCs was never divulged as far as we know.\n\nThe Gaudi 2 was etched in TSMC\u2019s 16 nanometer process and used 2.5D CoWoS\npackaging to link four stacks of HBM2 memory, at 8 GB per stack, for a total\nof 32 GB of memory with 1 TB/sec of aggregate bandwidth. The chip also had ten\n100 Gb/sec Ethernet RoCE ports to interconnect Gaudi processors within a\nserver node and across server nodes in a cluster, with a maximum of 128 fully\nconnected nodes. The device also had a PCI-Express 4.0 x16 controller to reach\nout to host CPUs.\n\nWith the Gaudi 2, which launched in May 2022 and which started shipping in\nvolume and on the Intel Developer Cloud in June 2023, the Habana team inside\nof Intel cranked it all up, thanks in large part to a move to TSMC\u2019s 7\nnanometer etching.\n\nThe shared SRAM memory was boosted from 24 MB to 48 MB. The number of TPCs was\nincreased by 3X to 24 units, and the number of GEMM units \u2013 now called the\nMatrix Math Engine \u2013 was doubled. The Ethernet port count went up by 2.4X to\n24 ports, which radically increased the scalability of Gaudi clusters, and\nmedia decoders were added to do pre-processing for AI vision applications. The\nTPCs supported FP32, TF32, BF16, FP16, and FP8 (both E4M3 and E5M2 variations)\ndata formats. The MME unit does matrix math and accumulates to FP32 format,\naccording to the documentation. (It is not clear if the GEMM is the same as\nthe MME. But if so, it does matrix math on 16-bit integers and accumulates to\n32-bit floating point.) The Gaudi 2 has the same PCI-Express 4.0 x16 link out\nto hosts, but has six HBM2E memory controllers and six HBM2E memory stacks at\n16 GB each, for a total of 96 GB of capacity and 2.4 TB/sec of bandwidth\nacross those six stacks.\n\nThat brings us to Gaudi 3 and the shift to TSMC 5 nanometer etching.\n\nHere is what the Gaudi 3 accelerator looks like:\n\nAnd here are the specs:\n\nThe Gaudi 3 is a much beefier device. The TPC design is in its fifth\ngeneration, according to Eitan Medina, chief operating officer at Habana Labs,\nwho has that same role inside of Intel. There are 64 of these TPCs on the\nGaudi 3 device, which is an increase of 50 percent over the Gaudi 2. There are\nalso eight MMEs, a factor of 4X more than the Gaudi 2.\n\nThis is a simplified diagram, according to Medina, but as you might expect\nthese days, there are two identical Gaudi 3 chiplets that are rotated 180\ndegrees from each other that implement half of the two dozen 200 Gb/sec\nEthernet ports, half of the media engines, and presumably one PCI-Express 5.0\nx8 port for each tile that presumably can be ganged up to make a single\nvirtual PCI-Express 5.0 x16 port shown on the simplified block diagram.\n\nThere is 48 MB of SRAM shared memory on each Gaudi 3 tile, and two blocks of\n16 TPCs and two blocks of two MMEs. The 96 MB of SRAM has 12.8 TB/sec of\naggregate bandwidth inside the Gaudi 3 complex. There are eight HBM2E memory\nstacks with a total of 128 GB of capacity and 3.7 TB/sec of bandwidth.\n\nAs far as we know, the TPCs in the Gaudi 3 device support the same FP32, TF32,\nBF16, FP16, and FP8 data formats as Gaudi 2 does, and does not support FP4\nprecision as the new Blackwell GPUs from Nvidia will and that the existing\nHopper GPUs from Nvidia do not.\n\n### Scaling Up Gaudi 3 In A Node And Out Across A Cluster\n\nAn AI accelerator is only as good as the clusters that can be built from it,\nand as has been the case from the beginning, the Gaudi team is integrating\nEthernet juiced with RDMA and other lossless features in the RoCE protocol\nextensions to get it done without InfiniBand.\n\nHere are the speeds and feeds of clusters at the FP8 precision that is\nsometimes used for AI training and that is increasingly being used for AI\ninference:\n\nAn eight-way Gaudi 3 node is rated at 14.7 petaflops at FP8 precision, and an\neight-way Hopper H100 node is rated at 15.8 petaflops at FP8 precision without\n2:1 sparsity support turned on. That is spitting distance given that not all\napplications can support that 2:1 sparsity. (With dense matrices, sparsity\nsupport don\u2019t do nothing.) Those original H100s from Nvidia also had only 80\nGB of HBM3 capacity but 3.35 TB/sec of bandwidth. Intel is getting more\ncapacity and more bandwidth by sticking with cheaper HBM2E, which is\ninteresting. The H200 from Nvidia has 141 GB of HBM3E and 4.8 TB/sec of\nbandwidth, which is 10.2 percent more memory capacity and 29.7 percent more\nbandwidth. (Ah, but at what price? The additional cost of a Grace CPU maybe?)\n\nThe Gaudi 3 accelerators inside of the nodes are connected using the same OSFP\nlinks to the outside world as happened with the Gaudi 2 designs, but in this\ncase the doubling of the speed means that Intel has had to add retimers\nbetween the Ethernet ports on the Gaudi 3 cards and the six 800 Gb/sec OSFP\nports that come out of the back of the system board. Of the 24 ports on each\nGaudi 3, 21 of them are used to make a high-bandwidth all-to-all network\nlinking those Gaudi 3 devices tightly to each other. Like this:\n\nAs you scale, you build a sub-cluster with sixteen of these eight-way Gaudi 3\nnodes, with three leaf switches \u2013 generally based on the 51.2 Tb/sec \u201cTomahawk\n5\u201d StrataXGS switch ASICs from Broadcom, according to Medina \u2013 that have half\nof their 64 ports running at 800 GB/sec pointing down to the servers and half\nof their ports pointing up to the spine network. You need three leaf switches\nto do the trick:\n\nTo get to 4,096 Gaudi 3 accelerators across 512 server nodes, you build 32\nsub-clusters and you cross link the 96 leaf switches with a three banks of\nsixteen spine switches, which will give you three different paths to link any\nGaudi 3 to any other Gaudi 3 through two layers of network. Like this:\n\nThis chart should have labeled those as Sub Clusters, not Clusters. But you\nget the idea.\n\nNow let\u2019s touch on performance briefly, which we will get into in more depth\nseparately. Medina showed off performance results that showed the Gaudi 3 was\nbetween 1.4X and 1.7X faster at training Llama2 7B and 13B and GPT-3 175B\nmodels than the Nvidia H100, and anywhere from 90 percent to 4X the\nperformance of the H100 on inferencing with Llama 2 7B and 70B and Falcon\n180B.\n\n### Form Factors Follow Function\n\nThat leaves us with the form factors. Here is the OAM version of the Gaudi 3\naccelerator, which in theory should be preferred by Microsoft and Meta\nPlatforms, who created the OAM form factor and open sourced it back in March\n2019. Take a gander:\n\nAnd here is the universal baseboard that takes eight of these and creates a\nsingle motherboard with all of those Gaudi 3 devices interconnected and six\nOSFP ports running at 800 Gb/sec to the outside world:\n\nThis UBB system is roughly analogous to Nvidia\u2019s HGX system boards, which have\nbeen used for its A100, H100, and soon its B100 accelerators.\n\nIntel is also going to deliver a PCI-Express 5.0 x16 variant of the Gaudi 3,\nwith passive cooling so it can be plugged directly into any servers that\nsupport those slots in a dual-width form factor:\n\nThe air-cooled Gaudi 3 devices with those massive skyscraper heatsinks have\nbeen sampling for a few weeks and the liquid-cooled ones with cold plates\nmounted on them are sampling here in Q2 2024. The air-cooled Gaudi 3s will be\nin volume production in Q3 and the liquid-cooled ones will be in volume\nproduction in Q4. Dell, Hewlett Packard Enterprise, Lenovo, and Supermicro\nhave all committed to make OEM systems based on these Gaudi 3 accelerators,\nand Gaudi 3 will also be available through the Intel Developer Cloud. Gaudi 2\nwas only in machines built by Supermicro. So the OEMs are taking Gaudi 3 more\nseriously than they did Gaudi 2 \u2014 and that has a lot to do with the low\nallocations the OEMs got during the Hopper GPU rollout.\n\n#### Sign up to our Newsletter\n\nFeaturing highlights, analysis, and stories from the week directly from us to\nyour inbox with nothing in between. Subscribe now\n\n### Related Articles\n\nCompute\n\n### Intel Puts Its Xe GPU Stakes In The Ground\n\nSeptember 2, 2020 Jeffrey Burt Compute 1\n\nFor about a decade, Intel has sold GPUs, in recent years with its integrated\nCPU-GPU devices used in client and entry servers. But for the most part, at\nits heart and despite acquisitions of FPGA and NNP makers, Intel is still a\nCPU maker. Nvidia and AMD have for years ...\n\nHPC\n\n### Putting TACC\u2019s \u201cStampede3\u201d Through The HBM Paces\n\nNovember 12, 2023 Ken Strandberg HPC 1\n\nWhen you host the workhorse supercomputers of the National Science Foundation,\nyou strive to provide the best possible solutions for your scientists. The\nStampede and Frontera systems at the Texas Advanced Computing Center (TACC) at\nthe University of Texas at Austin have been those workhorses. TACC continues\nto get National ...\n\nAI\n\n### The Performance Of MLPerf As A Ubiquitous Benchmark Is Lacking\n\nApril 8, 2022 Jeffrey Burt AI 0\n\nIndustry benchmarks are important because, no matter that comparisons are\nodious, IT organizations nonetheless have to make them to plot out the\narchitectures of their future systems. The MLPerf suite of AI benchmarks\ncreated by Google, Baidu, Harvard University, Stanford University, and the\nUniversity of California at Berkeley had a ...\n\n#### 1 Comment\n\n  1. Calamity Jim says:\n\nApril 9, 2024 at 2:38 pm\n\nSeeing how capable this Gaudi 3 is (a bit more than twice Gaudi2, or 4x\nGaudi1), I got to wonder what Intel still needs Falcon Shores for (why not\nGaudi 4, 5, 6, ...)? But then, reading this article, it dawned on me: it\ndoesn\u2019t have FP64! Merging the Xe GPU Max\u2019s FP64 vector engines with Gaudi\u2019s\nFP32-FP16-FP8 tensor and matrix units will likely be challenging, but is\nneeded to fully compete with the MI300s and GB200s of the world. The result\nshould be most interesting IMHO.\n\nReply\n\n### Leave a Reply Cancel reply\n\nThis site uses Akismet to reduce spam. Learn how your comment data is\nprocessed.\n\n###### About\n\nThe Next Platform is published by Stackhouse Publishing Inc in partnership\nwith the UK\u2019s top technology publication, The Register.\n\nIt offers in-depth coverage of high-end computing at large enterprises,\nsupercomputing centers, hyperscale data centers, and public clouds. Read\nmore...\n\n###### Newsletter\n\nFeaturing highlights, analysis, and stories from the week directly from us to\nyour inbox with nothing in between. Subscribe now\n\nAll Content Copyright The Next Platform\n\n", "frontpage": false}
