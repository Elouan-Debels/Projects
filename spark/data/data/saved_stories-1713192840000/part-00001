{"aid": "40038197", "title": "G\u00f6del Scheduler: a unified scheduler for online and offline workloads", "url": "https://www.cncf.io/blog/2024/04/02/godel-scheduler-open-sourced-a-unified-scheduler-for-online-and-offline-workloads/", "domain": "cncf.io", "votes": 1, "user": "xxisxuxu", "posted_at": "2024-04-15 08:51:48", "comments": 0, "source_title": "G\u00f6del Scheduler open-sourced: a unified scheduler for online and offline workloads", "source_text": "G\u00f6del Scheduler open-sourced: a unified scheduler for online and offline workloads | CNCF\n\nSkip to content Accessibility help\n\nBlog\n\n/\n\nMember Post\n\n# G\u00f6del Scheduler open-sourced: a unified scheduler for online and offline\nworkloads\n\nBy ByteDance April 2, 2024\n\nMember post by ByteDance\n\n## Background\n\nSince its open-source release in 2014, Kubernetes has rapidly become the de\nfacto standard for container orchestration. The infrastructure team at\nByteDance adopted Kubernetes early on to build our private cloud platform.\nOver the years, ByteDance\u2019s rapid growth across various business lines,\nincluding microservices, recommendation/advertising/search services, machine\nlearning & big data, and storage, has led to a substantial increase in the\ndemand for computing resources.\n\nInitially, ByteDance managed its online and offline workloads with separate\nresource pools, each dedicated to distinct business segments. To accommodate\nthe surge in online business demands during significant holidays and major\nevents, the infrastructure team usually needed to plan ahead by reallocating\nresources from offline to online pools to bolster the capacity for handling\nincreased online activities. While this temporary fix satisfied immediate\nrequirements, the inter-pool resource borrowing process proved to be time-\nconsuming, operationally heavy, and inefficient. Furthermore, maintaining\nseparate resource pools for online and offline workloads resulted in\nsignificant colocation costs, leaving little scope for enhancing resource\nutilization. Therefore, the infrastructure team sought to implement a unified\nsystem for scheduling and managing both online and offline workloads. This\ninitiative aimed to facilitate resource pooling, enhance resource utilization\nand elasticity, optimize costs and user experiences, and alleviate operational\nburdens.\n\n## Practice of Unified Scheduling\n\n### Enhancement beyond Kubernetes Default Scheduler:\n\nSince extensive use of Kubernetes in 2018, ByteDance continuously optimized\nvarious components of Kubernetes for functionality and performance. However,\nwith the containerization of recommendation/advertising/search services in\n2019, the native Kubernetes scheduler, in terms of both functionality and\nperformance, was farther away from meeting ByteDance\u2019s business requirements.\nIn terms of functionality, more granular resource scheduling capabilities and\nflexible preemption strategies were required. In terms of performance, the\nnative Kubernetes default scheduler could only achieve a scheduling throughput\nof around 10 pods per second in a cluster of 5000 nodes, often causing\nbusiness upgrades to be bottlenecked, far from meeting the requirements.\nTherefore, the team introduced a number of key optimizations to the Kubernetes\ndefault scheduler, including:\n\n### Functionality:\n\n  * Extended the scheduling capabilities to support non-native resources, such as memory bandwidth and network bandwidth.\n  * Supported for micro-topology scheduling.\n  * Refactored preemption implementation by providing a pluggable preemption framework to support extending preemption capabilities.\n\n### Performance:\n\n  * Optimized the data synchronization mechanism between Scheduler cache and Snapshot by refactoring data structure and further strengthening the concept of incremental update between snapshots.\n  * Cached scheduling results for homogenous scheduling units to reduce redundant calculations and improve efficiency.\n  * Optimized preemption implementation by reorganizing preemption-related data structures, applying pruning timely, and reducing unnecessary computation.\n\nBy implementing the aforementioned optimizations, we successfully enhanced our\ncontainerization capabilities to meet ByteDance\u2019s rapidly expanding needs.\nThis led to a remarkable 30-fold increase in scheduling throughput. That is,\nin a cluster comprising 10,000 nodes, we consistently achieved a scheduling\nrate of 300 pods per second.\n\n## G\u00f6del Scheduler\n\nIn 2020, ByteDance initiated a unified scheduling and resource management\nproject for both online and offline business operations. The objective was to\nenhance overall resource utilization, improve operational efficiency, and\nreduce maintenance overheads. Initially, the plan involved managing both\nonline and offline tasks through a singular scheduling system. However, this\napproach presented challenges, primarily due to the intricate nature of\noffline scheduling, which differed markedly from online processes, especially\nin its demand for high throughput.\n\nThe native Kubernetes scheduler, primarily designed for Pod-level scheduling,\nwas somewhat limited in its support for more complex \u201cJob\u201d scheduling\nsemantics and encountered performance limitations when dealing with these\nhigher-level demands. To effectively address these unique requirements and to\nbetter align with the diverse operational needs of ByteDance, the decision was\nmade to develop a bespoke, in-house distributed scheduler. This led to the\ncreation of the G\u00f6del Scheduler, specifically tailored to integrate with the\nKubernetes system and to handle the demanding and varied scheduling needs of\nByteDance\u2019s expansive and evolving business landscape.\n\nThe G\u00f6del Scheduler is a distributed system crafted to consolidate the\nscheduling of both online and offline workloads. This scheduler is an\nenhancement of the Kubernetes (K8s) Scheduler, designed to augment scalability\nand improve scheduling quality. It is adept at fulfilling the diverse\nfunctional and performance demands of ByteDance\u2019s online and offline\noperations. Key features of the G\u00f6del Scheduler include:\n\n  * Optimistic Concurrency: It incorporates optimistic concurrency concepts, moving the most time-consuming unit-to-node matching (filtering and scoring) to the scheduler component. This allows for concurrent execution and improves scheduling throughput in large-scale clusters.\n  * Two-Layer Scheduling Abstraction (Unit and Pod) and Framework: This provides more flexible batch scheduling capabilities, better supporting offline operations while also improving scheduling throughput and system scalability. The extended framework handles special scenarios more effectively.\n  * Rich Functionality and High Performance: It meets the demands of various operations including online, offline (batch and stream), and training tasks, achieving true unified scheduling.\n  * Compatibility with the Kubernetes Ecosystem: It can serve as an alternative to the K8s Scheduler, but due to performance and architectural optimizations, its framework interface is not entirely the same as the K8s Scheduler. However, its extensibility remains unaffected, and it can implement scheduling plugins like Kubernetes.\n\nBelow is the architecture diagram of G\u00f6del Scheduler.\n\nAs outlined, the G\u00f6del Scheduler consists of three primary components: the\nDispatcher, the Scheduler, and the Binder. Key to its architecture is the\nScheduler component, which is typically deployed in multiple shards to\nfacilitate optimistic concurrency scheduling. This multi-shard deployment\nenhances its efficiency and scalability. On the other hand, the Dispatcher and\nthe Binder are each deployed as single instances, a configuration that suits\ntheir specific roles and responsibilities within the G\u00f6del Scheduler system.\n\n### Dispatcher\n\nThe Dispatcher plays a pivotal role in managing application queuing,\ndistribution, and node partitioning. It is comprised of several key\ncomponents:\n\n  1. Sort Policy Manager: This module handles the queuing of applications. Currently, it implements FIFO and DRF/FairShare queuing strategies, the latter still pending production use. Future enhancements will introduce more sophisticated queuing strategies, including those based on priority values.\n  2. Dispatching Policy Manager: Its primary function is to allocate applications across various Scheduler instances. At present, the LoadBalance strategy is employed as the default. Future updates aim to make this feature more versatile and plugin-based.\n  3. Node Shuffler: This component is tasked with organizing cluster nodes relative to the number of Scheduler instances. It assigns each node to a specific node partition, with each Scheduler instance overseeing one partition. During the scheduling process, a Scheduler first considers nodes within its partition before exploring nodes in other partitions. This arrangement is dynamically adjusted in response to changes in node availability or Scheduler count.\n  4. Partition Rules: Currently, the system strives for an even distribution of nodes among Scheduler instances. Plans are underway to expand these partition strategies, enhancing their configurability.\n  5. Scheduler Maintainer: This element is responsible for monitoring the status of Scheduler instances. It tracks aspects like health status, workload, and the node count within each partition.\n  6. Reconciler: Operating periodically, the Reconciler oversees the status of various elements like Pods, Nodes, Schedulers, and SchedulingUnits. It addresses any errors, discrepancies, or deficiencies, ensuring system integrity and performance.\n\n### Scheduler\n\nThe Scheduler plays a critical role in the decision-making process for\nscheduling and preempting applications, although it does not execute these\ndecisions itself (that task is handled by the Binder). It operates on a two-\ntier framework: the Unit Scheduling Framework and the Pod Scheduling\nFramework. The entire scheduling procedure is segmented into three principal\nphases: Node Organizing, Unit Scheduling, and Unit Preempting.\n\n  1. Node Organizing: This phase involves filtering and sorting nodes to streamline the scheduling process and enhance its quality. It consists of two types of plugins:\n\n  * Locating Plugins: These filter nodes are based on specific application information.\n  * Grouping Plugins: These group nodes according to available resources or Job-level affinities.\n\n  2. Unit Scheduling: In this stage, nodes are matched and scored in alignment with application requests that have been filtered through the Node Organizing plugins. This process is analogous to the Kubernetes (K8s) Scheduler framework, encompassing:\n\n  * Filtering Plugins: These filter nodes are based on the requisites of the application requests.\n  * Scoring Plugins: These assign scores to nodes that have been filtered in the previous step.\n\n  3. Unit Preempting: If suitable nodes are not found during the Unit Scheduling phase, the Scheduler progresses to the preemption phase. Here, it tries to free up resources by preempting running application instances to make room for new ones. This phase includes:\n\n  * Victims Searching: Identifying potential victim applications that can be preempted.\n  * Candidates Sorting: Sorting both nodes and potential victims to identify the most appropriate choices for preemption.\n\n### Binder\n\nThe Binder plays a crucial role in the final stages of the scheduling process,\nfocusing on conflict detection, preemptive operations, and executing the\nbinding of applications to resources. It comprises three main components:\nConflictResolver, PreemptionOperator, and UnitBinder.\n\n  1. ConflictResolver: This component is tasked with detecting concurrent conflicts in the scheduling process. It operates in two modes:\n\n  * Cross Node Conflict Resolver: Checks for conflicts that might occur across different nodes.\n  * Single Node Conflict Resolver: Identifies conflicts within a single node. If any conflict is detected, the application is immediately rejected and rescheduled.\n\n  2. PreemptionOperator: In scenarios where no conflict exists but preemption is necessary, this operator takes charge. It executes the preemption by deleting the victims (applications or processes that need to be terminated to free up resources) and then awaits the final scheduling.\n  3. UnitBinder: This part of the Binder is responsible for the preparatory work required before binding, such as dynamically creating storage volumes, and then carries out the actual binding operation, linking applications to the designated resources.\n\nNoteworthy, the current version of the Binder integrates a PodGroup\ncontroller. This controller is responsible for managing the state and\nlifecycle of PodGroups. However, it\u2019s important to note that in a future\nversion we plan to remove this functionality from the Binder, transitioning it\ninto an independent controller.\n\n## Experience\n\nOver the past two years, the G\u00f6del Scheduler has been a cornerstone within\nByteDance, offering a wealth of scheduling features and semantics. It has\nefficiently and reliably supported the operations of ByteDance\u2019s diverse and\ncomplex business workloads.\n\nBuilding upon the foundation of architectural enhancements, ByteDance has\nimplemented profound performance optimizations drawing from its experience\nwith the Kubernetes native scheduler. Integrated with ByteDance\u2019s internally\nrefined Kubernetes system, the G\u00f6del Scheduler now boasts an impressive\nthroughput: 2000+ pods/s in a single shard and 5000+ pods/s across multiple\nshards. ByteDance\u2019s ongoing efforts to expand single-cluster capacity have\nculminated in their largest prod cluster reaching over 20,000 nodes and more\nthan 1,000,000 pods.\n\nAfter years of thorough internal practices and enhancements within ByteDance,\nG\u00f6del Scheduler has achieved a state of relative stability. In 2023, the top-\nnotch cloud computing conference, SoCC, accepted our paper on G\u00f6del Scheduler,\nhighlighting ByteDance\u2019s unified approach to large-scale resource management\nand scheduling. The RD team was also invited to present the work at the\nconference. For those interested, the G\u00f6del Scheduler paper is available at\nhttps://dl.acm.org/doi/10.1145/3620678.3624663.\n\nWith a commitment to contributing to the open-source community, the Bytedance\nteam decided to open-source the G\u00f6del Scheduler, offering a new scheduling\nsolution that enhances cloud-native experiences for both online and offline\nservices through its outstanding performance and comprehensive scheduling\ncapabilities.\n\n## Future Work\n\nLooking ahead, ByteDance is committed to the continual development of the\nG\u00f6del Scheduler, focusing on enriching its features and enhancing its\nscalability. A significant area of attention will be optimizing the scheduling\nthroughput in specific challenging scenarios, such as those involving high\nrates of deployment and frequent preemptions. Through innovative rescheduling\nstrategies, ByteDance aims to tackle the intricate balance between maintaining\nscheduling performance and enhancing its quality. The overarching goal is to\nnot only preserve the current scheduling throughput but also to substantially\nelevate the quality of scheduling.\n\nMoreover, ByteDance places a high priority on ecosystem development. Efforts\nwill be made to ensure G\u00f6del Scheduler\u2019s compatibility with leading systems\nand frameworks used in various business applications. This initiative will\ninclude integration with prominent big data and machine learning frameworks,\naccompanied by practical usage examples and comprehensive documentation.\n\nTo keep the community engaged and informed, a detailed roadmap for the G\u00f6del\nScheduler will be methodically laid out and made available on the G\u00f6del\nScheduler Repository. This will provide an opportunity for interested parties\nto track progress, contribute, and become active participants in the project.\n\nWhile the G\u00f6del Scheduler has undergone numerous iterations within ByteDance,\nbeen rigorously tested in various scenarios, and demonstrated its\neffectiveness, ByteDance acknowledges that there is still considerable\npotential for advancement in terms of generality and standardization.\nByteDance warmly invites and encourages members of the community to join in\nthe development of the G\u00f6del Scheduler, believing that collaborative efforts\nwill lead to even greater improvements and innovations.\n\nG\u00f6del Scheduler Project Repository: https://github.com/kubewharf/godel-\nscheduler\n\nShare\n\n## Other posts to check out\n\n### What is infrastructure from code?\n\nApril 10, 2024\n\n### Cloud Native Computing Foundation Announces Trend Micro has Doubled Down\non Cloud Native with Gold Membership Upgrade\n\nApril 10, 2024\n\n### K8s Benchmark Report: are organizations meeting NSA hardening checks?\n\nApril 9, 2024\n\nSubscribe for updates, event info, webinars, and the latest community news\n\nBy submitting this form, you acknowledge that your information is subject to\nThe Linux Foundation's Privacy Policy.\n\nJoin Now\n\nAll CNCF Sites\n\nCopyright \u00a9 2024 The Linux Foundation\u00ae. All rights reserved. The Linux\nFoundation has registered trademarks and uses trademarks. For a list of\ntrademarks of The Linux Foundation, please see our Trademark Usage page. Linux\nis a registered trademark of Linus Torvalds. Privacy Policy and Terms of Use.\n\nAccessibility Statement Submit an issue with this page\n\n", "frontpage": false}
