{"aid": "40000489", "title": "PostgreSQL Stories: A simple query with a big problem", "url": "https://render.com/blog/postgresql-simple-query-big-problem", "domain": "render.com", "votes": 1, "user": "rrampage", "posted_at": "2024-04-11 10:11:59", "comments": 0, "source_title": "PostgreSQL Stories: A simple query with a big problem | Render Blog", "source_text": "PostgreSQL Stories: A simple query with a big problem | Render Blog\n\nSign In\n\nGet Started\n\nApril 03, 2024\n\n# PostgreSQL Stories: A simple query with a big problem\n\nEric Fritz\n\nApril 03, 2024\n\nEric Fritz\n\nRender takes your infrastructure problems away and gives you a battle-tested,\npowerful, and cost-effective cloud with an outstanding developer experience.\nFocus on building your apps, shipping fast, and delighting your customers, and\nleave your cloud infrastructure to us.\n\nTry Render Free\n\nAt Render, we love PostgreSQL. As with any technology, it has its sharp edges.\nMost of the time, PostgreSQL invisibly optimizes our queries, but sometimes, a\nquery wreaks havoc. It\u2019s a fact of life for people who work with databases.\nThis February, our staging pipeline got blocked due to what looked like a\nsimple query:\n\n    \n    \n    SELECT e.id FROM events e JOIN postgres_dbs db ON (e.data ->> 'serviceId') = db.database_id LIMIT 1;\n\nSQL\n\nBoth sides of the join had relevant indexes, and the query included a LIMIT 1,\nbut the query was still taking hours to run. Scratching your head? We were\ntoo. (If you have a hypothesis already... we\u2019re hiring.) Here\u2019s the story of\nhow we identified, fixed, and debugged the core problems behind this query.\n\n## The canary: a blocked staging pipeline\n\nLet\u2019s start from the first signs of trouble. One day, we noticed that\ndeployments to our staging cluster had been blocked for hours. Our end-to-end\ntests hadn\u2019t run. Among other things, these tests create, modify, and destroy\ndatabase instances, and thus require \u201ctest\u201d database instances to be set up.\nThe blockage meant we couldn\u2019t deploy to production, because we require our\nproduction builds to first deploy to staging. The blocked tests also signaled\na possible problem in new code we\u2019d committed. Time to dig in.\n\n## First clues: hanging migration, spiking DB metrics\n\nWe opened an internal incident to investigate and unblock the staging deploy.\nWe quickly found two clues:\n\n  * A failing migration step: In the previous five deployment attempts, a specific database migration step had hung and never finished. We saw a timeout, but there were no error logs that pointed to a specific problem in code.\n  * Spiking memory and CPU usage: Graphs showed that around 8:30 that morning, free memory in the associated database dropped from a steady state of 20% to nearly 0%, while CPU utilization doubled.\n\nIn the staging database, free memory droppedCPU utilization doubled\n\n> Note: Each Render PostgreSQL instance has a timeline and metrics view that\n> shows the same information as the graphs here.\n\nSpikes in resource usage often occur when more users do more work during\nbusiness hours. However, this database lived in our staging environment, which\nhas a small, stable set of users and use cases. Thus, our first hypothesis was\nthat a recent code change had caused a performance regression.\n\n## Hard evidence: a four-hour-old transaction\n\nOf course, a code change was not the only possibility. We stepped back to\nsurvey the scene and gather more clues.\n\n  * Was it a problem with the environment itself? We logged into staging and manually ran a few basic operations. These operations worked.\n  * Was it a random problem, or predictable? We observed that the deployment pipeline always failed in the same way. Not all database queries were affected\u2014it was always the migration stage that would hang. This reinforced our sense that the problem was localized.\n\nWe scanned various charts in our internal metrics dashboard. One chart caught\nour attention: the age of the oldest open transaction. Since around 8:30 AM,\nthe age of the oldest open transaction had grown from single-digit seconds to\nover four hours.\n\nThe oldest transaction age had steadily increased\n\n> Note: This metric can also be fetched directly from PostgreSQL via the\n> pg_stat_activity system view:\n>  \n>  \n>     SELECT NOW() - MIN(xact_start) AS oldest_transaction_age FROM\n> pg_stat_activity;\n>\n> SQL\n\nIn general, it\u2019s best to keep transactions short. A longer transaction can be\nvalid for specific use cases, but it wasn\u2019t valid for us here. Nowhere in our\nplatform did we expect a transactional unit of work to process data, or to sit\nidle, for hours.\n\n## Aside: Why open transactions can create problems\n\nWhat\u2019s bad about an open transaction? It can cause symptoms similar to a query\nthat consumes a lot of CPU or memory. An open transaction can:\n\n  * Block other queries that need the same locks: Long-running transactions may hold table or row-level locks. This can block concurrent queries that need to read or update the same rows, and schema update operations that need exclusive access to a catalog object.\n  * Lead to worse query performance across the board. This is a more insidious side effect. An open transaction can interfere with critical processes such as periodic autovacuum operations and concurrent index creation, both of which support efficient queries. Autovacuum operations help maintain query performance by removing old versions of rows that will never be returned, so new queries can scan only relevant rows.\n  * Cause a database to shut down unexpectedly. Vacuuming does much more than free up space from old rows. If vacuuming is blocked for too long, a PostgreSQL instance can suffer a transaction identifier wraparound failure, in which the instance shuts itself down to prevent critical data loss. If this happens, you need emergency manual intervention to get the instance running again.\n\n## pg_stat_activity finds a suspect\n\nThe four-hour-old transaction was a big clue. Now we wanted to figure out\nwhich query was causing this. For this task, we revisited a trusted resource,\nthe pg_stat_activity system view. pg_stat_activity shows the activity of all\ncurrent running processes on the database instance\u2014both client backend\nprocesses as well as PostgreSQL-coordinated background processes. Using this\nview, we can easily find the set of workloads that are taking a long time to\ncomplete. Here\u2019s the query we ran:\n\n    \n    \n    WITH activity_with_age AS ( SELECT pid, query, NOW() - query_start AS query_age, query_start, state, wait_event_type, wait_event FROM pg_stat_activity ) SELECT * FROM activity_with_age WHERE state = 'active' AND query_age > '5 minutes'::interval ORDER BY query_age DESC, pid;\n\nSQL\n\nThis query returns metadata about each query that's been in the active state\nfor over five minutes. We narrowed our search to active queries because we'd\nseen an increase in both CPU utilization and maximum transaction age, and thus\nsuspected the long-lived query was not idle.\n\n> Note: In general, queries that sit in the idle in transaction state for a\n> long time can also cause similar changes in database behavior. To identify\n> these situations, you can slightly vary this query.\n\nFor this query, our results looked like this:\n\n    \n    \n    -[ RECORD 1 ]---+----------------------------------------------------------------------------------------------------- pid | 311124 query | SELECT e.id FROM events e JOIN postgres_dbs db ON (e.data ->> 'serviceId') = db.database_id LIMIT 1; query_age | 04:12:58.744124 query_start | 2024-02-28 08:40:03.78399-07 state | active wait_event_type | IO wait_event | DataFileRead -[ RECORD 2 ]---+----------------------------------------------------------------------------------------------------- pid | 313359 query | ... omitted ... query_age | 00:06:11.175428 query_start | 2024-02-28 12:46:51.04981-07 state | active wait_event_type | Lock wait_event | relation -[ RECORD 3 ]---+----------------------------------------------------------------------------------------------------- pid | ...\n\nUndefined\n\nIn this result set, we saw several long-running queries. The oldest query\nstarted around 8:40am\u2014the time our database metrics started going wonky. This\nseemed to be our culprit:\n\n    \n    \n    SELECT e.id FROM events e JOIN postgres_dbs db ON (e.data ->> 'serviceId') = db.database_id LIMIT 1;\n\nSQL\n\nHowever, this query looks innocuous. It has no tell-tale signs of trouble:\n\n  * It\u2019s simple: it doesn\u2019t have dozen-way joins, complex common table expressions that materialize large intermediate result sets, or window queries that slice-and-dice billions of rows.\n  * It\u2019s indexed: We verified that both sides of the join condition seemed properly indexed on their respective tables.\n  * It\u2019s bounded: The query has a LIMIT 1 to ensure it's not bringing back more data than needed.\n\nAt this point, a sensible theory could be that this culprit is a red herring\nand is itself blocked on another workload. We ruled out this hypothesis with\ntwo observations:\n\n  * First, this was the oldest recorded query actively running: it was unlikely another query was holding up its progress.\n  * Second, the \u201cwait events\u201d associated with the query showed that it was still fetching data from disk. In other words, the query was not blocked: it was actively doing work, just very slowly.\n\nNow we had a prime suspect. But we needed to prove guilt.\n\n## pg_blocking_pids() proves guilt\n\nThis mystery began with hanging migrations. We now needed to prove this\nsuspected query was preventing migrations from finishing. At this point, we\nonly knew the migration step in our staging pipeline had hung. We didn\u2019t know\nwhich specific migration(s) had hung. In fact, we were a bit puzzled as to how\nmigrations could be affected at all: the suspected query joins the events and\npostgres_dbs tables, but we hadn\u2019t changed the schema for the events or\npostgres_dbs tables in our recent commits. Our best friend pg_stat_activity\nonce again came to the rescue. We guessed the suspected query was holding a\nlock that was needed by a migration. So we queried pg_stat_activity to show\nall backend processes that are waiting on locks held by another backend\nprocess:\n\n    \n    \n    SELECT activity.pid, activity.usename, activity.query, blocking.pid AS blocking_id, blocking.query AS blocking_query FROM pg_stat_activity AS activity JOIN pg_stat_activity AS blocking ON blocking.pid = ANY(pg_blocking_pids(activity.pid));\n\nSQL\n\nA few notes about this query:\n\n  * This query uses the pg_blocking_pids function to self-join active queries and pair blocked queries with their blocker.\n  * pg_blocking_pids returns the backend processes that hold a lock (or are closer to the front of the queue for a lock) that is needed by the blocked query.\n\nThis query produced the following results:\n\n    \n    \n    -[ RECORD 1 ]--+----------------------------------------------------------------------------------------------------- pid | 313674 usename | postgres query | CREATE OR REPLACE VIEW postgres_dbs AS (SELECT * FROM all_postgres_dbs WHERE deleted_at IS NULL); blocking_id | 311124 blocking_query | SELECT e.id FROM events e JOIN postgres_dbs db ON (e.data ->> 'serviceId') = db.database_id LIMIT 1; -[ RECORD 2 ]--+----------------------------------------------------------------------------------------------------- pid | 344684 usename | postgres query | CREATE OR REPLACE VIEW postgres_dbs AS (SELECT * FROM all_postgres_dbs WHERE deleted_at IS NULL); blocking_id | 311124 blocking_query | SELECT e.id FROM events e JOIN postgres_dbs db ON (e.data ->> 'serviceId') = db.database_id LIMIT 1; -[ RECORD 3 ]--+----------------------------------------------------------------------------------------------------- pid | 344684 usename | postgres query | CREATE OR REPLACE VIEW postgres_dbs AS (SELECT * FROM all_postgres_dbs WHERE deleted_at IS NULL); blocking_id | 313674 blocking_query | CREATE OR REPLACE VIEW postgres_dbs AS (SELECT * FROM all_postgres_dbs WHERE deleted_at IS NULL);\n\nUndefined\n\n> Note: pg_stat_activity only shows a PostgreSQL instance\u2019s current workload,\n> so we had to run this query while a new migration attempt was active.\n> Otherwise, this query may have returned no results.\n\nThe results show that the suspected query blocks the following view creation\nstatement:\n\n    \n    \n    CREATE OR REPLACE VIEW.postgres_dbs AS ( SELECT * FROM all_postgres_dbs WHERE deleted_at IS NULL );\n\nSQL\n\nOn closer inspection, we see there\u2019s a web of dependencies:\n\n  * The suspected query (311124) is blocking two distinct CREATE OR REPLACE VIEW statements (313674 and 344684).\n  * One of the CREATE OR REPLACE VIEWS statements (313674) is blocking the other (344684).\n\nIt now made sense why migrations had hung:\n\n  * The view creation statement needed a lock on postgres_dbs. Behind the scenes, a CREATE OR REPLACE VIEW statement is implemented as a DROP of an existing view, followed atomically by the creation of the view as defined in the statement. This operation requires an exclusive lock on the view, because the view can't be altered if another transaction is actively reading it.\n  * But postgres_dbs was being read by the suspected query. For as long as the suspected query ran (four hours and counting), the view creation statements wouldn\u2019t be able to get an exclusive lock on postgres_db.\n  * Multiple deploy failures led multiple view creation statements to pile up. We had been failing and re-attempting deploys to our staging environment for hours. On each attempt, a new view creation statement had been initialized, which explained the multiple distinct view creation statements we saw.\n\nThe suspected query was now the guilty query. But there remained a big\nquestion: where had this query come from?\n\n## Finding the source of the guilty query\n\nWe had to find who had issued the guilty query. If our application or a cron\njob had issued it, the same problem could arise in production. We began by\nsearching our code. We make heavy use of GORM, a Go object-relational mapper\nand query building library, in our backend. Unfortunately, this makes it\ndifficult to search for exact query text and find its source. After a deep\nsearch of the package that queries our Event model, we didn\u2019t find any code\nthat builds this query. We began to think this query was issued from outside\nof our application. At this point, we realized... if we'd tweaked our queries\nto pg_stat_activity earlier, we could have short-circuited this question\nentirely. In particular, we would have requested the following columns:\n\n  * usename: the name of the user that issued the statement.\n  * application_name: a free-form tag used to identify the source application of a database client. For application clients, this value is empty by default, but you can (and should) supply it via the database connection string (e.g., postgres://myuser@my-host:5432/mydb?application_name=myapp). For clients that connect to the database instance directly via a shell, the application_name value is usually the name of the tool (e.g., psql or pgcli).\n\n> Note: The reference queries in our PostgreSQL troubleshooting guide include\n> these two columns.\n\nWe re-issued a query to pg_stat_activity to get these two columns for our\nguilty query. The results showed the guilty query had been issued via a psql\nshell by <REDACTED_PERSON>@render.com. The root cause was an ad-hoc query\nagainst our staging database from an engineer! The engineer had been working\non a feature related to tracking PostgreSQL events, and they had issued the\nquery to validate that a type of event data existed. They\u2019d seen the query was\ntaking a while to finish, so they hit Ctrl+C in their psql shell to cancel the\nquery, and went about their day. Unfortunately\u2014and very\nunintuitively\u2014canceling a psql query only cancels reading the query. The\nclient backend on the PostgreSQL instance continues to execute the query.\nUnknowingly, the engineer had issued a background process that had run for\nover four hours.\n\n## Quick fix: killing the query\n\nNow that we knew the long-running query was a one-off, we moved on to\nunblocking the pipeline. We simply killed the long-running query:\n\n    \n    \n    SELECT pg_terminate_backend(311124);\n\nSQL\n\nAlmost immediately, our migration step passed without failure, and the\nelevated metrics we noted began to normalize.\n\nThe oldest transaction age droppedFree memory recoveredCPU utilization\nreturned to normal\n\nMystery solved.\n\n## Next mystery: Why so slow?\n\nOr rather, mystery partially solved. One big question remained: why did this\nquery take so long? Without this understanding, we might unwittingly write\nanother query that triggers the same problem. Up until this point, our tools\npg_stat_activity and pg_blocking_pids() helped us understand active problems\nwithin our database. To investigate the core issues with the query itself, we\nneeded other tools. In fact, this investigation became a whole adventure in of\nitself... an adventure that deserves its own story. Stay tuned. In the\nmeantime, try running the queries you just read about. You can find them in\nour PostgreSQL performance troubleshooting guide.\n\nStart building with Render\n\nThe modern cloud for developers and teams.\n\nGet Started\n\nFeatures\n\n  * Autoscaling\n  * Private Networking\n  * Managed PostgreSQL\n  * Managed Redis\n  * Persistent Disks\n  * Infrastructure As Code\n  * Preview Environments\n  * Zero Downtime Deploys\n  * Render API\n\nServices\n\n  * Static Sites\n  * Web Services\n  * Private Services\n  * Background Workers\n  * Cron Jobs\n  * PostgreSQL\n  * Redis\u00ae\n\nCompany\n\n  * About\n  * Pricing\n  * Docs\n  * Changelog\n  * Blog\n  * Careers\n  * Privacy Policy\n  * Security\n  * Security.txt\n  * Terms Of Use\n\n  * Twitter\n  * LinkedIn\n  * GitHub\n\n  * Changelog\n  * Terms\n  * Privacy\n  * Security and Trust\n  * \u00a9 Render 2023\n\n", "frontpage": false}
