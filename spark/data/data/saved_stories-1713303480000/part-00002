{"aid": "40053399", "title": "The Free Lunch Is Over: A Fundamental Turn Toward Concurrency in Software (2005)", "url": "http://www.gotw.ca/publications/concurrency-ddj.htm", "domain": "gotw.ca", "votes": 2, "user": "belter", "posted_at": "2024-04-16 15:36:54", "comments": 0, "source_title": "The Free Lunch Is Over: A Fundamental Turn Toward Concurrency in Software", "source_text": "The Free Lunch Is Over: A Fundamental Turn Toward Concurrency in Software\n\n| The Free Lunch Is Over: A Fundamental Turn Toward Concurrency in Software  \n---  \n| On the blog| November 4: Other Concurrency Sessions at PDC November 3:\nPDC'09: Tutorial & Panel| October 26: Hoare on Testing October 23: Deprecating\nexport Considered for ISO C++0x  \n---|---|---  \n  \n## The Free Lunch Is Over A Fundamental Turn Toward Concurrency in Software\n\nBy Herb SutterThe biggest sea change in software development since the OO\nrevolution is knocking at the door, and its name is Concurrency.This article\nappeared in Dr. Dobb's Journal, 30(3), March 2005. A much briefer version\nunder the title \"The Concurrency Revolution\" appeared in C/C++ Users Journal,\n23(2), February 2005.Update note: The CPU trends graph last updated August\n2009 to include current data and show the trend continues as predicted. The\nrest of this article including all text is still original as first posted here\nin December 2004.Your free lunch will soon be over. What can you do about it?\nWhat are you doing about it?The major processor manufacturers and\narchitectures, from Intel and AMD to Sparc and PowerPC, have run out of room\nwith most of their traditional approaches to boosting CPU performance. Instead\nof driving clock speeds and straight-line instruction throughput ever higher,\nthey are instead turning en masse to hyperthreading and multicore\narchitectures. Both of these features are already available on chips today; in\nparticular, multicore is available on current PowerPC and Sparc IV processors,\nand is coming in 2005 from Intel and AMD. Indeed, the big theme of the 2004\nIn-Stat/MDR Fall Processor Forum was multicore devices, as many companies\nshowed new or updated multicore processors. Looking back, it\u2019s not much of a\nstretch to call 2004 the year of multicore.And that puts us at a fundamental\nturning point in software development, at least for the next few years and for\napplications targeting general-purpose desktop computers and low-end servers\n(which happens to account for the vast bulk of the dollar value of software\nsold today). In this article, I\u2019ll describe the changing face of hardware, why\nit suddenly does matter to software, and how specifically the concurrency\nrevolution matters to you and is going to change the way you will likely be\nwriting software in the future.Arguably, the free lunch has already been over\nfor a year or two, only we\u2019re just now noticing.\n\n### The Free Performance Lunch\n\nThere\u2019s an interesting phenomenon that\u2019s known as \u201cAndy giveth, and Bill\ntaketh away.\u201d No matter how fast processors get, software consistently finds\nnew ways to eat up the extra speed. Make a CPU ten times as fast, and software\nwill usually find ten times as much to do (or, in some cases, will feel at\nliberty to do it ten times less efficiently). Most classes of applications\nhave enjoyed free and regular performance gains for several decades, even\nwithout releasing new versions or doing anything special, because the CPU\nmanufacturers (primarily) and memory and disk manufacturers (secondarily) have\nreliably enabled ever-newer and ever-faster mainstream systems. Clock speed\nisn\u2019t the only measure of performance, or even necessarily a good one, but\nit\u2019s an instructive one: We\u2019re used to seeing 500MHz CPUs give way to 1GHz\nCPUs give way to 2GHz CPUs, and so on. Today we\u2019re in the 3GHz range on\nmainstream computers.The key question is: When will it end? After all, Moore\u2019s\nLaw predicts exponential growth, and clearly exponential growth can\u2019t continue\nforever before we reach hard physical limits; light isn\u2019t getting any faster.\nThe growth must eventually slow down and even end. (Caveat: Yes, Moore\u2019s Law\napplies principally to transistor densities, but the same kind of exponential\ngrowth has occurred in related areas such as clock speeds. There\u2019s even faster\ngrowth in other spaces, most notably the data storage explosion, but that\nimportant trend belongs in a different article.)If you\u2019re a software\ndeveloper, chances are that you have already been riding the \u201cfree lunch\u201d wave\nof desktop computer performance. Is your application\u2019s performance borderline\nfor some local operations? \u201cNot to worry,\u201d the conventional (if suspect)\nwisdom goes; \u201ctomorrow\u2019s processors will have even more throughput, and anyway\ntoday\u2019s applications are increasingly throttled by factors other than CPU\nthroughput and memory speed (e.g., they\u2019re often I/O-bound, network-bound,\ndatabase-bound).\u201d Right?Right enough, in the past. But dead wrong for the\nforeseeable future.The good news is that processors are going to continue to\nbecome more powerful. The bad news is that, at least in the short term, the\ngrowth will come mostly in directions that do not take most current\napplications along for their customary free ride.Over the past 30 years, CPU\ndesigners have achieved performance gains in three main areas, the first two\nof which focus on straight-line execution flow:| clock speed  \n---  \nexecution optimization  \ncache  \n  \nIncreasing clock speed is about getting more cycles. Running the CPU faster\nmore or less directly means doing the same work faster.\n\nOptimizing execution flow is about doing more work per cycle. Today\u2019s CPUs\nsport some more powerful instructions, and they perform optimizations that\nrange from the pedestrian to the exotic, including pipelining, branch\nprediction, executing multiple instructions in the same clock cycle(s), and\neven reordering the instruction stream for out-of-order execution. These\ntechniques are all designed to make the instructions flow better and/or\nexecute faster, and to squeeze the most work out of each clock cycle by\nreducing latency and maximizing the work accomplished per clock cycle.\n\nChip designers are under so much pressure to deliver ever-faster CPUs that\nthey\u2019ll risk changing the meaning of your program, and possibly break it, in\norder to make it run faster  \n---  \n  \nBrief aside on instruction reordering and memory models: Note that some of\nwhat I just called \u201coptimizations\u201d are actually far more than optimizations,\nin that they can change the meaning of programs and cause visible effects that\ncan break reasonable programmer expectations. This is significant. CPU\ndesigners are generally sane and well-adjusted folks who normally wouldn\u2019t\nhurt a fly, and wouldn\u2019t think of hurting your code... normally. But in recent\nyears they have been willing to pursue aggressive optimizations just to wring\nyet more speed out of each cycle, even knowing full well that these aggressive\nrearrangements could endanger the semantics of your code. Is this Mr. Hyde\nmaking an appearance? Not at all. That willingness is simply a clear indicator\nof the extreme pressure the chip designers face to deliver ever-faster CPUs;\nthey\u2019re under so much pressure that they\u2019ll risk changing the meaning of your\nprogram, and possibly break it, in order to make it run faster. Two noteworthy\nexamples in this respect are write reordering and read reordering: Allowing a\nprocessor to reorder write operations has consequences that are so surprising,\nand break so many programmer expectations, that the feature generally has to\nbe turned off because it\u2019s too difficult for programmers to reason correctly\nabout the meaning of their programs in the presence of arbitrary write\nreordering. Reordering read operations can also yield surprising visible\neffects, but that is more commonly left enabled anyway because it isn\u2019t quite\nas hard on programmers, and the demands for performance cause designers of\noperating systems and operating environments to compromise and choose models\nthat place a greater burden on programmers because that is viewed as a lesser\nevil than giving up the optimization opportunities.\n\nFinally, increasing the size of on-chip cache is about staying away from RAM.\nMain memory continues to be so much slower than the CPU that it makes sense to\nput the data closer to the processor\u2014and you can\u2019t get much closer than being\nright on the die. On-die cache sizes have soared, and today most major chip\nvendors will sell you CPUs that have 2MB and more of on-board L2 cache. (Of\nthese three major historical approaches to boosting CPU performance,\nincreasing cache is the only one that will continue in the near term. I\u2019ll\ntalk a little more about the importance of cache later on.)\n\nOkay. So what does this mean?\n\nA fundamentally important thing to recognize about this list is that all of\nthese areas are concurrency-agnostic. Speedups in any of these areas will\ndirectly lead to speedups in sequential (nonparallel, single-threaded, single-\nprocess) applications, as well as applications that do make use of\nconcurrency. That\u2019s important, because the vast majority of today\u2019s\napplications are single-threaded, for good reasons that I\u2019ll get into further\nbelow.\n\nOf course, compilers have had to keep up; sometimes you need to recompile your\napplication, and target a specific minimum level of CPU, in order to benefit\nfrom new instructions (e.g., MMX, SSE) and some new CPU features and\ncharacteristics. But, by and large, even old applications have always run\nsignificantly faster\u2014even without being recompiled to take advantage of all\nthe new instructions and features offered by the latest CPUs.\n\nThat world was a nice place to be. Unfortunately, it has already disappeared.\n\n### Obstacles, and Why You Don\u2019t Have 10GHz Today\n\nCPU performance growth as we have known it hit a wall two years ago. Most\npeople have only recently started to notice.\n\nYou can get similar graphs for other chips, but I\u2019m going to use Intel data\nhere. Figure 1 graphs the history of Intel chip introductions by clock speed\nand number of transistors. The number of transistors continues to climb, at\nleast for now. Clock speed, however, is a different story.\n\nFigure 1: Intel CPU Introductions (graph updated August 2009; article text\noriginal from December 2004)  \n---  \n  \nAround the beginning of 2003, you\u2019ll note a disturbing sharp turn in the\nprevious trend toward ever-faster CPU clock speeds. I\u2019ve added lines to show\nthe limit trends in maximum clock speed; instead of continuing on the previous\npath, as indicated by the thin dotted line, there is a sharp flattening. It\nhas become harder and harder to exploit higher clock speeds due to not just\none but several physical issues, notably heat (too much of it and too hard to\ndissipate), power consumption (too high), and current leakage problems.\n\nQuick: What\u2019s the clock speed on the CPU(s) in your current workstation? Are\nyou running at 10GHz? On Intel chips, we reached 2GHz a long time ago (August\n2001), and according to CPU trends before 2003, now in early 2005 we should\nhave the first 10GHz Pentium-family chips. A quick look around shows that,\nwell, actually, we don\u2019t. What\u2019s more, such chips are not even on the\nhorizon\u2014we have no good idea at all about when we might see them appear.\n\nWell, then, what about 4GHz? We\u2019re at 3.4GHz already\u2014surely 4GHz can\u2019t be far\naway? Alas, even 4GHz seems to be remote indeed. In mid-2004, as you probably\nknow, Intel first delayed its planned introduction of a 4GHz chip until 2005,\nand then in fall 2004 it officially abandoned its 4GHz plans entirely. As of\nthis writing, Intel is planning to ramp up a little further to 3.73GHz in\nearly 2005 (already included in Figure 1 as the upper-right-most dot), but the\nclock race really is over, at least for now; Intel\u2019s and most processor\nvendors\u2019 future lies elsewhere as chip companies aggressively pursue the same\nnew multicore directions.\n\nWe\u2019ll probably see 4GHz CPUs in our mainstream desktop machines someday, but\nit won\u2019t be in 2005. Sure, Intel has samples of their chips running at even\nhigher speeds in the lab\u2014but only by heroic efforts, such as attaching\nhideously impractical quantities of cooling equipment. You won\u2019t have that\nkind of cooling hardware in your office any day soon, let alone on your lap\nwhile computing on the plane.\n\n### TANSTAAFL: Moore\u2019s Law and the Next Generation(s)\n\n> \u201cThere ain\u2019t no such thing as a free lunch.\u201d \u2014R. A. Heinlein, The Moon Is a\n> Harsh Mistress\n\nDoes this mean Moore\u2019s Law is over? Interestingly, the answer in general seems\nto be no. Of course, like all exponential progressions, Moore\u2019s Law must end\nsomeday, but it does not seem to be in danger for a few more years yet.\nDespite the wall that chip engineers have hit in juicing up raw clock cycles,\ntransistor counts continue to explode and it seems CPUs will continue to\nfollow Moore\u2019s Law-like throughput gains for some years to come.\n\n|\n\n### Myths and Realities: 2 x 3GHz < 6 GHz\n\nSo a dual-core CPU that combines two 3GHz cores practically offers 6GHz of\nprocessing power. Right?Wrong. Even having two threads running on two physical\nprocessors doesn\u2019t mean getting two times the performance. Similarly, most\nmulti-threaded applications won\u2019t run twice as fast on a dual-core box. They\nshould run faster than on a single-core CPU; the performance gain just isn\u2019t\nlinear, that\u2019s all.Why not? First, there is coordination overhead between the\ncores to ensure cache coherency (a consistent view of cache, and of main\nmemory) and to perform other handshaking. Today, a two- or four-processor\nmachine isn\u2019t really two or four times as fast as a single CPU even for multi-\nthreaded applications. The problem remains essentially the same even when the\nCPUs in question sit on the same die.Second, unless the two cores are running\ndifferent processes, or different threads of a single process that are well-\nwritten to run independently and almost never wait for each other, they won\u2019t\nbe well utilized. (Despite this, I will speculate that today\u2019s single-threaded\napplications as actually used in the field could actually see a performance\nboost for most users by going to a dual-core chip, not because the extra core\nis actually doing anything useful, but because it is running the adware and\nspyware that infest many users\u2019 systems and are otherwise slowing down the\nsingle CPU that user has today. I leave it up to you to decide whether adding\na CPU to run your spyware is the best solution to that problem.)If you\u2019re\nrunning a single-threaded application, then the application can only make use\nof one core. There should be some speedup as the operating system and the\napplication can run on separate cores, but typically the OS isn\u2019t going to be\nmaxing out the CPU anyway so one of the cores will be mostly idle. (Again, the\nspyware can share the OS\u2019s core most of the time.)  \n---  \n  \nThe key difference, which is the heart of this article, is that the\nperformance gains are going to be accomplished in fundamentally different ways\nfor at least the next couple of processor generations. And most current\napplications will no longer benefit from the free ride without significant\nredesign.\n\nFor the near-term future, meaning for the next few years, the performance\ngains in new chips will be fueled by three main approaches, only one of which\nis the same as in the past. The near-term future performance growth drivers\nare:\n\nhyperthreading  \n---  \nmulticore  \ncache  \n  \nHyperthreading is about running two or more threads in parallel inside a\nsingle CPU. Hyperthreaded CPUs are already available today, and they do allow\nsome instructions to run in parallel. A limiting factor, however, is that\nalthough a hyper-threaded CPU has some extra hardware including extra\nregisters, it still has just one cache, one integer math unit, one FPU, and in\ngeneral just one each of most basic CPU features. Hyperthreading is sometimes\ncited as offering a 5% to 15% performance boost for reasonably well-written\nmulti-threaded applications, or even as much as 40% under ideal conditions for\ncarefully written multi-threaded applications. That\u2019s good, but it\u2019s hardly\ndouble, and it doesn\u2019t help single-threaded applications.\n\nMulticore is about running two or more actual CPUs on one chip. Some chips,\nincluding Sparc and PowerPC, have multicore versions available already. The\ninitial Intel and AMD designs, both due in 2005, vary in their level of\nintegration but are functionally similar. AMD\u2019s seems to have some initial\nperformance design advantages, such as better integration of support functions\non the same die, whereas Intel\u2019s initial entry basically just glues together\ntwo Xeons on a single die. The performance gains should initially be about the\nsame as having a true dual-CPU system (only the system will be cheaper because\nthe motherboard doesn\u2019t have to have two sockets and associated \u201cglue\u201d\nchippery), which means something less than double the speed even in the ideal\ncase, and just like today it will boost reasonably well-written multi-threaded\napplications. Not single-threaded ones.\n\nFinally, on-die cache sizes can be expected to continue to grow, at least in\nthe near term. Of these three areas, only this one will broadly benefit most\nexisting applications. The continuing growth in on-die cache sizes is an\nincredibly important and highly applicable benefit for many applications,\nsimply because space is speed. Accessing main memory is expensive, and you\nreally don\u2019t want to touch RAM if you can help it. On today\u2019s systems, a cache\nmiss that goes out to main memory often costs 10 to 50 times as much getting\nthe information from the cache; this, incidentally, continues to surprise\npeople because we all think of memory as fast, and it is fast compared to\ndisks and networks, but not compared to on-board cache which runs at faster\nspeeds. If an application\u2019s working set fits into cache, we\u2019re golden, and if\nit doesn\u2019t, we\u2019re not. That is why increased cache sizes will save some\nexisting applications and breathe life into them for a few more years without\nrequiring significant redesign: As existing applications manipulate more and\nmore data, and as they are incrementally updated to include more code for new\nfeatures, performance-sensitive operations need to continue to fit into cache.\nAs the Depression-era old-timers will be quick to remind you, \u201cCache is king.\u201d\n\n(Aside: Here\u2019s an anecdote to demonstrate \u201cspace is speed\u201d that recently hit\nmy compiler team. The compiler uses the same source base for the 32-bit and\n64-bit compilers; the code is just compiled as either a 32-bit process or a\n64-bit one. The 64-bit compiler gained a great deal of baseline performance by\nrunning on a 64-bit CPU, principally because the 64-bit CPU had many more\nregisters to work with and had other code performance features. All well and\ngood. But what about data? Going to 64 bits didn\u2019t change the size of most of\nthe data in memory, except that of course pointers in particular were now\ntwice the size they were before. As it happens, our compiler uses pointers\nmuch more heavily in its internal data structures than most other kinds of\napplications ever would. Because pointers were now 8 bytes instead of 4 bytes,\na pure data size increase, we saw a significant increase in the 64-bit\ncompiler\u2019s working set. That bigger working set caused a performance penalty\nthat almost exactly offset the code execution performance increase we\u2019d gained\nfrom going to the faster processor with more registers. As of this writing,\nthe 64-bit compiler runs at the same speed as the 32-bit compiler, even though\nthe source base is the same for both and the 64-bit processor offers better\nraw processing throughput. Space is speed.)\n\nBut cache is it. Hyperthreading and multicore CPUs will have nearly no impact\non most current applications.\n\nSo what does this change in the hardware mean for the way we write software?\nBy now you\u2019ve probably noticed the basic answer, so let\u2019s consider it and its\nconsequences.\n\n### What This Means For Software: The Next Revolution\n\nIn the 1990s, we learned to grok objects. The revolution in mainstream\nsoftware development from structured programming to object-oriented\nprogramming was the greatest such change in the past 20 years, and arguably in\nthe past 30 years. There have been other changes, including the most recent\n(and genuinely interesting) naissance of web services, but nothing that most\nof us have seen during our careers has been as fundamental and as far-reaching\na change in the way we write software as the object revolution.\n\nUntil now.\n\nStarting today, the performance lunch isn\u2019t free any more. Sure, there will\ncontinue to be generally applicable performance gains that everyone can pick\nup, thanks mainly to cache size improvements. But if you want your application\nto benefit from the continued exponential throughput advances in new\nprocessors, it will need to be a well-written concurrent (usually\nmultithreaded) application. And that\u2019s easier said than done, because not all\nproblems are inherently parallelizable and because concurrent programming is\nhard.\n\nI can hear the howls of protest: \u201cConcurrency? That\u2019s not news! People are\nalready writing concurrent applications.\u201d That\u2019s true. Of a small fraction of\ndevelopers.\n\nRemember that people have been doing object-oriented programming since at\nleast the days of Simula in the late 1960s. But OO didn\u2019t become a revolution,\nand dominant in the mainstream, until the 1990s. Why then? The reason the\nrevolution happened was primarily that our industry was driven by requirements\nto write larger and larger systems that solved larger and larger problems and\nexploited the greater and greater CPU and storage resources that were becoming\navailable. OOP\u2019s strengths in abstraction and dependency management made it a\nnecessity for achieving large-scale software development that is economical,\nreliable, and repeatable.\n\nConcurrency is the next major revolution in how we write software  \n---  \n  \nSimilarly, we\u2019ve been doing concurrent programming since those same dark ages,\nwriting coroutines and monitors and similar jazzy stuff. And for the past\ndecade or so we\u2019ve witnessed incrementally more and more programmers writing\nconcurrent (multi-threaded, multi-process) systems. But an actual revolution\nmarked by a major turning point toward concurrency has been slow to\nmaterialize. Today the vast majority of applications are single-threaded, and\nfor good reasons that I\u2019ll summarize in the next section.\n\nBy the way, on the matter of hype: People have always been quick to announce\n\u201cthe next software development revolution,\u201d usually about their own brand-new\ntechnology. Don\u2019t believe it. New technologies are often genuinely interesting\nand sometimes beneficial, but the biggest revolutions in the way we write\nsoftware generally come from technologies that have already been around for\nsome years and have already experienced gradual growth before they transition\nto explosive growth. This is necessary: You can only base a software\ndevelopment revolution on a technology that\u2019s mature enough to build on\n(including having solid vendor and tool support), and it generally takes any\nnew software technology at least seven years before it\u2019s solid enough to be\nbroadly usable without performance cliffs and other gotchas. As a result, true\nsoftware development revolutions like OO happen around technologies that have\nalready been undergoing refinement for years, often decades. Even in\nHollywood, most genuine \u201covernight successes\u201d have really been performing for\nmany years before their big break.\n\nConcurrency is the next major revolution in how we write software. Different\nexperts still have different opinions on whether it will be bigger than OO,\nbut that kind of conversation is best left to pundits. For technologists, the\ninteresting thing is that concurrency is of the same order as OO both in the\n(expected) scale of the revolution and in the complexity and learning curve of\nthe technology.\n\n### Benefits and Costs of Concurrency\n\nThere are two major reasons for which concurrency, especially multithreading,\nis already used in mainstream software. The first is to logically separate\nnaturally independent control flows; for example, in a database replication\nserver I designed it was natural to put each replication session on its own\nthread, because each session worked completely independently of any others\nthat might be active (as long as they weren\u2019t working on the same database\nrow). The second and less common reason to write concurrent code in the past\nhas been for performance, either to scalably take advantage of multiple\nphysical CPUs or to easily take advantage of latency in other parts of the\napplication; in my database replication server, this factor applied as well\nand the separate threads were able to scale well on multiple CPUs as our\nserver handled more and more concurrent replication sessions with many other\nservers.\n\nThere are, however, real costs to concurrency. Some of the obvious costs are\nactually relatively unimportant. For example, yes, locks can be expensive to\nacquire, but when used judiciously and properly you gain much more from the\nconcurrent execution than you lose on the synchronization, if you can find a\nsensible way to parallelize the operation and minimize or eliminate shared\nstate.\n\nPerhaps the second-greatest cost of concurrency is that not all applications\nare amenable to parallelization. I\u2019ll say more about this later on.\n\nProbably the greatest cost of concurrency is that concurrency really is hard:\nThe programming model, meaning the model in the programmer\u2019s head that he\nneeds to reason reliably about his program, is much harder than it is for\nsequential control flow.\n\nEverybody who learns concurrency thinks they understand it, ends up finding\nmysterious races they thought weren\u2019t possible, and discovers that they didn\u2019t\nactually understand it yet after all. As the developer learns to reason about\nconcurrency, they find that usually those races can be caught by reasonable\nin-house testing, and they reach a new plateau of knowledge and comfort. What\nusually doesn\u2019t get caught in testing, however, except in shops that\nunderstand why and how to do real stress testing, is those latent concurrency\nbugs that surface only on true multiprocessor systems, where the threads\naren\u2019t just being switched around on a single processor but where they really\ndo execute truly simultaneously and thus expose new classes of errors. This is\nthe next jolt for people who thought that surely now they know how to write\nconcurrent code: I\u2019ve come across many teams whose application worked fine\neven under heavy and extended stress testing, and ran perfectly at many\ncustomer sites, until the day that a customer actually had a real\nmultiprocessor machine and then deeply mysterious races and corruptions\nstarted to manifest intermittently. In the context of today\u2019s CPU landscape,\nthen, redesigning your application to run multithreaded on a multicore machine\nis a little like learning to swim by jumping into the deep end\u2014going straight\nto the least forgiving, truly parallel environment that is most likely to\nexpose the things you got wrong. Even when you have a team that can reliably\nwrite safe concurrent code, there are other pitfalls; for example, concurrent\ncode that is completely safe but isn\u2019t any faster than it was on a single-core\nmachine, typically because the threads aren\u2019t independent enough and share a\ndependency on a single resource which re-serializes the program\u2019s execution.\nThis stuff gets pretty subtle.\n\nThe vast majority of programmers today don\u2019t grok concurrency, just as the\nvast majority of programmers 15 years ago didn\u2019t yet grok objects  \n---  \n  \nJust as it is a leap for a structured programmer to learn OO (what\u2019s an\nobject? what\u2019s a virtual function? how should I use inheritance? and beyond\nthe \u201cwhats\u201d and \u201chows,\u201d why are the correct design practices actually\ncorrect?), it\u2019s a leap of about the same magnitude for a sequential programmer\nto learn concurrency (what\u2019s a race? what\u2019s a deadlock? how can it come up,\nand how do I avoid it? what constructs actually serialize the program that I\nthought was parallel? how is the message queue my friend? and beyond the\n\u201cwhats\u201d and \u201chows,\u201d why are the correct design practices actually correct?).\n\nThe vast majority of programmers today don\u2019t grok concurrency, just as the\nvast majority of programmers 15 years ago didn\u2019t yet grok objects. But the\nconcurrent programming model is learnable, particularly if we stick to\nmessage- and lock-based programming, and once grokked it isn\u2019t that much\nharder than OO and hopefully can become just as natural. Just be ready and\nallow for the investment in training and time, for you and for your team.\n\n(I deliberately limit the above to message- and lock-based concurrent\nprogramming models. There is also lock-free programming, supported most\ndirectly at the language level in Java 5 and in at least one popular C++\ncompiler. But concurrent lock-free programming is known to be very much harder\nfor programmers to understand and reason about than even concurrent lock-based\nprogramming. Most of the time, only systems and library writers should have to\nunderstand lock-free programming, although virtually everybody should be able\nto take advantage of the lock-free systems and libraries those people produce.\nFrankly, even lock-based programming is hazardous.)\n\n### What It Means For Us\n\nOkay, back to what it means for us.\n\n1\\. The clear primary consequence we\u2019ve already covered is that applications\nwill increasingly need to be concurrent if they want to fully exploit CPU\nthroughput gains that have now started becoming available and will continue to\nmaterialize over the next several years. For example, Intel is talking about\nsomeday producing 100-core chips; a single-threaded application can exploit at\nmost 1/100 of such a chip\u2019s potential throughput. \u201cOh, performance doesn\u2019t\nmatter so much, computers just keep getting faster\u201d has always been a na\u00efve\nstatement to be viewed with suspicion, and for the near future it will almost\nalways be simply wrong.\n\nApplications will increasingly need to be concurrent if they want to fully\nexploit continuing exponential CPU throughput gainsEfficiency and performance\noptimization will get more, not less, important  \n---  \n  \nNow, not all applications (or, more precisely, important operations of an\napplication) are amenable to parallelization. True, some problems, such as\ncompilation, are almost ideally parallelizable. But others aren\u2019t; the usual\ncounterexample here is that just because it takes one woman nine months to\nproduce a baby doesn\u2019t imply that nine women could produce one baby in one\nmonth. You\u2019ve probably come across that analogy before. But did you notice the\nproblem with leaving the analogy at that? Here\u2019s the trick question to ask the\nnext person who uses it on you: Can you conclude from this that the Human Baby\nProblem is inherently not amenable to parallelization? Usually people relating\nthis analogy err in quickly concluding that it demonstrates an inherently\nnonparallel problem, but that\u2019s actually not necessarily correct at all. It is\nindeed an inherently nonparallel problem if the goal is to produce one child.\nIt is actually an ideally parallelizable problem if the goal is to produce\nmany children! Knowing the real goals can make all the difference. This basic\ngoal-oriented principle is something to keep in mind when considering whether\nand how to parallelize your software.\n\n2\\. Perhaps a less obvious consequence is that applications are likely to\nbecome increasingly CPU-bound. Of course, not every application operation will\nbe CPU-bound, and even those that will be affected won\u2019t become CPU-bound\novernight if they aren\u2019t already, but we seem to have reached the end of the\n\u201capplications are increasingly I/O-bound or network-bound or database-bound\u201d\ntrend, because performance in those areas is still improving rapidly (gigabit\nWiFi, anyone?) while traditional CPU performance-enhancing techniques have\nmaxed out. Consider: We\u2019re stopping in the 3GHz range for now. Therefore\nsingle-threaded programs are likely not to get much faster any more for now\nexcept for benefits from further cache size growth (which is the main good\nnews). Other gains are likely to be incremental and much smaller than we\u2019ve\nbeen used to seeing in the past, for example as chip designers find new ways\nto keep pipelines full and avoid stalls, which are areas where the low-hanging\nfruit has already been harvested. The demand for new application features is\nunlikely to abate, and even more so the demand to handle vastly growing\nquantities of application data is unlikely to stop accelerating. As we\ncontinue to demand that programs do more, they will increasingly often find\nthat they run out of CPU to do it unless they can code for concurrency.\n\nThere are two ways to deal with this sea change toward concurrency. One is to\nredesign your applications for concurrency, as above. The other is to be\nfrugal, by writing code that is more efficient and less wasteful. This leads\nto the third interesting consequence:\n\n3\\. Efficiency and performance optimization will get more, not less,\nimportant. Those languages that already lend themselves to heavy optimization\nwill find new life; those that don\u2019t will need to find ways to compete and\nbecome more efficient and optimizable. Expect long-term increased demand for\nperformance-oriented languages and systems.\n\n4\\. Finally, programming languages and systems will increasingly be forced to\ndeal well with concurrency. The Java language has included support for\nconcurrency since its beginning, although mistakes were made that later had to\nbe corrected over several releases in order to do concurrent programming more\ncorrectly and efficiently. The C++ language has long been used to write heavy-\nduty multithreaded systems well, but it has no standardized support for\nconcurrency at all (the ISO C++ standard doesn\u2019t even mention threads, and\ndoes so intentionally), and so typically the concurrency is of necessity\naccomplished by using nonportable platform-specific concurrency features and\nlibraries. (It\u2019s also often incomplete; for example, static variables must be\ninitialized only once, which typically requires that the compiler wrap them\nwith a lock, but many C++ implementations do not generate the lock.) Finally,\nthere are a few concurrency standards, including pthreads and OpenMP, and some\nof these support implicit as well as explicit parallelization. Having the\ncompiler look at your single-threaded program and automatically figure out how\nto parallelize it implicitly is fine and dandy, but those automatic\ntransformation tools are limited and don\u2019t yield nearly the gains of explicit\nconcurrency control that you code yourself. The mainstream state of the art\nrevolves around lock-based programming, which is subtle and hazardous. We\ndesperately need a higher-level programming model for concurrency than\nlanguages offer today; I'll have more to say about that soon.\n\n### Conclusion\n\nIf you haven\u2019t done so already, now is the time to take a hard look at the\ndesign of your application, determine what operations are CPU-sensitive now or\nare likely to become so soon, and identify how those places could benefit from\nconcurrency. Now is also the time for you and your team to grok concurrent\nprogramming\u2019s requirements, pitfalls, styles, and idioms.\n\nA few rare classes of applications are naturally parallelizable, but most\naren\u2019t. Even when you know exactly where you\u2019re CPU-bound, you may well find\nit difficult to figure out how to parallelize those operations; all the most\nreason to start thinking about it now. Implicitly parallelizing compilers can\nhelp a little, but don\u2019t expect much; they can\u2019t do nearly as good a job of\nparallelizing your sequential program as you could do by turning it into an\nexplicitly parallel and threaded version.\n\nThanks to continued cache growth and probably a few more incremental straight-\nline control flow optimizations, the free lunch will continue a little while\nlonger; but starting today the buffet will only be serving that one entr\u00e9e and\nthat one dessert. The filet mignon of throughput gains is still on the menu,\nbut now it costs extra\u2014extra development effort, extra code complexity, and\nextra testing effort. The good news is that for many classes of applications\nthe extra effort will be worthwhile, because concurrency will let them fully\nexploit the continuing exponential gains in processor throughput.  \n  \n#### Copyright \u00a9 2009 Herb Sutter  \n  \n---\n\n", "frontpage": false}
