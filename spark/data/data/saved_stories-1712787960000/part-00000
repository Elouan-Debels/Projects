{"aid": "39992528", "title": "Fast, Automated and Empirical Prompt Engineering from Libretto (Demo)", "url": "https://www.getlibretto.com/blog/prompt-engineering-should-be-fast-automated-fully-empirical-heres-librettos-approach", "domain": "getlibretto.com", "votes": 1, "user": "cricketts", "posted_at": "2024-04-10 16:24:09", "comments": 0, "source_title": "Prompt Engineering Should Be Fast, Automated, & Fully Empirical \u2014 Here\u2019s Libretto\u2019s Approach | Empirical Prompt Engineering", "source_text": "Prompt Engineering Should Be Fast, Automated, & Fully Empirical \u2014 Here\u2019s Libretto\u2019s Approach | Empirical Prompt Engineering\n\n.\n\n# Prompt Engineering Should Be Fast, Automated, & Fully Empirical \u2014 Here\u2019s\nLibretto\u2019s Approach\n\nI\u2019ve been working in tech since the Mosaic and Netscape days, and I can say\nwithout a doubt that large language models are the most magical, mysterious,\nand infuriating technology to emerge over my 20+ year career.\n\nThe magic of LLMs is clear: ChatGPT regularly does things that I didn\u2019t think\ncomputers would be capable of in my lifetime. But the mystery follows quickly\nafter; LLMs solve hugely complex problems but fail at tasks an eight year old\nwould ace. Thus comes the infuriation: how do you make this tool do the right\nthing most of the time?\n\nTo me, this is the most interesting problem in generative AI. Solving it is\nthe difference between gen AI being fundamentally transformative or a vague\nnice-to-have. We have to unlock consistently great AI results to make gen AI\nuseful, and that\u2019s our mission at Libretto.\n\nIt\u2019s also why we raised $3.7 million in seed funding from XYZ Venture Capital\nand The General Partnership. Now that we have a solution that works, we want\nto get it to everyone.\n\nThe problem & solution\n\nSome of the most brilliant people and most impactful projects in the world are\nbeing blocked by bad prompts. We\u2019re all too used to telling computers what to\ndo, and them just doing it. Now we find ourselves in tedious negotiations with\nour machines: you have to cajole, coerce, and manipulate an LLM into doing\nwhat you want, with maddeningly mixed results. It might follow instructions\nten times only to go rogue on the eleventh. Nearly imperceptible changes in a\nprompt can cause divergent behaviors that are totally unpredictable. Prompts\nthat were working great can suddenly produce surprising results in production.\nAnd just because you crack the code on one model, doesn\u2019t mean those insights\nwill hold true for the next.\n\nLet\u2019s say you have 30 ideas for how to make a prompt better, but you don\u2019t\nknow what will work. Maybe the first thing you try will get the results you\nwant. Maybe the thirtieth will. Maybe none will. So many of us are burning\ndays and weeks trying to mold prompts that never quite succeed. And unlike\nbuilding traditional software, prompt engineering is almost entirely\nempirical. You\u2019ll never know if a task is possible or not \u2013 or how long it\u2019s\ngoing to take \u2013 without testing the heck out of as many prompts as you can.\n\nThis is what Libretto is hacking on right now \u2013 making this empirical facet of\nprompt engineering easy. Our hypothesis: we need a tool to test, improve, and\nmonitor LLM prompts rapidly and intelligently. Today\u2019s version of Libretto\nalready makes this possible\u2014it automates away the grunt work of prompt\nengineering to get you to the best possible answer fast and predictably.\n\nNow let\u2019s take a look under the hood.\n\nTesting: Finding feasibility, fast\n\nGenerally speaking, your first step should always be to see if an LLM can\nsolve the problem at hand. This usually requires loosely playing with\ndifferent models and prompt texts with a handful of example inputs for your\nprompt.\n\nAs you hone in on the answer, you need to build up your test set to be more\nrigorous in testing changes to your prompts. This quickly becomes a massive\nheadache. Every time you change your prompt, you have to run it over all your\ntests, and you too often end up skimming spreadsheets to see how the LLM did.\n\nAt Libretto, we track all of the test cases you\u2019ve built up for your various\nprompts, and keep a large library of ways to evaluate the answers that come\nback from LLMs.\n\nAutomated LLM evaluation is tricky and very much dependent on the type of\nprompt. For some prompts, like sentiment analysis or categorization prompts,\nyou can just do a string compare for your test case. If the LLM gets the right\nanswer, it passes. But for prompts that are more generative, like customer\nservice chats or retrieval augmented generation, you may want to use a fuzzy\nstring match or another LLM to grade the responses.\n\nIn Libretto, we have many different options for evaluating the LLM\u2019s response\nso that you can tailor your evaluation strategy to your prompt. We can test\nsentiment, toxicity, JSON structure, custom subjective criteria, BLEU, ROUGE,\nBERTScore, embedding similarity, and even custom-written evaluations.\n\nOnce you have test cases and ways to evaluate them, you\u2019re ready to rapidly\nimprove your prompts \u2013 and this is where the real magic of Libretto happens.\nWe provide a playground where you can modify your prompt or try out different\nmodels or parameters. Then, with the click of a button, you can run all your\ntests and get empirical, repeatable results. This gives you confidence that,\nwhenever you change your prompt, it\u2019s getting better, not worse. It also\nensures that you\u2019re including all the various tests you\u2019ve come up with, not\njust the one you\u2019re playing with at the moment.\n\nThe TLDR; You get to speed through that list of 30 prompt engineering\ntechniques to find the magic bullet.\n\nImproving: Building a real product\n\nBut what if we didn\u2019t even have to try those 30 techniques? Libretto\u2019s killer\nfeature is called Experiments, and it takes the pain and labor out of prompt\nengineering.\n\nThe idea behind Experiments is that Libretto can automatically try various\ntechniques, creating dozens, potentially hundreds, of variants of your prompt\nand figuring out which ones work best. In the time it takes you to go grab a\ncoffee, Libretto can power through a week\u2019s worth of your prompt engineering\nto-do list and give you a better version of your prompt along with the\nempirical evidence that shows how much better it is.\n\nNo more brainstorming new ways to argue or plead with the LLM, Libretto does\nthat for you. To see how we use Experiments to make a prompt 10 percentage\npoints more accurate in under 5 minutes, check out this demo:\n\nHere you can see how Libretto automates the choice of few-shot examples.\nGiving your LLM example questions and answers to work off of can significantly\nincrease accuracy, but this still depends on how helpful it finds the examples\nyou picked (finicky, as always).\n\nWhen you run a few-shot experiment in Libretto, we take a bunch of your test\ncases and stuff them into your prompt as few-shot examples, creating several\ndozen variants of your prompt. Then we test each of those variants against the\nrest of your test set so you get concrete results. Within minutes of launching\nthe experiment, you can see which few-shot examples work well and which ones\nflop.\n\nThis isn\u2019t just an academic exercise. A recent experiment I ran with Claude 3\nHaiku showed a 17 percentage point difference in prompt accuracy between the\nbest few-shot examples and the worst ones.\n\nEvery day, we\u2019re using Libretto Experiments to learn more about practical,\nempirical prompt engineering (more on few-shot best practices here and\nhere).We\u2019ve currently got experiments that automatically compare different\nmodels, add well-known \u201cmagic phrases\u201d to your prompts (like \u201ctake a deep\nbreath\u201d), and optimize your few-shot examples. We\u2019re working on adding many\nmore.\n\nMonitoring: Learning from your users\n\nOnce you\u2019ve optimized your prompt, it\u2019s time to put it into production. This\nis where you run into yet another snag: your users. You may have come up with\na bunch of good test cases, but I can guarantee you haven\u2019t anticipated all\nthe things your users will throw at your prompt. They will find ways to use\nand misuse it that never occurred to you. The only way through is to\ncontinuously monitor your prompts as they\u2019re out there in the world and pull\nthat knowledge into your prompt engineering.\n\nTo make this kind of monitoring possible, Libretto has a drop-in wrapper\nlibrary for popular LLM clients that records all of the arguments being sent\ninto your prompts and the results you get back from your LLMs. This allows you\nto see in great detail what\u2019s happening in production and how your customers\nare using your LLM prompts.\n\nThis is useful partly for debugging but also for finding those use cases that\nhadn\u2019t occurred to you \u2013 great ideas for future tests. Libretto highlights the\nproduction calls from your users that might be good candidates to add to your\nprompt tests, and we have a smooth and streamlined process for moving\nproduction data into your prompt test sets. Last but not least, we give you a\nway to record user feedback on the LLM response, which\u2019ll help you improve\nyour prompt template tests.\n\nPutting it all together\n\nWe hope this gives you a good sense of what we\u2019re working on here at Libretto\nand the pain we\u2019re looking to kill at each part of the process. Ultimately,\nthis is all about unleashing more creativity and productivity as we step into\nthis new era of LLMs and learn how they can reach their full potential \u2013 so we\ncan reach ours.\n\nAs I said at the start, we\u2019re sharing this expressly to get your thoughts on\nthis approach to realizing this mission. Our vision is that prompt engineering\nshould be automated, fast, and fully empirical for everyone, but we know that\nwe\u2019re at the beginning of this journey, and there\u2019s a tremendous amount to\nlearn.\n\nYou can sign up to join Libretto\u2019s beta today and see how easy it makes your\nprompt engineering across projects. Click here to sign up.\n\nWant more battle-tested insights into prompt engineering? Check out Libretto's\nblog, Notes from the Prompting Lab. Or go ahead and sign up for Libretto's\nBeta:\n\nSign Up For Libretto Beta\n\n\u00a9 2024 Libretto. All Rights Reserved.\n\nWebsite Design & Development by Thompson & Prince.\n\n", "frontpage": false}
