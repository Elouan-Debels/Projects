{"aid": "39981945", "title": "Zed Decoded: Async Rust", "url": "https://zed.dev/blog/zed-decoded-async-rust", "domain": "zed.dev", "votes": 18, "user": "ingve", "posted_at": "2024-04-09 17:38:59", "comments": 0, "source_title": "Zed - Code at the speed of thought", "source_text": "Zed - Code at the speed of thought\n\n\u2190 Back to Blog\n\n# Zed Decoded: Async Rust\n\nThorsten Ball\n\nAntonio Scandurra\n\nApril 9th, 2024\n\nWelcome to the first article in a new series called Zed Decoded. In Zed\nDecoded I'm going to take a close look at Zed \u2014 how it's built, which data\nstructures it uses, which technologies and techniques, what features it has,\nwhich bugs we ran into. The best part? I won't do this alone, but get to\ninterview and ask my colleagues here at Zed about everything I want to know.\n\nCompanion Video: Async Rust\n\nThis post comes with a 1hr companion video, in which Thorsten and Antonio\nexplore how Zed uses async Rust \u2014 in Zed. It's a loose conversation that\nfocuses on the code and dives a bit deeper into some topics that didn't fit\ninto the post.\n\nWatch the video here: https://youtu.be/gkU4NGSe21I\n\nThe first topic that was on my list: async Rust and how it's used in Zed. Over\nthe past few months I've become quite fascinated with async Rust \u2014 Zed's the\nfirst codebase I've worked in that uses it \u2014 so I decided to sit down and ask\nAntonio, one of Zed's co-founders, about how we use async Rust in Zed.\n\nWe won't get into the details of async Rust itself (familiarity with that is\nto be expected if you want to understand the nitty-gritty of the code we'll\nsee), but instead focus on how Zed uses async Rust to build a high-\nperformance, native application: what async code looks like on the application\nlevel, which runtime it uses, why it uses that runtime.\n\n## Writing async Rust with GPUI\n\nLet's jump right into the deep end. Here is a snippet of code that's\nrepresentative of async code in the Zed codebase:\n\n    \n    \n    fn show_cursor_names(&mut self, cx: &mut ViewContext<Self>) { self.show_cursor_names = true; cx.notify(); cx.spawn(|this, mut cx| async move { cx.background_executor().timer(CURSORS_VISIBLE_FOR).await; this.update(&mut cx, |this, cx| { this.show_cursor_names = false; cx.notify() }) .ok() }) .detach(); }\n\nIt's a function from our Editor. When it's called, Zed shows the names of the\nowners of each cursor: your name or the names of the people you're\ncollaborating with. It's called, for example, when the editor is re-focused,\nso you can quickly see who's doing what and where.\n\nWhat show_cursor_names does is the following:\n\n  * Toggle on Editor.show_cursor_names and trigger a re-render of the editor. When Editor.show_cursor_names is true, cursor names will be rendered.\n  * Spawn a task that sleeps for CURSOR_VISIBLE_FOR, turn the cursors off, and trigger another re-render.\n\nIf you've ever written async Rust before, you can spot some familiar elements\nin the code: there's a .spawn, there's an async move, there's an await. And if\nyou've ever used the async_task crate before, this might remind you of code\nlike this:\n\n    \n    \n    let ex = Executor::new(); ex.spawn(async { loop { Timer::after(Duration::from_secs(1)).await; } }) .detach();\n\nThat's because Zed uses async_task for its Task type. But in this example\nthere's an Executor \u2014 where is that in the Zed code? And what does\ncx.background_executor() do? Good questions, let's find answers.\n\n## macOS as our async runtime\n\nOne remarkable thing about async Rust is that it allows you to choose your own\nruntime. That's different from a lot of other languages (such as JavaScript)\nin which you can also write asynchronous code. Runtime isn't a term with very\nsharp definition, but for our purposes here, we can say that a runtime is the\nthing that runs your asynchronous code and provides you with utilities such as\n.spawn and something like an Executor.\n\nThe most popular of these runtimes is probably tokio. But there's also smol,\nembassy and others. Choosing and switching runtimes comes with tradeoffs, they\nare only interchangable to a degree, but it is possible.\n\nIn Zed for macOS, as it turns out, we don't use any one of these. We also\ndon't use async_task's Executor. But there has to be something to execute the\nasynchronous code, right? Otherwise I wouldn't be typing these lines in Zed.\n\nSo what then does cx.spawn do and what is the cx.background_executor()? Let's\ntake a look. Here are three relevant methods from GPUI's AppContext:\n\n    \n    \n    // crates/gpui/src/app.rs impl AppContext { pub fn background_executor(&self) -> &BackgroundExecutor { &self.background_executor } pub fn foreground_executor(&self) -> &ForegroundExecutor { &self.foreground_executor } /// Spawns the future returned by the given function on the thread pool. The closure will be invoked /// with [AsyncAppContext], which allows the application state to be accessed across await points. pub fn spawn<Fut, R>(&self, f: impl FnOnce(AsyncAppContext) -> Fut) -> Task<R> where Fut: Future<Output = R> + 'static, R: 'static, { self.foreground_executor.spawn(f(self.to_async())) } // [...] }\n\nAlright, two executors, foreground_executor and background_executor, and both\nhave .spawn methods. We already saw background_executor's .spawn above in\nshow_cursor_names and here, in AppContext.spawn, we see the\nforeground_executor counterpart.\n\nOne level deeper, we can see what foreground_executor.spawn does:\n\n    \n    \n    // crates/gpui/src/executor.rs impl ForegroundExecutor { /// Enqueues the given Task to run on the main thread at some point in the future. pub fn spawn<R>(&self, future: impl Future<Output = R> + 'static) -> Task<R> where R: 'static, { let dispatcher = self.dispatcher.clone(); fn inner<R: 'static>( dispatcher: Arc<dyn PlatformDispatcher>, future: AnyLocalFuture<R>, ) -> Task<R> { let (runnable, task) = async_task::spawn_local(future, move |runnable| { dispatcher.dispatch_on_main_thread(runnable) }); runnable.schedule(); Task::Spawned(task) } inner::<R>(dispatcher, Box::pin(future)) } // [...] }\n\nThere's a lot going on here, a lot of syntax, but what happens can be boiled\ndown to this: the .spawn method takes in a future, turns it into a Runnable\nand a Task, and asks the dispatcher to run it on the main thread.\n\nThe dispatcher here is a PlatformDispatcher. That's the GPUI equivalent of\nasync_task's Executor from above. It has Platform in its name because it has\ndifferent implementations for macOS, Linux, and Windows. But in this post,\nwe're only going to look at macOS, since that's our best-supported platform at\nthe moment and Linux/Windows implementations are still work-in-progress.\n\nSo what does dispatch_on_main_thread do? Does this now call an async runtime?\nNo, no runtime there either:\n\n    \n    \n    // crates/gpui/src/platform/mac/dispatcher.rs impl PlatformDispatcher for MacDispatcher { fn dispatch_on_main_thread(&self, runnable: Runnable) { unsafe { dispatch_async_f( dispatch_get_main_queue(), runnable.into_raw().as_ptr() as *mut c_void, Some(trampoline), ); } } // [...] } extern \"C\" fn trampoline(runnable: *mut c_void) { let task = unsafe { Runnable::<()>::from_raw(NonNull::new_unchecked(runnable as *mut ())) }; task.run(); }\n\ndispatch_async_f is where the call leaves the Zed codebase, because\ndispatch_async_f is actually a compile-time generated binding to the\ndispatch_async_f function in macOS' Grand Central Dispatch's (GCD).\ndispatch_get_main_queue(), too, is such a binding.\n\nThat's right: Zed, as a macOS application, uses macOS' GCD to schedule and\nexecute work.\n\nWhat happens in the snippet above is that Zed turns the Runnable \u2014 think of it\nas a handle to a Task \u2014 into a raw pointer and passes it to dispatch_async_f\nalong with a trampoline, which puts it on its main_queue.\n\nWhen GCD then decides it's time to run the next item on the main_queue, it\npops it off the queue, and calls trampoline, which takes the raw pointer,\nturns it back into a Runnable and, to poll the Future behind its Task, calls\n.run() on it.\n\nAnd, as I learned to my big surprise: that's it. That's essentially all the\ncode necessary to use GCD as a \"runtime\" for async Rust. Where other\napplications use tokio or smol, Zed uses thin wrappers around GCD and crates\nsuch as async_task.\n\nWait, but what about the BackgroundExecutor? It's very, very similar to the\nForegroundExecutor, with the main difference being that the BackgroundExecutor\ncalls this method on PlatformDispatcher:\n\n    \n    \n    impl PlatformDispatcher for MacDispatcher { fn dispatch(&self, runnable: Runnable, _: Option<TaskLabel>) { unsafe { dispatch_async_f( dispatch_get_global_queue(DISPATCH_QUEUE_PRIORITY_HIGH.try_into().unwrap(), 0), runnable.into_raw().as_ptr() as *mut c_void, Some(trampoline), ); } } }\n\nThe only difference between this dispatch method and dispatch_async_f from\nabove is the queue. The BackgroundExecutor doesn't use the main_queue, but a\nglobal queue.\n\nLike I did when I first read through this code, you now might wonder: why?\n\nWhy use GCD? Why have a ForegroundExecutor and a BackgroundExecutor? What's so\nspecial about the main_queue?\n\n## Never block the main thread\n\nIn a native UI application, the main thread is important. No, the main thread\nis holy. The main thread is where the rendering happens, where user input is\nhandled, where the operating system communicates with the application. The\nmain thread should never, ever block. On the main thread, the responsiveness\nof your app lives or dies.\n\nThat's true for Cocoa applications on macOS too. Rendering, receiving user\ninput, communication with macOS, and other platform concerns have to happen on\nthe main thread. And since Zed wants perfect cooperation with macOS to ensure\nhigh-performance and responsiveness, it does two things.\n\nFirst, it uses GCD to schedule its work \u2014 on and off the main thread \u2014 so that\nmacOS can maintain high responsiveness and overall system efficiency.\n\nSecond, the importance of the main thread is baked into GPUI, the UI\nframework, by explicitly making the distinction between the ForegroundExecutor\nand the BackgroundExecutor, both of which we saw above.\n\nAs a writer of application-level Zed code, you should always be mindful of\nwhat happens on the main thread and never put too much blocking work on it. If\nyou were to put, say, a blocking sleep(10ms) on the main thread, rendering the\nUI now has to wait for that sleep() to finish, which means that rendering the\nnext frame would take longer than 8ms \u2014 the maximum frame time available if\nyou want to achieve 120 FPS. You'd \"drop a frame\", as they say.\n\nKnowing that, let's take a look at another small snippet of code. This time\nit's from the built-in terminal in Zed, a function that searches through the\ncontents of the terminal buffer:\n\n    \n    \n    // crates/terminal/src/terminal.rs pub struct Terminal { term: Arc<Mutex<alacritty_terminal::Term<ZedListener>>>, // [... other fields ...] } pub fn find_matches( &mut self, mut searcher: RegexSearch, cx: &mut ModelContext<Self>, ) -> Task<Vec<RangeInclusive<AlacPoint>>> { let term = self.term.clone(); cx.background_executor().spawn(async move { let term = term.lock(); all_search_matches(&term, &mut searcher).collect() }) }\n\nThe first line in find_matches, the self.term.clone(), happens on the main\nthread and is quick: self.term is an Arc<Mutex<...>>, so cloning only bumps\nthe reference count on the Arc. The call to .lock() then only happens in the\nbackground, since .lock() might block. It's unlikely that there will be\ncontention for this lock in this particular code path, but if there was\ncontention, it wouldn't freeze the UI, only a single background thread. That's\nthe pattern: if it's quick, you can do it on the main thread, but if it might\ntake a while or even block, put it on a background thread by using\ncx.background_executor().\n\nHere's another example, the project-wide search in Zed (\u2318-shift-f). It pushes\nas much heavy work as possible onto background threads to ensure Zed stays\nresponsive while searching through tens of thousands of files in your project.\nHere's a simplified and heavily-commented excerpt from Project.search_local\nthat shows the main part of the search:\n\n    \n    \n    // crates/project/src/project.rs // Spawn a Task on the background executor. The Task finds all files on disk // that contain >1 matches for the given `query` and sends them back over // the `matching_paths_tx` channel. let (matching_paths_tx, matching_paths_rx) = smol::channel::bounded(1024); cx.background_executor() .spawn(Self::background_search( // [... other arguments ... ] query.clone(), matching_paths_tx, )) .detach(); // Setup a channel on which we stream results to the UI. let (result_tx, result_rx) = smol::channel::bounded(1024); // On the main thread, spawn a Task that first... cx.spawn(|this, mut cx| async move { // ... waits for the background thread to return the filepaths of // the maximum number of files that we want to search... let mut matching_paths = matching_paths_rx .take(MAX_SEARCH_RESULT_FILES + 1) .collect::<Vec<_>>() .await; // ... then loops over the filepaths in chunks of 64... for matching_paths_chunk in matching_paths.chunks(64) { let mut chunk_results = Vec::new(); for matching_path in matching_paths_chunk { // .... opens each file.... let buffer = this.update(&mut cx, |this, cx| { this.open_buffer((*worktree_id, path.clone()), cx) })?; // ... and pushes into `chunk_results` a Task that // runs on the main thread and ... chunk_results.push(cx.spawn(|cx| async move { // ... waits for the file to be opened ... let buffer = buffer.await?; // ... creates a snapshot of its contents ... let snapshot = buffer.read_with(&cx, |buffer, _| buffer.snapshot())?; // ... and again starts a Task on the background executor, // which searches through the snapshot for all results. let ranges = cx .background_executor() .spawn(async move { query .search(&snapshot, None) .await .iter() .collect::<Vec<_>>() }) .await; Ok((buffer, ranges)) })); } // On the main thread, non-blocking, wait for all buffers to be searched... let chunk_results = futures::future::join_all(chunk_results).await; for result in chunk_results { if let Some((buffer, ranges)) = result.log_err() { // send the results over the results channel result_tx .send(SearchResult::Buffer { buffer, ranges }) .await?; } } } }) .detach(); result_rx\n\nIt's a lot of code \u2014 sorry! \u2014 but there's not a lot more going on than the\nconcepts we already talked about. What's noteworthy here and why I wanted to\nshow it is the ping-pong between the main thread and background threads:\n\n  * main thread: kicks off the search and hands the query over to background thread\n  * background thread: finds files in project with >1 occurrences of query in them, sends results back over channel as they come in\n  * main thread: waits until background thread has found MAX+1 results, then drops channel, which causes background thread to exit\n  * main thread: spawns multiple other main-thread tasks to open each file & create a snapshot.\n  * background threads: search through buffer snapshot to find all results in a buffer, sends results back over channel\n  * main thread: waits for background thread to find results in all buffers, then sends them back to the caller of the outer search_local method\n\nEven though this method can be optimized and the search made a lot faster (we\nhaven't gotten around to that yet), it can already search thousands of files\nwithout blocking the main thread, while still using multiple CPU cores.\n\n## Async-Friendly Data Structures, Testing Executors, and More\n\nI'm pretty sure that the previous code excerpt raised a lot of questions that\nI haven't answered yet: how exactly is it possible to send a buffer snapshot\nto a background thread? How efficient is it do that? What if I want to modify\nsuch a snapshot on another thread? How do you test all this?\n\nAnd I'm sorry to say that I couldn't fit all of the answers into this post.\nBut there is a companion video in which Antonio and I did dive into a lot of\nthese areas and talked about async-friendly data structures, copy-on-write\nbuffer snapshots, and other things. Antonio also gave a fantastic talk about\nhow we do property-testing of async Rust code in the Zed code base that I\nhighly recommend. I also promise that in the future there will be a post about\nthe data structures underlying the Zed editor.\n\nUntil next time!\n\n## Interested in trying Zed out?\n\nYou can try Zed today on macOS. Download now!\n\nThorsten Ball\n\nAntonio Scandurra\n\n04/09/24\n\n\u00a9 2024 Zed Industries.\n\nEULA \u00b7 Attributions \u00b7 Sign in\n\n#### Product\n\n  * Download\n  * FAQ\n  * Roadmap\n  * EULA\n  * CLA\n\n#### Developers\n\n  * Docs\n  * Releases\n  * GitHub\u2197\n  * Status\u2197\n\n#### Community\n\n  * Blog\n  * Feedback\n  * Discussions\u2197\n  * Merch\u2197\n\n#### Company\n\n  * About\n  * Team\n  * Twitter\u2197\n\n", "frontpage": true}
