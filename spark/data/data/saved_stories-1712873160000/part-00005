{"aid": "40003701", "title": "Redis <> RQScheduler <> Celery in Django Back End for Scheduling and Concurrency", "url": "https://suprsend.tech/redis-rqscheduler-celery-for-dynamic-task-scheduling-and-concurrent-execution-in-django-backend", "domain": "suprsend.tech", "votes": 4, "user": "Derek122", "posted_at": "2024-04-11 16:05:01", "comments": 0, "source_title": "Redis <-> RQScheduler <-> Celery for Dynamic Task Scheduling and Concu", "source_text": "Redis <-> RQScheduler <-> Celery for Dynamic Task Scheduling and Concu\n\n#\n\nNotification Service API\n\n# Notification Service API\n\n# Redis <-> RQScheduler <-> Celery for Dynamic Task Scheduling and Concurrent\nExecution in Django Backend\n\nNik L\n\n\u00b7Apr 11, 2024\u00b7\n\n6 min read\n\nIn our previous setup, the initial problem seemed straightforward:\nimplementing a scheduling mechanism for database queries using goroutines.\nThis approach worked well with minimal resources and SQLite in a Go service.\nHowever, when integrating this functionality into our SaaS platform, we\nencountered new challenges related to dynamic scheduling and concurrent task\nexecution.\n\nWe needed to synchronize data from clients' data warehouses to our data store\non a scheduled basis.\n\nRecreating the previous setup posed two main challenges:\n\n  1. Dynamic Scheduling: Our original architecture allowed users to link their data warehouses, execute database queries, and synchronize subscribers on a predefined schedule (e.g., hourly, daily). Initially, this seemed manageable due to our use of an embedded SQLite database and the assumption of limited simultaneous executions. Furthermore, leveraging Golang's goroutines and the Asynq library facilitated efficient scheduling without needing a separate process. However, transitioning to our SaaS platform introduced the need for dynamic scheduling from various processes and concurrent execution of these schedules.\n\n  2. Concurrent Task Execution: The process involved executing queries, processing results, and syncing subscribers concurrently.\n\nWe devised a solution leveraging Redis's sorted set data structure to address\nthese challenges. This approach would allow for efficient task scheduling and\nmanagement, adapting to dynamic schedule changes.\n\n## Dynamic Scheduling:\n\nWe envisioned a scenario where tasks could be scheduled at any time, either\nvia cron schedules or fixed intervals. Our scheduler needed to prioritize and\nefficiently manage tasks, adapting to dynamic changes.\n\nWe chose Redis's Sorted Set because it stores and retrieves tasks based on\nexecution time or priority. Internally, Redis implements sorted sets using a\nhash table and a skip list, providing fast access and maintaining sorted\norder. Tasks are stored as unique strings with associated scores representing\nexecution time or priority. Lower scores denote higher priority, enabling\nquick retrieval of tasks due for execution. Redis commands such as ZADD and\nZRANGEBYSCORE facilitate adding tasks to the sorted set and retrieving the\nhighest priority task, respectively.\n\n### Let\u2019s understand with an example:\n\nSuppose we have a task scheduling system with different priorities (low,\nmedium, high) and execution times. We want to schedule tasks such that high-\npriority tasks are executed before low-priority tasks, even if a low-priority\ntask has an earlier execution time. To achieve this, we can use a scoring\nalgorithm that combines the priority and execution time into a single score.\n\nExample scoring algorithm:\n\n    \n    \n    def calculate_score(priority, execution_time): # Convert execution_time to a UNIX timestamp unix_timestamp = execution_time.timestamp() # Assign numeric values to priorities (lower value means higher priority) priority_values = {'low': 3, 'medium': 2, 'high': 1} # Calculate the score by combining the priority value and UNIX timestamp score = unix_timestamp + (10**9 * priority_values[priority]) return score\n\nNow, let's add tasks to the Redis Sorted Set using the ZADD command:\n\n    \n    \n    // Connect to Redis r = redis.Redis() # Add tasks with their calculated scores r.zadd('scheduled_tasks', { 'Task A (low)': calculate_score('low', datetime(2023, 3, 15, 10, 0, 0)), 'Task B (medium)': calculate_score('medium', datetime(2023, 3, 15, 10, 15, 0)), 'Task C (high)': calculate_score('high', datetime(2023, 3, 15, 10, 30, 0)), 'Task D (low)': calculate_score('low', datetime(2023, 3, 15, 10, 45, 0)), })\n\nTo retrieve tasks due for execution, we can use the ZRANGEBYSCORE command with\nthe current UNIX timestamp as the minimum score and a large value (e.g., +inf)\nas the maximum score:\n\n    \n    \n    import datetime # Get the current UNIX timestamp current_timestamp = datetime.datetime.utcnow().timestamp() # Retrieve tasks due for execution due_tasks = r.zrangebyscore('scheduled_tasks', current_timestamp, '+inf')\n\nThis approach ensures that tasks with higher priority are executed before\ntasks with lower priority, even if they have later execution times.\n\nNow that the scoring and scheduling part is clear let\u2019s try to understand how\nwe can leverage this to build a robust system that can schedule tasks from a\nseparate producer process and utilize scheduler, worker, and Redis to function\nin sync.\n\n  * We would need a producer process/ processes to put the task in Redis using ZADD in Redis\u2019s sorted set.\n\n  * We would need a scheduler to continuously poll for tasks in Redis using ZRANGEBYSCORE and current timestamp and assign the task to existing workers.\n\n  * Finally, we would need a worker process to execute the task and produce heartbeats when the task is completed so that the scheduler can update the execution progress.\n\nIn our case, the API server was our producer.\n\n## Implementation:\n\nWe evaluated various libraries that would utilize this unique functionality\nprovided by Redis, and we found that rq_scheduler in Python ticks all the\nboxes.\n\nWe also evaluated:\n\n  1. APScheduler: It lacked a separate process for scheduler and worker, which is required since we would ideally want to decouple these processes from our main API server.\n\n  2. Celerybeat: Celerybeat didn\u2019t support dynamic scheduling and hence wasn\u2019t ideal.\n\n  3. RQ-scheduler: This implements exactly the algorithm explained above and was ideal for our use case; also, its availability in Django was a plus.\n\nNow, this is how the final architecture looked like:\n\nSuprSend's new architecture around handling multi-tenancy\n\n### For Concurrent DB Writes\n\nOur previous setup, SQLite, wouldn\u2019t work for distributed applications like\nours because:\n\n  1. Concurrency Limitations: SQLite's file-based locking can cause contention issues in scenarios with high concurrent writes.\n\n  2. File-based Locking: SQLite's reliance on file-level locks impedes concurrent write operations in a distributed environment.\n\n  3. Limited Scalability: SQLite's serverless design becomes a bottleneck as the number of nodes and concurrent writes increases.\n\n  4. ACID Compliance Challenges: Ensuring ACID properties across distributed nodes introduces complexities for SQLite.\n\n  5. Data Integrity Concerns: File-based locking can lead to data corruption or loss of integrity in distributed systems.\n\n  6. No Built-in Network Protocol: SQLite's direct communication with the local file system limits its use in distributed environments.\n\nConsidering the situation where we had to handle distributed writes from\nmultiple processes on the same DB. We chose to use Redis or Postgres for our\ndistributed application. Since each query execution involved handling multiple\nstates and processing results in batches to alleviate server load, we opted\nfor Postgres as our database.\n\nPostgres solves all the abovementioned issues related to distributed and\nconcurrent writes and scalability support, which was ideal for our use case.\nThe only drawback was potentially a little extra compute cost to cloud\nproviders for Postgres usage. Still, the cost paid for a bad customer\nexperience is much larger and potentially catastrophic.\n\nWell, after architecting the solution efficiently, processing the queries,\nwhich can sometimes even fetch a billion rows (or more), was another critical\nproblem to solve, which we solved by creating a separate service to process\nthe tasks as seen in the architecture diagram. Which processed the tasks and\nsent events to SuprSend internally for subscriber addition.\n\nAnyways, this is how SuprSend notification infrastructure architecture works\nto abstract the pain of building and maintaining notification service on your\nend. What do you think?\n\n## Why We Use Both RQ and Celery?\n\nRQ to schedule the task and celery to handle task execution; since tasks can\nbe long/heavy, we didn't want rq workers to get blocked or in error cases,\nhandle restarts. And as you rightly said, we didn't want to spend extra\nefforts managing and actively maintaining the RQ backend in the future,\nwhereas we already actively manage and maintain celery. Hence, Celery would be\nbetter suited to handle task execution and free up extra bandwidth and load\nfor RQ workers.\n\nAlso, it's better to stick with the native workers and scheduler provided by\nthe task queue system you're using (either Celery or RQ). This ensures\ncompatibility, simplicity, and optimal resource utilization. Also, we'd want\nRQ workers to handle smaller tasks in the future, whereas coupling Celery\nworkers with RQ schedulers introduces an extra layer of complexity, i.e.,\nconfiguring celery workers to also listen to RQ's task queue, and handling\nfailures, errors, retries, which just makes it even more complex.\n\nYou could try this functionality directly on the platform. Signup now for\nfree.\n\nIf you learnt something from this content, consider sharing it with your dev\nfriends or on Hackernews :)\n\n## Subscribe to my newsletter\n\nRead articles from Notification Service API directly inside your inbox.\nSubscribe to the newsletter, and don't miss out.\n\nPythonDjangocelerynotificationsPostgreSQLSQLiteGo LanguageAPIsReactJavaScript\n\n### Written by\n\n# Nik L\n\nDeveloper from SuprSend.\n\nDeveloper from SuprSend.\n\nShare this\n\n### More articles\n\nNik L\n\n# Optimizing Task Scheduling and Concurrent Operations Through Redis &\nPostgres in Python Backend\n\nThe problem initially appeared straightforward, but it quickly became apparent\nthat we had underesti...\n\nNik L\n\n# 7 Components of our Notification Service we Shifted from Devs to PMs\n\nA good notification service is more than a communication channel; it can\nbridge the gap between the ...\n\nNik L\n\n# What Is A Good Notification Service and How We Built One for SuprSend?\n\nIn today\u2019s digital age, effective communication has become a cornerstone of\ncustomer engagement and ...\n\n\u00a92024 Notification Service API\n\nArchive\u00b7Privacy policy\u00b7Terms\n\nWrite on Hashnode\n\nPowered by Hashnode - Home for tech writers and readers\n\n", "frontpage": false}
