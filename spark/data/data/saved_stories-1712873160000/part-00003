{"aid": "40003695", "title": "OpenEQA: From word models to world models", "url": "https://ai.meta.com/blog/openeqa-embodied-question-answering-robotics-ar-glasses/", "domain": "meta.com", "votes": 2, "user": "mfiguiere", "posted_at": "2024-04-11 16:04:48", "comments": 0, "source_title": "OpenEQA: From word models to world models", "source_text": "OpenEQA: From word models to world models\n\nResearch\n\nBlog\n\nResources\n\nAbout\n\nResearch\n\nBlog\n\nResources\n\nAbout\n\nEmbodied AI\n\nOpenEQA: From word models to world models\n\nApril 11, 2024\n\nTakeaways:\n\n  * We\u2019re releasing the Open-Vocabulary Embodied Question Answering (OpenEQA) benchmark, which measures an AI agent\u2019s understanding of physical spaces via questions like \u201cWhere did I leave my badge?\u201d\n  * We benchmarked state-of-art vision+language models (VLMs) and found a significant gap between human-level performance and even the best models. In fact, for questions that require spatial understanding, today\u2019s VLMs are nearly \u201cblind\u201d\u2014access to visual content provides no significant improvement over language-only models.\n  * We hope releasing OpenEQA will help motivate and facilitate open research into helping AI agents understand and communicate about the world it sees, an essential component for artificial general intelligence.\n\nImagine an embodied AI agent that acts as the brain of a home robot or a\nstylish pair of smart glasses. Such an agent needs to leverage sensory\nmodalities like vision to understand its surroundings and be capable of\ncommunicating in clear, everyday language to effectively assist people. This\nis akin to building a \u201cworld model\u201d\u2014an agent\u2019s internal representation of the\nexternal world, that is queryable through language. It\u2019s a long-term vision\nand a daunting research challenge\u2014one that Meta is actively exploring.\n\nToday, we\u2019re introducing the Open-Vocabulary Embodied Question Answering\n(OpenEQA) framework\u2014a new benchmark to measure an AI agent\u2019s understanding of\nits environment by probing it with open-vocabulary questions. This is similar\nto how we might assess a human\u2019s understanding of a concept, by asking them\nquestions and evaluating their answers. OpenEQA contains two tasks: (1)\nepisodic memory EQA, in which an embodied AI agent answers questions based on\nits recollection of past experiences, and (2) active EQA, in which the agent\nmust take action within the environment to gather necessary information and\nanswer questions.\n\nEQA has direct applications too, and even a basic version of it can simplify\nyour everyday life. For example, let\u2019s say you\u2019re getting ready to leave the\nhouse and can\u2019t find your office badge. You could ask your smart glasses where\nyou left it, and the agent might respond that the badge is on the dining table\nby leveraging its episodic memory. Or if you were hungry on the way back home,\nyou could ask your home robot if there\u2019s any fruit left. Based on its active\nexploration of the environment, it might respond that there are ripe bananas\nin the fruit basket. See the video at the top of this post to see EQA in\naction.\n\nSounds simple enough, right? After all, LLMs have excelled in tasks many\npeople find challenging, like passing the SAT or bar exams. But the reality is\nthat even today\u2019s most advanced models struggle to match human performance\nwhen it comes to EQA, another manifestation of Moravec\u2019s paradox. That\u2019s why\nwe\u2019re also releasing the OpenEQA benchmark, so researchers can test their own\nmodels and see how they stack up against humans.\n\nWhy EQA? From \u201cword models\u201d to \u201cworld models\u201d\n\nWe\u2019ve seen exciting developments in the space of large language models (LLMs),\nwhich seem to have captured a basic linguistic understanding of the world.\nLLMs can answer all kinds of questions based on their historical knowledge,\nbut they have no idea what is currently going on in the world around them. By\nenhancing LLMs with the ability to \u201csee\u201d the world and situating them in a\nuser\u2019s smart glasses or on a home robot, we can open up new applications and\nadd value to people\u2019s lives.\n\nIt\u2019s an exciting problem statement because, as Jitendra Malik puts it, it\nshowcases the difference between building world models, and word models. In\nother words, rather than simply predicting the next token in a string, an\nembodied AI agent that excels at EQA would show that it\u2019s grounded in an\nunderstanding of the physical world. Such world models are an important step\ntoward our vision of artificial general intelligence (AGI).\n\nTo that end, EQA is a tool that probes if an AI agent really understands what\nis going on in the world around them. After all, when we want to determine how\nwell a human understands a concept, we ask them questions and form an\nassessment based on their answers. We can do the same with embodied AI agents.\n\nOpenEQA: A novel benchmark for Embodied AI\n\nOpenEQA is the first open-vocabulary benchmark for EQA, which we believe will\nhelp researchers track future progress in multimodal learning and scene\nunderstanding. The benchmark features over 1,600 non-templated pairs of\nquestions and answers from human annotators that are representative of real-\nworld use cases, as well as pointers to more than 180 videos and scans of\nphysical environments. Our question-and-answer pairs were validated by\ndifferent human annotators to ensure that the questions are answerable and the\nanswers provided are correct.\n\nOpenEQA also comes equipped with LLM-Match, an automatic evaluation metric for\nscoring open vocabulary answers. In fact, through blind user studies, we found\nthat LLM-Match is as correlated to humans as two humans are to each other.\n\nWe used OpenEQA to benchmark several state-of-art vision+language foundation\nmodels (VLMs) and found a significant gap between even the most performant\nmodels (GPT-4V at 48.5%) and human performance (85.9%). Of particular\ninterest, for questions that require spatial understanding, even the best VLMs\nare nearly \u201cblind\u201d\u2014i.e., they perform not much better than text-only models,\nindicating that models leveraging visual information aren\u2019t substantially\nbenefitting from it and are falling back on priors about the world captured in\ntext to answer visual questions. As an example, for the question \u201cI\u2019m sitting\non the living room couch watching TV. Which room is directly behind me?\u201d, the\nmodels guess different rooms essentially at random without significantly\nbenefitting from visual episodic memory that should provide an understanding\nof the space. This suggests that additional improvement on both perception and\nreasoning fronts are needed before embodied AI agents powered by such models\nare ready for primetime.\n\nOpenEQA combines challenging open-vocabulary questions with the ability to\nanswer in natural language. This results in a straightforward benchmark that\ndemonstrates a strong understanding of the environment\u2014and poses a\nconsiderable challenge to current foundational models. We hope this work\nmotivates additional research into helping AI understand and communicate about\nthe world it sees.\n\nAt FAIR, we\u2019re working to build world models capable of performing well on\nOpenEQA, and we welcome others to join us in that effort.\n\nRead the paper\n\nGet the dataset\n\nShare:\n\nOur latest updates delivered to your inbox\n\nSubscribe to our newsletter to keep up with Meta AI news, events, research\nbreakthroughs, and more.\n\nJoin us in the pursuit of what\u2019s possible with AI.\n\nSee all open positions\n\nRelated Posts\n\nComputer Vision\n\nIntroducing Segment Anything: Working toward the first foundation model for\nimage segmentation\n\nApril 5, 2023\n\nRead post\n\nFEATURED\n\nResearch\n\nMultiRay: Optimizing efficiency for large-scale AI models\n\nNovember 18, 2022\n\nRead post\n\nFEATURED\n\nML Applications\n\nMuAViC: The first audio-video speech translation benchmark\n\nMarch 8, 2023\n\nRead post\n\nWho We Are\n\nAbout\n\nPeople\n\nCareers\n\nEvents\n\nLatest Work\n\nResearch\n\nInfrastructure\n\nBlog\n\nResources\n\nOur Actions\n\nResponsibilities\n\nNewsletter\n\nSign Up\n\nWho We Are\n\nWho We AreAboutPeopleCareersEvents\n\nLatest Work\n\nLatest WorkResearchInfrastructureBlogResources\n\nOur Actions\n\nOur ActionsResponsibilities\n\nNewsletter\n\nNewsletterSign Up\n\nPrivacy Policy\n\nTerms\n\nCookies\n\nMeta \u00a9 2024\n\nAllow the use of cookies from Meta on this browser?\n\nWe use essential cookies and similar technologies to help:\n\nProvide and improve content on Meta Products\n\nProvide a safer experience by using information we receive from cookies on and\noff Meta Products\n\nProvide and improve Meta Company Products for people using a Meta or Oculus\naccount\n\nWe use tools on Meta from other companies that also use cookies. These tools\nare used for things like:\n\n  * Advertising and measurement services off of Meta Products\n  * Analytics\n  * Providing certain features\n  * Improving our services\n\nYou can allow the use of all cookies, just essential cookies or you can choose\nmore options below. You can learn more about cookies and how we use them, and\nreview or change your choice at any time in our Cookie Policy.\n\nEssential cookies\n\nThese cookies are required to use Meta Company Products. They\u2019re necessary for\nMeta websites to work as intended.\n\nOptional cookies\n\nOptional cookies from other companies\n\nWe use tools from other companies for advertising and measurement services off\nof Meta Company Products, analytics, and to provide certain features and\nimprove our services for you. These companies also use cookies.\n\nIf you allow these cookies:\n\n  * We\u2019ll be able to better personalize ads for you off of Meta Products, and measure their performance\n  * Features on our products will not be affected\n  * Other companies will receive information about you when you use cookies\n\nIf you don\u2019t allow these cookies:\n\n  * We won\u2019t use cookies from other companies to help personalize ads for you off of Meta Products or measure ads performance\n  * Some features on our products may not work\n\nOther ways you can control tracking\n\nAd settings\n\nIf you have added your Meta or Oculus account to the same Accounts Center as\nyour Facebook or Instagram account, you can manage how different data is used\nto personalize ads in ad settings. To show you better ads, we use data that\nadvertisers and other partners provide us about your activity off Meta Company\nProducts, including websites and apps. You can control whether we use this\ndata to show you ads in your ad settings.\n\nThe Facebook Audience Network is a way for advertisers to show you ads in apps\nand websites off the Meta Company Products. One of the ways Audience Network\nshows relevant ads is by using your ad preferences to determine which ads you\nmay be interested in seeing.\n\nAd preferences\n\nIn Ad preferences, you can choose whether we show you ads and make choices\nabout the information used to show you ads.\n\nYou can opt out of seeing online interest-based ads from Meta and other\nparticipating companies through the Digital Advertising Alliance in the US,\nthe Digital Advertising Alliance of Canada in Canada or the European\nInteractive Digital Advertising Alliance in Europe, or through your mobile\ndevice settings, if you are using Android, iOS 13 or an earlier version of\niOS. Please note that ad blockers and tools that restrict our cookie use may\ninterfere with these controls.The advertising companies we work with generally\nuse cookies and similar technologies as part of their services. To learn more\nabout how advertisers generally use cookies and the choices they offer, you\ncan review the following resources:\n\n  * Digital Advertising Alliance\n  * Digital Advertising Alliance of Canada\n  * European Interactive Digital Advertising Alliance\n\nYour browser or device may offer settings that allow you to choose whether\nbrowser cookies are set and to delete them. These controls vary by browser,\nand manufacturers may change both the settings they make available and how\nthey work at any time. As of 5 October 2020, you may find additional\ninformation about the controls offered by popular browsers at the links below.\nCertain parts of Meta Products may not work properly if you have disabled\nbrowser cookies. Please be aware these controls are distinct from the controls\nthat Instagram and Facebook offer.\n\n  * Google Chrome\n  * Internet Explorer\n  * Firefox\n  * Safari\n  * Safari Mobile\n  * Opera\n\n", "frontpage": false}
