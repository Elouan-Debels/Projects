{"aid": "40003696", "title": "Why reliable AI requires a paradigm shift", "url": "https://blog.apiad.net/p/reliable-ai-is-harder-than-you-think", "domain": "apiad.net", "votes": 1, "user": "apiad", "posted_at": "2024-04-11 16:04:52", "comments": 0, "source_title": "Why reliable AI requires a paradigm shift", "source_text": "Why reliable AI requires a paradigm shift\n\n# Mostly Harmless Ideas\n\nShare this post\n\n#### Why reliable AI requires a paradigm shift\n\nblog.apiad.net\n\n#### Discover more from Mostly Harmless Ideas\n\nOpinions, essays, tutorials, educational articles, and short comments on\nComputer Science research, practice, and education.\n\nOver 2,000 subscribers\n\nContinue reading\n\nSign in\n\n# Why reliable AI requires a paradigm shift\n\n### Hallucinations are the fundamental barrier for the widespread use of AI,\nand they won't be solved anytime soon.\n\nAlejandro Piad Morffis\n\nApr 11, 2024\n\n10\n\nShare this post\n\n#### Why reliable AI requires a paradigm shift\n\nblog.apiad.net\n\n7\n\nShare\n\nIn the context of AI systems, hallucinations refer to the phenomenon where an\nAI model generates outputs that appear plausible and coherent but do not\naccurately reflect real-world facts or the system's intended purpose. These\nhallucinations can manifest in various forms, such as fabricated information,\nnonsensical responses, or outputs that are inconsistent with the input data.\nHallucinations can occur in various AI applications, from natural language\nprocessing to computer vision and beyond.\n\nAddressing the issue of hallucinations in AI is crucial for ensuring the\nreliable and trustworthy deployment of these systems. Hallucinations can lead\nto erroneous decision-making, false conclusions, and potentially harmful\noutcomes, especially in critical applications such as healthcare, finance, and\npublic safety, just to mention a few. By understanding the causes and\nmechanisms behind hallucinations, researchers and developers can work to\nmitigate these issues, improving the overall robustness and reliability of AI\nsystems.\n\nIn this issue, I want to explore the nature and impact of hallucinations in\ncurrent generative AI models, explicitly focusing on language models. However,\nI believe these insights can be extrapolated to other domains where generative\nmodels are used.\n\nThe central thesis of this article is that although hallucinations can be\nreduced or mitigated with a variety of practical approaches, the core issue is\na fundamental flaw in the assumptions about the nature of language and truth\nthat are intrinsic to the prevalent language modeling paradigms used today. If\nthis thesis is correct, we won't be able to solve AI hallucinations entirely\nwith incremental improvements to current tech; we will need a new machine\nlearning paradigm altogether.\n\nMostly Harmless Ideas is a reader-supported publication. To receive new posts\nand support my work, consider becoming a free or paid subscriber.\n\n##\n\nWhat are Hallucinations in AI?\n\nThe term \"hallucination\" in the context of AI refers to the phenomenon where a\nlarge language model (LLM) or other generative AI system produces outputs that\nappear plausible and coherent but do not accurately reflect reality or the\nsystem's intended purpose. These hallucinations manifest as the generation of\nfalse, inaccurate, or nonsensical information that the AI presents with\nconfidence as if it were factual.\n\nFirst, a caveat. Unlike human hallucinations, which involve perceiving things\nthat are not real, AI hallucinations are associated with the model producing\nunjustified responses or beliefs rather than perceptual experiences. The name\n\"hallucination\" is, therefore, imperfect, and it often leads to mistakes as\npeople tend to anthropomorphize these models and make erroneous assumptions\nabout how they work and the causes of these failures. However, we will stick\nto this name in this article because it is the prevalent nomenclature used\neverywhere people talk about AI. Just keep in mind we're talking about\nsomething completely different from what the term \"hallucination\" means in\ngeneral.\n\nBefore diving into the why of AI hallucinations, let's distinguish them from\nother typical failures of generative models, such as out-of-distribution\nerrors or biased outputs.\n\nOut-of-distribution errors occur when an AI model is presented with input data\nsignificantly different from its training data, causing it to produce\nunpredictable or nonsensical outputs. In these cases, the model's limitations\nare clear, and it is evident that the input is outside of its capabilities.\nThis is just a generalization error that usually points to either 1) the\nmodel's hypothesis space is too constrained to entirely capture the actual\ndistribution of data, or 2) the available data or training regimes are\ninsufficient to pinpoint a general enough hypothesis.\n\nHallucinations are more insidious than out-of-distribution errors because they\nhappen within the input distribution, where the model is supposedly well-\nbehaved. Even worse, due to the stochastic nature of generative models,\nhallucinations tend to happen entirely randomly. This means that for the same\ninput, the model can hallucinate once out of 100 times, making it almost\nimpossible to evaluate and debug.\n\nBiased outputs, on the other hand, arise when an AI model's training data or\nalgorithms contain inherent biases, leading to the generation of outputs that\nreflect those biases, such as stereotypes or prejudices. These are often not\nhallucinations but realistic reproductions of human biases that pervade our\nsocieties: The model produces something that reflects the reality underlying\nits training data. It's just that such a reality is an ugly one. Dealing with\nbiases in AI is one of the most critical challenges in making AI safe, but it\nis an entirely different problem that we can tackle in a future issue.\n\nHallucinations, in contrast, involve the AI model generating information that\nis not necessarily biased but completely fabricated or detached from reality.\nThis makes detecting them far more difficult because the model's responses\nappear confident and coherent, and there is no obvious telltale that helps\nhuman evaluators quickly identify them.\n\n##\n\nReal-World Implications of AI Hallucinations\n\nThe occurrence of hallucinations in AI systems, particularly in large language\nmodels (LLMs), can have significant consequences, especially in high-stakes\napplications such as healthcare, finance, or public safety. For example, a\nhealthcare AI model incorrectly identifying a malignant skin lesion as benign\ncan doom a patient. On the other hand, identifying a benign skin lesion as\nmalignant could lead to unnecessary medical interventions, also causing harm\nto the patient. Similarly, in the financial sector, hallucinated outputs from\nan AI system could result in poor investment decisions with potentially\ndevastating economic impacts.\n\nHowever, even in low-stakes applications, the insidious nature of\nhallucinations makes them a fundamental barrier to the widespread adoption of\nAI. For example, imagine you're using an LLM to generate summaries from audio\ntranscripts of a meeting, extracting relevant talking points and actionable\nitems. Suppose the model tends to hallucinate occasionally, either failing to\nextract one key item or producing a spurious item. In that case, it will be\nvirtually impossible for anyone to detect that without manually revising the\ntranscript, thus rendering the whole application of AI in this domain useless.\n\nFor this reason, one of the critical challenges in addressing the real-world\nimplications of language model hallucinations is the difficulty in effectively\ncommunicating the limitations of these systems to end-users. LLMs are trained\nto produce fluent, coherent outputs that appear plausible, even when factually\nincorrect. If the end-users of an AI system are not sufficiently informed to\nreview the system's output with a critical eye, they may never spot any\nhallucinations. This leads to a chain of mistakes as the errors from the AI\nsystem propagate upstream through the layers of decision-makers in an\norganization. Ultimately, you could be making a terrible decision that seems\nentirely plausible given all the available information because the source of\nthe error\u2014an AI hallucination\u2014is impossible to detect.\n\nThus, developing and deploying LLMs with hallucination capabilities raises\ncritical ethical considerations. There is a need for responsible AI\ndevelopment practices that prioritize transparency, accountability, and the\nmitigation of potential harms. This includes establishing clear guidelines for\ntesting and validating LLMs before real-world use and implementing robust\nmonitoring and oversight mechanisms to identify and address hallucinations as\nthey arise.\n\nCrucially, there are absolutely zero generative AI systems today that can\nguarantee they don't hallucinate. This tech is simply unreliable in\nfundamental ways, so every actor in this domain, from developers to users,\nmust be aware there will be hallucinations in your system, and you must have\nguardrails in place to deal with the output of unreliable AIs. And this is so\nperverse because we are used to software just working. Whenever software\ndoesn't do what it should, that's a bug. However, hallucinations are not a bug\nof AI, at least in the current paradigm. As we will see in the next section,\nthey are an inherent feature of how generative models work.\n\n##\n\nWhy do Hallucinations Happen?\n\nThere are many superficial reasons for hallucinations, from data and modeling\nproblems to issues with prompting. However, the underlying cause of all\nhallucinations, at least in large language models, is that the current\nlanguage modeling paradigm used in these systems is, by design, a\nhallucination machine. Let's unpack that.\n\nGenerative AI models, including LLMs, rely on capturing statistical patterns\nin their training data to generate outputs. Rather than storing explicit\nfactual claims, LLMs implicitly encode information as statistical correlations\nbetween words and phrases. This means the models do not have a clear, well-\ndefined understanding of what is true or false. They can just generate\nplausibly sounding text.\n\nThe reason this mainly works is that generating plausibly sounding text has a\nhigh probability of reproducing something that is true, provided you are\ntrained on mostly truthful data. However, large language models (LLMs) are\ntrained on vast corpora of text data from the internet, which contains\ninaccuracies, biases, and even fabricated information. So, these models have\n\"seen\" many true sentences and thus picked up correlations between words that\ntend to generate true sentences. Still, they've also seen many variants of the\nsame sentences that are slightly or even entirely wrong.\n\nSo, one of the primary reasons for the occurrence of hallucinations is the\nlack of grounding in authoritative knowledge sources. Without a strong\nfoundation in verified, factual knowledge, the models struggle to distinguish\ntruth from falsehood, generating hallucinated outputs. But this is far from\nthe only problem. Even if you only train on factual information\u2014assuming there\nwould be enough of such high-quality data to begin with\u2014the statistical nature\nof language models makes them susceptible to hallucination.\n\nSuppose your model has only seen truthful sentences and learned the\ncorrelations between words in these sentences. Imagine there are two very\nsimilar sentences, both factually true, that differ in just a couple of\nwords\u2014maybe a date and a name, for example, \"Person A was born in year X\" and\n\"Person B was born in year Y\". Given the way these models work, the\nprobability of generating a mixed-up sentence like \"Person B was born in year\nX\" is only slightly smaller than generating either of the original sentences.\n\nWhat's going on here is that the statistical model implicitly assumes that\nsmall changes in the input (the sequence of words) lead to small changes in\nthe output (the probability of generating a sentence). In more technical\nterms, the statistical model assumes a smooth distribution, which is necessary\nbecause the size of the data the model needs to encode is orders of magnitude\nlarger than the memory (i.e., the number of parameters) in the model. Thus,\nthe models must compress the training corpus, and compression implies losing\nsome information.\n\nIn other words, statistical language models inherently assume that sentences\nthat are very similar to what they have seen in the training data are also\nplausible. They encode a smooth representation of language, and that's fine as\nlong as you don't equate plausible with factual. See, these models weren't\ndesigned with factuality in mind. They were initially designed for tasks like\ntranslation, where plausibility and coherence are all that matter. It's only\nwhen you turn them into answering machines that you run into a problem.\n\nThe problem is there is nothing smooth about facts. A sentence is either\nfactual or not; there are no degrees of truthfulness\u2014for the most part, let's\nnot get dragged into epistemological discussions here. But LLMs cannot, by\ndesign, define a crisp frontier between true and false sentences. All the\nfrontiers are fuzzy, so there is no clear cutoff point where you can say that\nif a sentence has less than X perplexity value, it is false. And even if you\ncould define such a threshold, it would be different for all sentences.\n\nYou may ask why we can't avoid using this smooth representation altogether.\nThe reason is that you want to generate sentences that are not in the training\nset. This means you must somehow guess that some sentences you have never seen\nare also plausible, and guessing means you must make some assumptions. The\nsmoothness hypothesis is very reasonable\u2014and computationally convenient, as\nthese models are trained with gradient descent, which requires smoothness in\nthe loss function\u2014again, as long as you don't care about factuality. If you\ndon't compress the training data in this smooth, lossy way, you will simply\nnot be able to generate novel sentences at all.\n\nIn summary, this is why the current paradigm of generative AI will always\nhallucinate, no matter how good your data is and how elaborate your training\nprocedures or guardrails are. The statistical language modeling paradigm, at\nits core, is a hallucination machine. It concocts plausibly-sounding sentences\nby mixing and matching words seen together in similar contexts in the training\nset. It has no inherent notion of whether a given sentence is true or false.\nAll it can tell is that it looks like sentences that appear in the training\nset.\n\nNow, a silver lining could be that even if some false sentences are\nunavoidably generated, we can train the system to minimize their occurrence by\nshowing it lots and lots of high-quality data. Can we push the probability of\na hallucination to a sufficiently low value that, in practice, almost never\nhappens? Unfortunately, no. Recent research suggests that if there is a\nsentence that can be generated at all, no matter how low its base probability,\nthen there is a prompt that will generate it with almost 100% certainty. This\nmeans that if we introduce malicious actors into our equation, we can never be\nsure our system can't be jailbroken.\n\n##\n\nMitigating Hallucinations in AI\n\nSo far, we've argued that hallucinations are inherently impossible to\neliminate completely. But this doesn't mean we can't do anything about it in\npractice. I want to end this article with a summary of mitigation approaches\nused today by researchers and developers.\n\nOne key strategy is incorporating external knowledge bases and fact-checking\nsystems into the AI models. The risk of generating fabricated or inaccurate\noutputs can be reduced by grounding the models in authoritative, verified\ninformation sources.\n\nResearchers are also exploring ways to develop more robust model architectures\nand training paradigms less susceptible to hallucinations. This may involve\nincreasing model complexity, incorporating explicit reasoning capabilities, or\nusing specialized training data and loss functions.\n\nEnhancing the transparency and interpretability of AI models is also crucial\nfor addressing hallucinations. By making the models' decision-making processes\nmore transparent, it becomes easier to identify and rectify the underlying\ncauses of hallucinations.\n\nAlongside these technical approaches, developing standardized benchmarks and\ntest sets for hallucination assessment is crucial. This will enable\nresearchers and developers to quantify the prevalence and severity of\nhallucinations and compare the performance of different models. Thus, if you\ncan't completely eliminate the problem, at least you can quantify it and make\ninformed decisions about where and when it is safe enough to deploy a\ngenerative model.\n\nFinally, addressing the challenge of hallucinations in AI requires an\ninterdisciplinary approach involving collaboration between AI researchers,\ndomain experts, and authorities in fields like scientific reasoning, legal\nargumentation, and other relevant disciplines. By fostering cross-disciplinary\nknowledge sharing and research, the understanding and mitigation of\nhallucinations can be further advanced.\n\n##\n\nConclusion\n\nThe issue of hallucinations in AI systems, particularly in large language\nmodels, poses a significant challenge to the reliable and trustworthy\ndeployment of these powerful technologies. Hallucinations, where AI models\ngenerate plausible-sounding but factually inaccurate outputs, can have severe\nconsequences in high-stakes applications and undermine user trust.\n\nThe underlying causes of hallucinations stem from the fundamental limitations\nof current language modeling approaches, including the lack of grounding in\nauthoritative knowledge sources, the reliance on statistical patterns in\ntraining data, and the inherent difficulty in reliably distinguishing truth\nfrom falsehood using statistics alone. These models were designed to generate\nplausible output similar to but not exactly the same as the training data,\nwhich, by definition, requires making up stuff.\n\nThese limitations highlight the need for more advanced techniques to better\nunderstand the nuances of language and factual claims, probably involving some\nfundamental paradigm shifts in machine learning that take us beyond what pure\nstatistical models can achieve.\n\nMitigating hallucinations in practice requires a multifaceted approach\ninvolving incorporating external knowledge bases and fact-checking systems,\ndeveloping more robust model architectures and training paradigms, leveraging\nhuman-in-the-loop and interactive learning strategies, and improving model\ntransparency and interpretability. Standardized benchmarks and test sets for\nhallucination assessment and interdisciplinary collaboration between AI\nresearchers, domain experts, and authorities in related fields will be crucial\nfor advancing the understanding and mitigation of this challenge.\n\nI hope this article has given you some food for thought. We went a bit deep\ninto the technical details of how generative models work, but that is\nnecessary to understand why these issues are so hard to solve. If you like\nthis type of article, please let me know with a comment, and feel free to\nsuggest future topics of interest.\n\nThank you for reading Mostly Harmless Ideas. This post is public, so feel free\nto share it.\n\nShare\n\n### Subscribe to Mostly Harmless Ideas\n\nBy Alejandro Piad Morffis \u00b7 Launched a year ago\n\nOpinions, essays, tutorials, educational articles, and short comments on\nComputer Science research, practice, and education.\n\n10 Likes\n\n\u00b7\n\n4 Restacks\n\n10\n\nShare this post\n\n#### Why reliable AI requires a paradigm shift\n\nblog.apiad.net\n\n7\n\nShare\n\n7 Comments\n\nZan TafakariRise with Zan Tafakari1 hr agoLiked by Alejandro Piad MorffisI\nlove how you distinguish between Hallucinations, out of distribution erros,\nand bias.I think for a given prompt, you could also map out the distribution\nof hallucinations, and may find that it's actually fat tailed? That would be\ninteresting.The other thing I always think about is \"hallucination\ninheritance\". Once an LLM has hallucinated something, it might be feeding into\nanother LLM which would inherit that hallucination, add more, or modify it. It\neventually becomes an untraceable game of chinese whispers.I can see this\nhappening in electronic health records, where lots of ambient AI tools are\nalready out there summarising doctor-patient conversations.What are your\nthoughts?Expand full commentLike (1)ReplyShare  \n---  \n  \nMichael WoudenbergPolymathic Being1 hr agoLiked by Alejandro Piad\nMorffisFantastic description of LLMs. I've always said they were intended to\nbe linguistically accurate, not factually accurate but I haven't thought that\nthe hallucinations are a feature, not a bug.Expand full commentLike\n(1)ReplyShare  \n---  \n  \n1 reply by Alejandro Piad Morffis\n\n5 more comments...\n\nLovelaice, an LLM-powered assistant for hackers without self-regard\n\nA chatbot you can use directly from your terminal to blow up you computer on\nyour own terms.\n\nFeb 20 \u2022\n\nAlejandro Piad Morffis\n\n17\n\nShare this post\n\n#### Lovelaice, an LLM-powered assistant for hackers without self-regard\n\nblog.apiad.net\n\n2\n\nChat with your PDF\n\nBuild a simple streamlit app using LLMs and vector databases to answer\narbitrary questions from a PDF document\n\nFeb 10 \u2022\n\nAlejandro Piad Morffis\n\n44\n\nShare this post\n\n#### Chat with your PDF\n\nblog.apiad.net\n\n19\n\nThe Science of Computation\n\nAn upcoming ebook about all things Computer Science\n\nJan 20 \u2022\n\nAlejandro Piad Morffis\n\n43\n\nShare this post\n\n#### The Science of Computation\n\nblog.apiad.net\n\n22\n\nReady for more?\n\n\u00a9 2024 Alejandro Piad Morffis\n\nPrivacy \u2219 Terms \u2219 Collection notice\n\nStart WritingGet the app\n\nSubstack is the home for great writing\n\nShare\n\n## Create your profile\n\n## Only paid subscribers can comment on this post\n\nAlready a paid subscriber? Sign in\n\n#### Check your email\n\nFor your security, we need to re-authenticate you.\n\nClick the link we sent to , or click here to sign in.\n\n", "frontpage": false}
