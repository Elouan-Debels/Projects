{"aid": "39996759", "title": "Race to build AI software engineers", "url": "https://dariuszsemba.com/blog/race-to-build-ai-software-engineers/", "domain": "dariuszsemba.com", "votes": 1, "user": "DSemba", "posted_at": "2024-04-10 23:29:03", "comments": 0, "source_title": "Race to build AI software engineers", "source_text": "Race to build AI software engineers\n\nDariuszSemba\n\nArticles\n\n# Race to build AI software engineers\n\nDariusz Semba \u2022 Wed Apr 10 2024 \u2022\n\n#generativeai #softwareengineering #llms #aiagents\n\n### The race to build a truly useful AI software engineer is on! \ud83d\udd25\ud83d\udd25\ud83d\udd25\n\nIn just 3 weeks, we\u2019ve witnessed multiple independent breakthroughs on the\nfront of AI agents solving real-world GitHub issues.\n\nTimeline of events:\n\n  * October 10th, 2023: \ud83d\udcc4 SWE-bench dataset/paper published, \ud83c\udfaf 1.96% accuracy (best model)\n\n\u23e9 5 months later \u23e9\n\n  * March 12th, 2024: \ud83d\udde3\ufe0f Devin announced, \ud83c\udfaf 13.86% accuracy\n\n  * March 26th, 2024: \ud83d\udcdc MAGIS paper, \ud83c\udfaf 13.94% accuracy\n\n  * April 2nd, 2024: \ud83d\ude80 Release of open-source SWE-agent, \ud83c\udfaf 12.29% accuracy (by the authors of SWE-bench)\n\n  * April 8th, 2024: \ud83d\ude80 AutoCodeRover paper/implementation, \ud83c\udfaf 16.06% \u2192 23.72% accuracy improvement on SWE-bench lite (compared to SWE-agent)\n\nAlthough the results may seem similar, in each case researchers found their\nown unique ways to improve on the benchmark.\n\n## AI research is a constant race\n\nWas it the release of Devin that really forced everyone else to reveal their\nprogress? Or had the release of the SWE-bench and/or general advancements in\nAI set off a timed explosion of developments? \ud83e\udd14\n\nSurely the announcement of Devin put some pressure on the rest of the\nresearchers. The authors of SWE-agent even decided to release their\ncode/results early on and have yet to publish their paper. At the same time,\nSWE-agent being open-source turns up the heat too - pushes Cognition Labs\n(Devin\u2019s creators) to improve the technology further and make it accessible to\neveryone. The techniques developed by the researches vary across solutions,\nhence there might be some potential for combining their learnings to obtain\neven better results. \ud83d\udcc8\n\n## Importance of benchmarks\n\nBenchmarks allow to evaluate and compare various systems. They measure\nprogress and hence point toward an end goal. Just like any dataset we use to\ntrain our models on, benchmarks should reflect the real problem, embody all\nits complexity and real-world challenges to be solved.\n\nEvery time an AI model reaches >90% accuracy on a given benchmark, another\none, more challenging is created, where the same model performs poorly.\n\nSWE-bench is an attempt to apply LLMs in the setting of real-world software\nengineering. It\u2019s definitely challenging, simple RAG + GPT/Claude don\u2019t yield\nany good results.\n\n### Limitations of SWE-bench\n\nSWE-bench itself is not ideal. It comprises 2,294 examples from 12 popular\nopen-source projects, all written in Python. It covers only a small specific\ndomain of problems, leaving out other languages and problems often encountered\nin enterprise setting.\n\nSome SWE-bench examples (e.g. coming from a certain repository) might be less\ndifficult than others. MAGIS framework scores from about 0% on some\nrepositories to as high as 40% accuracy on one specific repository.\n\nEvaluating against SWE-bench might get costly in some scenarios - the more\ncomplex the agent, the more prompts/requests to LLMs under the hood. To\naddress this problem the authors created SWE-bench Lite with only a subset of\nexamples, that\u2019s diverse enough.\n\n## Will programmers get replaced soon?\n\n14% doesn\u2019t seem a lot. On the other hand: if you could make even just 14% of\nbugs disappear simply by waving a magic wand - that\u2019s a real deal.\n\nUnfortunately (or fortunately for programmers), that\u2019s not the case.\n\nSWE-bench dataset uses human-written tests to verify that the bugs were solved\ncorrectly by AI. Without those tests, you would end up with a lot of useless,\npotentially detrimental, changes in your code. The solution can be to solve\nevery issue by writing tests first, and only then you could actually\nautomatically resolve a given issue 14% of time. Does it translate to 14% time\nsavings? Not necessarily. Writing tests requires knowing what went wrong and\nthat constitutes most of the work.\n\nOther alternatives would be to:\n\n  * Measure the accuracy of the system, which writes tests by itself and decides on its own whether it solved the bug correctly. If the predictions were reliable, humans could use such a system to automatically solve a small percentage of the bugs.\n  * Adopt Devin\u2019s approach of close involvement of humans in the process - multiple interactions, ability to inspect the ongoing work - both assure that the outcome is favorable.\n\nWhat do you think? Should engineers be concerned, or is there cause for\nexcitement?\n\n\u2190 Back to Blog\n\n", "frontpage": false}
