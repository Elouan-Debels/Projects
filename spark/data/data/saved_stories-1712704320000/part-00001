{"aid": "39981553", "title": "Google Cloud AI Hypercomputer Architecture", "url": "https://cloud.google.com/blog/products/compute/whats-new-with-google-clouds-ai-hypercomputer-architecture", "domain": "cloud.google.com", "votes": 2, "user": "kaycebasques", "posted_at": "2024-04-09 17:10:08", "comments": 0, "source_title": "What\u2019s new with Google Cloud\u2019s AI Hypercomputer architecture | Google Cloud Blog", "source_text": "What\u2019s new with Google Cloud\u2019s AI Hypercomputer architecture | Google Cloud Blog\n\nJump to Content\n\nCloud\n\nBlog\n\nContact sales Get started for free\n\nCloud\n\nBlog\n\nSolutions & technology\n\nSecurity\n\nEcosystem\n\nIndustries\n\n  * Solutions & technology\n  * Ecosystem\n  * Developers & Practitioners\n  * Transform with Google Cloud\n\n  * AI & Machine Learning\n  * API Management\n  * Application Development\n  * Application Modernization\n  * Chrome Enterprise\n  * Compute\n  * Containers & Kubernetes\n  * Data Analytics\n  * Databases\n  * DevOps & SRE\n  * Maps & Geospatial\n  * Security\n  * Infrastructure\n  * Infrastructure Modernization\n  * Networking\n  * Productivity & Collaboration\n  * SAP on Google Cloud\n  * Storage & Data Transfer\n  * Sustainability\n\n  * Security & Identity\n  * Threat Intelligence\n\n  * IT Leaders\n  * Industries\n  * Partners\n  * Startups & SMB\n  * Training & Certifications\n  * Inside Google Cloud\n  * Google Cloud Next & Events\n  * Google Maps Platform\n  * Google Workspace\n\n  * Financial Services\n  * Healthcare & Life Sciences\n  * Manufacturing\n  * Media & Entertainment\n  * Public Sector\n  * Retail\n  * Supply Chain\n  * Telecommunications\n\nContact sales Get started for free\n\nCompute\n\n#\n\nWhat\u2019s new with Google Cloud\u2019s AI Hypercomputer architecture\n\nApril 9, 2024\n\n##### Mark Lohmeyer\n\nVP & GM, Compute and AI Infrastructure\n\n##### Google Cloud Next is live!\n\nSee the latest announcements from Next '24.\n\nJoin us\n\nAdvancements in AI are unlocking use-cases previously thought impossible.\nLarger and more complex AI models are enabling powerful capabilities across a\nfull range of applications involving text, code, images, videos, voice, music,\nand more. As a result, leveraging AI has become an innovation imperative for\nbusinesses and organizations around the world, with the potential to boost\nhuman potential and productivity.\n\nHowever, the AI workloads powering these exciting use-cases place incredible\ndemands on the underlying compute, networking, and storage infrastructure. And\nthat\u2019s only one aspect of the architecture: customers also face the challenge\nof integrating open-source software, frameworks, and data platforms, while\noptimizing for resource consumption to harness the power of AI cost-\neffectively. Historically, this has required manually combining component-\nlevel enhancements, which can lead to inefficiencies and bottlenecks.\n\nThat\u2019s why today we\u2019re pleased to announce significant enhancements at every\nlayer of our AI Hypercomputer architecture. This systems-level approach\ncombines performance-optimized hardware, open software and frameworks, and\nflexible consumption models to enable developers and businesses to be more\nproductive, because the overall system runs with higher performance and\neffectiveness, and the models generated are served more efficiently.\n\nIn fact, just last month, Forrester Research recognized Google as a Leader in\nThe Forrester WaveTM: AI Infrastructure Solutions^1, Q1 2024, with the highest\nscores of any vendor evaluated in both the Current Offering and Strategy\ncategories in this report.\n\nThe announcements we\u2019re making today span every layer of the AI Hypercomputer\narchitecture:\n\n  * Performance-optimized hardware enhancements including the general availability of Cloud TPU v5p, and A3 Mega VMs powered by NVIDIA H100 Tensor Core GPUs, with higher performance for large-scale training with enhanced networking capabilities\n\n  * Storage portfolio optimizations for AI workloads including Hyperdisk ML, a new block storage service optimized for AI inference/serving workloads, and new caching capabilities in Cloud Storage FUSE and Parallelstore, which improve training and inferencing throughput and latency\n\n  * Open software advancements including the introduction of JetStream \u2014 a throughput- and memory-optimized inference engine for large language models (LLMs) that offers higher performance per dollar on open models like Gemma 7B, and JAX and PyTorch/XLA releases that improve performance on both Cloud TPUs and NVIDIA GPUs\n\n  * New flexible consumption options with Dynamic Workload Scheduler, including calendar mode for start time assurance, and flex start mode for optimized economics\n\nLearn more about AI Hypercomputer with a rare look inside one of our data\ncenters:\n\n### Advances in performance-optimized hardware\n\nCloud TPU v5p GA We\u2019re thrilled to announce the general availability of Cloud\nTPU v5p, our most powerful and scalable TPU to date. TPU v5p is a next-\ngeneration accelerator that is purpose-built to train some of the largest and\nmost demanding generative AI models. A single TPU v5p pod contains 8,960 chips\nthat run in unison \u2014 over 2x the chips in a TPU v4 pod. Beyond the larger\nscale, TPU v5p also delivers over 2x higher FLOPS and 3x more high-bandwidth\nmemory on a per chip basis. It also delivers near-linear improvement in\nthroughput as customers use larger slices, achieving 11.97X throughput for a\n12x increase in slice size (from 512 to 6144 chips).\n\nComprehensive GKE support for TPU v5p To enable training and serving the\nlargest AI models on GKE across large-scale TPU clusters, today we\u2019re also\nannouncing the general availability of both Google Kubernetes Engine (GKE)\nsupport for Cloud TPU v5p and TPU multi-host serving on GKE. TPU multi-host\nserving on GKE allows customers to manage a group of model servers deployed\nover multiple hosts as a single logical unit, so they can be managed and\nmonitored centrally.\n\n\u201cBy leveraging Google Cloud\u2019s TPU v5p on Google Kubernetes Engine (GKE),\nLightricks has achieved a remarkable 2.5X speed-up in training our text-to-\nimage and text-to-video models compared to TPU v4. GKE ensures that we are\nable to smoothly leverage TPU v5p for the specific training jobs that need the\nperformance boost.\u201d - Yoav HaCohen, PhD, Core Generative AI Research Team\nLead, Lightricks\n\nExpanded NVIDIA H100 GPU capabilities with A3 Mega GA and Confidential Compute\nWe\u2019re also expanding our NVIDIA GPU capabilities with additions to the A3 VM\nfamily, which now includes A3 Mega. A3 Mega, powered by NVIDIA H100 GPUs, will\nbe generally available next month and offers double the GPU-to-GPU networking\nbandwidth of A3. Confidential Computing will also be coming to the A3 VM\nfamily, in preview later this year. Enabling confidential VMs on the A3\nmachine series protects the confidentiality and integrity of sensitive data\nand AI workloads and mitigates threats from unauthorized access. Enabling\nConfidential Computing on the A3 VM family encrypts the data transfers between\nthe Intel TDX-enabled CPU and NVIDIA H100 GPU via protected PCIe, and requires\nno code changes.\n\nBringing NVIDIA Blackwell GPUs to Google Cloud We also recently announced that\nwe will be bringing NVIDIA\u2019s newest Blackwell platform to our AI Hypercomputer\narchitecture in two configurations. Google Cloud customers will have access to\nVMs powered by both the NVIDIA HGX B200 and GB200 NVL72 GPUs. The new VMs with\nHGX B200 GPU is designed for the most demanding AI, data analytics, and HPC\nworkloads, while the upcoming VMs powered by the liquid-cooled GB200 NVL72 GPU\nwill enable a new era of computing with real-time LLM inference and massive-\nscale training performance for trillion-parameter scale models.\n\nCustomers leveraging both Google Cloud TPU and GPU-based services Character.AI\nis a powerful, direct-to-consumer AI computing platform where users can easily\ncreate and interact with a variety of characters. Character.AI is using Google\nCloud\u2019s AI Hypercomputer architecture across GPU- and TPU-based infrastructure\nto meet the needs of its rapidly growing community.\n\n\u201cCharacter.AI is using Google Cloud's Tensor Processor Units (TPUs) and A3 VMs\nrunning on NVIDIA H100 Tensor Core GPUs to train and infer LLMs faster and\nmore efficiently. The optionality of GPUs and TPUs running on the powerful AI-\nfirst infrastructure makes Google Cloud our obvious choice as we scale to\ndeliver new features and capabilities to millions of users. It\u2019s exciting to\nsee the innovation of next-generation accelerators in the overall AI\nlandscape, including Google Cloud TPU v5e and A3 VMs with H100 GPUs. We expect\nboth of these platforms to offer more than 2X more cost-efficient performance\nthan their respective previous generations.\u201d - Noam Shazeer, CEO, Character AI\n\n### Storage optimized for AI/ML workloads\n\nTo improve AI training, fine-tuning, and inference performance, we've added a\nnumber of enhancements to our storage products, including caching, which keeps\nthe data closer to your compute instances, so you can train much faster. Each\nof these improvements also maximizes GPU and TPU utilization, leading to\nhigher energy efficiency and cost optimization.\n\nCloud Storage FUSE (generally available) is a file-based interface for Google\nCloud Storage that harnesses Cloud Storage capabilities for more complex AI/ML\napps by providing file access to our high-performance, low-cost cloud storage\nsolutions. Today we announced that new caching capabilities are generally\navailable. Cloud Storge FUSE caching improves training throughput by 2.9X and\nimproves serving performance for one of our own foundation models by 2.2X.\n\nParallelstore now also includes caching (in preview). Parallelstore is a high-\nperformance parallel filesystem optimized for AI/ML and HPC workloads. New\ncaching capabilities enable up to 3.9X faster training times and up to 3.7X\nhigher training throughput, compared to native ML framework data loaders.\n\nFilestore (generally available) is optimized for AI/ML models that require low\nlatency, file-based data access. The network file system-based approach allows\nall GPUs and TPUs within a cluster to simultaneously access the same data,\nwhich improves training times by up to 56%, optimizing the performance of your\nAI workloads and boosting your most demanding AI projects.\n\nWe\u2019re also pleased to introduce Hyperdisk ML in preview, our next-generation\nblock storage service optimized for AI inference/serving workloads. Hyperdisk\nML accelerates model load times up to 12X compared to common alternatives, and\noffers cost efficiency through read-only, multi-attach, and thin provisioning.\nIt enables up to 2,500 instances to access the same volume and delivers up to\n1.2 TiB/s of aggregate throughput per volume \u2014 over 100X greater performance\nthan Microsoft Azure Ultra SSD and Amazon EBS io2 BlockExpress.\n\n### Advancements in our open software\n\nStarting from frameworks and spanning the full software stack, we\u2019re\nintroducing open-source enhancements that enable customers to improve time-to-\nvalue for AI workloads by simplifying the developer experience while improving\nperformance and cost efficiency.\n\nJAX and high-performance reference implementations We\u2019re pleased to introduce\nMaxDiffusion, a new high-performance and scalable reference implementation for\ndiffusion models. We\u2019re also introducing new LLM models in MaxText, including\nGemma, GPT3, LLAMA2 and Mistral across both Cloud TPUs and NVIDIA GPUs.\nCustomers can jump-start their AI model development with these open-source\nimplementations and then customize them further based on their needs.\n\nMaxText and MaxDiffusion models are built on JAX, a cutting-edge framework for\nhigh-performance numerical computing and large-scale machine learning. JAX in\nturn is integrated with the OpenXLA compiler, which optimizes numerical\nfunctions and delivers excellent performance at scale, allowing model builders\nto focus on the math and let the software drive the most effective\nimplementation. We\u2019ve heavily optimized JAX and OpenXLA performance on Cloud\nTPU and also partnered closely with NVIDIA to optimize OpenXLA performance on\nlarge Cloud GPU clusters.\n\nAdvancing PyTorch support As part of our commitment to PyTorch, support for\nPyTorch/XLA 2.3 will follow the upstream release later this month. PyTorch/XLA\nenables tens of thousands of PyTorch developers to get the best performance\nfrom XLA devices such as TPUs and GPUs, without having to learn a new\nframework. The new release brings features such as single program, multiple\ndata (SPMD) auto-sharding, and asynchronous distributed checkpointing, making\nrunning a distributed training job much easier and more scalable.\n\nAnd for PyTorch users in the Hugging Face community, we worked with Hugging\nFace to launch Optimum-TPU, a performance-optimized package that will help\ndevelopers easily train and serve Hugging Face models on TPUs.\n\nJetstream: New LLM inference engine We\u2019re introducing Jetstream, an open-\nsource, throughput- and memory-optimized LLM inference engine for XLA devices,\nstarting with TPUs, that offers up to 3x higher inferences per dollar on Gemma\n7B and other open models. As customers bring their AI workloads to production,\nthere\u2019s an increasing demand for a cost-efficient inference stack that\ndelivers high performance. JetStream supports models trained with both JAX and\nPyTorch/XLA, and includes optimizations for popular open models such as Llama\n2 and Gemma.\n\nOpen community models in collaboration with NVIDIA Additionally, as part of\nthe NVIDIA and Google collaboration with open community models, Google models\nwill be available as NVIDIA NIM inference microservices to provide developers\nwith an open, flexible platform to train and deploy using their preferred\ntools and frameworks.\n\n### New Dynamic Workload Scheduler modes\n\nDynamic Workload Scheduler is a resource management and job scheduling service\nthat\u2019s designed for AI workloads. Dynamic Workload Scheduler improves access\nto AI computing capacity and helps you optimize your spend for AI workloads by\nscheduling all the accelerators needed simultaneously, and for a guaranteed\nduration. Dynamic Workload Scheduler offers two modes: flex start mode (in\npreview) for enhanced obtainability with optimized economics, and calendar\nmode (in preview) for predictable job start times and durations.\n\nFlex start jobs are cued to run as soon as possible, based on resource\navailability, making it easier to obtain TPU and GPU resources for jobs that\nhave a flexible start time. Flex start mode is now integrated across Compute\nEngine Managed Instance Groups, Batch, and Vertex AI Custom Training, in\naddition to Google Kubernetes Engine (GKE). With flex start, you can now run\nthousands of AI/ML jobs with increased obtainability across the various TPU\nand GPU types offered in Google Cloud.\n\nCalendar mode offers short-term reserved access to AI-optimized computing\ncapacity. You can reserve collocated GPUs for up to 14 days, which can be\npurchased up to 8 weeks in advance. This new mode extends Compute Engine\nfuture reservation capabilities. Your reservations are confirmed, based on\navailability, and the capacity is delivered to your project on your requested\nstart date. You can then simply create VMs targeting the capacity block for\nthe entire duration of the reservation.\n\n\u201cDynamic Workload Scheduler improved on-demand GPU obtainability by 80%,\naccelerating experiment iteration for our researchers. Leveraging the built-in\nKueue and GKE integration, we were able to take advantage of new GPU capacity\nin Dynamic Workload Scheduler quickly and save months of development work.\u201d -\nAlex Hays, Software Engineer, Two Sigma\n\n### AI anywhere with Google Distributed Cloud\n\nThe acceleration of AI adoption by enterprises has highlighted the need for\nflexible deployment options to process or securely analyze data closer to\nwhere it is generated. Google Distributed Cloud (GDC) brings the power of\nGoogle's cloud services wherever you need them \u2014 in your own data center or at\nthe edge. Today we introduced several enhancements to GDC, including a\ngenerative AI search package solution powered by Gemma, an expanded ecosystem\nof partner solutions, new compliance certifications and more. Learn more about\nhow to use GDC to run AI anywhere.\n\n### Our growing momentum with Google AI infrastructure\n\nAt Next this week we\u2019re launching incredible AI innovation across everything\nfrom AI platforms and models to AI assistance with Gemini for Google Cloud \u2014\nall underpinned by a foundation of AI-optimized infrastructure. All of this\ninnovation is driving incredible momentum for our customers. In fact, nearly\n90% of generative AI unicorns and more than 60% of funded gen AI startups are\nGoogle Cloud customers.\n\n\u201cRunway\u2019s text-to-video platform is powered by AI Hypercomputer. At the base,\nA3 VMs, powered by NVIDIA H100 GPUs gave our training a significant\nperformance boost over A2 VMs, enabling large-scale training and inference for\nour Gen-2 model. Using GKE to orchestrate our training jobs enables us to\nscale to thousands of H100s in a single fabric to meet our customers\u2019 growing\ndemand.\u201d - Anastasis Germanidis, CTO and Co-Founder, Runway\n\n\"By moving to Google Cloud and leveraging the AI Hypercomputer architecture\nwith G2 VMs powered by NVIDIA L4 GPUs and Triton Inference Server, we saw a\nsignificant boost in our model inference performance while lowering our\nhosting costs by 15% using novel techniques enabled by the flexibility that\nGoogle Cloud offers.\u201d -Ashwin Kannan, Sr. Staff Machine Learning Engineer,\nPalo Alto Networks\n\n\"Writer's platform is powered by Google Cloud A3 and G2 VMs powered by NVIDIA\nH100 and L4 GPUs. With GKE we're able to efficiently train and inference over\n17 large language models (LLMs) that scale up to over 70B parameters. We\nleverage Nvidia NeMo Framework to build our industrial strength models which\ngenerate 990,000 words a second with over a trillion API calls per month.\nWe're delivering the highest quality inferencing models that exceed those from\ncompanies with larger teams and bigger budgets and all of that is possible\nwith the Google and Nvidia partnership.\u201d - Waseem Alshikh Cofounder and CTO,\nWriter\n\nLearn more about AI Hypercomputer at the Next sessions below, and ask your\nsales representative about how you can apply these capabilities within your\nown organization.\n\n  * SPTL205 - Workload-optimized and AI-powered Infrastructure\n  * ARC108 - Take large scale AI from research to production with Google Cloud's AI Hypercomputer\n  * IHLT303 - How Lightricks is powering generative image models with Cloud TPUs and AI Hypercomputer\n\n^1. Forrester Research, The Forrester WaveTM: AI Infrastructure Solutions, Q1\n2024, Mike Gualtieri, Sudha Maheshwari, Sarah Morana, Jen Barton, March 17,\n2024\n\n^The Forrester WaveTM is copyrighted by Forrester Research, Inc. Forrester and\nForrester WaveTM are trademarks of Forrester Research, Inc. The Forrester Save\nis a graphical representation of Forrester\u2019s call on a market and is plotted\nusing a detailed spreadsheet with exposed scores, weightings, and comments.\nForrester does not endorse any vendor, product, or service depicted in the\nForrester WaveTM. Information is based on the best available resources.\nOpinions reflect judgment at the time and are subject to change.\n\nPosted in\n\n  * Compute\n  * AI & Machine Learning\n  * Google Cloud Next\n  * Systems\n\n##### Related articles\n\nCompute\n\n### Introducing Google Axion Processors, our new Arm-based CPUs\n\nBy Amin Vahdat \u2022 7-minute read\n\nCompute\n\n### What\u2019s new in Google Cloud\u2019s workload-optimized infrastructure\n\nBy Salil Suri \u2022 11-minute read\n\nCompute\n\n### IT pros\u2019 top five questions, and how we are answering them at Google Cloud\nNext\n\nBy Jarrad Swain \u2022 5-minute read\n\nHPC\n\n### Rocky Linux 8 and CentOS 7 versions of HPC VM image now generally\navailable\n\nBy Rohit Ramu \u2022 6-minute read\n\n### Footer Links\n\n#### Follow us\n\n  * Google Cloud\n  * Google Cloud Products\n  * Privacy\n  * Terms\n  * Cookies management controls\n\n  * Help\n\n", "frontpage": false}
