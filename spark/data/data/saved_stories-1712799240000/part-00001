{"aid": "39994840", "title": "Recreating the Works of Steve Reich and Brian Eno Using JavaScript", "url": "https://teropa.info/blog/2016/07/28/javascript-systems-music.html", "domain": "teropa.info", "votes": 3, "user": "udit99", "posted_at": "2024-04-10 19:32:07", "comments": 0, "source_title": "JavaScript Systems Music", "source_text": "JavaScript Systems Music\n\n# JavaScript Systems Music\n\n## Learning Web Audio by Recreating The Works of Steve Reich and Brian Eno\n\nPosted on Thursday Jul 28, 2016 by Tero Parviainen (@teropa)\n\nSystems music is an idea that explores the following question: What if we\ncould, instead of making music, design systems that generate music for us?\n\nThis idea has animated artists and composers for a long time and emerges in\nnew forms whenever new technologies are adopted in music-making.\n\nIn the 1960s and 70s there was a particularly fruitful period. People like\nSteve Reich, Terry Riley, Pauline Oliveros, and Brian Eno designed systems\nthat resulted in many landmark works of minimal and ambient music. They worked\nwith the cutting edge technologies of the time: Magnetic tape recorders,\nloops, and delays.\n\nToday our technological opportunities for making systems music are broader\nthan ever. Thanks to computers and software, they're virtually endless. But to\nme, there is one platform that's particularly exciting from this perspective:\nWeb Audio. Here we have a technology that combines audio synthesis and\nprocessing capabilities with a general purpose programming language:\nJavaScript. It is a platform that's available everywhere \u2014 or at least we're\ngetting there. If we make a musical system for Web Audio, any computer or\nsmartphone in the world can run it.\n\nWith Web Audio we can do something Reich, Riley, Oliveros, and Eno could not\ndo all those decades ago: They could only share some of the output of their\nsystems by recording them. We can share the system itself. Thanks to the\nunique power of the web platform, all we need to do is send a URL.\n\nIn this guide we'll explore some of the history of systems music and the\npossibilities of making musical systems with Web Audio and JavaScript. We'll\npay homage to three seminal systems pieces by examining and attempting to\nrecreate them: \"It's Gonna Rain\" by Steve Reich, \"Discreet Music\" by Brian\nEno, and \"Ambient 1: Music for Airports\", also by Brian Eno.\n\n## Table of Contents\n\n  * \"Is This for Me?\"\n  * How to Read This Guide\n  * The Tools You'll Need\n  * Steve Reich - It's Gonna Rain (1965)\n\n    * Setting Up itsgonnarain.js\n    * Loading A Sound File\n    * Playing The Sound\n    * Looping The Sound\n    * How Phase Music Works\n    * Setting up The Second Loop\n    * Adding Stereo Panning\n    * Putting It Together: Adding Phase Shifting\n    * Exploring Variations on It's Gonna Rain\n  * Brian Eno - Ambient 1: Music for Airports, 2/1 (1978)\n\n    * The Notes and Intervals in Music for Airports\n    * Setting up musicforairports.js\n    * Obtaining Samples to Play\n    * Building a Simple Sampler\n    * A System of Loops\n    * Playing Extended Loops\n    * Adding Reverb\n    * Putting It Together: Launching the Loops\n    * Exploring Variations on Music for Airports\n  * Brian Eno - Discreet Music (1975)\n\n    * Setting up discreetmusic.js\n    * Synthesizing the Sound Inputs\n\n      * Setting up a Monophonic Synth with a Sawtooth Wave\n      * Filtering the Wave\n      * Tweaking the Amplitude Envelope\n      * Bringing in a Second Oscillator\n      * Emulating Tape Wow with Vibrato\n    * Understanding Timing in Tone.js\n\n      * Transport Time\n      * Musical Timing\n    * Sequencing the Synth Loops\n    * Adding Echo\n    * Adding Tape Delay with Frippertronics\n    * Controlling Timbre with a Graphic Equalizer\n\n      * Setting up the Equalizer Filters\n      * Building the Equalizer Control UI\n  * Going Forward\n\n## \"Is This for Me?\"\n\nThis guide is targeted for people who know some JavaScript and are interested\nin making music with it.\n\nWe'll be using ES2015 JavaScript and knowing at least some of it will make it\na lot easier to follow along. However, we won't be doing anything hugely\nsophisticated, so advanced knowledge is not required.\n\nYou don't have to be a musician or an expert in music theory to follow this\nguide. I'm neither of those things. I'm figuring things out as I go and it's\nperfectly fine if you do too. I believe that this kind of stuff is well within\nreach for anyone who knows a bit of programming, and you can have a lot of fun\nwith it even if you aren't a musician.\n\nOne thing that definitely won't hurt though is an interest in experimental\nmusic! This will get weird at times.\n\n## How to Read This Guide\n\nThe guide is divided into three chapters, one for each of the musical pieces\nwe'll study and recreate:\n\n  * In \"It's Gonna Rain\" we'll get acquainted with the Web Audio API and playing sounds with it.\n  * In \"Music for Airports\" we'll learn more details about playing sample-based music and implementing audio processing effects such as reverb.\n  * In \"Discreet Music\" we'll study audio synthesis and sequencing with the Tone.js library and build a more elaborate effects graph.\n\nEach chapter stands on its own, so you can just pick one and read it. However,\nI will refer to previous chapters from time to time and I recommend working\nthrough the whole guide in order. This is especially true if you haven't used\nthe Web Audio API before.\n\nYou'll get the most out of the guide if you write your own version of the code\nevery step of the way. This will also allow you to further experiment with the\nsystems. But you don't have to do that if you don't want to. I've set things\nup so that you can hear and see everything right on this page.\n\n## The Tools You'll Need\n\nFor running the experiments you'll need a web browser that supports ES2015\nJavaScript and the Web Audio API. The latest versions of Chrome and Firefox\nwill work fine, as will Safari 10 (currently in beta). MS Edge should be OK\ntoo, but Internet Explorer will not work.\n\nIf you also want to code along, you need a couple of other things:\n\n  * A code editor, such as Atom or Visual Studio Code, for authoring the code.\n  * An installation of Node.js. We'll use it to run a local web server.\n\n##\n\nSteve Reich\n\nIt's Gonna Rain\n\n1965\n\n> Performing and listening to a gradual musical process resembles: pulling\n> back a swing, releasing it, and observing it gradually come to rest; turning\n> over an hour glass and watching the sand slowly run through the bottom;\n> placing your feet in the sand by the ocean's edge and watching, feeling, and\n> listening to the waves gradually bury them.\n>\n> Steve Reich, Music as a Gradual Process, 1968\n\nThe first piece we're going to explore takes us back to the mid-1960s. This\nwas a time right after the Cuban Missile Crisis, when the end of the world by\nnuclear annihilation did not seem all that implausible.\n\nIn this tense atmosphere, young American composer Steve Reich was\nexperimenting with using reel-to-reel magnetic tape recorders for the purpose\nof making music.\n\nA reel-to-reel tape recorder. Photo: Alejandro Linares Garcia.\n\nOne of the exciting new possibilities of this technology was that you could\nrecord your own sounds and then play them back. Reich was using this\npossibility to make field recordings. In 1965 he happened upon San Francisco's\nUnion Square, where a Pentecostal preacher called Brother Walter was bellowing\nabout the end of the world.\n\nFrom this tape Reich would construct what is considered to be his first major\nwork and one of the definitive pieces of American minimal music: \"It's Gonna\nRain\".\n\nAs you listen to it, I'd like you to pay particularly close attention to the\nmovement that begins around the two minute mark and lasts for about six\nminutes. This is the part produced by the process that we're about to examine.\n\nNow, this is not exactly easy listening. It gets pretty intense.\nNotwithstanding the pigeon drummer, it even challenges most people's\nconception of what \"music\" even means. This is basically just looped human\nspeech, isn't it?\n\nTo me, what this piece seems to be saying is \"we're going to take this\nsubsecond apocalyptic sound clip and we're going to listen the shit out of\nit\". And as we listen, the clip starts shifting and layering on top of itself.\nThe words disappear and it all just becomes pure sound. Even though no new\nsonic information is being introduced, you keep noticing new things all the\ntime \u2014 things that were there all along but you just hadn't payed attention\nto. Until, at the end, the words re-emerge again and we're back where we\nstarted and not quite sure what just happened.\n\nTo say I actually enjoy listening to this piece would probably be stretching\nit. It wouldn't be among the records I'd take with me on a desert island. But\nit is certainly fascinating and kind of hypnotic too. If you allow it to, it\ndoes evoke a certain kind of mental atmosphere.\n\n> All music to some degree invites people to bring their own emotional life to\n> it. My early pieces do that in an extreme form, but, paradoxically, they do\n> so through a very organized process, and it\u2019s precisely the impersonality of\n> that process that invites this very engaged psychological reaction.\n>\n> Steve Reich, 1996\n\nBecause of the simplicity of this piece, it's a great way for us to begin\nexploring Web Audio. So let's see how we might be able to reconstruct it in a\nweb browser.\n\n### Setting up itsgonnarain.js\n\nThe source code for this project is available on GitHub.\n\nTo host our JavaScript version of It's Gonna Rain, we're going to need a bit\nof HTML and a bit of JavaScript. Create a new directory for this project and\nadd two files into it:\n\n  * itsgonnarain/\n\n    * index.html\n    * itsgonnarain.js\n\nThen set the contents of index.html as follows:\n\n    \n    \n    <!DOCTYPE html> <html> <body> <script src=\"https://cdn.rawgit.com/mohayonao/web-audio-api-shim/master/build/web-audio-api-shim.min.js\"></script> <script src=\"https://cdnjs.cloudflare.com/ajax/libs/fetch/1.0.0/fetch.min.js\"></script> <script src=\"itsgonnarain.js\"></script> </body> </html>\n\nThis is a very simple HTML document that just loads a couple of JavaScript\nfiles to the page:\n\n  * The web-audio-api-shim library, which irons out some browser inconsistencies with the Web Audio API so that we don't have to deal with them.\n  * A polyfill for the Fetch API, which also isn't fully supported across all browsers yet.\n  * The file that will host our own code, itsgonnarain.js.\n\nWe're not going to use an ES2015 transpiler in these code examples, and will\njust rely on native browser support of the latest JavaScript features. If you\nwant to share your work with the wider world, you'll definitely also want to\ntranspile your code with a tool like Babel.\n\nAlso add some content to the itsgonnarain.js file:\n\n    \n    \n    console.log(\"It's gonna rain\");\n\nThis will print a message to the browser's development console. It'll give us\na chance to check that everything is working once we load up the page in a web\nbrowser. But before we can do that, we'll need to serve it up with a web\nserver. Open up a command line and install the lite-server Node.js package:\n\n    \n    \n    npm install -g lite-server\n\nThe npm command comes with Node.js, so if it isn't working for you make sure\nyou have Node installed.\n\nWe now have a package we can use to spin up a web server, and we can do so by\nrunning the following command from inside the itsgonnarain project directory:\n\n    \n    \n    lite-server\n\nWe'll keep the server running throughout the project. You should now be able\nto open http://localhost:3000 in a web browser to load the project (or it may\nalready have opened automatically for you). It'll just be a blank page, but if\nyou go and open the browser's developer tools you should see the It's gonna\nrain message in the console. We're ready to start playing some sounds!\n\nI recommend always keeping the developer console open while working on the\nproject. If something goes wrong with the code, this is where the browser will\ntell you about it.\n\n### Loading A Sound File\n\nWhat we're going to do first is simply play a sound sample on our webpage.\nBefore we can play it though, we'll need to load it.\n\nWhat I did for my version was to take an MP3 file of the Reich piece and\nextract a short clip from the first 15 seconds or so. Unfortunately I can't\ngive you that clip to download, but you can easily obtain it (it's on e.g.\nAmazon or iTunes). Or you can use some other piece of audio that you think\nwould be interesting to use as raw material - it's entirely up to you!\n\nWhatever audio you use, make sure it's in a format that web browsers can\nunderstand (.wav, .mp3, .ogg, etc.) and then place the file inside the project\ndirectory.\n\nOnce we have an audio file, we can add some JavaScript code to load it in:\n\nitsgonnarain.js\n\n    \n    \n    fetch('itsgonnarain.mp3') .then(response => response.arrayBuffer()) .then(arrayBuffer => console.log('Received', arrayBuffer)) .catch(e => console.error(e));\n\nHere we are using the Fetch API to send a request for the audio file over the\nnetwork (which in this case will just go to our local web server). The file\nwill eventually be received back from the server, and what fetch() returns\nback to us is a Promise object that will give us a chance to react when that\nhappens.\n\nWhat we do with the Promise is call its .then() method and supply it with a\ncallback function we want to run when the response comes in. In the callback\nwe invoke the arrayBuffer() method of the response, letting it know that we\nwant to have the incoming data as a binary ArrayBuffer object.\n\nThen we have to wait a bit more to actually receive all that data, so we get\nback another Promise object. This one will get fulfilled when the browser has\nfinished receiving every last bit of the (binary) data. Into this second\nPromise we attach a callback function that simply logs the resulting\nArrayBuffer into the developer console. We also include a catch rejection\nhandler, which will log any errors that might occur during this whole process,\nso that if something goes wrong it'll also be shown in the developer console.\n\nYou should now see this message:\n\nWe now have the raw MP3 data loaded to our page.\n\n### Playing The Sound\n\nWe still have to do one more thing before we can play the MP3 data we now\nhave. We have to decode it into a playable form. This is where we bring in the\nWeb Audio API, which has the facilities for doing that.\n\nWith Web Audio, everything begins with something called an AudioContext. This\nis an object that handles all the audio processing we're going to do and\ninternally manages any communication with the audio hardware and the resources\nassociated with it. Before we do anything else, we need to create one of those\nAudioContexts. We'll be using it for many things, but right now we're going to\ncall its decodeAudioData method to turn our MP3 ArrayBuffer into a decoded\nAudioBuffer:\n\nitsgonnarain.js\n\n    \n    \n    let audioContext = new AudioContext(); fetch('itsgonnarain.mp3') .then(response => response.arrayBuffer()) .then(arrayBuffer => audioContext.decodeAudioData(arrayBuffer)) .then(audioBuffer => console.log('Decoded', audioBuffer)) .catch(e => console.error(e));\n\nAs we see here, decodeAudioData actually also returns a Promise instead of\ngiving us the audio data right away. That's because the data may take a while\nto decode and the browser does it in the background, letting us get on with\nother business in the meantime. So what we now have is a chain of three nested\nPromises. After the last one we do our console logging. The message in the\ndeveloper console should show that we have about 14 seconds of stereo audio\ndata \u2013 or whatever the duration of your sound file happens to be.\n\nThis is something we can now play, so let's go right ahead and do that. We're\ngoing to use the AudioContext again, this time to create something called a\nbuffer source. This is an object that knows how to play back an AudioBuffer.\nWe give it the buffer we have, connect it, and start it. The result is that we\ncan hear the sound play from the browser window \u2013 our equivalent of hitting\nthe play button on Reich's tape recorder.\n\nitsgonnarain.js\n\n    \n    \n    let audioContext = new AudioContext(); fetch('itsgonnarain.mp3') .then(response => response.arrayBuffer()) .then(arrayBuffer => audioContext.decodeAudioData(arrayBuffer)) .then(audioBuffer => { let sourceNode = audioContext.createBufferSource(); sourceNode.buffer = audioBuffer; sourceNode.connect(audioContext.destination); sourceNode.start(); }) .catch(e => console.error(e));\n\nWhat we've done here is set up our very first audio-processing graph, which\nconsists of just two nodes, one connected to the next:\n\n  * An AudioBufferSourceNode that reads in the AudioBuffer data and streams it to other nodes.\n  * The AudioContext's built-in AudioDestinationNode, which makes the sound audible on the machine's speakers.\n\nBasic Web Audio graph with a single source\n\nIn general, all audio processing with the Web Audio API happens with such\ngraphs, through which audio signals flow, starting from source nodes and\nending up in destination nodes, possibly going through a number of\nintermediate nodes. These graphs may contain different kinds of nodes that\ngenerate, transform, measure, or play audio. We'll soon be constructing more\nelaborate graphs, but this is all you need if you simply want to play back a\nsound.\n\n### Looping The Sound\n\nWhat we have so far plays the audio file once and then stops. What we really\nwant to do though to begin recreating the magic of \"It's Gonna Rain\" is make\nit loop over and over again.\n\nSteve Reich was using magnetic tape, which in its usual configuration spools\nfrom one reel to the other. But there's a brilliant hack, pioneered by Pierre\nSchaeffer, that allows you to circumvent this process: You can splice the tape\ninto a loop which gets you an infinitely repeating sample with a duration of a\nfew seconds or less.\n\nFor Reich, as for anyone working with tape, this meant a laborious process of\nfinding the right bit of tape, cutting it up and gluing it back together\nagain. For us, it's much, much easier. This is because the\nAudioBufferSourceNode interface we're using happens to have a loop flag we can\njust set to true:\n\nitsgonnarain.js\n\n    \n    \n    let sourceNode = audioContext.createBufferSource(); sourceNode.buffer = audioBuffer; sourceNode.loop = true; sourceNode.connect(audioContext.destination); sourceNode.start();\n\nThe effect of this is that the whole sample loops round and round forever.\n\nWe should still do the equivalent of cutting up the little piece that says\n\"It's Gonna Rain\" and loop just that bit. We could go into an audio editor and\ncreate an .mp3 file that only contains that part, but we don't actually need\nto. AudioBufferSourceNode has attributes called loopStart and loopEnd for\ncontrolling this. They allow setting the offsets, in seconds, of where the\nloop should start and end. With some trial and error you can find good values\nfor them:\n\nitsgonnarain.js\n\n    \n    \n    let sourceNode = audioContext.createBufferSource(); sourceNode.buffer = audioBuffer; sourceNode.loop = true; sourceNode.loopStart = 2.98; sourceNode.loopEnd = 3.80; sourceNode.connect(audioContext.destination); sourceNode.start();\n\nWith these settings, the sample still starts playing from the very beginning\nand then gets \"stuck\" looping between 2.98 and 3.80, resulting in a loop with\na duration of 0.82 seconds.\n\nIf we only want to play the looping part, we should also begin playing the\nsample from the loop start offset, which we can do by giving that same value\nto the start() method of the buffer source:\n\nitsgonnarain.js\n\n    \n    \n    let sourceNode = audioContext.createBufferSource(); sourceNode.buffer = audioBuffer; sourceNode.loop = true; sourceNode.loopStart = 2.98; sourceNode.loopEnd = 3.80; sourceNode.connect(audioContext.destination); sourceNode.start(0, 2.98);\n\nThe start() method optionally takes up to three arguments, and we're giving\ntwo of them here:\n\n  * When to start playing. We want to start playing immediately, so we set it to zero. This is also the default but since we want to provide that second argument, we also need to come up with something for the first.\n  * The offset at which to start playing the buffer. We set it to 2.98 - the beginning of our loop.\n\n### How Phase Music Works\n\nAnd now we come to the real technical trick that underlies \"It's Gonna Rain\":\nWhat we are hearing are two identical tape loops playing at the same time.\nThere's one in the left ear and one in the right. But one of those loops is\nrunning ever so slightly faster than the other.\n\nThe difference is so small you don't even notice it at first, but after a\nwhile it amounts to a phase shift between the two loops that keeps growing\nbigger and bigger as time goes by. First it sounds like there's an echo. Then\nthe echo transforms into a repeating effect, and then it builds into a series\nof strange combinations of sounds as different parts of the sample overlap\nwith others. At the halfway point the loops start to approach each other\nagain, and we go back through repetition and echo to unison.\n\nBrian Eno has likened this process to a moir\u00e9 pattern, where two simple\nidentical geometrical patterns are superimposed to give rise to something much\nmore complex than the original.\n\nIt's a remarkably simple trick compared to the apparent complexity of what you\nactually hear. And from a 0.82 second loop we get six minutes of music. That's\npretty good return on investment!\n\nThis is called phase music, which is a kind of systems music. One way to think\nabout it is that Reich, as the author of the piece, did not write a score for\nit. He just set the inputs and parameters for a phase-shifting process which\nhe would then execute, generating the music.\n\n> Though I may have the pleasure of discovering musical processes and\n> composing the musical material to run through them, once the process is set\n> up and loaded it runs by itself.\n>\n> Steve Reich, Music as a Gradual Process\n\n### Setting up The Second Loop\n\nTo reproduce the phase-shifting process, we're going to need another instance\nof the \"It's gonna rain\" sound loop. Let's extract the initialization of our\nsource node into a new function called startLoop, which we can then call\ntwice. Two source nodes that both play the same AudioBuffer are created and\nstarted:\n\nitsgonnarain.js\n\n    \n    \n    let audioContext = new AudioContext(); function startLoop(audioBuffer) { let sourceNode = audioContext.createBufferSource(); sourceNode.buffer = audioBuffer; sourceNode.loop = true; sourceNode.loopStart = 2.98; sourceNode.loopEnd = 3.80; sourceNode.connect(audioContext.destination); sourceNode.start(0, 2.98); } fetch('itsgonnarain.mp3') .then(response => response.arrayBuffer()) .then(arrayBuffer => audioContext.decodeAudioData(arrayBuffer)) .then(audioBuffer => { startLoop(audioBuffer); startLoop(audioBuffer); }) .catch(e => console.error(e));\n\nThe only effect of doing this so far is that the sound we hear becomes louder.\nThere are now two sources playing the loop in unison, which amps up the\nvolume.\n\nWeb Audio graph with two looped sources.\n\n### Adding Stereo Panning\n\nIn Reich's piece the two loops are playing in separate stereo channels, one in\nyour left ear and one in the right. We should do the same.\n\nThe AudioBufferSourceNode interface has no property for controlling this.\nInstead there's a more general purpose solution for it in the Web Audio API,\nwhich is to add specialized StereoPannerNodes to the audio graph. They can be\nused to pan their incoming audio signal to any point in the stereo field.\n\nStereoPannerNode has a pan attribute that can be set to a number between -1\n(all the way to the left) and 1 (all the way to the right), with the default\nbeing 0 (in the center). We set up our loops so that one is fully on the left\nand the other on the right:\n\nitsgonnarain.js\n\n    \n    \n    let audioContext = new AudioContext(); function startLoop(audioBuffer, pan = 0) { let sourceNode = audioContext.createBufferSource(); let pannerNode = audioContext.createStereoPanner(); sourceNode.buffer = audioBuffer; sourceNode.loop = true; sourceNode.loopStart = 2.98; sourceNode.loopEnd = 3.80; pannerNode.pan.value = pan; sourceNode.connect(pannerNode); pannerNode.connect(audioContext.destination); sourceNode.start(0, 2.98); } fetch('itsgonnarain.mp3') .then(response => response.arrayBuffer()) .then(arrayBuffer => audioContext.decodeAudioData(arrayBuffer)) .then(audioBuffer => { startLoop(audioBuffer, -1); startLoop(audioBuffer, 1); }) .catch(e => console.error(e));\n\nWe are no longer connecting the source nodes directly to the AudioContext\ndestination. We connect them to the panner nodes, and then the panner nodes to\nthe destination. The audio signals flow through the panners:\n\nWeb audio graph with two loops panned to different ends of the stereo field.\n\n### Putting It Together: Adding Phase Shifting\n\nFinally we're ready introduce the phase shifting. This is done so that one of\nthe loops plays slightly faster than the other. The difference should be so\nsmall that you don't perceive it at first, but large enough so that a\nnoticeable phase shift starts accumulating over time.\n\nThe playback speed of an AudioBufferSourceNode can be set using the\nplaybackRate attribute. The default value is 1 and setting it to something\nelse makes it go slower or faster. For example, setting it to 2 doubles the\nplayback speed, also halving the duration in the process.\n\nFor our purposes, I've found setting a value around 1.002 for one loop and\nkeeping the other as 1 to work well. When the original loop is 0.82 seconds\nlong, the second one thus becomes 0.82 / 1.002 \u2248 0.818 seconds long.\n\nitsgonnarain.js\n\n    \n    \n    let audioContext = new AudioContext(); function startLoop(audioBuffer, pan = 0, rate = 1) { let sourceNode = audioContext.createBufferSource(); let pannerNode = audioContext.createStereoPanner(); sourceNode.buffer = audioBuffer; sourceNode.loop = true; sourceNode.loopStart = 2.98; sourceNode.loopEnd = 3.80; sourceNode.playbackRate.value = rate; pannerNode.pan.value = pan; sourceNode.connect(pannerNode); pannerNode.connect(audioContext.destination); sourceNode.start(0, 2.98); } fetch('itsgonnarain.mp3') .then(response => response.arrayBuffer()) .then(arrayBuffer => audioContext.decodeAudioData(arrayBuffer)) .then(audioBuffer => { startLoop(audioBuffer, -1); startLoop(audioBuffer, 1, 1.002); }) .catch(e => console.error(e));\n\nAnd with that, we have \"It's Gonna Rain\"!\n\n#### \"It's Gonna Rain\"\n\nWeb audio graph with two loops, panned and with phasing.\n\n### Exploring Variations on It's Gonna Rain\n\nThere's a lot of exploration we can do around the process we've set up. Here's\na couple of ideas you can try to see what happens:\n\n  * Try different playback rates. How much faster can you make one of the loops compared to the other before it turns from \"phasing\" to something else?\n  * Set different values for panning. Does the piece sound different if you only partially pan the loops, or don't pan them at all? How about dynamic panning that changes over time? (There are several methods in the Web Audio API for gradually ramping parameters like pan or playbackRate up or down)\n\nAnother route of exploration is using different sound sources for the piece.\nYou can try looping different parts of itsgonnarain.mp3, some of them shorter,\nsome of them longer. Why not also try sourcing samples from audio you've found\nfrom the Internet or recorded yourself? You could try speech samples or even\npieces of songs. The same process can result in wildly different results given\ndifferent inputs. Here are some loops I've made:\n\n##### \"Ways to Survive\" (Kendrick Lamar)\n\n##### \"Rhythm Was Generating\" (Patti Smith)\n\n##### \"Does It Make Common Sense?\" (Laurie Anderson)\n\nSteve Reich himself would later develop the tape phasing technique further in\nthe 1966 piece Come Out, whose civil rights theme is still unsettlingly\npertinent 50 years later.\n\nReich also took the tape phasing idea and applied it to live performance in\nPiano Phase in 1967 and Drumming in 1970. Alexander Chen has made a great Web\nAudio version of Piano Phase.\n\n##\n\nBrian Eno\n\nAmbient 1: Music for Airports, 2/1\n\n1978\n\n> Once I started working with generative music in the 1970s, I was flirting\n> with ideas of making a kind of endless music \u2014 not like a record that you'd\n> put on and which would play for a while and finish. I like the idea of a\n> kind of eternal music, but I didn't want it to be eternally repetitive,\n> either. I wanted it to be eternally changing.\n>\n> Brian Eno, 2007\n\nPrior to the mid-1970s, Brian Eno was mainly known for his art rock bona\nfides, first as a member of Roxy Music and then as a solo artist. But this\nwould soon change. Eno was interested in the experimental techniques of John\nCage, Terry Riley, and Steve Reich. He was \u2013 and still is \u2013 also something of\na tech geek. He was an early user of synthesizers and would later become a big\nproponent of the idea of the whole music studio as a compositional tool.\n\nIn 1975 Eno got into an accident. He was overrun by a speeding London taxi and\nwas left hospitalized. During his time in the hospital something happened that\nled him to the conceptualization of \"ambient music\" \u2013 a term everyone is by\nnow familiar with.\n\nNow, Ambient 1: Music for Airports was not the first record Eno made as a\nresult of this epiphany. That would have been \"Discreet Music\" a couple of\nyears earlier, something that we're going to look into in the next chapter.\nBut \"Music for Airports\" has a track on it that is very directly influenced by\nboth the concept and the execution of Steve Reich's tape pieces. It's the\nsecond track on side A.\n\nAs a musical experience, this is clearly nothing like \"It's Gonna Rain\"\nthough. Instead of a frantic loop of apocalyptic sermonizing, we're treated\nwith gentle waves of vocal harmonies. It actually kind of sounds like music in\nthe traditional sense!\n\nBut even though the results are very different, there are very close\nsimilarities in the underlying systems that produce the music.\n\n### The Notes and Intervals in Music for Airports\n\nBefore we discuss the system though, let's discuss the inputs.\n\nThe raw materials Eno used on this track are seven different \"aahhh\" sounds\n(\"sung by three women and myself\"). Each one of those sounds is on a different\npitch.\n\nIn terms of musical notes, what we are hearing are F, A-flat, and from the\nnext octave C, D-flat, E-flat, F, and A-flat. These, I'm told, together form\n\"a D-flat major seventh chord with an added ninth\". It's really quite\nbeautiful:\n\nThis chord evokes a kind of reflective mood, which is very common throughout\nEno's ambient work. You can get an idea of what he was going for in this\ninterview clip (I like the sound of \"well, if you die, it doesn't really\nmatter\"):\n\nThe note selection is also important because of the way the notes may combine.\nWe'll be feeding these individual notes into a kind of generative system, and\nwe will have little control over which combinations of two or more notes may\noccur. Any succession of notes might be played back to back forming a melody,\nor on top of each other forming a harmony.\n\nHere are all the possible two note intervals in this chord:\n\nA_\u266d| C'| D_\u266d'| E_\u266d'| F'| A_\u266d'  \n---|---|---|---|---|---  \nF  \nA_\u266d  \nC'  \nD_\u266d'  \nE_\u266d'  \nF'  \n  \nAmong all these intervals, there's just one that's sharply dissonant (a minor\nsecond) and sounds a bit off. Can you hear it? However, since there's just one\nand it'll thus not occur very frequently, it'll only serve to add a bit of\nspice to the music. Looks like whatever the system may throw at us will\ngenerally sound pretty good.\n\n### Setting up musicforairports.js\n\nThe source code for this project is available on GitHub.\n\nWith this background knowledge, we're ready to start writing our own little\nversion of this piece. Let's begin by setting up the project. The basic\nstructure will be very similar to what we did with \"It's Gonna Rain\".\n\nCreate a new directory and add two files to it:\n\n  * musicforairports/\n\n    * index.html\n    * musicforairports.js\n\nThen set the contents of index.html as follows:\n\n    \n    \n    <!DOCTYPE html> <html> <body> <script src=\"https://cdn.rawgit.com/mohayonao/web-audio-api-shim/master/build/web-audio-api-shim.min.js\"></script> <script src=\"https://cdnjs.cloudflare.com/ajax/libs/fetch/1.0.0/fetch.min.js\"></script> <script src=\"musicforairports.js\"></script> </body> </html>\n\nWe can leave the JavaScript file empty for now. If we just launch the web\nserver from this directory, we're all set to start hacking:\n\n    \n    \n    lite-server\n\n### Obtaining Samples to Play\n\nWe're going to build a system that plays sounds based on the musical chord\nthat we just heard. But we haven't really discussed what sounds those are\nactually going to be.\n\nEno used a set of samples based on voices he'd recorded himself. We have\nlittle hope of reproducing those voices exactly, so we're not going to even\ntry. We can use readymade samples obtained from the Internet instead.\nSpecifically, we're going to use samples from the freely available Sonatina\nSymphonic Orchestra sound bank. This is a very nice sampling of the sounds of\nindividual instruments in a symphonic orchestra - violins, brass instruments,\npercussion, etc.\n\nGo ahead and download and then unpack the Sonatina ZIP archive. It's pretty\nlarge so it might take a while to transfer and you'll need space for it. Then\nmove or copy the Samples directory to the musicforairports project.\n\n  * musicforairports/\n\n    * index.html\n    * musicforairports.js\n    * Samples\n\n      * 1st Violins\n\n        * 1st-violins-piz-rr1-a#3.wav\n        * etc.\n      * etc.\n\nBasically, we want to set things up so that we'll be able to load any of the\nsamples into our JavaScript code, which means they need to be inside a\ndirectory we're serving using the lite-server.\n\nIf you're feeling adventurous, there's a lot of more experimentation you can\ndo around sample selection, beyond the sounds found in the Sonatina library.\nYou could:\n\n  * Find other samples online, in places like freesound.org.\n  * Generate samples from GarageBand or some other piece of software or equipment.\n  * Record your own voice or a musical instrument.\n  * Synthesize the sound, as we do in the next chapter.\n\n### Building a Simple Sampler\n\nWe now have a sample library and we should write some code that will allow us\nto play different musical notes using the various musical instruments in that\nlibrary. In other words, we're going to write a primitive JavaScript sampler.\n\nIf you look into the various subdirectories under Samples you'll notice that\nthere are many .wav files for each musical instrument, for various notes. But\nthere's a problem: There aren't samples for every musical note. For example,\nlooking at the alto flute, we've got samples for A_#, C_#, E, and G on three\ndifferent octaves, but that's only four notes per octave. There are twelve\nsemitones in an octave, so eight out of twelve are missing!\n\nThis is not really a problem though, and it's a pretty common occurrence in\nsample libraries. An obvious reason for this is preserving storage and\nbandwidth: You may have noticed that the Sonatina library is already half a\ngigabyte in size. If it included all the notes of all the instruments, that\nnumber could easily triple.\n\nWhat we will do for each note that is missing a sample, is to find the nearest\nnote that does have a sample and pitch-shift that sample to the new note. This\ntechnique for \"filling in the blanks\" is commonly used by samplers.\n\nLet's look at the samples for the grand piano - an instrument I find fits well\nwith this piece. It has samples on about seven octaves, a few in each octave.\nIf we visualize the whole chromatic scale for octaves 4, 5, and 6, we see what\nwe have and what's missing:\n\nC4| C_#4 D_\u266d4| D4| D_#4 E_\u266d4| E4| F4| F_#4 G_\u266d4| G4| G_#4 A_\u266d4| A4| A_#4 B_\u266d4|\nB4  \n---|---|---|---|---|---|---|---|---|---|---|---  \nC5| C_#5 D_\u266d5| D5| E_#5 E_\u266d5| E5| F5| F_#5 G_\u266d5| G5| G_#5 A_\u266d6| A5| A_#5 B_\u266d5|\nB5  \nC6| C_#6 D_\u266d6| D6| D_#6 E_\u266d6| E6| F6| F_#6 G_\u266d6| G6| G_#6 A_\u266d7| A6| A_#6 B_\u266d6|\nB6  \n  \nWhat our sampler should do is find for each of these notes the nearest note\nthat has a sample. When we do that we have the keymap for this instrument:\n\nC4| C_#4 D_\u266d4| D4| D_#4 E_\u266d4| E4| F4| F_#4 G_\u266d4| G4| G_#4 A_\u266d5| A4| A_#4 B_\u266d4|\nB4  \n---|---|---|---|---|---|---|---|---|---|---|---  \nC5| C_#5 D_\u266d5| D5| E_#5 E_\u266d5| E5| F5| F_#5 G_\u266d5| G5| G_#5 A_\u266d6| A5| A_#5 B_\u266d5|\nB5  \nC6| C_#6 D_\u266d6| D6| D_#6 E_\u266d6| E6| F6| F_#6 G_\u266d6| G6| G_#6 A_\u266d7| A6| A_#6 B_\u266d6|\nB6  \n  \nThe main function of our sampler will be one that takes the name of an\ninstrument and a note to play. We'll expect it to return a sample that we'll\nbe able to play (or, rather, a Promise that resolves to a sample):\n\nmusicforairports.js\n\n    \n    \n    function getSample(instrument, noteAndOctave) { }\n\nFor each instrument we'll store a little \"sample library\" data structure,\nwhich records what note samples we have for that instrument, and where those\nsample files can be located. Let's put the grand piano samples for octaves 4-6\nin it:\n\nmusicforairports.js\n\n    \n    \n    const SAMPLE_LIBRARY = { 'Grand Piano': [ { note: 'A', octave: 4, file: 'Samples/Grand Piano/piano-f-a4.wav' }, { note: 'A', octave: 5, file: 'Samples/Grand Piano/piano-f-a5.wav' }, { note: 'A', octave: 6, file: 'Samples/Grand Piano/piano-f-a6.wav' }, { note: 'C', octave: 4, file: 'Samples/Grand Piano/piano-f-c4.wav' }, { note: 'C', octave: 5, file: 'Samples/Grand Piano/piano-f-c5.wav' }, { note: 'C', octave: 6, file: 'Samples/Grand Piano/piano-f-c6.wav' }, { note: 'D#', octave: 4, file: 'Samples/Grand Piano/piano-f-d#4.wav' }, { note: 'D#', octave: 5, file: 'Samples/Grand Piano/piano-f-d#5.wav' }, { note: 'D#', octave: 6, file: 'Samples/Grand Piano/piano-f-d#6.wav' }, { note: 'F#', octave: 4, file: 'Samples/Grand Piano/piano-f-f#4.wav' }, { note: 'F#', octave: 5, file: 'Samples/Grand Piano/piano-f-f#5.wav' }, { note: 'F#', octave: 6, file: 'Samples/Grand Piano/piano-f-f#6.wav' } ] };\n\nSo now we're going to need to find, for any note given to getSample, the\nnearest note we have in the sample library. Before we can do that, we need to\nnormalize the input notes a little bit.\n\nFirstly, we should split the incoming note string (\"B#4\") into the note\ncharacters (\"B#\") and the octave digit (\"4\"). We can do so with a regular\nexpression that pulls the two parts into separate capture groups.\n\nmusicforairports.js\n\n    \n    \n    function getSample(instrument, noteAndOctave) { let [, requestedNote, requestedOctave] = /^(\\w[b#]?)(\\d)$/.exec(noteAndOctave); requestedOctave = parseInt(requestedOctave, 10); }\n\nThe group (\\w[b#]?) captures a single word character optionally followed by a\n'b' or a '#', and (\\d) captures a single digit. We then also parse the octave\nfrom a string to a integer number.\n\nThe Sonatina library contains samples for natural notes and sharp (#) notes\nonly, but we would like to also support flat (\u266d) notes. We'll exploit the fact\nthat for every flat note there is an \"enharmonically equivalent\" sharp note.\nThis basically just means the same note may be written in two different ways,\nsharp or flat.\n\nWe can make a little function that eliminates flats completely by turning them\ninto their equivalent sharps. For example, for the flat Bb we get the\nequivalent sharp A#:\n\nmusicforairports.js\n\n    \n    \n    function flatToSharp(note) { switch (note) { case 'Bb': return 'A#'; case 'Db': return 'C#'; case 'Eb': return 'D#'; case 'Gb': return 'F#'; case 'Ab': return 'G#'; default: return note; } } function getSample(instrument, noteAndOctave) { let [, requestedNote, requestedOctave] = /^(\\w[b#]?)(\\d)$/.exec(noteAndOctave); requestedOctave = parseInt(requestedOctave, 10); requestedNote = flatToSharp(requestedNote); }\n\nIn order to find the \"nearest\" note for a given input note, we need a way to\nmeasure the distance between two notes. This calls for a purely numeric note\nrepresentation.\n\nFirst of all, there are twelve notes (\"semitones\") in each octave, and each\noctave begins from the C note:\n\nmusicforairports.js\n\n    \n    \n    const OCTAVE = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B'];\n\nUsing this information, given a note and an octave, we can come up with an\ninteger number that uniquely identifies that note:\n\n  1. Identify the octave's base note by multiplying the octave number by 12: For the note D3, the octave is 3 so we get 3 * 12 = 36.\n  2. Find the index of the note within the octave. D3 is the third note in the octave, D, so its index is 2.\n  3. Sum those together to get the note's numeric value. For D3, 36 + 2 = 38.\n\nmusicforairports.js\n\n    \n    \n    const OCTAVE = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']; function noteValue(note, octave) { return octave * 12 + OCTAVE.indexOf(note); }\n\nBased on this, we can form a function that calculates the distance between any\ntwo notes. It takes two note names and octaves and simply subtracts one's\nvalue from the other:\n\nmusicforairports.js\n\n    \n    \n    function getNoteDistance(note1, octave1, note2, octave2) { return noteValue(note1, octave1) - noteValue(note2, octave2); }\n\nNow we have the building blocks of a function that finds the nearest sample\ngiven an instrument's sample bank, note, and octave. It sorts the sample bank\narray by the absolute distance to the requested note. It then returns the\nsample with the shortest distance, which will be the first sample in the\nsorted array.\n\nmusicforairports.js\n\n    \n    \n    function getNearestSample(sampleBank, note, octave) { let sortedBank = sampleBank.slice().sort((sampleA, sampleB) => { let distanceToA = Math.abs(getNoteDistance(note, octave, sampleA.note, sampleA.octave)); let distanceToB = Math.abs(getNoteDistance(note, octave, sampleB.note, sampleB.octave)); return distanceToA - distanceToB; }); return sortedBank[0]; }\n\nThe final helper function we'll need for our sampler is one that actually\nloads a sample file from the server. The code for that is very similar to the\none we had for \"It's Gonna Rain\". It fetches and decodes the sample file. For\nthis we also need to instantiate an AudioContext:\n\nmusicforairports.js\n\n    \n    \n    let audioContext = new AudioContext(); function fetchSample(path) { return fetch(encodeURIComponent(path)) .then(response => response.arrayBuffer()) .then(arrayBuffer => audioContext.decodeAudioData(arrayBuffer)); }\n\nNote that we use the encodeURIComponent function to sanitize the file path\nbefore giving it to fetch. It's needed because our filenames may have '#'\ncharacters, which need to be escaped when used inside URLs.\n\nNow we can put all of this together, so that getSample always returns an\nAudioBuffer as well as a number that denotes the distance between the sample\nthat was requested and the one that was found.\n\nmusicforairports.js\n\n    \n    \n    function getSample(instrument, noteAndOctave) { let [, requestedNote, requestedOctave] = /^(\\w[b\\#]?)(\\d)$/.exec(noteAndOctave); requestedOctave = parseInt(requestedOctave, 10); requestedNote = flatToSharp(requestedNote); let sampleBank = SAMPLE_LIBRARY[instrument]; let sample = getNearestSample(sampleBank, requestedNote, requestedOctave); let distance = getNoteDistance(requestedNote, requestedOctave, sample.note, sample.octave); return fetchSample(sample.file).then(audioBuffer => ({ audioBuffer: audioBuffer, distance: distance })); }\n\nIn the case of the grand piano sample bank, the distances between requested\nand found samples range from -1 to 1 because we always have a sample at most\none semitone away. But other instruments may have longer distances. This\ndistance determines the amount of pitch-shifting we'll have to do.\n\nPitch-shifting is done by adjusting the frequency of the sound wave formed by\na sample, and there's a simple formula for determining the size of the\nadjustment.\n\nThe key idea is that the relationship between musical notes and the underlying\nsound frequencies is exponential. Going up an octave always doubles the\nfrequency of the sound wave, so if we want to pitch-shift a whole octave we\nneed to double the frequency:\n\nGoing an octave up from C to C' means doubling the frequency.\n\nSince an octave is divided into 12 semitones, going up a single semitone means\nmultiplying the frequency by a fractional power of two: 2^1/12. And going up n\nsemitones means multiplying the frequency by 2^n/12. The same applies when\ngoing down, but then we multiply by negative exponents: 2^-n/12:\n\nGoing up or down n semitones means multiplying the frequency by a fractional\npower of 2.\n\nWhen we are playing sample files, we can't really just \"set the frequency\" of\na sample directly. But what we can do is set the playback rate of the sample,\nbecause this will compress or expand its sound wave in time by the same ratio.\nTo achieve a lower note, we simply play the sample more slowly, and to reach a\nhigher note we play it more quickly.\n\nArmed with this information we can come up with an expression that, given the\ndistance to a note, gives us the playback rate we need to use. A zero distance\ngives the original playback rate, 1, since 2^0/12 = 2^0 = 1, whereas distance\nof one semitone should return a playback rate of 2^1/12 \u2248 1.059. This will\nmake the sound play slightly faster, causing our ears to perceive it as a\nhigher note.\n\n    \n    \n    let playbackRate = Math.pow(2, distance / 12);\n\nNow we can form a function that actually plays any musical note for any\ninstrument. It uses getSample to fetch the nearest sample, waits for the\nresult, and then plays back the sample with the appropriate playback rate.\n\nmusicforairports.js\n\n    \n    \n    function playSample(instrument, note) { getSample(instrument, note).then(({audioBuffer, distance}) => { let playbackRate = Math.pow(2, distance / 12); let bufferSource = audioContext.createBufferSource(); bufferSource.buffer = audioBuffer; bufferSource.playbackRate.value = playbackRate; bufferSource.connect(audioContext.destination); bufferSource.start(); }); }\n\nA general purpose sampler should also cache the sample buffers somewhere in\nmemory instead of fetching them again each time they're needed. But this is\ngood enough for us right now.\n\nHere's the complete code we have written so far, combined with some test code\nthat plays the seven notes from \"Airports\" at one second intervals:\n\nmusicforairports.js\n\n    \n    \n    const SAMPLE_LIBRARY = { 'Grand Piano': [ { note: 'A', octave: 4, file: 'Samples/Grand Piano/piano-f-a4.wav' }, { note: 'A', octave: 5, file: 'Samples/Grand Piano/piano-f-a5.wav' }, { note: 'A', octave: 6, file: 'Samples/Grand Piano/piano-f-a6.wav' }, { note: 'C', octave: 4, file: 'Samples/Grand Piano/piano-f-c4.wav' }, { note: 'C', octave: 5, file: 'Samples/Grand Piano/piano-f-c5.wav' }, { note: 'C', octave: 6, file: 'Samples/Grand Piano/piano-f-c6.wav' }, { note: 'D#', octave: 4, file: 'Samples/Grand Piano/piano-f-d#4.wav' }, { note: 'D#', octave: 5, file: 'Samples/Grand Piano/piano-f-d#5.wav' }, { note: 'D#', octave: 6, file: 'Samples/Grand Piano/piano-f-d#6.wav' }, { note: 'F#', octave: 4, file: 'Samples/Grand Piano/piano-f-f#4.wav' }, { note: 'F#', octave: 5, file: 'Samples/Grand Piano/piano-f-f#5.wav' }, { note: 'F#', octave: 6, file: 'Samples/Grand Piano/piano-f-f#6.wav' } ] }; const OCTAVE = ['C', 'C#', 'D', 'D#', 'E', 'F', 'F#', 'G', 'G#', 'A', 'A#', 'B']; let audioContext = new AudioContext(); function fetchSample(path) { return fetch(encodeURIComponent(path)) .then(response => response.arrayBuffer()) .then(arrayBuffer => audioContext.decodeAudioData(arrayBuffer)); } function noteValue(note, octave) { return octave * 12 + OCTAVE.indexOf(note); } function getNoteDistance(note1, octave1, note2, octave2) { return noteValue(note1, octave1) - noteValue(note2, octave2); } function getNearestSample(sampleBank, note, octave) { let sortedBank = sampleBank.slice().sort((sampleA, sampleB) => { let distanceToA = Math.abs(getNoteDistance(note, octave, sampleA.note, sampleA.octave)); let distanceToB = Math.abs(getNoteDistance(note, octave, sampleB.note, sampleB.octave)); return distanceToA - distanceToB; }); return sortedBank[0]; } function flatToSharp(note) { switch (note) { case 'Bb': return 'A#'; case 'Db': return 'C#'; case 'Eb': return 'D#'; case 'Gb': return 'F#'; case 'Ab': return 'G#'; default: return note; } } function getSample(instrument, noteAndOctave) { let [, requestedNote, requestedOctave] = /^(\\w[b\\#]?)(\\d)$/.exec(noteAndOctave); requestedOctave = parseInt(requestedOctave, 10); requestedNote = flatToSharp(requestedNote); let sampleBank = SAMPLE_LIBRARY[instrument]; let sample = getNearestSample(sampleBank, requestedNote, requestedOctave); let distance = getNoteDistance(requestedNote, requestedOctave, sample.note, sample.octave); return fetchSample(sample.file).then(audioBuffer => ({ audioBuffer: audioBuffer, distance: distance })); } function playSample(instrument, note) { getSample(instrument, note).then(({audioBuffer, distance}) => { let playbackRate = Math.pow(2, distance / 12); let bufferSource = audioContext.createBufferSource(); bufferSource.buffer = audioBuffer; bufferSource.playbackRate.value = playbackRate; bufferSource.connect(audioContext.destination); bufferSource.start(); }); } // Temporary test code setTimeout(() => playSample('Grand Piano', 'F4'), 1000); setTimeout(() => playSample('Grand Piano', 'Ab4'), 2000); setTimeout(() => playSample('Grand Piano', 'C5'), 3000); setTimeout(() => playSample('Grand Piano', 'Db5'), 4000); setTimeout(() => playSample('Grand Piano', 'Eb5'), 5000); setTimeout(() => playSample('Grand Piano', 'F5'), 6000); setTimeout(() => playSample('Grand Piano', 'Ab5'), 7000);\n\n### A System of Loops\n\nNow that we have an instrument to play, we can shift our focus to the system\nthat will play it.\n\nEno's piece was generated by seven tape loops. Each one contained a recording\nof a single vocal \"aaaahh\" sound on one of the seven pitches. The loops all\nhad different durations, between about 20-30 seconds. When you play such a\ncombination of loops together, you get a similar kind of relative shifting of\nsounds as we have in \"It's Gonna Rain\". As the loops turn over at different\ntimes, the sounds and silences on the tapes constantly reorganize and\n\"generate\" new combinations.\n\nOnce again, it's really a very simple system compared to what the results\nsound like! This conceptual simplicity does not mean the piece was easy to\nexecute though. The 20-30 second durations of these tape loops were way more\nthan the 1-2 seconds that the tape recorders of the time were equipped to\nhandle. Eno had to resort to some epic hacks to make it all work.\n\n> One of the notes repeats every 23 1/2 seconds. It is in fact a long loop\n> running around a series of tubular aluminum chairs in Conny Plank's studio.\n> The next lowest loop repeats every 25 7/8 seconds or something like that.\n> The third one every 29 15/16 seconds or something. What I mean is they all\n> repeat in cycles that are called incommensurable \u2014 they are not likely to\n> come back into sync again.\n>\n> Brian Eno, 1996\n\nAnd how much of unique music can we generate with this technique? This depends\non the relative durations of the loops (or, \"lengths of tape\") that we choose.\nIf we have two loops, both with a duration of 2 seconds, the duration of our\nwhole piece is 2 seconds since the whole thing just keeps repeating.\n\nBut if we make the duration of the second loop 3 seconds instead of 2, the\nduration of our whole piece doubles to 6 seconds. This is because the next\ntime the two loops start at the same time is after six seconds. That's when\nthe system will have exhausted all of its musical combinations.\n\nBy adjusting the two numbers we can make the piece much longer. If we make one\nloop only slightly longer than the other, it'll take a while for them to meet\nagain. This is how Reich was able to squeeze out 6 minutes worth of music from\ntwo sub-second loops. He varied the playback rate rather than the tape length,\nbut the result was effectively the same.\n\nBut it is when we bring in a third loop that the system really takes off. We\nwon't have heard all the combinations the system is able to produce until we\nreach a moment when all of the three loops meet at the same time. That's going\nto take some time.\n\nYou can see how bringing in four more loops with similarly differing durations\nresults in a piece of music that's very long indeed. That's pretty striking,\ngiving the extremely limited amount of sonic material and the simplicity of\nthe system.\n\nNow of course, you don't have to listen for very long until you've figured out\nwhat type of music is being made. You can listen to it for a thousand years\nand it'll never play \"Hotline Bling\". It simply varies the relative timings of\nits seven notes. But in any case, anyone who buys a copy of the \"Music for\nAirports\" album only gets to hear a tiny 9 minute clip of the vast amount of\nmusic this system is capable of making.\n\n### Playing Extended Loops\n\nLet's construct these loops in our project. Recall that with \"It's Gonna Rain\"\nwe looped sounds by simply setting the loop property of the buffer source\nnodes to true, which made the sounds repeat. This time the loops are longer -\nmuch longer than the individual sample files in fact, so we can't apply the\nsame trick here.\n\nEno had to set up physical contraptions to make his tape loops go around the\nstudio space. Our equivalent of this is to use JavaScript's built-in\nsetInterval function, which executes a given callback function with a regular\ntime interval. If we make a callback that plays one of the sounds every 20\nseconds, we have an \"extended\" loop. Replace the test setTimeout test code in\nmusicforairports.js with this:\n\nmusicforairports.js\n\n    \n    \n    setInterval(() => playSample('Grand Piano', 'C4'), 20000);\n\nWith this version we do need to wait for about 20 seconds before the first\nnote plays too, which we may not want to do. Instead we can write a function\nthat plays the sound once immediately and after that sets up the interval:\n\nmusicforairports.js\n\n    \n    \n    function startLoop(instrument, note, loopLengthSeconds) { playSample(instrument, note); setInterval(() => playSample(instrument, note), loopLengthSeconds * 1000); } startLoop('Grand Piano', 'C4', 20);\n\nSince we're going to set up several loops, we should also have the possibility\nto add initial delays to them, so that all the sounds won't play at exactly\nthe same time in the beginning. So, we should have another number that delays\nthe playing of the sound within the loop. It controls at which section of our\nvirtual \"tape\" the sound is recorded.\n\nWe can achieve this by adding a delay argument to the playSample function. It\npasses a new argument to the bufferSource.start() method - the time at which\nthe sound should begin playing, relative to the AudioContext's current time:\n\nmusicforairports.js\n\n    \n    \n    function playSample(instrument, note, delaySeconds = 0) { getSample(instrument, note).then(({audioBuffer, distance}) => { let playbackRate = Math.pow(2, distance / 12); let bufferSource = audioContext.createBufferSource(); bufferSource.buffer = audioBuffer; bufferSource.playbackRate.value = playbackRate; bufferSource.connect(audioContext.destination); bufferSource.start(audioContext.currentTime + delaySeconds); }); }\n\nWe already saw this argument with \"It's Gonna Rain\", but we were just using 0\nas the value then to get immediate playback.\n\nTo test this version, we can set up a five second delay to our loop:\n\nmusicforairports.js\n\n    \n    \n    function startLoop(instrument, note, loopLengthSeconds, delaySeconds) { playSample(instrument, note, delaySeconds); setInterval( () => playSample(instrument, note, delaySeconds), loopLengthSeconds * 1000 ); } startLoop('Grand Piano', 'C4', 20, 5);\n\nOur Web Audio node graph at this point is very simple. There's at most just\none AudioBufferSourceNode playing a sound, since we've got a single loop\nrunning:\n\nWeb Audio graph with one source added by the extended loop.\n\nWhat's different this time though is that the node graph is not static. Every\n20 seconds we attach a new source node. After it's finished playing it'll get\nautomatically removed, at which point the graph contains nothing but the\ndestination node.\n\nThe source is removed and destroyed once it finishes playing.\n\nHaving dynamic graphs such as this is very common with Web Audio. In fact, you\ncan't even start an AudioBufferSourceNode more than once. Every time you want\nto play a sound, you make a new node, even when it's for a sound you've played\nbefore.\n\nWhile the simple setInterval() based solution will work just fine for us, I\nshould mention that it is not precise enough for most Web Audio applications.\nThis is because JavaScript does not give exact guarantees about the timing of\nsetInterval(). If something else is making the computer busy at the moment\nwhen the timer should fire, it may get delayed. It's not at all uncommon for\nthis to happen.\n\nFour our purposes though, I actually like that it's a bit inexact. We're\ndealing with the kind of application where scheduling is very porous anyway.\nFurthermore, this was a characteristic of the equipment Eno was using as well.\nThe motors in his tape recorders were not exact to a millisecond precision,\nespecially when he stretched them way beyond what they were originally\ndesigned for.\n\nFor the kinds of applications you do need precision for, you'll need to\ncalculate the exact offsets manually and give them to bufferSource.start().\nThere are good articles and libraries that help with this. We will also see\nhow to achieve exact timing with the Tone.js library in the next chapter.\n\n### Adding Reverb\n\nIf you compare our grand piano loop to how Eno's original piece sounds, you'll\nhear a big difference.\n\nOf course, this is mostly due to the fact that we're using completely\ndifferent instruments, resulting in an entirely different timbre. But that's\nnot the only difference. You'll notice that Eno's sounds seem to emerge from\nthe ether very softly and then also fade back down gently, whereas our samples\nhave a much rougher entrance and exit.\n\nThere are a number of ways we could adjust this. One very effective way to\nchange the perception of the sound is to add some reverberation to it, making\nit seem like the sound is reflecting in a physical space. This will especially\naffect the part where the sound stops playing: Instead of all sound\ndisappearing instantly, the reflections in the virtual space will linger.\n\nThe Web Audio API comes with an interface called ConvolverNode that can\nachieve this kind of an effect. It applies something called a convolution to a\nsound signal, by combining it with another sound signal called an acoustic\nimpulse response. The exact math behind this is beyond the scope of this guide\nand to be honest I don't fully understand it. But it sounds pretty damn cool,\nwhich is the most important thing.\n\nTo use ConvolverNode we need to obtain an impulse response sample that will be\nused to convolve our audio source. As it happens, someone by the name of\nAirWindows has made a free one called \"airport terminal\", which is\nfantastically appropriate for our purposes and our theme. It's not actually\nrecorded in a real airport terminal, but it simulates the massive amount of\nreverb you get in one.\n\nI've made a suitable stereo .wav file of this impulse response, which you can\nget here. Place it within the musicforairports directory. Then modify the code\nthat starts our audio loop so that it first fetches this file as an audio\nbuffer and then creates a convolver node based on it. The node is given to\nstartLoop as a new argument:\n\nmusicforairports.js\n\n    \n    \n    fetchSample('AirportTerminal.wav').then(convolverBuffer => { let convolver = audioContext.createConvolver(); convolver.buffer = convolverBuffer; convolver.connect(audioContext.destination); startLoop('Grand Piano', 'C4', convolver, 20, 5); });\n\nThen modify startLoop so that it passes the new argument (called destination)\nonward to playSample:\n\nmusicforairports.js\n\n    \n    \n    function startLoop(instrument, note, destination, loopLengthSeconds, delaySeconds) { playSample(instrument, note, destination, delaySeconds); setInterval( () => playSample(instrument, note, destination, delaySeconds), loopLengthSeconds * 1000 ); }\n\nAnd finally, modify playSample so that it sends its audio signal to the given\ndestination instead of the audio context's final destination node. This causes\nall of the samples to flow through the convolver node:\n\nmusicforairports.js\n\n    \n    \n    function playSample(instrument, note, destination, delaySeconds = 0) { getSample(instrument, note).then(({audioBuffer, distance}) => { let playbackRate = Math.pow(2, distance / 12); let bufferSource = audioContext.createBufferSource(); bufferSource.buffer = audioBuffer; bufferSource.playbackRate.value = playbackRate; bufferSource.connect(destination); bufferSource.start(audioContext.currentTime + delaySeconds); }); }\n\nThis makes a big difference in the sound produced! As far as reverberation\neffects go, it's certainly among the most dramatic and it completely changes\nthe nature of the music. If it proves too massive to your liking, try one of\nthe others from AirWindows or use one from this database compiled by The\nUniversity of York.\n\nHere's what the Web Audio node graphs produced by this code look like:\n\nWeb Audio graph with a single source and a convolver.\n\n### Putting It Together: Launching the Loops\n\nWith our sampler and reverbed loops, we can now prop up the whole system that\nproduces our approximation of \"Music for Airports 2/1\". This is simply done by\ncalling startLoop several times, once for each of the musical notes that we\nwant to play.\n\nHere we need to come up with a bunch of numeric values: We need to decide the\nduration of each loop, and the delay by which each sound is positioned inside\nits loop. Of these two, the loop durations are much more important since, as\nwe saw earlier, making them incommensurable will cause the system to produce a\nvery long piece of unique music. The delays on the other hand have less of an\nimpact. They basically control how the music begins but very soon their role\nwill diminish as the differences in the loop durations take over.\n\nHere's an example set of numbers. Their exact values are not hugely important.\nThe story tells that Eno merely cut the tapes up to what he thought were\n\"reasonable lengths\" without giving it much thought. We may just as well do\nthe same.\n\nmusicforairports.js\n\n    \n    \n    fetchSample('AirportTerminal.wav').then(convolverBuffer => { let convolver = audioContext.createConvolver(); convolver.buffer = convolverBuffer; convolver.connect(audioContext.destination); startLoop('Grand Piano', 'F4', convolver, 19.7, 4.0); startLoop('Grand Piano', 'Ab4', convolver, 17.8, 8.1); startLoop('Grand Piano', 'C5', convolver, 21.3, 5.6); startLoop('Grand Piano', 'Db5', convolver, 22.1, 12.6); startLoop('Grand Piano', 'Eb5', convolver, 18.4, 9.2); startLoop('Grand Piano', 'F5', convolver, 20.0, 14.1); startLoop('Grand Piano', 'Ab5', convolver, 17.7, 3.1); });\n\nA snapshot of the Web Audio graph formed by the Music for Airports system.\n\nAnd here is this particular system in action:\n\n#### \"Music for Airports\"\n\nIt's a fascinating feeling to set one of these systems off. You created it but\nyou don't know what it's going to do. To me this captures some of the magic I\nfelt when I was just getting started with programming, which I don't feel that\noften these days as a working software engineer. Even though there's\nabsolutely nothing random about the system \u2013 its behavior is fully\ndeterministic \u2013 you really have no idea what it's going to sound like. Those\ntwelve numbers specify the system's behavior completely but the system seems\nmuch more complicated than that. The complexity is emergent.\n\nEno has compared making musical systems like these to \"gardening\", as opposed\nto \"architecting\". We plant the seeds for the system, but then let it grow by\nitself and surprise us by the music that it makes.\n\n> What this means, really, is a rethinking of one's own position as a creator.\n> You stop thinking of yourself as me, the controller, you the audience, and\n> you start thinking of all of us as the audience, all of us as people\n> enjoying the garden together. Gardener included.\n>\n> Brian Eno, Composers as Gardeners\n\n### Exploring Variations on Music for Airports\n\nThis is a much more complex systems than \"It's Gonna Rain\", and as such\nprovides a lot more room for all kinds of experimentation. See if any of the\nfollowing tickles your interest:\n\n  * Make the durations considerably shorter or longer. You may notice that the silences are just as important as the sounds in a piece like this. Having things playing constantly can easily feel crowded.\n  * Play with different instruments from the Sonatina library or from elsewhere.\n  * How about mixing two or more instruments in the same system? Maybe add more loops to accommodate them.\n  * Change the musical chords played. For example, playing a straightforward C major scale will result in a very different mood than what we have currently.\n  * Try different impulse responses to change the physical \"space\" in which the sound plays.\n  * Try randomizing the loop durations and/or delays, so that a different system is generated every time the page loads.\n\nHere's an example of a variation I came up with. I used the grand piano but\naccidentally divided the playback rates of the samples by three, causing them\nto sound completely different from what I was expecting.\n\n#### \"Music for Airports\" - Oblique Piano\n\nThe result not only sounds nothing like a grand piano, but also changes the\nchord. As we've seen, frequencies and musical notes have an exponential\nrelationship, so when I simply divide the frequency (= playback rate) by a\nconstant number, a new set of musical intervals forms. I don't even know what\nthey are in this case, but I like them.\n\nWhat essentially happened here is that I caused a bug and it made the thing\nbetter. When's the last time you remember that happening? Yet this is the kind\nof thing that often happens when you're playing around with these kinds of\nmusical systems. It's a good idea to listen carefully when it does, because\nwhat you did by mistake may end up sounding better than the thing you were\noriginally trying to do. As one of Eno's Oblique Strategies cards says, \"Honor\nthy error as a hidden intention\".\n\nIn another variation I switched the instrument to the English horn, which is\nalso included in the Sonatina sample library:\n\n#### \"Music for Airports\" - Cor Anglais\n\n##\n\nBrian Eno\n\nDiscreet Music\n\n1975\n\n> I\u2019ve met a lot of children who were born listening to one record in\n> particular, which is Discreet Music. I should think by now I\u2019ve met about 60\n> or 70 kids who came out of the womb listening to that record, which of\n> course, is any marketing department's dream: Get in there right at the\n> beginning, you know?\n>\n> Brian Eno, 2013\n\nWhile the second track on \"Music for Airports\" is possibly the simplest of\nEno's systems music releases, it was not the first. That honor is bestowed on\nDiscreet Music, recorded several years earlier, on one spring day in 1975.\n\nIt wasn't called \"ambient\" at the time (one contemporary review just called it\n\"a haunting minimalist experiment\"), but it's still a landmark piece in the\ndevelopment of ambient music, just as much as Music for Airports is.\n\n\"Discreet Music\" is very explicitly a piece of systems music. In fact, Eno\nactually described the system that produced the music in the liner notes of\nthe album release, and also included a diagram of how the whole thing was put\ntogether. This shows how central the \"systems\" idea was in his thinking at the\ntime.\n\nBrian Eno included a systems diagram in the liner notes of the Discreet Music\nrecord.\n\nAs we see here, and as we also hear from the results, this system is much more\nelaborate than the simple tape loops we've been studying so far. There are\nfamiliar things such as tapes and loops, but there are also new things such as\nsynthesized sound. It'll take a bit more effort for us to reconstruct, but\nwe'll get there!\n\n### Setting up discreetmusic.js\n\nThe source code for this project is available on GitHub.\n\nLet's set up a new project for our rendition of Discreet Music. Its structure\nis the same as in our other projects. Create a new directory and add two files\nto it:\n\n  * discreetmusic/\n\n    * index.html\n    * discreetmusic.js\n\nThen set the contents of index.html as follows:\n\n    \n    \n    <!DOCTYPE html> <html> <body> <script src=\"https://cdn.rawgit.com/mohayonao/web-audio-api-shim/master/build/web-audio-api-shim.min.js\"></script> <script src=\"https://cdnjs.cloudflare.com/ajax/libs/fetch/1.0.0/fetch.min.js\"></script> <script src=\"https://tonejs.github.io/CDN/r7/Tone.js\"></script> <script src=\"discreetmusic.js\"></script> </body> </html>\n\nThe contents of this file are the same as before, except that this time we're\nincluding the Tone.js library onto the page. This is a fantastic open source\nJavaScript library that provides all kinds of useful abstractions on top of\nthe standard Web Audio API. We'll use it in several places as we construct\nthis project.\n\nWe can now launch our dev server from this directory and start playing with\nit:\n\n    \n    \n    lite-server\n\n### Synthesizing the Sound Inputs\n\nIn the pieces we've looked at previously, all sounds have been sampled human\nvoices or musical instruments. Discreet Music is very different in the sense\nthat all sounds are electronically produced with a synthesizer \u2013 something\nthat was still pretty exotic in the mid-70s. Eno synthesized the music with\nhis EMS Synthi AKS modular analog synthesizer.\n\nThe EMS Synthi AKS modular - the model Eno used to make Discreet Music and\nmany other things. Photo: Andr\u00e9s Galeotti\n\nThe Synthi was Eno's workhorse for much of the 1970s, and it can be heard not\nonly on Eno's own albums but also on many collaborations. It is all over David\nBowie's Berlin trilogy, for example. The device had a convenient portable form\nfactor: It was built into a briefcase that you could take with you, kind of\nlike a laptop.\n\nThere's a neat JavaScript emulator of the Synthi too, though it doesn't have\nthe keyboard or the sequencer.\n\nNow, the thing about old analog synthesizers is that they all have distinct\nsounds. In these machines, sound signals were not treated in the exact digital\nform as they mostly are today, but were passed along in analog form through\nall kinds of electronic circuitry. This meant that the signals were subject to\nvarious subtle imperfections and disturbances of the physical world, causing\neach model to have its unique brand of wobble and distortion.\n\nThe EMS Synthi is no exception to this. We're not going to be able to emulate\nexactly what it produces, so we're not going to even try. Instead, we'll\nsettle for something that sounds close enough, for some definition of \"close\".\n\nThe Web Audio API includes all the requisite parts for doing modular\nsynthesis. It supports oscillators for producing sound waves, filters for\nfiltering them, and all kinds of other processing nodes for further modifying\nthe sounds, some of which we've already used in this guide.\n\nBut the thing is, these APIs are pretty low level. Setting up a synthesizer\nthat approaches the capabilities of a real-world one would require quite a bit\nof code if we were to do it all directly with Web Audio nodes. This is where\nTone.js comes in. Among the things it includes are abstractions over most of\nthe low-level details of modular synths, allowing us to build different kinds\neasily.\n\nThe following steps demonstrate the construction of a particular synth sound\nthat I came up with for this project. Obviously, there's a huge amount of\npossible variation here and you'll be able to come up with all kinds of\ndifferent sounds by tweaking the parameters and trying different kinds of\nsynths. This is especially true if you know your way around sound synthesis.\n\n#### Setting up a Monophonic Synth with a Sawtooth Wave\n\nThe first thing we'll do is set up a simple Tone.js MonoSynth object. This is\na monophonic synthesizer, just like the EMS Synthi, which means it's able to\nproduce one sound at a time.\n\nWe'll then connect the synth to the Tone.js Master output and play a middle C\nnote with a duration of one second, so that we can hear what it sounds like.\n\ndiscreetmusic.js\n\n    \n    \n    let synth = new Tone.MonoSynth(); synth.toMaster(); synth.triggerAttackRelease('C4', 1);\n\nAlthough we can't really see it, what we've constructed here is a Web Audio\nnode graph. It lives inside an AudioContext that Tone.js has constructed for\nus, and outputs to its destination node.\n\nThe sound here is based on a square wave (), which is the default for\nMonoSynth. Let's change it to a sawtooth wave (). This will result in a very\ndifferent kind of sound:\n\ndiscreetmusic.js\n\n    \n    \n    let synth = new Tone.MonoSynth({ oscillator: {type: 'sawtooth'} }); synth.toMaster(); synth.triggerAttackRelease('C4', 1);\n\n#### Filtering the Wave\n\nThough the sawtooth wave we have right now is a good start, its sharpness\nbares no resemblance to the warm synth washes of Discreet Music. We can get\nmuch closer to that by filtering the wave with a low-pass filter. This will\ntake out some of the higher frequencies of the wave, leaving us with a very\ndifferent, softer sound ().\n\nThe MonoSynth comes with a built-in filtering capability. All we need to do is\ntweak its FrequencyEnvelope. If we allow the filter to pass just two octaves\nworth of frequencies above a base frequency of 200Hz, we get a \"muffled\" sound\nthat's much closer to what we want:\n\ndiscreetmusic.js\n\n    \n    \n    let synth = new Tone.MonoSynth({ oscillator: {type: 'sawtooth'}, filterEnvelope: { baseFrequency: 200, octaves: 2 } }); synth.toMaster(); synth.triggerAttackRelease('C4', 1);\n\nAs you listen to this sound, you may notice that it changes during its\nlifetime. This is because the filter is actually taking on different values\nover time, based on the default values of its Attack-Decay-Sustain-Release\n(\"ADSR\") envelope. In particular, the filter takes a while to kick in and then\na bit more time to stabilize because of its attack and decay settings. We\ndon't want that for what we're doing, so let's set those both to zero, so that\nthe filter reaches its sustain level immediately:\n\ndiscreetmusic.js\n\n    \n    \n    let synth = new Tone.MonoSynth({ oscillator: {type: 'sawtooth'}, filterEnvelope: { baseFrequency: 200, octaves: 2, attack: 0, decay: 0 } }); synth.toMaster(); synth.triggerAttackRelease('C4', 1);\n\n#### Tweaking the Amplitude Envelope\n\nOur synth currently starts playing at pretty much full volume immediately, and\nalso ends very quickly once it's released after one second. In Discreet Music\nboth of these sound softer: Synths don't come in quite so abruptly and also\nlinger for a while before they go. Some of this is due to the echo effect\nwe'll add later, but some of it we'll build right into the synth, by changing\nits amplitude ADSR envelope.\n\nFirstly, if we set the attack to 0.1, the sound will \"ramp up\" for a tenth of\na second as it begins:\n\ndiscreetmusic.js\n\n    \n    \n    let synth = new Tone.MonoSynth({ oscillator: {type: 'sawtooth'}, envelope: { attack: 0.1 }, filterEnvelope: { baseFrequency: 200, octaves: 2, attack: 0, decay: 0 } }); synth.toMaster(); synth.triggerAttackRelease('C4', 1);\n\nSecondly, if we set the release of the synth to happen over a linear curve of\nfour seconds, we get a very slow fading out of the sound after it's released.\n\ndiscreetmusic.js\n\n    \n    \n    let synth = new Tone.MonoSynth({ oscillator: {type: 'sawtooth'}, envelope: { attack: 0.1, release: 4, releaseCurve: 'linear' }, filterEnvelope: { baseFrequency: 200, octaves: 2, attack: 0, decay: 0 } }); synth.toMaster(); synth.triggerAttackRelease('C4', 1);\n\nThis doesn't sound quite right though - the sound is muting out almost\nimmediately instead of going out evenly over four seconds. This is because of\nour filter: It also has a release setting, which causes the filter frequency\nto drop to the base value of 200Hz. We don't want that, so let's go ahead and\neliminate the filter's release by setting it to a very high number, meaning\nthat it'll happen over such a long period of time that we'll never hear it.\n\ndiscreetmusic.js\n\n    \n    \n    let synth = new Tone.MonoSynth({ oscillator: {type: 'sawtooth'}, envelope: { attack: 0.1, release: 4, releaseCurve: 'linear' }, filterEnvelope: { baseFrequency: 200, octaves: 2, attack: 0, decay: 0, release: 1000 } }); synth.toMaster(); synth.triggerAttackRelease('C4', 1);\n\nThat's more like it!\n\n#### Bringing in a Second Oscillator\n\nThe current sound we have is based on a single filtered sawtooth oscillator.\nWe can gain a fuller sound if we combine it with a second oscillator producing\na different kind of wave, such as a sine wave ().\n\nWe could set up another MonoSynth for this purpose and play both synths\nsimultaneously. But we won't have to, since Tone.js comes with a suitable\nabstraction for this: DuoSynth packages two MonoSynths behind a convenient\ninterface for controlling both at the same time.\n\nHere's a DuoSynth, where the first voice is the MonoSynth we constructed\nbefore, and the second voice produces a sine wave. Both voices use the same\namplitude and filter envelopes.\n\ndiscreetmusic.js\n\n    \n    \n    let synth = new Tone.DuoSynth({ voice0: { oscillator: {type: 'sawtooth'}, envelope: { attack: 0.1, release: 4, releaseCurve: 'linear' }, filterEnvelope: { baseFrequency: 200, octaves: 2, attack: 0, decay: 0, release: 1000 } }, voice1: { oscillator: {type: 'sine'}, envelope: { attack: 0.1, release: 4, releaseCurve: 'linear' }, filterEnvelope: { baseFrequency: 200, octaves: 2, attack: 0, decay: 0, release: 1000 } } }); synth.toMaster(); synth.triggerAttackRelease('C4', 1);\n\nSince both synths use the same envelopes, we can pull them out into variables\nso we don't have to repeat all their values twice:\n\ndiscreetmusic.js\n\n    \n    \n    let envelope = { attack: 0.1, release: 4, releaseCurve: 'linear' }; let filterEnvelope = { baseFrequency: 200, octaves: 2, attack: 0, decay: 0, release: 1000 }; let synth = new Tone.DuoSynth({ voice0: { oscillator: {type: 'sawtooth'}, envelope, filterEnvelope }, voice1: { oscillator: {type: 'sine'}, envelope, filterEnvelope } }); synth.toMaster(); synth.triggerAttackRelease('C4', 1);\n\nWe can hear the two synths now but the sound is not quite right. There's a\nkind of eerie quality to it, which is a combination of two things: Vibrato\n(which we'll look at next), and the fact that the two oscillators are not\nvibrating at the same frequency.\n\nThe DuoSynth has a harmonicity attribute that controls the difference between\nthe frequencies of the two oscillators. By default it's set to 2, causing the\nsecond voice to vibrate twice as frequently as (or \"an octave above\") the\nfirst one. While this is nice for many situations, we don't want that right\nnow. We can disable it by setting harmonicity to 1 so that both oscillators\nhave the same frequency and are playing exactly the same note.\n\ndiscreetmusic.js\n\n    \n    \n    let synth = new Tone.DuoSynth({ harmonicity: 1, voice0: { oscillator: {type: 'sawtooth'}, envelope, filterEnvelope }, voice1: { oscillator: {type: 'sine'}, envelope, filterEnvelope } });\n\n#### Emulating Tape Wow with Vibrato\n\nThe other extra quality in our sound that was introduced by the DuoSynth is a\nvibrato effect, making the sound's pitch oscillate slightly around the base\ntone.\n\nWhile we don't exactly want vibrato in our synth, we can exploit the same\nfeature to emulate something else: Eno's equipment caused some tape wow in the\noriginal piece, adding continuous slowly changing pitch shifts into the\nrecording. What we can do is add a very slow and very slight vibrato to our\nsynth to emulate a similar effect. It's a pale imitation of the warm analog\nflutter of the original, but I find it a nice touch anyway.\n\nI found a frequency of 0.5Hz and an depth of 0.1 suitable for this. What we\nwant is an almost imperceptible effect \u2013 enough that you hear a difference but\nnot so much that it draws attention to itself.\n\ndiscreetmusic.js\n\n    \n    \n    let synth = new Tone.DuoSynth({ harmonicity: 1, voice0: { oscillator: {type: 'sawtooth'}, envelope, filterEnvelope }, voice1: { oscillator: {type: 'sine'}, envelope, filterEnvelope }, vibratoRate: 0.5, vibratoAmount: 0.1 });\n\nThis synth is now something we can work with.\n\n### Understanding Timing in Tone.js\n\nBefore we start building up the melodies that we're going to play with our\nsynth, let's talk a little bit about how exactly we're going to control the\ntiming of those melodies.\n\nIn the \"Music for Airports\" system we simply used JavaScript's built-in\nsetInterval() function to make things repeat at (almost) regular intervals.\nWith this system we have more things to control than just a few virtual tape\nloops. We need a better API than setInterval() for timing them.\n\nThis is another place where Tone.js can help, and it can do so in a couple of\nways.\n\n#### Transport Time\n\nTone.js has its own timeline, along which things can be scheduled to occur\nover time. For example, we could construct a Loop to achieve something very\nsimilar as setInterval(): To execute a callback function every 2 seconds.\n\n    \n    \n    new Tone.Loop(() => { synth.triggerAttackRelease('C4', 1); }, 2).start(); // Tone's Transport needs to be started for any loops to become active Tone.Transport.start();\n\nBut there's a lot more to the Loop API than this. For example, you can set a\nloop to only repeat a specific number of times, or add some randomness by only\nhaving the loop trigger at a given probability. Most importantly though, the\nloop API lets us do precise timing.\n\nBecause of the single-threaded nature of JavaScript, there is no way to\nguarantee that a given callback function is invoked at an exact moment in\ntime. There is a clever way for us to get around this with Tone.js:\n\n  1. Tone.js always executes Loop callbacks slightly before their scheduled triggering time.\n  2. It passes each callback one argument: The exact time it was supposed to be executed at. Because of (1) this will always be a little bit in the future.\n  3. We use that time argument when we play our synth. For example, we can give triggerAttackRelease a third argument to control when it should trigger.\n\nTone.js invokes a loop callback slightly before its scheduled time and gives\nit the scheduled time as an argument.\n\nThis lets us play a sound at an exact interval:\n\n    \n    \n    new Tone.Loop(time => { synth.triggerAttackRelease('C4', 1, time); }, 2).start(); Tone.Transport.start();\n\n#### Musical Timing\n\nScheduling things to happen over seconds and milliseconds is one thing, but\nit's not how most music in the world is measured. Instead, we measure things\nusing time signatures: Beats and measures.\n\nIn Tone.js we can use these musical time signatures instead of absolute times\nto schedule things. For example, we can schedule a loop to execute once per\nevery measure (or \"bar\") with the 1m syntax where m is short for \"measure\":\n\n    \n    \n    new Tone.Loop(time => { synth.triggerAttackRelease('C4', 1, time); }, '1m').start(); Tone.Transport.start();\n\nIn Tone's default 4/4 time signature (which is what we're going to use), this\nwill happen every four beats (ONE-two-three-four-ONE-two-three-four).\n\nWe can also divide the measure into half notes (2n), quarter notes (4n),\neighth notes (8n), and sixteenth notes (16n). We can play a note, say, on the\nsecond beat of every measure by delaying the trigger by one quarter note (one-\nTWO-three-four-one-TWO-three-four):\n\n    \n    \n    new Tone.Loop(time => { // Trigger one quarter note from now, and hold for one eighth note synth.triggerAttackRelease('C4', '8n', '+4n'); }, '1m').start(); Tone.Transport.start();\n\nWhen we want to combine different measures, we can use simple arithmetic\nexpressions ('1m + 16n'), or use a special shorthand\n\"measures:quarters:sixteenths\" syntax:\n\n  * 1:1:1 = 1m + 4n + 16n\n  * 0:0:2 = 2 * 16n = 8n\n  * 0:1:2 = 4n + 2 * 16n = 4n + 8n\n  * 1:2:3 = 1m + 2 * 4n + 3 * 16n = 1m + 2n + 8n + 16n\n\nPerhaps the most useful part about using musical timing from our point of view\nis that using this scheme, the absolute timing of events can be changed by\nsetting the BPM (beats-per-minute) value of the transport. For example, we can\nmake everything in our system play in double speed by bumping the BPM from the\ndefault of 120 to 240;\n\n    \n    \n    Tone.Transport.bpm.value = 240;\n\nThis, in fact, is a trick Brian Eno commonly uses: He often works on his\nambient pieces at double speed and then slows them down before release. This\nwas also the case with Discreet Music. It's not simply about working at a\nfaster pace, but about affecting how the results turn out:\n\n> I have a theory that, as a maker you tend to put in twice as much as you\n> need as a listener. You tend to plug every hole. You're always looking for\n> that charge, so you put more and more in to get it. But as a listener you're\n> much less demanding. You can take things that are much simpler, much more\n> open, and much slower.\n>\n> It's often happened that I've made a piece and ended up slowing it down by\n> as much as half. Discreet Music is an example: that's half the speed at\n> which it was recorded.\n>\n> Brian Eno, 1979\n\nAs you work on this piece, you may want to double the Transport BPM to get a\nfeel for the speed at which Eno actually worked on it.\n\n### Sequencing the Synth Loops\n\nAt the core of Discreet Music is an assortment of little melodic phrases,\nplayed with a synth similar to the one we've just constructed. If you listen\nto the original, you'll hear some of them playing somewhat to the left of\ncenter in the stereo field and some to the right. These seven phrases form all\nthe sonic inputs to the system:\n\nLeft| Right  \n---|---  \n  \nThese phrases are arranged onto two repeating sequences that have slightly\ndifferent durations: One for the left channel, and one for the right. Both\nsequences are looped. The result of this is yet another instance of a phasing\nloop system, the kind of which we've seen already. As the two loops repeat\nthey form different combinations over time.\n\nSince the two sequences use different stereo panning, and may also play at the\nsame time because of the phasing that's going on, we can't really use the same\nMonoSynth for both. We need two synths, and we need to pan them separately.\nLet's extract our synth initialization code into a function that we can then\ncall twice to make two identical synths.\n\ndiscreetmusic.js\n\n    \n    \n    function makeSynth() { let envelope = { attack: 0.1, release: 4, releaseCurve: 'linear' }; let filterEnvelope = { baseFrequency: 200, octaves: 2, attack: 0, decay: 0, release: 1000 }; return new Tone.DuoSynth({ harmonicity: 1, voice0: { oscillator: {type: 'sawtooth'}, envelope, filterEnvelope }, voice1: { oscillator: {type: 'sine'}, envelope, filterEnvelope }, vibratoRate: 0.5, vibratoAmount: 0.1 }); } let leftSynth = makeSynth(); let rightSynth = makeSynth();\n\nWe'll then set up two Tone.js Panners (which are pretty much just wrappers for\nthe Web Audio StereoPannerNode that we've already seen). We connect each synth\nto a panner and set the panners to opposite locations of the stereo field:\n\ndiscreetmusic.js\n\n    \n    \n    let leftSynth = makeSynth(); let rightSynth = makeSynth(); let leftPanner = new Tone.Panner(-0.5).toMaster(); let rightPanner = new Tone.Panner(0.5).toMaster(); leftSynth.connect(leftPanner); rightSynth.connect(rightPanner);\n\nWe're setting the pannings to -0.5 and 0.5, putting them to the left and right\nof center, but not fully to the left and right so that some mixing of channels\nwill occur.\n\nEno's Synthi AKS had a primitive sequencer (which he called a \"digital recall\nsystem\" at the time) onto which the sequences were programmed. What we can do\nto substitute for this is to model each sequence as a Tone.js Loop. We have\none Loop for the left synth, and one for the right:\n\ndiscreetmusic.js\n\n    \n    \n    let leftSynth = makeSynth(); let rightSynth = makeSynth(); let leftPanner = new Tone.Panner(-0.5).toMaster(); let rightPanner = new Tone.Panner(0.5).toMaster(); leftSynth.connect(leftPanner); rightSynth.connect(rightPanner); new Tone.Loop(time => { // We'll play leftSynth here }, '34m').start(); new Tone.Loop(time => { // We'll play rightSynth here }, '37m').start(); Tone.Transport.start();\n\nWith the default BPM of 120 and time signature of 4/4, the left loop's wall\nclock duration will be about 68 seconds, and the right loop's 74 seconds. This\nis roughly the same as in the original Discreet Music. Since the loops have\ndifferent durations, the resulting phasing effect causes the sounds to shift\nrelative to each other over time, just like in \"It's Gonna Rain\" and\n\"Airports\". In this case there'll be about 41 minutes of unique music before\nthe loops start at the same time again.\n\nHere's what the audio graph of this system looks like:\n\nThe Web Audio graph of two panned synths. The Tone DuoSynths and Panners are\nabstractions. Under the hood, each DuoSynth contains several oscillator, gain,\nand filter nodes.\n\nLet's put the first melodic phrase of the left loop in. It triggers the C5\nnote, and then moves to the D5 note, and starts right at the beginning of the\nloop:\n\ndiscreetmusic.js\n\n    \n    \n    new Tone.Loop(time => { // Trigger C5, and hold for a full note (measure) + two 1/4 notes leftSynth.triggerAttackRelease('C5', '1:2', time); // Switch note to D5 after two 1/4 notes without retriggering leftSynth.setNote('D5', '+0:2'); }, '34m').start(); new Tone.Loop(time => { // We'll play rightSynth here }, '37m').start();\n\nThis phrase will play right when the page loads. It then repeats after just\nover a minute when the loop turns.\n\nWe can then fill in the rest of the phrases of the left sequence:\n\ndiscreetmusic.js\n\n    \n    \n    new Tone.Loop(time => { leftSynth.triggerAttackRelease('C5', '1n + 2n', time); leftSynth.setNote('D5', '+2n'); // Trigger E4 after 6 measures and hold for two 1/4 notes. leftSynth.triggerAttackRelease('E4', '0:2', '+6:0'); // Trigger G4 after 11 measures + a two 1/4 notes, and hold for two 1/4 notes. leftSynth.triggerAttackRelease('G4', '0:2', '+11:2'); // Trigger E5 after 19 measures and hold for 2 measures. // Switch to G5, A5, G5 after delay of a 1/4 note + two 1/16 notes each. leftSynth.triggerAttackRelease('E5', '2:0', '+19:0'); leftSynth.setNote('G5', '+19:1:2'); leftSynth.setNote('A5', '+19:3:0'); leftSynth.setNote('G5', '+19:4:2'); }, '34m').start();\n\nThe math here can be a bit tricky to follow because we're talking in musical\nmeasures and not absolute times. But if you play with the numbers and listen\nto the results you can get the hang of it pretty quickly.\n\nAnd then we'll do the same for the right sequence, which will complete all the\nsynth inputs to the system. Here's the full source code so far.\n\ndiscreetmusic.js\n\n    \n    \n    function makeSynth() { let envelope = { attack: 0.1, release: 4, releaseCurve: 'linear' }; let filterEnvelope = { baseFrequency: 200, octaves: 2, attack: 0, decay: 0, release: 1000 }; return new Tone.DuoSynth({ harmonicity: 1, voice0: { oscillator: {type: 'sawtooth'}, envelope, filterEnvelope }, voice1: { oscillator: {type: 'sine'}, envelope, filterEnvelope }, vibratoRate: 0.5, vibratoAmount: 0.1 }); } let leftSynth = makeSynth(); let rightSynth = makeSynth(); let leftPanner = new Tone.Panner(-0.5).toMaster(); let rightPanner = new Tone.Panner(0.5).toMaster(); leftSynth.connect(leftPanner); rightSynth.connect(rightPanner); new Tone.Loop(time => { leftSynth.triggerAttackRelease('C5', '1:2', time); leftSynth.setNote('D5', '+0:2'); leftSynth.triggerAttackRelease('E4', '0:2', '+6:0'); leftSynth.triggerAttackRelease('G4', '0:2', '+11:2'); leftSynth.triggerAttackRelease('E5', '2:0', '+19:0'); leftSynth.setNote('G5', '+19:1:2'); leftSynth.setNote('A5', '+19:3:0'); leftSynth.setNote('G5', '+19:4:2'); }, '34m').start(); new Tone.Loop(time => { // Trigger D4 after 5 measures and hold for 1 full measure + two 1/4 notes rightSynth.triggerAttackRelease('D4', '1:2', '+5:0'); // Switch to E4 after one more measure rightSynth.setNote('E4', '+6:0'); // Trigger B3 after 11 measures + two 1/4 notes + two 1/16 notes. Hold for one measure rightSynth.triggerAttackRelease('B3', '1m', '+11:2:2'); // Switch to G3 after a 1/2 note more rightSynth.setNote('G3', '+12:0:2'); // Trigger G4 after 23 measures + two 1/4 notes. Hold for a half note. rightSynth.triggerAttackRelease('G4', '0:2', '+23:2'); }, '37m').start(); Tone.Transport.start();\n\nWhat we have now are two looping sequences, going about their way on their own\ntimings and forming different combinations against each other:\n\nThis is starting to resemble Discreet Music, though it does still sound very\nsparse and not all that interesting. That will change soon.\n\nI don't know exactly how Eno achieved what we've done here. As far as I can\ntell, the Synthi AKS could hold just one sequence at a time. Furthermore, it\nwas monophonic so it couldn't play more than one note at a time. It wouldn't\nseem to be possible to run two sequences of different durations\nsimultaneously.\n\nThe best theory I can come up with is that Eno actually ran the system twice \u2013\nonce per each sequence \u2013 and simply overdubbed each run to the same master\ntape.\n\n### Adding Echo\n\nThe Discreet Music system also contained an echo unit, through which all\nsounds coming from the synthesizer were routed before they reached the tape.\nWe can simulate this by using a Tone.js FeedbackDelay node. It takes the\nincoming signal, holds on to it for a given delay, and then feeds part of it\nback to the signal again, causing each sound to repeat (\"echo\") a number of\ntimes.\n\nA FeedbackDelay node partially repeats the incoming signal a number of times.\nIt can be used to construct an echo effect.\n\nThe node is parameterized with two numbers:\n\n  * delayTime - how long the delay should be, or \"how long does it take before you hear the echo?\"\n  * feedback - how much of the input to feed back in, or \"how loud is the echo?\". This also indirectly controls how many times the signal will be repeated \u2013 until it reaches zero.\n\nLet's put in a delay time of a sixteenth note and a feedback of 0.2. The delay\nis connected to the master output, and the panners are now connected to the\ndelay:\n\ndiscreetmusic.js\n\n    \n    \n    let leftPanner = new Tone.Panner(-0.5); // No longer connected to master! let rightPanner = new Tone.Panner(0.5); // No longer connected to master! let echo = new Tone.FeedbackDelay('16n', 0.2); leftSynth.connect(leftPanner); rightSynth.connect(rightPanner); leftPanner.connect(echo); rightPanner.connect(echo); echo.toMaster();\n\nSince we are now layering our synth sound on top of itself, it becomes a bit\ntoo loud and you may hear some distortion because of this. We can make it more\nmanageable by adjusting the volume of our synths, by something like -20dB:\n\ndiscreetmusic.js\n\n    \n    \n    return new Tone.DuoSynth({ harmonicity: 1, volume: -20, voice0: { oscillator: {type: 'sawtooth'}, envelope, filterEnvelope }, voice1: { oscillator: {type: 'sine'}, envelope, filterEnvelope }, vibratoRate: 0.5, vibratoAmount: 0.1 });\n\nThis is what our system audio graph looks like now:\n\nThe Web Audio graph of two panned, echoed synths.\n\nAnd here's how it sounds:\n\nIf you listen to the very last minutes of Discreet Music, you'll hear an echo\nthat's much longer and more distinct than before.\n\nIt sounds like Eno changed the echo delay during recording, which is easily\ndone with the controls on the Echoplex unit he was using. Should you want to\ndo the same, you can simply adjust the delayTime and feedback attributes our\nfeedback delay (try something like 2n and 0.4).\n\n### Adding Tape Delay with Frippertronics\n\nAnd now we arrive at the defining feature of the Discreet Music system: A tape\ndelay contortion commonly called Frippertronics.\n\nThe basic idea of this system is that whatever sound goes into it is repeated\nagain after a few seconds, and then again after a few seconds more, and again,\nuntil it gradually fades out.\n\nNow, this may sound pretty similar to the \"echo\" feedback delay we just added\nin the previous section. And that's because that's precisely what it does. It\nfeeds the audio signal back to the system with a delay. The difference is that\nin this case the delay is very long \u2013 several seconds long, in fact. It\ndoesn't sound like an echo. It sounds like a completely new sound that just\nhappens to be a repetition of a sound that occured a few seconds ago.\n\nThese days such effects can easily be achieved with digital delays, but in the\nearly 1970s this was not the case. Delay units like the Echoplex could not\nprovide anything close to the delay times we need here. Frippertronics is a\nvery clever hack that gets around this. It involves the same kind of tape\nrecorders we've been discussing with relation to tape loops, but configured a\nbit differently.\n\nHow this works is that you have two tape recorders. You plug your input (the\nsynth, in our case) to one tape recorder, which then records that input onto a\ntape. But from this tape recorder you then feed the tape directly to the other\ntape recorder, where it is played back. The output signal of the second tape\nrecorder is fed back into the first as another input, so that it's recorded to\nthe tape again, and this is how it keeps repeating every few seconds.\n\nThe Frippertronics tape delay system, as illustrated on the Discreet Music\nliner notes.\n\nSo, after a sound is recorded to a tape on the first recorder, the tape\ntravels to the second recorder, and then it is played back there. The exact\ndelay is controlled by the physical distance between the two tape machines,\nbecause that directly affects how long the tape takes to travel between them.\nIn the case of Discreet Music, the delay is about six seconds.\n\nEno and frequent collaborator, Robert Fripp of King Crimson, developed this\nsystem for their joint releases, such as (No Pussyfooting). The idea was that\nFripp would play things on his guitar and Eno would use the delay system to\nlayer the guitar sound on top of itself.\n\nHere's a nice video introduction to how a Frippertronics system can be set up\nand used with an electric guitar.\n\nThe system wasn't originally Fripp and Eno's invention though: Terry Riley and\nPauline Oliveros had experimented with such systems as much as a decade\nearlier. Riley called it the Time Lag Accumulator and recorded pieces like\nPoppy Nogood and the Phantom Band with it. Oliveros used similar systems to\nrecord some fantastically eerie tape pieces like Bye Bye Butterfly.\n\nBut how should we approach emulating this kind of tape delay system? We could\nuse a Tone.js FeedbackDelay again, but I think in this case it's more\nappropriate to drop to the lower level of the native Web Audio API, both\nbecause laying bare the internal structure of the system will help understand\nhow it works, and because I feel it's closer to the DIY ethos of the original\nFrippertronics. As we do this, we'll see that Tone.js integrates well with the\nlower level Web Audio API nodes, and we can mix and match Tone nodes and raw\nnodes in the same graph.\n\nThe first thing we'll do is create a Web Audio DelayNode, and set its delay to\nsix seconds, which is about what it is in Discreet Music. We then connect our\necho delay to this new delay node. Both the echo delay and the new delay are\nconnected to the audio context destination. Each sound is played right when it\ncomes out of the synth and then repeated again six seconds later.\n\ndiscreetmusic.js\n\n    \n    \n    let leftSynth = makeSynth(); let rightSynth = makeSynth(); let leftPanner = new Tone.Panner(-0.5); let rightPanner = new Tone.Panner(0.5); let echo = new Tone.FeedbackDelay('16n', 0.2); let delay = Tone.context.createDelay(6.0); // Borrow the AudioContext from Tone.js delay.delayTime.value = 6.0; leftSynth.connect(leftPanner); rightSynth.connect(rightPanner); leftPanner.connect(echo); rightPanner.connect(echo); echo.toMaster(); echo.connect(delay); delay.connect(Tone.context.destination);\n\nThe Web Audio graph of two panned, echoed synths played twice: Once\nimmediately and once after a delay.\n\nWith this, the music gets off to a good start, but it's not quite right.\nEverything repeats twice and then goes away. We want things to stick around\nlonger than that.\n\nThe trick is to connect the delay back to itself. That means the sounds will\njust keep coming back after every 6 seconds.\n\ndiscreetmusic.js\n\n    \n    \n    let leftSynth = makeSynth(); let rightSynth = makeSynth(); let leftPanner = new Tone.Panner(-0.5); let rightPanner = new Tone.Panner(0.5); let echo = new Tone.FeedbackDelay('16n', 0.2); let delay = Tone.context.createDelay(6.0); delay.delayTime.value = 6.0; leftSynth.connect(leftPanner); rightSynth.connect(rightPanner); leftPanner.connect(echo); rightPanner.connect(echo); echo.toMaster(); echo.connect(delay); delay.connect(Tone.context.destination); delay.connect(delay);\n\nWhat we've done here is formed a loop within the audio node graph.\n\nThe Web Audio graph of two panned, echoed synths played continuously through a\nlooped delay.\n\nThis is much better, but now we have another problem. As we listen to the\nsystem for a while, it grows into an ear-shattering cacophony of melodies. It\ndoes sound interesting but it's not quite what we're after. That's because\nevery sound coming in from the synth never goes away! It keeps looping\nforever.\n\nWhat we need is a way to fade the sound out a bit each time it passes through\nthe loop we've made. After each delay it should be a bit quieter than before.\nWe can do this by introducing a GainNode to the loop. It is used to control\nvolume, and what we'll do with it is lower the volume of the signal by a\nquarter each time it passes through.\n\ndiscreetmusic.js\n\n    \n    \n    let leftSynth = makeSynth(); let rightSynth = makeSynth(); let leftPanner = new Tone.Panner(-0.5); let rightPanner = new Tone.Panner(0.5); let echo = new Tone.FeedbackDelay('16n', 0.2); let delay = Tone.context.createDelay(6.0); let delayFade = Tone.context.createGain(); delay.delayTime.value = 6.0; delayFade.gain.value = 0.75; leftSynth.connect(leftPanner); rightSynth.connect(rightPanner); leftPanner.connect(echo); rightPanner.connect(echo); echo.toMaster(); echo.connect(delay); delay.connect(Tone.context.destination); delay.connect(delayFade); delayFade.connect(delay);\n\nThe Web Audio graph of a complete Frippertronics system: Two panned, echoed\nsynths played continuously through a looped delay. The delay loop goes through\na gain that gradually fades out the repeated signal.\n\nThat's more like it! Here's what the system we have now sounds like:\n\n### Controlling Timbre with a Graphic Equalizer\n\nAt this point there's just one more thing for us to do, and it involves the\nfinal piece in the Discreet Music diagram that we haven't addressed yet: The\ngraphic equalizer.\n\nWhat's meant by this is one of these things \u2013 an interface for controlling the\nvolumes of different frequency bands in the audio signal:\n\nPhoto: Rik Ruff\n\nThe term graphic equalizer comes from the neat UI design of this interface:\nThe knobs on each slider not only provide the means to control the volume of a\ncertain frequency band, but also visualize the resulting frequency graph. If\nyou plot a mental curve through the center of each slider knob, you get an\nidea of the frequency response of the equalizer. No separate graph display is\nrequired for that.\n\nThere are many reasons one may want to use a graphic equalizer when recording\nor playing back sound. For most of my lifetime I've used them to just make\nmusic sound as loud as possible while listening. But in this case we have a\ndifferent, very specific purpose: To control the harmonic spectrum of our\nsynthesizers while they're playing, resulting in different timbres.\n\nThe oscillators and filters we've set up for our synth are generating sounds\nat certain frequencies. Among them are the fundamental frequencies of each\nnote. For example, when we play a C5 pitch, it plays a sound at the\nfundamental frequency of that pitch, 523.25Hz. But this is not the only\nfrequency played. There are sounds played at a number of frequencies,\nparticularly at certain integer multiples of the fundamental frequency, called\nharmonics.\n\nOur synth driven through Chris Wilson's Web Audio Oscilloscope. The lower half\nof the image shows a frequency breakdown of the sound at a point in time. The\nleftmost bars display the fundamental frequency, and to the right of them\nthere are several peaks for the various harmonics of the synth. With an\nequaliser we can manipulate the sound by eliminating or boosting some of those\nharmonics.\n\nThe relationships between the fundamental frequency and the harmonics result\nin the distinct sound of our synth (as they do for any musical instrument).\n\nA graphic equalizer allows us to change these things at runtime. We can, for\nexample, emphasize some harmonics and eliminate others. Or we may bring down\nthe fundamental frequency itself, after which we may start hearing a\ncompletely different note!\n\nWhat's notable about this from the perspective of our discussion is that here\nEno introduced some manual inputs to his system, which was otherwise running\nfully automatically. When you listen to Discreet Music, you'll hear that the\nsound seems to become distinctly different in certain points. Sometimes its\nlouder, sometimes quieter. Sometimes its thinner, sometimes fuller. What\nyou're hearing is Eno adjusting the controls on his graphic equalizer to\ndifferent settings during the recording. But that's all he's doing, the system\nitself is playing automatically.\n\nSo, what we're going to do is build a little graphic equalizer UI of our own,\nwhich we can then use to control our system as it is playing. We can do so\nwith a bunch of HTML controls which we connect to Web Audio nodes.\n\n#### Setting up the Equalizer Filters\n\nWe're going to control the gain of the sound signal separately on a number of\nfrequencies. Before we do anything else, let's specify these frequencies in a\nconstant:\n\ndiscreetmusic.js\n\n    \n    \n    const EQUALIZER_CENTER_FREQUENCIES = [ 100, 125, 160, 200, 250, 315, 400, 500, 630, 800, 1000, 1250, 1600, 2000, 2500, 3150, 4000, 5000, 6300, 8000, 10000 ];\n\nThese are all hertz values, about 1/3 of an octave apart from each other\n(remember the exponential relationship between notes and frequencies?).\nThey're called ISO preferred frequencies and you'll find them on most so-\ncalled \"1/3 octave\" equalizers, which are the ones that have a big row of\n30-31 knobs on them.\n\nWe are not going to use 30 bands in our equalizer though. We only have 21.\nThat's because I've left out some of the lowest and highest frequencies\ncompletely. A general purpose graphic equalizer has knobs for the whole\naudible spectrum of about 20Hz to 20kHz, but our domain specific equalizer\nonly needs to deal with the middle frequencies that are present in Discreet\nMusic. In the interest of user experience we won't add unnecessary controls\nfor the very low and high ends.\n\nWhat we'll do next is add a filter node for each of these frequencies. Each\nfilter can either boost or hinder that particular frequency.\n\nThe Web Audio API comes with the BiquadFilterNode interface that allows for\nsignal filtering. There are different kinds of filters available and we can\nswitch between them using the type attribute. The type we need here is\npeaking: It controls a certain frequency band \"peaking\" at a specific\nfrequency. Here's how we could create a peaking filter for boosting the 1kHz\nfrequency by 12 decibels:\n\n    \n    \n    let filter = Tone.context.createBiquadFilter(); filter.type = 'peaking'; filter.frequency.value = 1000; filter.Q.value = 4.31; filter.gain.value = 12;\n\nThe Q attribute controls the width of the frequency band around the base\nfrequency. It is defined as \"center frequency divided by the desired\nbandwidth\". We always want a bandwidth of a 1/3 octave since that is the\ndistance between our adjacent knobs, and for this 4.31 is a convenient magic\nnumber.\n\nNow, we have a number of frequencies to control separately, and that means we\nneed a number of individual filter nodes, one for each frequency. Let's create\nall those nodes:\n\ndiscreetmusic.js\n\n    \n    \n    let leftSynth = makeSynth(); let rightSynth = makeSynth(); let leftPanner = new Tone.Panner(-0.5); let rightPanner = new Tone.Panner(0.5); let equalizer = EQUALIZER_CENTER_FREQUENCIES.map(frequency => { let filter = Tone.context.createBiquadFilter(); filter.type = 'peaking'; filter.frequency.value = frequency; filter.Q.value = 4.31; filter.gain.value = 0; return filter; }); let echo = new Tone.FeedbackDelay('16n', 0.2); let delay = Tone.context.createDelay(6.0); let delayFade = Tone.context.createGain();\n\nHere we initialize a variable called equalizer, which will be an array of\nBiquadFilterNodes. Each of their gains is initialized to zero, which means\nthat they will just pass the signal through without modifying it.\n\nNext, we'll add these nodes into our audio graph. What we can do is simply\nconnect them in series, so that the signal passes from one filter to the next.\nIn Eno's systems diagram the graphic equalizer goes right after the synth,\nbefore the echo. So that's where we'll put it. The signal passes from the\nsynth panners to the first filter in the equalizer. From the last filter in\nthe equalizer we connect it to the echo:\n\ndiscreetmusic.js\n\n    \n    \n    leftSynth.connect(leftPanner); rightSynth.connect(rightPanner); leftPanner.connect(equalizer[0]); rightPanner.connect(equalizer[0]); equalizer.forEach((equalizerBand, index) => { if (index < equalizer.length - 1) { // Connect to next equalizer band equalizerBand.connect(equalizer[index + 1]); } else { // This is the last band, connect it to the echo equalizerBand.connect(echo); } }); echo.toMaster(); echo.connect(delay); delay.connect(Tone.context.destination); delay.connect(delayFade); delayFade.connect(delay);\n\nHere's the audio graph we have now:\n\nThe complete Discreet Music Web Audio graph: Panned synths, equalizer filters,\necho, Frippertronics.\n\nThe system doesn't yet sound any different because none of the filters have\nnonzero gains. That's what we'll change next.\n\n#### Building the Equalizer Control UI\n\nWe are still missing the \"graphic\" part of the graphic equalizer. What we're\ngoing to do is add to our HTML document a set of slider knobs, one for each\nfrequency band in our equalizer. This will emulate the UI of an actual\nhardware graphic equalizer, which Eno would have been using.\n\nNow, HTML5 has a standard range input, which renders a slider for controlling\na numeric value. Unfortunately this input leaves a lot to be desired,\nespecially in terms of how to control its look and feel consistently across\nbrowsers. Because of this, we're going to bring in an additional library which\nmakes things a lot easier. It's called noUiSlider.\n\nLet's add this library to the project. We need some JavaScript as well as some\nCSS for it. While we're at it, let's add a container <div> into which we'll\nrender our equalizer:\n\nindex.html\n\n    \n    \n    <!DOCTYPE html> <html> <head> <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/noUiSlider/8.5.1/nouislider.min.css\"> </head> <body> <div class=\"eq\"></div> <script src=\"https://cdn.rawgit.com/mohayonao/web-audio-api-shim/master/build/web-audio-api-shim.min.js\"></script> <script src=\"https://cdnjs.cloudflare.com/ajax/libs/fetch/1.0.0/fetch.min.js\"></script> <script src=\"https://tonejs.github.io/CDN/r7/Tone.js\"></script> <script src=\"https://cdnjs.cloudflare.com/ajax/libs/noUiSlider/8.5.1/nouislider.min.js\"> </script> <script src=\"discreetmusic.js\"></script> </body> </html>\n\nSwitching to the JavaScript code, we'll add a new function called\ninitEqualizerUI, which takes the container div and the array of filter nodes\nas arguments. We'll call it right at the end of the file, after we've started\nthe system:\n\ndiscreetmusic.js\n\n    \n    \n    Tone.Transport.start(); initEqualizerUI(document.querySelector('.eq'), equalizer);\n\nIn this function we're going to go through each frequency band in our\nequalizer and set up a knob UI for it. What we'd like to have for each knob is\nthe following HTML structure - a wrapper <div> containing two elements: The\nslider element itself and a label element that displays what frequency we're\ncontrolling:\n\n    \n    \n    <div class=\"slider-wrapper\"> <div class=\"slider\"></div> <label>1K</label> </div>\n\nHere's the function we need to generate and render this HTML structure for\neach frequency band:\n\ndiscreetmusic.js\n\n    \n    \n    function initEqualizerUI(container, equalizer) { equalizer.forEach(equalizerBand => { let frequency = equalizerBand.frequency.value; let wrapper = document.createElement('div'); let slider = document.createElement('div'); let label = document.createElement('label'); wrapper.classList.add('slider-wrapper'); slider.classList.add('slider'); label.textContent = frequency >= 1000 ? `${frequency / 1000}K` : frequency; wrapper.appendChild(slider); wrapper.appendChild(label); container.appendChild(wrapper); }); }\n\nNow we can bring in the noUiSlider library and let it construct a range slider\nfor each of the <div class=\"slider\"></div> elements we're producing:\n\ndiscreetmusic.js\n\n    \n    \n    function initEqualizerUI(container, equalizer) { equalizer.forEach(equalizerBand => { let frequency = equalizerBand.frequency.value; let wrapper = document.createElement('div'); let slider = document.createElement('div'); let label = document.createElement('label'); wrapper.classList.add('slider-wrapper'); slider.classList.add('slider'); label.textContent = frequency >= 1000 ? `${frequency / 1000}K` : frequency; noUiSlider.create(slider, { start: 0, // The initial gain, 0dB range: {min: -12, max: 12}, // The allowed gain range, -12dB..12dB step: 0.1, // Adjust the gain in 0.1dB increments orientation: 'vertical', // Render a vertical slider direction: 'rtl' // -12dB goes at the bottom, 12dB at the top }); wrapper.appendChild(slider); wrapper.appendChild(label); container.appendChild(wrapper); }); }\n\nThis will make the sliders visible, but we still need a bit of CSS to make\nthem look better:\n\nindex.html\n\n    \n    \n    <!DOCTYPE html> <html> <head> <link rel=\"stylesheet\" href=\"https://cdnjs.cloudflare.com/ajax/libs/noUiSlider/8.5.1/nouislider.min.css\"> <style> .eq .slider-wrapper { /* Float the wrappers so the sliders will render side by side */ float: left; } .eq .slider { /* Put some horizontal space around each slider */ margin: auto 5px; /* Set the slider heights */ height: 150px; } /* Make the frequency labels smaller and lighter */ .eq label { color: #777; display: block; font-size: 0.7em; text-align: center; } </style> </head> <body> <div class=\"eq\"> </div> <script src=\"https://tonejs.github.io/CDN/r7/Tone.js\"></script> <script src=\"https://cdnjs.cloudflare.com/ajax/libs/noUiSlider/8.5.1/nouislider.min.js\"></script> <script src=\"discreetmusic.js\"></script> </body> </html>\n\nNow we have a perfectly acceptable looking graphic equalizer!\n\nThere's a tiny problem with it though, which is that it doesn't actually work.\nSliding the knobs has no effect. This is simply because we haven't connected\nthe slider values to the gain attributes of the BiquadFilterNodes.\n\nThis is easily fixed. We just need to listen to the update event that\nnoUiSlider sends when the slider's value changes. It gives us the slider value\npackaged into an array of strings (it's an array because the library supports\nmultiple knobs per slider). We get the first value from the array and coerce\nit to a number to get the current gain number:\n\ndiscreetmusic.js\n\n    \n    \n    function initEqualizerUI(container, equalizer) { equalizer.forEach(equalizerBand => { let frequency = equalizerBand.frequency.value; let wrapper = document.createElement('div'); let slider = document.createElement('div'); let label = document.createElement('label'); wrapper.classList.add('slider-wrapper'); slider.classList.add('slider'); label.textContent = frequency >= 1000 ? `${frequency / 1000}K` : frequency; noUiSlider.create(slider, { start: 0, range: {min: -12, max: 12}, step: 0.1, direction: 'rtl', orientation: 'vertical', }); slider.noUiSlider.on('update', ([value]) => { let gain = +value; }); wrapper.appendChild(slider); wrapper.appendChild(label); container.appendChild(wrapper); }); }\n\nThen we can just plop that value into the gain attribute of the filter node\nfor this frequency:\n\ndiscreetmusic.js\n\n    \n    \n    slider.noUiSlider.on('update', ([value]) => { let gain = +value; equalizerBand.gain.value = gain; });\n\nAnd finally we have a fully functional version of the Discreet Music system:\n\n#### \"Discreet Music\"\n\nYou can adjust the knobs to alter the timbre of the system. You can get the\nmost dramatic results by bringing the lower half of the bands all the way down\nand the upper half all the way up. This'll eliminate many of the fundamental\npitches and highlight the overtone frequencies.\n\nDo remember that because of the way we've set this system up, the immediate\ncontrol you have on the music is limited: You're not controlling everything\nyou hear, but just the sounds that are currently being generated from the\nsynthesizer. Anything that's being repeated by Frippertronics isn't affected\nbecause it doesn't pass through the equalizer.\n\nAlso, as you listen to the music, there's no need to feel compelled to\nconstantly fiddle with the equalizer. You can just adjust it from time to time\nand then let it be. This is what Eno did when he was recording Discreet Music,\nbecause he was busy:\n\n> Once I got it going the phone started ringing, people started knocking on\n> the door, and I was answering the phone and adjusting all this stuff as it\n> ran. I almost made that without listening to it. It was really automatic\n> music.\n>\n> Brian Eno, 1978\n\n## Going Forward\n\nIn this guide we've been reconstructing systems made by artists in the 1960s\nand 1970s using technology that's completely different from the original. The\ntechnical restrictions Reich and Eno faced \u2013 fixed, short tape durations and\nthe absence of any kind of runtime computation \u2013 simply don't exist when\nworking with Web Audio.\n\nMaking music with such artificial restrictions feels a bit funny, but then\nagain, there's a certain elegance to the utter simplicity of these systems.\nAlso, restrictions are often good when you're trying to create. Being\noverwhelmed with the technical possibilities of the platform can easily block\nyou from coming up with anything interesting, and simply ruling out 80% of\nyour options can unblock your creative flow. This is especially true when\nyou're just getting started with the platform. As one of the masters of\nelectronic music has put it:\n\n> When I first started using Max it was a bit intimidating, you'd get blank\n> canvas syndrome, that moment of \"what should I do? I could do anything!\"\n> Once I started to build stuff, narrow it down, reduce the capabilities, you\n> start to get more ideas. It's all about narrowing down for me. There has to\n> be a seed.\n>\n> Sean Booth of Autechre, 2016\n\nThere's certainly a similar quality to Web Audio: There's so much you can do\nthat it can be intimidating. But if you limit your options, as we've done here\nby pretending we're working with the technologies of yesteryear, it suddenly\nbecomes a lot easier to experiment.\n\nBut interesting as these pieces are to reverse engineer and reconstruct, I\ndon't think they really make use of the unique possibilities we have with Web\nAudio. Whereas the systems constructed by Reich and Eno were physically\nconstrained to their respective studios, we can simply and trivially just send\nthe systems over to anyone in the world. It's just code and it runs in any web\nbrowser. There's at least a couple of possibilities this opens up that we\nhaven't even touched on in this guide.\n\nFirstly, the systems could be interactive, bringing the listener in as an\nactive participant instead of a passive receiver. Brian Eno himself has\ncreated such systems into his generative music apps for iOS, made in\ncollaboration with Peter Chilvers.\n\nSecondly, since we're working on the web platform, we are by definition\nconnected to the outside world. Listeners could easily be connected to each\nother, making the interactive experience a social one as well. These systems\ncould also potentially be connected to various servers and APIs around the\nworld, providing possibilities for, say, music generated from real-time data\nsources.\n\nLimited though these systems are, I hope they've given you an idea of the\nkinds of things we can do with Web Audio. They've certainly done so for me.\n\nTero Parviainen is an independent software developer and writer.\n\nTero is the author of two books: Build Your Own AngularJS and Real Time Web\nApplication development using Vert.x 2.0. He also likes to write in-depth\narticles on his blog, some examples of this being The Full-Stack Redux\nTutorial and JavaScript Systems Music.\n\n  * Things that Excite Me about Angular 2\n  * Signals and Sine Waves\n\n  * javascript\n  * music\n  * webaudio\n\n\u00a9 2017 Tero Parviainen\n\nContact: @teropa / tero@teropa.info\n\n", "frontpage": false}
