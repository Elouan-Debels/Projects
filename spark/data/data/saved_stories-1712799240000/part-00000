{"aid": "39994836", "title": "Twinny: Locally hosted (or API hosted) AI code completion for Visual Studio Code", "url": "https://github.com/rjmacarthy/twinny", "domain": "github.com/rjmacarthy", "votes": 1, "user": "tosh", "posted_at": "2024-04-10 19:31:52", "comments": 0, "source_title": "GitHub - rjmacarthy/twinny: The most no-nonsense locally hosted (or API hosted) AI code completion plugin for Visual Studio Code, like GitHub Copilot but 100% free!", "source_text": "GitHub - rjmacarthy/twinny: The most no-nonsense locally hosted (or API\nhosted) AI code completion plugin for Visual Studio Code, like GitHub Copilot\nbut 100% free!\n\nSkip to content\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nrjmacarthy / twinny Public\n\n  * Notifications\n  * Fork 69\n  * Star 1.1k\n\nThe most no-nonsense locally hosted (or API hosted) AI code completion plugin\nfor Visual Studio Code, like GitHub Copilot but 100% free!\n\nmarketplace.visualstudio.com/items?itemname=rjmacarthy.twinny\n\n### License\n\nMIT license\n\n1.1k stars 69 forks Branches Tags Activity\n\nStar\n\nNotifications\n\n# rjmacarthy/twinny\n\nThis commit does not belong to any branch on this repository, and may belong\nto a fork outside of the repository.\n\n2 Branches\n\n252 Tags\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\nrjmacarthy3.11.4ef38e35 \u00b7\n\n## History\n\n793 Commits  \n  \n### .github\n\n|\n\n### .github\n\n| add publish to open vsx workflow step  \n  \n### .vscode\n\n|\n\n### .vscode\n\n| fix launch.json revert  \n  \n### assets\n\n|\n\n### assets\n\n| add new document feature add codicons  \n  \n### scripts\n\n|\n\n### scripts\n\n| mkdir wasms dir  \n  \n### src\n\n|\n\n### src\n\n| use boolean  \n  \n### .editorconfig\n\n|\n\n### .editorconfig\n\n| initial commit  \n  \n### .eslintrc.cjs\n\n|\n\n### .eslintrc.cjs\n\n| initial commit  \n  \n### .gitignore\n\n|\n\n### .gitignore\n\n| initial commit  \n  \n### .prettierrc\n\n|\n\n### .prettierrc\n\n| initial commit  \n  \n### .vscode-test.js\n\n|\n\n### .vscode-test.js\n\n| update completion formatter add tests  \n  \n### .vscodeignore\n\n|\n\n### .vscodeignore\n\n| fix ignore  \n  \n### CODE_OF_CONDUCT.md\n\n|\n\n### CODE_OF_CONDUCT.md\n\n| Create CODE_OF_CONDUCT.md  \n  \n### CONTRIBUTING.md\n\n|\n\n### CONTRIBUTING.md\n\n| update contributing  \n  \n### LICENSE\n\n|\n\n### LICENSE\n\n| update licence  \n  \n### README.md\n\n|\n\n### README.md\n\n| Update README.md for correct model pull  \n  \n### example\n\n|\n\n### example\n\n| add custom providers initial commit  \n  \n### package-lock.json\n\n|\n\n### package-lock.json\n\n| 3.11.4  \n  \n### package.json\n\n|\n\n### package.json\n\n| 3.11.4  \n  \n### tsconfig.json\n\n|\n\n### tsconfig.json\n\n| update completion formatter add tests  \n  \n### webpack.config.js\n\n|\n\n### webpack.config.js\n\n| fix output path  \n  \n## Repository files navigation\n\n# twinny\n\nAre you fed up of all of those so called \"free\" Copilot alternatives with\npaywalls and signups? Fear not my developer friend!\n\nTwinny is the most no-nonsense locally hosted (or api hosted) AI code\ncompletion plugin for Visual Studio Code and any compatible editors (like\nVSCodium) designed to work seamlessly with:\n\n  * Ollama\n  * llama.cpp\n  * oobabooga/text-generation-webui\n  * LM Studio\n  * LiteLLM\n  * Ollama Web UI\n\nLike Github Copilot but 100% free!\n\nInstall twinny on the Visual Studio Code extension marketplace\n\n## Latest version v3.11.0\n\nThe twinny extension has received an update, bringing it to version 3.11.0.\nThis release is categorized as a minor update, as it may potentially break\nexisting configurations.\n\nOne of the key changes in this version is the way API settings are handled.\nInstead of configuring API settings within the twinny extension itself,\nproviders are now managed through the extension's provider settings. This\nchange streamlines the process of switching between different models,\nproviders, and APIs without the need to access the extension settings\ndirectly.\n\n## Main features\n\n#### Fill in the middle code completion\n\nGet AI based suggestions in real time. While coding you can let twinny\nautocomplete the code as you are typing.\n\n#### Chat with AI about your code\n\nThrough the side bar, have a conversation with your model and get explanations\nabout a function, ask it to write tests, ask for a refactor and much more.\n\n#### Other features\n\n  * Works online or offline\n  * Highly configurable api endpoints for fim and chat\n  * Conforms to the OpenAI API standard\n  * Single or multiline fill-in-middle completions\n  * Customisable prompt templates to add context to completions\n  * Generate git commit messages from staged changes (CTRL+SHIFT+T CTRL+SHIFT+G)\n  * Easy installation via vscode extensions marketplace or by downloading and running a binary directly\n  * Customisable settings to change API provider, model name, port number and path\n  * Ollama, llamacpp, oobabooga and LM Studio API compatible\n  * Accept code solutions directly to editor\n  * Create new documents from code blocks\n  * Copy generated code solution blocks\n  * Chat history preserved per workspace\n\n## \ud83d\ude80 Getting Started\n\n### With Ollama\n\n  1. Install the VS code extension link (or if VSCodium)\n  2. Twinny is configured to use Ollama by default as the backend, you can install Ollama here: ollama\n  3. Choose your model from the library (eg: codellama:7b)\n\n    \n    \n    ollama run codellama:7b-instruct ollama run codellama:7b-code\n\n  4. Open VS code (if already open a restart might be needed) and press ctr + shift + T to open the side panel.\n\nYou should see the \ud83e\udd16 icon indicating that twinny is ready to use.\n\n  5. See Keyboard shortcuts to start using while coding \ud83c\udf89\n\n### With llama.cpp / LM Studio / Oobabooga / LiteLLM or any other provider.\n\n  1. Install the VS code extension link (or if VSCodium)\n  2. Get llama.cpp / LM Studio / Oobabooga / LiteLLM\n  3. Download and run the model locally using the chosen provider\n  4. Open VS code (if already open a restart might be needed) and press ctr + shift + T to open the side panel.\n  5. From the top \u2699\ufe0f icon open the settings page and in the Api Provider panel change from ollama to llamacpp (or others respectively).\n  6. Update the settings for chat provider, port and hostname etc to be the correct. Please adjust carefully for other providers.\n  7. In the left panel you should see the \ud83e\udd16 icon indicating that twinny is ready to use.\n  8. See Keyboard shortcuts to start using while coding \ud83c\udf89\n\n### With other providers\n\nTwinny supports the OpenAI API specification so in theory any provider should\nwork as long as it supports the specification.\n\nThe easiest way to use OpenAI API through twinny is to use LiteLLM as your\nprovider as a local proxy, it works seamlessly if configured correctly.\n\nIf you find that isn't the case please open an issue with details of how you\nare having problems.\n\n#### Note!\n\nThe option for chat model name and fim model name are only applicable to\nOllama and Oobabooga providers.\n\n## Model support\n\nTwinny chat works with any model as long as it can run on your machine or in\nthe cloud and it exposes a OpenAI API compliant endpoint.\n\nChoosing a model is influenced a lot by the machine it will be running, a\nsmaller model will give you a faster response but with a loss in accuracy.\n\nThere are two functionalities that twinny are expecting from a model:\n\n### Models for Chat\n\nAmong LLM models, there are models called \"instruct models\", which are\ndesigned for a question & answer mode of chat.\n\nAll instruct models should work for chat generations, but the templates might\nneed editing if using something other than codellama (they need to be updated\nwith the special tokens).\n\n  * For computers with a good GPU, use: deepseek-coder:6.7b-base-q5_K_M (or any other good instruct model).\n\n### Models for FIM (fill in the middle) completions\n\nFor FIM completions, you need to use LLM models called \"base models\". Unlike\ninstruct models, base models will only try to complete your prompt. They are\nnot designed to answer questions.\n\nIf using Llama the model must support the Llama special tokens.\n\n  * For computers with a good GPU, use: deepseek-coder:base or codellama-code (or any other good model that is optimised for code completions).\n  * For slower computers or computers using only CPU, use deepseek-coder:1.3b-base-q4_1 (or any other small base model).\n\n## Keyboard shortcuts\n\nShortcut| Description  \n---|---  \nALT+\\| Trigger inline code completion  \nCTRL+SHIFT+/| Stop the inline code generation  \nTab| Accept the inline code generated  \nCTRL+SHIFT+t| Open twinny sidebar  \nCTRL+SHIFT+t CTRL+SHIT+g| Generate commit messages from staged changes  \n  \n## Workspace context\n\nIn the settings there is an option called useFileContext this will keep track\nof sessions, keystrokes, visits and recency of visited files in the current\nworkspace. This can be enabled to help improve the quality of completions,\nit's turned off by default.\n\n## Known issues\n\n  * If the server settings are incorrectly set chat and fim completion will not work, if this is the case please open an issue with your error message.\n  * Sometimes a restart of vscode is required for new settings to take effect, please open an issue if you are having problems with this.\n  * Using file context often causes unreliable completions for FIM because small models get confused when provided with more than one file context.\n  * See open issues on github to see any known issues that are not yet fixed.\n  * LiteLLM fim template needs invetigation\n\nIf you have a problem with Twinny or have any suggestions please report them\non github issues. Please include your vscode version and OS details in your\nissue.\n\n## Contributing\n\nWe are actively looking for contributors who want to help improve the project,\nif you are interested in helping out please reach out on twitter.\n\nContributions are welcome please open an issue describing your changes and\nopen a pull request when ready.\n\nThis project is under MIT licence, please read the LICENSE file for more\ninformation.\n\n## Disclaimer\n\nThis plugin is provided \"as is\" and is under active development. This means\nthat at times it may not work fully as expected.\n\n## About\n\nThe most no-nonsense locally hosted (or API hosted) AI code completion plugin\nfor Visual Studio Code, like GitHub Copilot but 100% free!\n\nmarketplace.visualstudio.com/items?itemName=rjmacarthy.twinny\n\n### Topics\n\nartificial-intelligence private free vscode-extension code-generation code-\ncompletion copilot code-chat llamacpp llama2 ollama codellama ollama-chat\nollama-api\n\n### Resources\n\nReadme\n\n### License\n\nMIT license\n\n### Code of conduct\n\nCode of conduct\n\nActivity\n\n### Stars\n\n1.1k stars\n\n### Watchers\n\n11 watching\n\n### Forks\n\n69 forks\n\nReport repository\n\n## Releases 43\n\nv3.11.0 Latest\n\nApr 8, 2024\n\n\\+ 42 releases\n\n## Sponsor this project\n\n  * https://www.paypal.me/rjmacarthy\n\n## Packages 0\n\nNo packages published\n\n## Contributors 15\n\n## Languages\n\n  * TypeScript 82.1%\n  * CSS 16.3%\n  * JavaScript 1.6%\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": false}
