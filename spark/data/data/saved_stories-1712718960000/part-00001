{"aid": "39984315", "title": "Jetstream: New LLM Inference Engine", "url": "https://github.com/google/JetStream", "domain": "github.com/google", "votes": 1, "user": "gfortaine", "posted_at": "2024-04-09 21:13:58", "comments": 0, "source_title": "GitHub - google/JetStream: JetStream is a throughput and memory optimized engine for LLM inference on XLA devices, starting with TPUs (and GPUs in future -- PRs welcome).", "source_text": "GitHub - google/JetStream: JetStream is a throughput and memory optimized\nengine for LLM inference on XLA devices, starting with TPUs (and GPUs in\nfuture -- PRs welcome).\n\nSkip to content\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\ngoogle / JetStream Public\n\n  * Notifications\n  * Fork 12\n  * Star 59\n\nJetStream is a throughput and memory optimized engine for LLM inference on XLA\ndevices, starting with TPUs (and GPUs in future -- PRs welcome).\n\n### License\n\nApache-2.0 license\n\n59 stars 12 forks Branches Tags Activity\n\nStar\n\nNotifications\n\n# google/JetStream\n\nThis commit does not belong to any branch on this repository, and may belong\nto a fork outside of the repository.\n\n10 Branches\n\n1 Tags\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\nJoeZijunZhouUpdate README.md with user guides (#34)c3203c1 \u00b7\n\n## History\n\n38 Commits  \n  \n### .github\n\n|\n\n### .github\n\n| Release v0.2.0 (#31)  \n  \n### benchmarks\n\n|\n\n### benchmarks\n\n| add sample_idx for debugging (#32)  \n  \n### jetstream\n\n|\n\n### jetstream\n\n| Enable pylint linter and pyink formatter (#29)  \n  \n### .gitignore\n\n|\n\n### .gitignore\n\n| Update benchmark to run openorca dataset (#21)  \n  \n### AUTHORS\n\n|\n\n### AUTHORS\n\n| JetStream init version  \n  \n### CONTRIBUTING.md\n\n|\n\n### CONTRIBUTING.md\n\n| JetStream init version  \n  \n### LICENSE\n\n|\n\n### LICENSE\n\n| JetStream init version  \n  \n### MANIFEST.in\n\n|\n\n### MANIFEST.in\n\n| Release v0.2.0 (#31)  \n  \n### README.md\n\n|\n\n### README.md\n\n| Update README.md with user guides (#34)  \n  \n### pylintrc\n\n|\n\n### pylintrc\n\n| Enable pylint linter and pyink formatter (#29)  \n  \n### requirements.in\n\n|\n\n### requirements.in\n\n| JetStream init version  \n  \n### requirements.txt\n\n|\n\n### requirements.txt\n\n| JetStream init version  \n  \n### setup.py\n\n|\n\n### setup.py\n\n| Release v0.2.0 (#31)  \n  \n## Repository files navigation\n\n# JetStream is a throughput and memory optimized engine for LLM inference on\nXLA devices, starting with TPUs (and GPUs in future -- PRs welcome).\n\n## About\n\nJetStream is a fast library for LLM inference and serving on TPUs (and GPUs in\nfuture -- PRs welcome).\n\n## Documentation\n\n  * Online Inference with MaxText on v5e Cloud TPU VM [README]\n  * Online Inference with Pytorch on v5e Cloud TPU VM [README]\n  * Serve Gemma using TPUs on GKE with JetStream\n  * JetStream Standalone Local Setup\n\n# JetStream MaxText Inference on v5e Cloud TPU VM User Guide\n\n## Outline\n\n  1. Prerequisites: Prepare your GCP project and connect to Cloud TPU VM\n  2. Download the JetStream and MaxText github repository\n  3. Setup your MaxText JetStream environment\n  4. Convert Model Checkpoints\n  5. Run the JetStream MaxText server\n  6. Send a test request to the JetStream MaxText server\n  7. Run benchmarks with the JetStream MaxText server\n  8. Clean up\n\n## Prerequisites: Prepare your GCP project and connect to Cloud TPU VM\n\nFollow the steps in Manage TPU resources | Google Cloud to create a Cloud TPU VM (Recommend TPU type: v5litepod-8) and connect to the Cloud TPU VM.\n\n## Step 1: Download JetStream and the MaxText github repository\n\n    \n    \n    git clone -b jetstream-v0.2.0 https://github.com/google/maxtext.git git clone -b v0.2.0 https://github.com/google/JetStream.git\n\n## Step 2: Setup MaxText\n\n    \n    \n    # Create a python virtual environment for the demo. sudo apt install python3.10-venv python -m venv .env source .env/bin/activate # Setup MaxText. cd maxtext/ bash setup.sh\n\n## Step 3: Convert Model Checkpoints\n\nYou can run the JetStream MaxText Server with Gemma and Llama2 models. This\nsection describes how to run the JetStream MaxText server with various sizes\nof these models.\n\n### Use a Gemma model checkpoint\n\n  * You can download a Gemma checkpoint from Kaggle.\n  * After downloading checkpoints, copy them to your GCS bucket at $CHKPT_BUCKET.\n\n    * gsutil -m cp -r ${YOUR_CKPT_PATH} ${CHKPT_BUCKET}\n    * Please refer to the conversion script for an example of $CHKPT_BUCKET.\n  * Then, using the following command to convert the Gemma checkpoint into a MaxText compatible unscanned checkpoint.\n\n    \n    \n    # bash ../JetStream/jetstream/tools/maxtext/model_ckpt_conversion.sh ${MODEL} ${MODEL_VARIATION} ${CHKPT_BUCKET} # For gemma-7b bash ../JetStream/jetstream/tools/maxtext/model_ckpt_conversion.sh gemma 7b ${CHKPT_BUCKET}\n\nNote: For more information about the Gemma model and checkpoints, see About\nGemma.\n\n### Use a Llama2 model checkpoint\n\n  * You can use a Llama2 checkpoint you have generated or one from the open source community.\n  * After downloading checkpoints, copy them to your GCS bucket at $CHKPT_BUCKET.\n\n    * gsutil -m cp -r ${YOUR_CKPT_PATH} ${CHKPT_BUCKET}\n    * Please refer to the conversion script for an example of $CHKPT_BUCKET.\n  * Then, using the following command to convert the Llama2 checkpoint into a MaxText compatible unscanned checkpoint.\n\n    \n    \n    # bash ../JetStream/jetstream/tools/maxtext/model_ckpt_conversion.sh ${MODEL} ${MODEL_VARIATION} ${CHKPT_BUCKET} # For llama2-7b bash ../JetStream/jetstream/tools/maxtext/model_ckpt_conversion.sh llama2 7b ${CHKPT_BUCKET} # For llama2-13b bash ../JetStream/jetstream/tools/maxtext/model_ckpt_conversion.sh llama2 13b ${CHKPT_BUCKET}\n\nNote: For more information about the Llama2 model and checkpoints, see About\nLlama2.\n\n## Step4: Run the JetStream MaxText server\n\n### Create model config environment variables for server flags\n\nYou can export the following environment variables based on the model you\nused.\n\n  * You can copy and export the UNSCANNED_CKPT_PATH from the model_ckpt_conversion.sh output.\n\n#### Create Gemma-7b environment variables for server flags\n\n  * Configure the flags passing into the JetStream MaxText server\n\n    \n    \n    export TOKENIZER_PATH=assets/tokenizer.gemma export LOAD_PARAMETERS_PATH=${UNSCANNED_CKPT_PATH} export MAX_PREFILL_PREDICT_LENGTH=1024 export MAX_TARGET_LENGTH=2048 export MODEL_NAME=gemma-7b export ICI_FSDP_PARALLELISM=1 export ICI_AUTOREGRESSIVE_PARALLELISM=-1 export ICI_TENSOR_PARALLELISM=1 export SCAN_LAYERS=false export WEIGHT_DTYPE=bfloat16 export PER_DEVICE_BATCH_SIZE=4\n\n#### Create Llama2-7b environment variables for server flags\n\n  * Configure the flags passing into the JetStream MaxText server\n\n    \n    \n    export TOKENIZER_PATH=assets/tokenizer.llama2 export LOAD_PARAMETERS_PATH=${UNSCANNED_CKPT_PATH} export MAX_PREFILL_PREDICT_LENGTH=1024 export MAX_TARGET_LENGTH=2048 export MODEL_NAME=llama2-7b export ICI_FSDP_PARALLELISM=1 export ICI_AUTOREGRESSIVE_PARALLELISM=-1 export ICI_TENSOR_PARALLELISM=1 export SCAN_LAYERS=false export WEIGHT_DTYPE=bfloat16 export PER_DEVICE_BATCH_SIZE=6\n\n#### Create Llama2-13b environment variables for server flags\n\n  * Configure the flags passing into the JetStream MaxText server\n\n    \n    \n    export TOKENIZER_PATH=assets/tokenizer.llama2 export LOAD_PARAMETERS_PATH=${UNSCANNED_CKPT_PATH} export MAX_PREFILL_PREDICT_LENGTH=1024 export MAX_TARGET_LENGTH=2048 export MODEL_NAME=llama2-13b export ICI_FSDP_PARALLELISM=1 export ICI_AUTOREGRESSIVE_PARALLELISM=-1 export ICI_TENSOR_PARALLELISM=1 export SCAN_LAYERS=false export WEIGHT_DTYPE=bfloat16 export PER_DEVICE_BATCH_SIZE=2\n\n### Run the following command to start the JetStream MaxText server\n\n    \n    \n    cd ~/maxtext python MaxText/maxengine_server.py \\ MaxText/configs/base.yml \\ tokenizer_path=${TOKENIZER_PATH} \\ load_parameters_path=${LOAD_PARAMETERS_PATH} \\ max_prefill_predict_length=${MAX_PREFILL_PREDICT_LENGTH} \\ max_target_length=${MAX_TARGET_LENGTH} \\ model_name=${MODEL_NAME} \\ ici_fsdp_parallelism=${ICI_FSDP_PARALLELISM} \\ ici_autoregressive_parallelism=${ICI_AUTOREGRESSIVE_PARALLELISM} \\ ici_tensor_parallelism=${ICI_TENSOR_PARALLELISM} \\ scan_layers=${SCAN_LAYERS} \\ weight_dtype=${WEIGHT_DTYPE} \\ per_device_batch_size=${PER_DEVICE_BATCH_SIZE}\n\n### JetStream MaxText Server flag descriptions:\n\n  * tokenizer_path: file path to a tokenizer (should match your model)\n  * load_parameters_path: Loads the parameters (no optimizer states) from a specific directory\n  * per_device_batch_size: decoding batch size per device (1 TPU chip = 1 device)\n  * max_prefill_predict_length: Maximum length for the prefill when doing autoregression\n  * max_target_length: Maximum sequence length\n  * model_name: Model name\n  * ici_fsdp_parallelism: The number of shards for FSDP parallelism\n  * ici_autoregressive_parallelism: The number of shards for autoregressive parallelism\n  * ici_tensor_parallelism: The number of shards for tensor parallelism\n  * weight_dtype: Weight data type (e.g. bfloat16)\n  * scan_layers: Scan layers boolean flag\n\nNote: these flags are from MaxText config\n\n## Step 5: Send test request to JetStream MaxText server\n\n    \n    \n    cd ~ python JetStream/jetstream/tools/requester.py\n\nThe output will be similar to the following:\n\n    \n    \n    Sending request to: dns:///[::1]:9000 Prompt: Today is a good day Response: to be a fan\n\n## Step 6: Run benchmarks with JetStream MaxText server\n\nNote: The JetStream MaxText Server is not running with quantization\noptimization in Step 3. To get best benchmark results, we need to enable\nquantization (Please use AQT trained or fine tuned checkpoints to ensure\naccuracy) for both weights and KV cache, please add the quantization flags and\nrestart the server as following:\n\n    \n    \n    # Enable int8 quantization for both weights and KV cache export QUANTIZATION=int8 export QUANTIZE_KVCACHE=true # For Gemma 7b model, change per_device_batch_size to 12 to optimize performance. export PER_DEVICE_BATCH_SIZE=12 cd ~/maxtext python MaxText/maxengine_server.py \\ MaxText/configs/base.yml \\ tokenizer_path=${TOKENIZER_PATH} \\ load_parameters_path=${LOAD_PARAMETERS_PATH} \\ max_prefill_predict_length=${MAX_PREFILL_PREDICT_LENGTH} \\ max_target_length=${MAX_TARGET_LENGTH} \\ model_name=${MODEL_NAME} \\ ici_fsdp_parallelism=${ICI_FSDP_PARALLELISM} \\ ici_autoregressive_parallelism=${ICI_AUTOREGRESSIVE_PARALLELISM} \\ ici_tensor_parallelism=${ICI_TENSOR_PARALLELISM} \\ scan_layers=${SCAN_LAYERS} \\ weight_dtype=${WEIGHT_DTYPE} \\ per_device_batch_size=${PER_DEVICE_BATCH_SIZE} \\ quantization=${QUANTIZATION} \\ quantize_kvcache=${QUANTIZE_KVCACHE}\n\n### Benchmarking Gemma-7b\n\nInstructions\n\n  * Download the ShareGPT dataset\n  * Make sure to use the Gemma tokenizer (tokenizer.gemma) when running Gemma 7b.\n  * Add --warmup-first flag for your 1st run to warmup the server\n\n    \n    \n    # Activate the python virtual environment we created in Step 2. cd ~ source .env/bin/activate # download dataset wget https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json # run benchmark with the downloaded dataset and the tokenizer in maxtext # You can control the qps by setting `--request-rate`, the default value is inf. python JetStream/benchmarks/benchmark_serving.py \\ --tokenizer /home/$USER/maxtext/assets/tokenizer.gemma \\ --num-prompts 1000 \\ --dataset sharegpt \\ --dataset-path ~/ShareGPT_V3_unfiltered_cleaned_split.json \\ --max-output-length 1024 \\ --request-rate 5 \\ --warmup-first true\n\n### Benchmarking Llama2-*b\n\n    \n    \n    # Same as Gemma-7b except for the tokenizer (must use a tokenizer that matches your model, which should now be tokenizer.llama2). python JetStream/benchmarks/benchmark_serving.py \\ --tokenizer maxtext/assets/tokenizer.llama2 \\ --num-prompts 1000 \\ --dataset sharegpt \\ --dataset-path ~/ShareGPT_V3_unfiltered_cleaned_split.json \\ --max-output-length 1024 \\ --request-rate 5 \\ --warmup-first true\n\n## Clean Up\n\n    \n    \n    # Clean up gcs buckets. gcloud storage buckets delete ${MODEL_BUCKET} gcloud storage buckets delete ${BASE_OUTPUT_DIRECTORY} gcloud storage buckets delete ${DATASET_PATH} # Clean up repositories. rm -rf maxtext rm -rf JetStream # Clean up python virtual environment rm -rf .env\n\n# JetStream Standalone Local Setup\n\n## Getting Started\n\n### Setup\n\n    \n    \n    pip install -r requirements.txt\n\n### Run local server & Testing\n\nUse the following commands to run a server locally:\n\n    \n    \n    # Start a server python -m jetstream.core.implementations.mock.server # Test local mock server python -m jetstream.tools.requester # Load test local mock server python -m jetstream.tools.load_tester\n\n### Test core modules\n\n    \n    \n    # Test JetStream core orchestrator python -m jetstream.core.orchestrator_test # Test JetStream core server library python -m jetstream.core.server_test # Test mock JetStream engine implementation python -m jetstream.engine.mock_engine_test # Test mock JetStream token utils python -m jetstream.engine.utils_test\n\n## About\n\nJetStream is a throughput and memory optimized engine for LLM inference on XLA\ndevices, starting with TPUs (and GPUs in future -- PRs welcome).\n\n### Resources\n\nReadme\n\n### License\n\nApache-2.0 license\n\n### Code of conduct\n\nCode of conduct\n\n### Security policy\n\nSecurity policy\n\nActivity\n\nCustom properties\n\n### Stars\n\n59 stars\n\n### Watchers\n\n10 watching\n\n### Forks\n\n12 forks\n\nReport repository\n\n## Releases 1\n\nv0.2.0 Latest\n\nApr 5, 2024\n\n## Packages 0\n\nNo packages published\n\n## Contributors 8\n\n## Languages\n\n  * Python 94.0%\n  * Shell 6.0%\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": false}
