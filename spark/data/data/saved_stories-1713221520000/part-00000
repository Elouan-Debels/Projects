{"aid": "40042941", "title": "The end of the \"best open LLM\"", "url": "https://www.interconnects.ai/p/compute-efficient-open-llms", "domain": "interconnects.ai", "votes": 4, "user": "vsreekanti", "posted_at": "2024-04-15 16:50:52", "comments": 0, "source_title": "The end of the \u201cbest open LLM\u201d", "source_text": "The end of the \u201cbest open LLM\u201d - by Nathan Lambert\n\nShare this post\n\n#### The end of the \u201cbest open LLM\u201d\n\nwww.interconnects.ai\n\n#### Discover more from Interconnects\n\nLinking important ideas of AI. The border between high-level and technical\nthinking. Read by leading engineers, researchers, and investors on Wednesday\nmornings.\n\nOver 12,000 subscribers\n\nContinue reading\n\nSign in\n\n# The end of the \u201cbest open LLM\u201d\n\n### Modeling the compute versus performance tradeoff of many open LLMs.\n\nNathan Lambert\n\nApr 15, 2024\n\n5\n\nShare this post\n\n#### The end of the \u201cbest open LLM\u201d\n\nwww.interconnects.ai\n\nShare\n\nSince starting this blog with boots on the ground building the open LLM\necosystem I\u2019ve declared there a \u201cbest open LLM\u201d three times: Llama 2, Mixtral,\nand DBRX. For each of these releases, following the starting gun that went off\nwith LLaMA in early 2023, the most practical LLM for almost any use case was\ngoing to come downstream of these models. Each of these was a substantial jump\nforward in the performance per parameter tradeoff of open LLMs (relative to a\nfixed compute budget).\n\nOpen models simply had so much to gain by scaling up. It\u2019s extremely easy to\nsee that a 70 billion parameter model captures so much more nuance than a 7\nbillion parameter model trained similarly. I\u2019ve highlighted the three \u201cbest\u201d\nmodels on a chart from Maxime Labonne on Twitter, modified via the addition of\nGrok 1 from xAI.\n\nThe shift from Llama 2 to Mixtral was mostly about performance compression,\nmoving from 70 to 13 billion active parameters with similar scores. The step\nto the DBRX model was about taking proportional steps up in performance and\nsize \u2014 without taking into account training token counts, more on that later.\n\nSoon after the DBRX release, the landscape of openly available LLMs has\nchanged notably with two models, Cohere\u2019s Command R+ and Mixtral\u2019s new 8x22B\nbase model.\n\n  * Command R+ is the first openly available model that has beaten any GPT-4 version on LMSYS\u2019s ChatBotArena. Can a model with a strict non-commercial license take the label of \u201cbest open model?\u201d\n\n  * Mistral 8x22B is a new stronger base model released on April 10th with little information. Do models like Mistral 8x22B or DBRX with 36 or 39 billion parameters respectively actually support the ecosystem so that it\u2019s \u201cbest\u201d for everyone?\n\nFor example, some of my former colleagues at HuggingFace quickly responded\nwith a fine-tuning of the new Mistral model, training it with a largely\nunproven alignment method, Odds Ratio Preference Optimization (ORPO), on 32\nH100 GPUs for 1 to 2 hours. When you assume that any decent model needs some\ntrial and error margins on compute costs, there are very few players in the\nalignment space with this type of compute to throw around. This new model\ndidn\u2019t entirely pass my vibes test on HuggingChat, but try it for yourself.\n\nThe last few weeks of releases have marked a meaningful shift in the community\nof people using open models. We\u2019ve known for a while that no one model will\nserve everyone equally, but it has realistically not been the case. On The\nRetort, I described it as open ML entering the trough of disillusionment (of\nthe Gartner Hype Cycle).\n\nThis article, and its main declaration that there isn\u2019t much of a point trying\nto argue for a singular best open LLM, is actually late. The release of xAI\u2019s\nGrok 1 weights actually marked this most clearly. Yi 34 B and Qwen 1.5 72B\nreally could\u2019ve marked this transition point too. Qwen 1.5 72B Chat is\npractically tied with GPT4 on the LMSYS leaderboard, coming in slightly below,\nand was released about 5 months ago. As far as I can tell, the Yi and Qwen\nmodels largely are under-adopted due to the bias of the narrative in aligning\nopen models \u2014 my own included. These models could have been labeled as the\nstate-of-the-art LLM, but that\u2019s not really the point.\n\nInterconnects is a reader-supported publication. Consider becoming a\nsubscriber.\n\n###\n\nCompute efficient open LLMs\n\nTo me, the best model going forward is going to be based on the weighted\nperformance per parameter and training token count. Ultimately, a model keeps\ngetting better the longer you train it. Most open model providers could train\nlonger, but it hasn\u2019t been worth their time. We\u2019re starting to see that\nchange.\n\nIf we start by plotting the total parameter count versus the MMLU score, it\u2019s\nobvious that it isn\u2019t that insightful. Models are clumped at popular parameter\ncounts, but it makes progress look too rosy and simple.\n\nMost of the gains made in the last year and a half of open models are purely\nfrom throwing more compute at the problem. I made a little model showcasing\nhow open models compare on compute versus MMLU tradeoff. Note, I assumed the\ntoken count of Mistral\u2019s models to be 10T tokens, which is based entirely on\nrumor and much higher than most of the models (other than DBRX at 12T). Raw\ndata is available here and please let me know if you have more training token\ncounts I couldn\u2019t find.\n\nLet\u2019s start with the two Llama series to showcase the compute versus MMLU\ntrend. Reminder, MMLU, Massive Multitask Language Understanding, is the\ncurrent benchmark that folks track most closely for base models (and fine-\ntuned models go to LMSYS ChatBotArena). Compared to the first image, the\nconcentration of the trend is remarkable! More compute simply equals more\nperformance. The question is: What is your compute budget for each model? How\ndo you optimize that as a fixed resource?\n\nNormalized by compute, it shows that most of the gains from the Llama 2 line\nof work come from scaling compute. There are more gains, as MMLU is just one\nmeasurement, but it\u2019s important to illustrate how simple compute can be in\nterms of performance \u2014 even for open models. Now, let\u2019s extend this figure\nwith more of the open base LLMs from 2023.\n\nMost of these are in line, with Yi 34B and Mixtral being the ones that look\nthe most interesting. Here\u2019s an annotated version showing where Mixtral could\nbe with different assumed token training counts.\n\nIf we assume that Mixtral 8x7B was trained on the same number of tokens as\nLlama 2, 2 trillion, it would look like a total outlier, which is unlikely.\nNow, let\u2019s add the models for 2024, which shows that models are not only\ngetting bigger, but they\u2019re getting better. The models are improving in MMLU\nwith constant compute budgets.\n\nFor models at the size of DBRX or Mixtral 8x22B, it\u2019s pretty infeasible for\nmost of the open community to do much with them. This size class will continue\nto be functional, but the models that are 10x bigger (as a Llama 3 variant is\nrumored to be), fill a different niche in the open community. The most\nimportant models will represent improvements in capability density, rather\nthan shifting the frontier.\n\nThe core difference between open and closed LLMs on these charts is how\nundertrained open LLMs often are. The only open model confirmed to be trained\non a lot of tokens is DBRX. Many models being in the 2-3 trillion token range\nrepresents a huge change from the industry model providers, where their base\nmodels are probably trained on all of their effective tokens. In some ways,\nit\u2019s easier to make the model better by training longer compared to anything\nelse, if you have the data.\n\nFinally, here\u2019s a zoomed-in version with the models most folks care about. As\nmore open-base models come out, I\u2019ll continue to use this model to showcase\nthe efficiency of the model.\n\nI\u2019m excited to add Llama 2.5 or 3\u2019s performance to these plots in the near\nfuture. In the past, the Llama models were released into an open plain with\nalmost no competitors. The large delay we\u2019ve seen is likely due to the change\nin culture from being the only player, where shipping fast is the easiest way\nto win, to being one of many players. I also expect Meta to be trying to fine-\ntune a model that scores well on LMSYS\u2019s leaderboard given the cultural weight\nof that evaluation and the relative failure of Llama 2 Chat\u2019s release.\n\nShare\n\n###\n\nNewsletter stuff\n\nI\u2019m giving a lecture on Thursday at Stanford\u2019s CS25, Transformers United\nseminar course. I\u2019ve put together a cool collection of open artifacts to go\nalong with my slides.\n\nAdditionally, I\u2019ve created collections to track all the artifacts I link in\nthe models section, so it\u2019s easier to go back and find a fine-tuned model you\nknew you saw 3 months ago.\n\nModels, datasets, and other tools (2024 artifacts, 2023 artifacts)\n\n  * A new smol MoE model, JetMoE, was released by a multi-institution (some academics) group! Great to see this.\n\n  * Another small base model, MiniCPM, was released by OpenBMB, maybe most notable for having a vision variant. OpenBMB has a pretty good track record on releases! Paper is here.\n\n  * Stable LM 2 12B was released, and the chat version particularly looks fairly strong!\n\n  * The miscellaneous fine-tunes are continuing to pump out. This project, bagel, is doing Tulu style work by mixing tons of different datasets.\n\n  * CodeGemma and RecurrentGemma came out, which is nice to see from Google, but not a ton of reason for excitement immediately for me.\n\n  * A cool new model, Rho, showed up from Microsoft that decides on which pretraining data to include by passing weights through a reward model! Awesome!\n\n  * Progress on \u201cflattening\u201d mixture of expert models continues \u2014 Mistral 22b \u2014 i.e. making it so only 2 experts exist and are used for every batch, lowering compute requirements.\n\nLinks\n\n  * This was a great talk from Tatsu Hashimoto (Alpaca team prof.) on how to contribute to the open ecosystem as an academic.\n\n  * You really should watch 3b1b\u2019s introduction to Attention.\n\nHousekeeping\n\n  * Audio of this post is available (soon) in podcast form or on YouTube.\n\n  * My real podcast is at retortai.com.\n\n  * Paid subscriber Discord access in email footer.\n\n  * Referrals \u2192 paid sub: Use the Interconnects Leaderboard.\n\n  * Student discounts in About page.\n\n### Subscribe to Interconnects\n\nBy Nathan Lambert \u00b7 Hundreds of paid subscribers\n\nLinking important ideas of AI. The border between high-level and technical\nthinking. Read by leading engineers, researchers, and investors on Wednesday\nmornings.\n\n7 Likes\n\n\u00b7\n\n1 Restack\n\n5\n\nShare this post\n\n#### The end of the \u201cbest open LLM\u201d\n\nwww.interconnects.ai\n\nShare\n\nComments\n\nThe Q* hypothesis: Tree-of-thoughts reasoning, process reward models, and\nsupercharging synthetic data\n\nEmergency special: The information we need to understand what Q* is was right\nin front of us, but the memes are more fun than reality.\n\nNov 24, 2023 \u2022\n\nNathan Lambert\n\n88\n\nShare this post\n\n#### The Q* hypothesis: Tree-of-thoughts reasoning, process reward models, and\nsupercharging synthetic data\n\nwww.interconnects.ai\n\n5\n\nBehind the curtain: what it feels like to work in AI right now (April 2023)\n\nFear, FOMO, and the scientific exodus driven by ChatGPT\n\nApr 5, 2023 \u2022\n\nNathan Lambert\n\n92\n\nShare this post\n\n#### Behind the curtain: what it feels like to work in AI right now (April\n2023)\n\nwww.interconnects.ai\n\n21\n\nLlama 2: an incredible open LLM\n\nMeta is continuing to deliver high-quality research artifacts and not backing\ndown from pressure against open source.\n\nJul 18, 2023 \u2022\n\nNathan Lambert\n\n59\n\nShare this post\n\n#### Llama 2: an incredible open LLM\n\nwww.interconnects.ai\n\n7\n\nReady for more?\n\n\u00a9 2024 Nathan Lambert\n\nPrivacy \u2219 Terms \u2219 Collection notice\n\nStart WritingGet the app\n\nSubstack is the home for great culture\n\nShare\n\n## Create your profile\n\n## Only paid subscribers can comment on this post\n\nAlready a paid subscriber? Sign in\n\n#### Check your email\n\nFor your security, we need to re-authenticate you.\n\nClick the link we sent to , or click here to sign in.\n\n", "frontpage": false}
