{"aid": "40004029", "title": "The 300+ configuration knobs of PostgreSQL", "url": "https://www.crystaldb.cloud/blog/post/why-is-configuring-postgresql-so-complicated", "domain": "crystaldb.cloud", "votes": 1, "user": "itunpredictable", "posted_at": "2024-04-11 16:32:23", "comments": 0, "source_title": "Why is configuring PostgreSQL so complicated? | CrystalDB", "source_text": "Why is configuring PostgreSQL so complicated? | CrystalDB\n\nProductPricingWhy CrystalDB?DocsResources\n\nLoginSign up\n\nGeneral\n\n# Why is configuring PostgreSQL so complicated?\n\n25 Mar 2024 - 10 Min Read\n\nPostgres promises simplicity and power that is perhaps too good to be true. To\nget the most out of your database you must manage its 300+ configuration knobs\ncarefully.\n\n### Overview\n\nGetting started with Postgres is remarkably easy. On most Linux distributions\nyou can have a database up and running with a one-line command. This is great\nfor developers, who can get started quickly and build applications rapidly.\nHowever, this simplicity belies the true complexity of running an advanced\nopen source database.\n\nLook under the hood and you will find over 300 configuration knobs that\ndetermine whether Postgres will run your application reliably and efficiently.\nSetting these knobs requires substantial expertise, especially when in\nenvironments that require high availability, stringent security, or in\napplications that achieve significant scale.\n\nThe power of Postgres arises directly from its powerful abstractions: the\nrelational model and the SQL programming language. The relational model\nprovides tables, a convenient way of organizing data that hides, or abstracts\naway, the process of laying out the data in memory or storage. SQL provides a\npowerful tool for combining data from multiple tables, and bears a closer\nresemblance to human languages than most programming languages. SQL is also\nunique among popular languages because it lets the database fill in the\nalgorithmic details of a computation. Programmers describe the result they\nwant computed, but they do not tell Postgres how to compute it.\n\nDoes Postgres promise something too good to be true? It claims to do all sorts\nof hard work for programmers: recording data reliably, organizing it for\nefficient access, producing efficient algorithms for combining it, managing\ncaching, ensuring quick recovery when system faults occur\u2014the list goes on.\n\nThe caveat is the configuration. Postgres ships with a formidable set of\ncapabilities, including a sophisticated cost-based query optimizer, state-of-\nthe-art concurrency control, and robust logging and recovery mechanisms. In\nundemanding scenarios these mechanisms take care of everything automatically.\nIn production circumstances, by contrast, they are more like tools to help you\nget the job done.\n\nIn the remainder of this post, we explore the 300+ configuration knobs in\nPostgres. This is one of the best ways to learn about its capabilities.\n\n### The PostgreSQL Configuration\n\nThe size of the Postgres configuration has grown over time. From 146 knobs in\nversion 8.0, released in 2005, it leaped to 270 in version 10.0, released in\n2017. In version 16.0, released in 2023, the number of knobs has reached 362.\n\nWe can dive deeper into this configuration to see what the knobs actually do.\nPostgres exposes all of the configuration settings via the pg_settings view\nand helpfully provides a description of each parameter. Look at the categories\nof settings in Postgres version 15.6.\n\n    \n    \n    postgres=> select substring(category, '^([^/]*)/?.*') major_category, count(*) ct from pg_settings group by major_category order by ct desc; major_category | ct -------------------------------------+---- Query Tuning | 48 Client Connection Defaults | 42 Reporting and Logging | 41 Write-Ahead Log | 38 Resource Usage | 34 Connections and Authentication | 25 Developer Options | 21 Replication | 21 Preset Options | 20 Autovacuum | 13 Statistics | 12 Version and Platform Compatibility | 8 Lock Management | 5 Error Handling | 4 (15 rows)\n\nWhat do these 332 all do?\n\n#### Query Tuning (48 settings)\n\nThere are often many ways to run a SQL statement. All produce equivalent\nresults, but may have very different performance characteristics, requiring\ndifferent amounts of memory, CPU time, and disk IO to complete. Postgres uses\na sophisticated cost-based query optimizer to pick the best plan. It scores\nalternative execution plans according to a cost model, then chooses the best\none. The power to pick through alternatives is fabulous, but it has several\nlimitations, not the least among them the requirement to set the weights for\nall of the parameters. For we may have some intuition for what\ncpu_index_tuple_cost, cpu_operator_cost, and cpu_tuple_cost mean, but how do\nwe set them well? And how should they compare to random_page_cost or\nseq_page_cost?\n\n#### Client Connection Defaults (42 settings)\n\nQuite a few of these relate to locale and formatting, and as such defaults for\nthe application configuration. Examples include character encoding\n(client_encoding), date and time formatting (lc_time), or monetary formatting\n(lc_monetary). Setting these aside, there are still 29 parameters that impact\nSQL processing statement behavior. These include settings like\ntransaction_isolation, transaction_deferrable, or lock_timeout.\n\n#### Connections and Authentication (25 settings)\n\nThis category contains settings for managing database connections and\nauthenticating users. The most common database tuning knob is max_connections,\nwhich specifies how many clients can connect to the database simultaneously.\nSetting this too high can exhaust system resources, invoking the OOM killer,\nbut setting it too low limits the amount of work the database can do. The ssl\nparameter enables SSL encryption for connections. Today this should always be\non, and Postgres shows its age with a default off setting. However but setting\nup SSL requires generating and storing certificates, so turning it on is not\nas easy as flipping a switch.\n\n#### Developer Options (21 settings)\n\nThese are settings designed for developers of Postgres. They are interesting\nto understand, but you will not need to tune them in a production system.\nExamples include jit_dump_bitcode, which directs the Just-In-Time (JIT)\ncompiler to LLVM bitcode, wal_consistency_checking which adds extra checks of\nthe write-ahead logs, and ignore_checksum_failure allows the system to\ncontinue operating even if data corruption is detected.\n\n#### Replication (21 settings)\n\nReplication is crucial for databases that meet high availability standards.\nThere are several settings that control replication. For example,\nmax_slot_wal_keep_size determines the maximum amount of write-ahead log data\nto keep for replication slots. A higher settings keeps more data, using disk\nspace but allowing a replica to catch up after a longer period of\ndisconnection. The hot_standby_feedback informs the primary node about long-\nrunning queries on read replicas. This can ensure that vacuum garbage\ncollection activity on the primary system does not remove records needed to\ncomplete these queries, but doing doing so can risk table bloat.\n\n#### Preset Options (20 settings)\n\nThese are settings that are usually set at the time of database initialization\nand rarely changed afterward. data_checksums enhance data integrity by\nenabling checksums on data pages, which helps detect corruption introduces a\nperformance overhead. debug_assertions enable internal sanity checks. This\nprovide additional error detection, but is generally needed only for\ntroubleshooting bugs in the Postgres code itself.\n\n#### Autovacuum (13 settings)\n\nAutovacuum settings control the automated background process that cleans up\nand optimizes the database. autovacuum_naptime sets the minimum delay between\nautovacuum runs, affecting the responsiveness to accumulation of garbage.\nautovacuum_vacuum_cost_limit determines the throttling of the autovacuum\nprocesses. Throttling these processes helps protect foreground application\nprocessing, but risks higher garbage accumulation and table bloat. In worst-\ncase scenarios autovacuum falls ever further behind, creating the need to\npause the database to perform a full vacuum.\n\n#### Statistics (12 settings)\n\nStatistics settings affect the collection of performance and operational\nmetrics. log_parser_stats and track_io_timing enable detailed logging of query\nparsing and I/O timing, respectively. Collecting more statistics can provide\nvaluable performance insight, but may generate large amounts of log data and\nimpact the performance of the database.\n\n#### Version and Platform Compatibility (8 settings)\n\nThese are used for ensuring backwards compatibility with older versions of\nPostgres. For example, transform_null_equals alters the behavior of the =\noperator with NULL values to conform to legacy behavior. backslash_quote\ncontrols how backslash characters are treated in string literals, ensuring\ncompatibility with certain applications.\n\n#### Lock Management (5 settings)\n\nLock management settings fine-tune the behavior of locks. These settings\nbecome important under high concurrency, and setting them incorrectly can lead\nto crippling performance bottlenecks. For example, max_locks_per_transaction,\nwhich is misleadingly named, specifies the size of the lock table and\nconsequently the average number of locks each transaction can hold.\ndeadlock_timeout determines how long the database waits before checking for\ndeadlocks, impacting responsiveness in situations where deadlocks occur.\n\n#### Error Handling (4 settings)\n\nThese settings dictate how the database responds to errors. data_sync_retry\nallows the database to retry synchronization to disk after a failure, which\nenhances resilience but can hide underlying disk issues, possibly masking\ncorruption. restart_after_crash controls whether the database should\nautomatically restart after a crash, which makes sense in some settings but is\nnot appropriate in settings when a failover configuration is available.\n\n### Summary\n\nDigging through the Postgres configuration is a little bit like taking a tour\nof the internals of the database. The developers of Postgres have put a\nconfiguration knob on many of the interesting database capabilities and\nmechanisms, so you learn something about each one of them when you explore the\nconfiguration.\n\nThis exploration also reinforces what many of us already know from experience:\nconfiguring Postgres so that it runs your workload optimally requires\nsignificant expertise. Many developers do not even know where to start when\nfaced with the 300+ configuration knobs. Even experts need to spend\nsignificant time analyzing a system to understand and address the root causes\nperformance bottlenecks.\n\nThe complexity of the Postgres configuration undermines the benefits of the\npowerful abstractions: the relational model and the SQL language. We have been\npromised a database that takes care of all of the complex details of storing\ninformation reliably and transforming it efficiently. Postgres delivers on\nthis promise to a great extent\u2014as much as any database does\u2014but while it\nsimplifies code, it shifts much complexity to the configuration.\n\nGeneral\n\nAbout author\n\nJS\n\nJohann Schleier-SmithCEO and Founder\n\nGet started now\n\n## Self-Managing Serverless PostgreSQL\n\nBook demoSign up\n\n\u00a9 2024\n\nProductWhy CrystalDB?PricingDocsFAQsSecurityBlog\n\n", "frontpage": false}
