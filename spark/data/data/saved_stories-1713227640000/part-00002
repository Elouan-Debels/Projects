{"aid": "40044075", "title": "Latency Is a Curve", "url": "https://iop.systems/blog/latency-is-a-curve/", "domain": "iop.systems", "votes": 1, "user": "TotallyNotOla", "posted_at": "2024-04-15 18:32:03", "comments": 0, "source_title": "Latency Is a Curve", "source_text": "Latency Is a Curve\n\n# Latency Is A Curve\n\n## Understanding Latency Series, Part 1\n\n### Questions This Post Answers...\n\n  * What is SLO, and why does it matter?\n  * How do you determine the timeout setting when calling a dependency service?\n  * Why is average latency deceptive?\n  * What is quantile, and how does it relate to percentile?\n  * What is a histogram, and why is it the key to understanding latency?\n\n### Owning A Service\n\nImagine yourself being the proud owner of an online software called\nCoolService. You want to promise to your many users that CoolService will\nalways return a result within 2 seconds, and at least 90% of the time it\nshould be a success. To many software engineers, this type of promise is\nformally known Service Level Objective (SLO).\n\nYou want to make a promise on wait time, aka latency, because you intuitively\nunderstand that having reasonably low and predictable latency is important.\nPeople can be impatient, especially on the Internet where distractions abound.\nBut how do you get a rigorous and quantified understanding of service latency\nto guide operations? And just as importantly, can that understanding be\nobtained with a reasonable of effort?\n\n### A Tale of Four Dependencies\n\nTo understand CoolSerivce\u2019s latency, we first need to understand how it is\nsetup. CoolService has four dependencies \u2014 A, B, C, and D \u2014 as shown in Figure\n1. Your service needs some data from each of them for every request, and then\nit spends exactly 0.4 seconds to compute the response. The requests for data\nfrom these four dependencies can be issued concurrently, which you do to save\ntime. The four dependencies, like most things in real life, do fail to respond\nevery once in a while. As a responsible developer, you use client-side\ntimeouts to detect such failures. But what should the timeout value be?\n\nFigure 1. Service Diagram\n\n#### Sending a Request == Taking a Chance\n\nEvery time CoolService asks for data from one of its dependencies, it is\ntaking a chance. CoolService (or anybody else with access to these\ndependencies) can measure request latency \u2014 the amount of time it waits on\neach request \u2014 and over many requests it can learn a great deal about these\ndependencies.\n\nStatistics lends many tools to study probabilistic measurements like\nlatencies. The fundamental model it uses to describe such measurements is\ncalled a statistical distribution. The simplest, and most common property of a\ndistribution is the mean. In our case, it can be obtained by adding up all the\nlatencies and dividing that overall time by the total number of requests.\nComputing the mean is cheap and fast. But is it good enough?\n\n#### Why mean latency is not enough\n\nAmazingly, the mean latency of the four dependencies is exactly the same \u2014 0.6\nseconds. You wonder if you could set the timeout to be 1 second based on that\nobservation. Unfortunately, CoolService was not cool with these settings \u2014\nWhile A always returned data in time, B timed out 20% of the time, C timed out\n10% of the time, and D timed out over 25% the time. What\u2019s going on?\n\nIt turned out the four dependencies behave quite differently in terms of\nlatency despite having the same average. Aggressively aggregating latency\nreadings, such as computing the mean, results in a significant loss of\ninsight^[1]. We need more information to fully understand how a service is\nbehaving.\n\n#### Seeing the full picture\n\nLife as a developer would be easy if we simply know how A, B, C, D would\nbehave. For a moment, let's pretend that is the case: A always takes 0.6\nseconds to process a request. B returns results in 0.4 seconds 80% of the\ntime, but for the other 20%, it takes 1.4 seconds. C's processing time is a\nuniformly random number between 1 and 1.1 seconds. Response time from D is\ngoverned by 2.4\u00d7p3, where p is a random number between 0 and 1.\n\nThe relationship between latency and their likelihood for A, B, C, D are shown\nas Value Distributions in Figure 2. By definition, the shaded areas are equal\nto the mean latency of each service, which happen to be the same for all\ndependencies.\n\nFigure 2. Service latency quantile\n\nBy looking at the charts, it is also clear why the 1 second timeout setting\nwould have very different effect on these four services. Can we obtain this\nknowledge without being an oracle?\n\n### Measurements, Quantiles, and Histogram\n\nThe answer\u2014with caveats which we can ignore for now\u2014is yes. If we obtain all\nthe latency measurements as observed by CoolService and sort them for each\ndependency, we can answer any question about latency distribution up to the\nprecision of our measurements. The knowledge we obtain through measuring\nlatencies is illustrated in Figure 3. The theoretic distributions are plotted\nin grey as a reference.\n\nFigure 3. Latency distribution from randomm samples\n\nThe more measurements we collect, the closer they reflect the underlying\nlatency distribution. You can use the slider to increase the number of latency\nmeasurements, and notice how the curves become smoother as they approach the\nreference.\n\n#### Quantiles\n\nWhen we sort and rank all the latency measurements from fastest to slowest, we\ncan answer questions such as \u201cHow long would I have to wait for A if I want to\nhear back at least 9 out 10 times?\u201d You may notice this questions sounds very\nsimilar to SLO, the promise you made to the users. The answers to such\nquestions, by varying the numbers in \u201cm out of n times\u201d, are called the\nlatency quantiles. Generally, we need at least n measurements to calculate all\nthe n-quantiles. Having more measurements therefore allows us to provider\nfiner-grain quantiles, leading to a more accurate description of the\ndistribution and a smoother curve when plotted.\n\nIn software systems, the most common n chosen for n-quantile is 100. The\ncorresponding quantiles are called percentiles, often denoted by the p-\nprefix, such as p50 (the median), p99. Percentiles are so ubiquitous, that\npeople continue to use this name even when they track finer-grain quantiles,\nsuch as p99.9 (technically 999th 1000-quantile) and p99.99 (technically 9999th\n10000-quantile).\n\nHistogram\n\nWhile more measurements could produce a smoother, more filled-out curve, they\nalso take up more space. A high performance server can respond to millions of\nrequests per second, and tracking request latency alone could produce 10s of\ngigabytes of data per hour. Fortunately, most of the valuable questions in\nreal life does not require precise answers. For example, when we set client-\nside timeouts, we probably want to leave some head room above how long the\nservices normally take, so losing a few percent of precision hardly matters.\nThis means we can compress the data we gather greatly by dividing values into\nnon-overlapping ranges, a technique called binning. Using bins to represent\nthe entire range of the latency values and converting a precise reading to a\ncount in a bin are the foundations of a histogram, an approximation of both\nthe empirical measurements we gather and the underlying distribution.\n\nRanges in a histogram could be dynamic and data-dependent, or they could be\npredetermined and static. How to setup ranges is a topic of both academic and\npractical importance. A couple of popular choices include HDRHistogram^[2] and\nDDSketch (blog, paper), but there are many more.\n\nBelow, we plot the quantile-latency relationships of A, B, C, D again, this\ntime as histograms. You probably notice immediately that dots and lines become\nrectangles, indicating the effect of binning. For simplicity, our bins in this\narticle evenly divide up the latency range.\n\nFigure 4. Service latency histograms\n\nReal-world latency histograms can sometimes look rugged and noisy depending on\nthe quantity of latency samples and how they are gathered. Using the latency\ndistribution of service D as an example, you will note that more samples and\nfiner-grain bins lead to a smoother latency histogram.\n\nFigure 5a. Latency histogram from random samples (of D)\n\n### Identifying Important Latency Thresholds\n\nUsually, as in the case of running CoolService, service owners have some idea\nof what kind of latency is good and what is alarming, or unacceptable. We can\nuse color-encoding to intuitively interpret the latencies to reflect those\nranges:\n\nIn the color scale above, green represents latencies within our previously\nconfigured timeout (1 second), yellow shows latencies higher than that but\nstill acceptable (1.6 seconds, considering processing time needed by\nCoolService), and red means high latencies that will violate SLO.\n\nBelow, you can also toggle the colors on and off\u2014 coloring helps us remember\nimportant thresholds. We can also draw a bright warning line to remind\nourselves when the latency becomes unacceptable.\n\nFigure 5b. Latency histogram from random samples (of D)\n\nEquipped with these charts, it's easy to see where your problem lay. D was\nsimply too slow for your goal \u2014 the p90 of D's latency is above 1.7 seconds\nbut you need it to be under 1.6 seconds. Now what?\n\nTo see whether we have a shot at this problem, please read Part 2 of our\nUnderstanding Latency Series: Hedging or Scattering? A Mixed Bag of\nDistributed Latencies.\n\n### Takeaway\n\nTo understand and use latency to your advantage, you need latency samples, a\nrobust histogram implementation, and an aggregation mechanism that allows you\nto harvest the latency histograms for services you care about.\n\n#### Capturing the latency curve... with histograms\n\nAmong software practitioners and researchers, it is becoming more broadly\naccepted that we need to care more than just the mean latency. Typically a few\nmore quantiles are computed at where the service is running, such as p50, p90,\nand p99 of the latency readings. Having a few numbers is already a big\nimprovement over one, and they are still pretty cheap to store. However, there\nisn\u2019t really consensus about how many, and what quantiles of latency are\nadequate or sufficient. This is partly because the percentiles that would\nmatter is highly situational. As shown by our example above, it has a lot to\ndo with the goal of the service and request dispatch strategy, among other\nthings.\n\nSo what should service owners and their users do? Our recommendation is to\ncapture latency with histogram at least at small scale, such as from just one\ninstance. The most notable benefit of a histogram instead of a few quantiles\nis that a histogram preserves the general shape of the curve much better,\nwhich is critical to our understanding of the performance characteristics. It\nalso allows users to delay the decision on what quantiles of the latency\ndistribution are important, and pick those numbers on demand. Developers can\nconstruct multiple hypothetical scenarios (\u201cwhat-ifs\u201d) and (re-)focus on the\npart of the latency curve that matters in each scenario, perform numerical\nsimulations quickly, before making a bigger investment of actual\nimplementation.\n\n### Authors' Note\n\nThis post is authored by Yao Yue with help from Yuri Vishnevsky. Yao Yue is a\nco-founder of IOP Systems. We are a group of full-stack performance experts,\nbuilding performance-specific monitoring and automated benchmarking tools to\nimprove the efficiency and user experience of complex, large-scale production\nsystems.\n\nThanks to Rebecca Isaacs, Jacob Rus, Anthony Roberts, Nick Morgan, and Khawaja\nShams for reading an earlier version of the post and providing feedback. Any\nremaining mistakes are solely the author's responsibility.\n\n### Footnotes\n\n  1. ^ A broader point was made eloquently in this blog post.\n\n  2. ^ We like HDRHistogram so much that we created a base-2 version of the same idea, which is even faster and offers finer-grain control. We have implemented in several languages: Rust, C (header, source), and Javascript.\n\n## Get new posts via email\n\n\u00a9 IOP Systems, Inc.\n\n", "frontpage": false}
