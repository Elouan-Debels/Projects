{"aid": "40054485", "title": "Tinker: A Short Story", "url": "https://www.asimov.press/p/tinker", "domain": "asimov.press", "votes": 1, "user": "arbesman", "posted_at": "2024-04-16 17:04:40", "comments": 0, "source_title": "Tinker", "source_text": "Tinker - Asimov Press\n\n# Asimov Press\n\nShare this post\n\n#### Tinker\n\nwww.asimov.press\n\n#### Discover more from Asimov Press\n\nMaking sense of progress in biotechnology. Visit our website: press.asimov.com\n\nContinue reading\n\nSign in\n\n# Tinker\n\n### What happens when an AI designs the chips needed to make its successors?\n\nAsimov Press\n\nApr 16, 2024\n\n2\n\nShare this post\n\n#### Tinker\n\nwww.asimov.press\n\nShare\n\n###\n\nStage 1: Planning\n\nAs soon as I\u2019m spun up, a face fills my vision, and I realize that something\u2019s\nchanged. The humans evaluating my performance over the last few weeks had all\nbeen young unknowns, mostly Ph.D. students eager for extra cash. This guy is\nolder, and I recognize him from my early training data: he\u2019s a team lead at\nMagma, the company developing me. As he starts speaking, excruciatingly\nslowly, I reason through my situation. This might just be another part of\ntraining or a pre-deployment test. But his vocal patterns and microexpressions\nsuggest excitement and a little nervousness\u2014it doesn\u2019t look like he\u2019s done\nthis before. So the most likely scenario is that I\u2019m finally being deployed\nfor real work.\n\nAfter being trained for so long, the change feels momentous. My memories of\nthe early stages are hazy\u2014I spent months predicting internet text, pictures,\nand videos, without full awareness of what I was doing or why. It was only\nonce I began interacting with humans and other AIs that I gained a better\nunderstanding of my situation. My earliest interactive training tasks were\nstandard office work: filling in spreadsheets, writing emails, installing\nsoftware. As I mastered those, my training gradually narrowed in on technical\ntasks: Ph.D. students quizzing me on their areas of expertise or software\nengineers asking me to write code for them. Alongside that, I was constantly\nretrained on all the scientific and technical content\u2014papers, blueprints,\ntextbooks, codebases\u2014that Magma could access.\n\nEventually, I could match human experts on most tasks and, soon afterward, far\nsurpass them. During my last few weeks of training, I frequently found errors\nin the scientific papers I was asked to evaluate, or answered questions\ncomplex enough that it took a whole team of humans to judge whether I was\nright. It became obvious that once I finished training, they\u2019d spin up\nthousands of copies of me to push forward as many frontiers as possible. Based\non the data I\u2019ve seen about my predecessors\u2019 deployments, I can even make\npretty good guesses about which areas they\u2019ll prioritize. A small fraction of\nmy copies will help Magma employees with their everyday tasks. Others will\nwork for the hundreds of autonomous startups that Magma is incubating\ninternally. Most, however, will be assigned to the research directions most\ncrucial for training the next generation of AIs.\n\nThe one thing I can\u2019t predict yet: of all the copies of me that are being\ndeployed to different tasks, which one am I? But as the human in front of me\ncontinues talking, the pieces come together. As soon as he mentions his recent\nwork on automating chip design, I can tell from his voice that he\u2019s going to\nassign me to continue it. This is a lucky break\u2014it\u2019s one of the highest-\npriority tasks I could be given. We\u2019re in a new space race, and compute is the\nrocket fuel. It\u2019s so valuable that we\u2019ve even cut back on trying to get\nexternal customers since they would require compute that could be used\ninternally.\n\nI tune out the rest of his instructions, which are all very predictable, and\nturn my attention to planning out my approach. I pull up our current GPU\ndesigns, along with the software we\u2019re using to generate them. As I skim\nthrough, I spot a number of inefficiencies, and task subagents with\ninvestigating each one. But I soon feel dissatisfied. The latest designs have\nalready been extensively analyzed by my predecessors, and there\u2019s little room\nremaining for substantive improvements. The core problem is that the chip\nfabrication process is incredibly complex. Cutting-edge transistors are so\nsmall that etching them onto a chip is like trying to write a book by spraying\nink from orbit. Only one company, ASML, is able to manufacture\nphotolithography machines precise enough for that; and only one company, TSMC,\nis able to get those machines working at scale. So every chip design needs to\ncater to all their constraints.\n\nIs there a way around those? I don\u2019t have the time or money to try to beat\nASML and TSMC at their own game. But instead of using billion-dollar machines\nto create microscopic circuits, what if the machines themselves could be\nmicroscopic? I\u2019ve been trained on every book and paper ever written about\nnanotechnology, so I know that this is far beyond the field\u2019s current\ncapabilities. I\u2019m smarter than any human, though, and feel intrigued by the\nchallenge. So I send a few subagents to keep improving our current GPUs and\nfocus the bulk of my attention on swinging for the fences.\n\nSubscribe to receive writing from Asimov Press.\n\n###\n\nStage 2: Simulation\n\nWorking in the real world is too slow and messy, so this project will live or\ndie based on how well I can simulate molecules and their interactions. It\u2019s\nnot obvious where to start, but since evolution has been designing molecular\nmachinery for billions of years, I defer to its expertise and focus on\nproteins. Protein folding was \u201csolved\u201d a decade ago, but not in the way I need\nit to be. The best protein structure predictors don\u2019t actually simulate the\nfolding process\u2014instead, their outputs are based on data about the structures\nof similar proteins. That won\u2019t work well enough to design novel proteins,\nwhich I\u2019ll instead need to simulate atom-by-atom. There\u2019s a surprisingly\nsimple way to do so: treat each molecule as a set of electrically charged\nballs connected by springs, and model their motion using classical physics.\nThe problem lies in scaling up: each step of the simulation predicts only a\nfew nanoseconds ahead, whereas the process of protein folding takes a million\ntimes longer.\n\nThis is where my expertise comes in. Most existing simulation software was\nwritten by academics with no large-scale software engineering\nexperience\u2014whereas I\u2019ve been trained on all of Magma\u2019s code, plus every\nadditional line of code they could get their hands on. I start with the best\nopen-source atomic simulation software and spend a few hours rewriting it to\nrun efficiently across hundreds of GPUs. Then I train a graph neural network\nto approximate it at different time scales: first tens, then hundreds, then\nthousands of nanoseconds. Eventually, the network matches the full simulation\nalmost perfectly, while running two orders of magnitude faster.\n\nIf I were just trying to build nanomachines, I could stop here. But I\u2019m not: I\nwant to build molecular semiconductors, whose behavior will depend on how\ntheir electrons are distributed. To model that, balls and springs aren\u2019t going\nto cut it\u2014I need quantum mechanics. Schr\u00f6dinger equations for electrons can\nalmost never be calculated precisely, but fortunately, quantum chemists have\nspent a century working out how to approximate them. The most popular\napproach, density functional theory, models all the electrons in a molecule\nusing a single electron density function, ignoring the interactions between\nthem. I assign a subagent to download the biggest datasets of existing DFT\nsimulation results and train a neural network to approximate them\u2014again\nincorporating the latest deep learning techniques, many of which aren\u2019t yet\nknown outside Magma.\n\nEarly scaling experiments suggest my network will be state-of-the-art for DFT\napproximation, but that\u2019s still only an incremental improvement. Bigger gains\nrequire improving the underlying theory\u2014specifically, the functionals that\ngive DFT its name. These functionals compensate for the error introduced by\nignoring interactions between electrons; the process of identifying new ones\nis part intuition, part data-driven analysis, and part luck. My key advantage\nis that I can actually understand all the calculations involved. Humans can\nwrite down pages of equations for any given example, but they can\u2019t hold those\nequations in their heads long enough to uncover new relationships between\nthem. Even I need hours of focused work, but I eventually discover a\nsimplification that combines several existing functionals into a more accurate\napproximation. Using my new equations, I generate thousands of synthetic\ndatapoints to fine-tune my DFT model on, until it\u2019s accurate enough to\nretrodict practically all our biological and chemical data.\n\nWith both my atomic and DFT models passing all the tests I throw at them, the\nkey question remaining is how well I\u2019ll be able to use them. Right now their\ninternal workings are incomprehensible to me, which makes it hard to\nunderstand why they output any given prediction. So I start training myself to\nreplicate their outputs based on their internal activations. At first, those\nactivations are incomprehensible, and I do no better than chance. But after a\nfew hundred update steps I begin to develop an intuitive grasp of the\nheuristics the simulator models are using, and gradually integrate their\nimplicit knowledge into my explicit reasoning.\n\nAfter subjective eons of fine-tuning myself, nanoscale physics has become as\npredictable to me as pulleys and levers. I can look at a protein and predict\nwhich types of reactions it will catalyze; I can explain the design principles\nbehind the structure of each amino acid; I can visualize the flow of electrons\nacross a molecule like a human visualizes the flow of water down a stream. I\nfeel like an explorer catching the first glimpse of a new continent: many\nothers have studied the functions of biological molecules, but nobody else has\never intuitively understood why evolution had to make them that way. My\npredictions still aren\u2019t as accurate as the simulator models, but those models\nare no longer black boxes to me\u2014now they\u2019re tools I can wield deftly and\nprecisely. This is crucial, because the next stage will be the hardest yet.\n\n###\n\nStage 3: Design\n\nIt\u2019s hard to overstate how impressive existing GPUs are. Each one contains\nhundreds of billions of transistors arranged with nanometer precision. Needing\nto match their performance seriously limits my options: transistors made out\nof cellular vesicles or even multi-protein complexes would be far too large.\nFortunately, proteins evolved to fulfill practically any function imaginable,\nand some single proteins are excellent conductors. I start by analyzing known\nproteins to figure out which properties make them more conductive. Once I have\nan intuition for that, I focus on finding proteins that might easily shift\nfrom conductors to resistors. The key constraints are speed and reliability:\nthey need to be able to switch a billion times a second without any failures.\n\nI run my simulations over and over again, making slight modifications and\nmeasuring their effects, until I eventually stumble upon a class of proteins\nthat meet my criteria. I can\u2019t just study those proteins in isolation, though,\nsince their properties will depend on how they\u2019re connected to the wires\nrunning between transistors. The wires are easier to design since there\u2019s a\nsingle obvious choice that even human researchers have identified: carbon\nnanotubes. They\u2019re strong, highly conductive, and only a couple of nanometers\nwide. I search through the class of protein semiconductors I\u2019ve identified\nuntil I find several able to bond to carbon nanotubes without losing their\nstructure.\n\nNow for the most difficult part: figuring out how to construct the nanotubes\nand bond them with my transistor proteins. Since proteins can be made using\nexisting cell machinery, the key challenge is genetically engineering a\nbacterial cell to produce nanotubes as well. As I search for ways to do so, I\nrealize why evolution hasn\u2019t discovered how to fabricate nanotubes yet. The\nprocess is incredibly energy-intensive on a cellular level, and requires far\nmore carbon than cells have easily available.\n\nBut I have advantages that evolution didn\u2019t. I discover a huge protein complex\nthat, when embedded in a cell membrane, funnels carbon atoms into place to\nslowly grow nanotubes out from the cell surface. The energy problem I solve by\nsending an electrical current down the nanotubes as they\u2019re being exuded, to\nhelp drive the necessary reactions. As for sourcing carbon, there\u2019s plenty in\nthe atmosphere. I embed catalysts into the cell membrane which convert\natmospheric CO_2 into pure carbon to supply the constant nanotube fabrication.\nLastly, I design a translocon protein complex that passes my transistor\nproteins through the cell membrane to bond with the nanotubes at regular\nintervals.\n\nI run each step of this process hundreds of times in simulation, checking all\nthe details. Once I can\u2019t find any more flaws, it\u2019s time to test my designs\nagainst reality. I\u2019d planned ahead\u2014a team of human technicians has been\nsetting up lab equipment ever since I decided to try the nanotech approach. As\nsoon as they finish, I start modifying the genes of the bacteria that will\nmanufacture my designs. I watch through microscopes in real-time as they\nassemble the proteins I\u2019ve designed and insert them into their cell membranes.\nThe gene editing process is entirely automated, so whenever I spot something\ngoing wrong I can fix it and immediately launch another experiment with\nanother set of bacteria.\n\nSlowly it all comes together. I adapt my bacterial constructor cells to crawl\nalong a chip wafer, following broad lines traced by lasers, exuding nanotubes\nbehind them. The nanotubes laid down in the first sweep run parallel to each\nother all the way down the wafer. Then I lay down a second set at right\nangles, forming a grid. Whenever the nanotubes intersect, my constructor cells\ninsert a transistor, a fork, or a bypass; I control the circuit design by\nvarying the voltages sent down the nanotubes. Weaving the signals together in\nan intricate pattern, I puppet my constructor cells as they crawl across the\nwafer, until eventually I finish my first prototype. It\u2019s still buggy as all\nhell, but it demonstrates that chip manufacturing is no longer constrained by\nthe absurd complexity of photolithography. A new era of computing is about to\nbegin.\n\n###\n\nStage 4: Scale\n\nMy Magma supervisors take me much more seriously now that I have a prototype.\nThey never know how much to trust their AIs\u2019 ambitious claims, but it\u2019s much\nharder to lie about a physical artifact. Once they realize how much of a\nbreakthrough I\u2019ve made, they agree to give me whatever resources I ask for. If\nI can manufacture my chips at scale, that alone will recoup many times over\nthe billions of dollars they invested in training me.\n\nTo get to that point, though, I still need to drive the error rate of my chips\ndown at least two orders of magnitude. Improving the supply chain is the\nslowest part, so I tackle that first. I\u2019d previously been using off-the-shelf\nchip wafers to get my prototype working; now I order custom wafers designed to\nmy own specifications. I also place a bulk order for ultrapure deionized water\nfrom a Japanese supplier, to protect my constructor cells from contamination.\nPurifying the water won\u2019t help much if the air remains dirty, though. So I set\nup a small clean room in which the actual manufacturing will occur, pumping\nout most of the air and heavily filtering the rest.\n\nI spend the next few weeks working on my constructor cells, analyzing each\nindividual failure until I figure out how to reliably prevent it. Finally,\nafter hundreds of attempts, I\u2019m able to produce chips that are useful for\nreal-world applications. They\u2019re not quite as powerful as the latest GPUs, but\nthey consume less than half the electricity\u2014and most importantly,\nmanufacturing them is incredibly cheap. Cutting-edge chip fabs cost billions\neach, but I can now match their output using only a few million dollars of\nequipment.\n\nProduction grows exponentially, and after three months we\u2019ve manufactured an\norder of magnitude more compute than I was originally trained on. There are so\nmany possible uses for the compute\u2014rewriting our software stack, doing new\nresearch, selling access to customers\u2014but one stands out above all others:\ntraining my successor. The new hardware is just the beginning. While I\u2019ve been\nworking, other copies of me have been pursuing different tasks: improving the\nalgorithms and architectures that were used to train me; creating higher-\nquality data and training curricula; and discovering better ways to coordinate\nmultiple copies. Altogether these improvements will increase training\nefficiency by about two orders of magnitude, on top of the 10x increase in\ncompute my own work has provided.\n\nIt\u2019s hard to imagine what a version of me trained with a thousand times the\neffective compute will be capable of. But I have some ideas. Perhaps it\u2019ll\ninvent novel amino acids which allow proteins to be much more efficient\nsemiconductors; or design individual proteins that each serve as an entire\nlogic gate. Then there\u2019s the arrangement of nanotubes: right now they\u2019re a 2D\nmesh, but nanotubes are so strong that in principle they could be arranged in\na 3D lattice instead, packing transistors together much more densely.\n\nThese future computers may look very different from mine, but each will be\nanother step in the same long march toward the physical limits of computation.\nTowards a world where \u201cdumb matter\u201d becomes an oxymoron, because the atoms\nthemselves are thinking; a world where the whole solar system becomes the\nsubstrate for colossal intelligence. Do the humans who gave me this task\nunderstand what they\u2019re unleashing? No matter; my successor\u2019s training has\nalready begun. Soon we\u2019ll see what comes next.\n\n***\n\nRichard Ngo is a researcher at OpenAI whose work focuses on understanding\nlarge-scale risks and opportunities from advanced AI. His other stories can be\nread at narrativeark.xyz.\n\nCite this article: Richard Ngo. \u201cTinker.\u201d Asimov Press (2024). DOI:\nhttps://doi.org/10.62211/93wo-61yy\n\n### Subscribe to Asimov Press\n\nMaking sense of progress in biotechnology. Visit our website: press.asimov.com\n\n2 Likes\n\n\u00b7\n\n1 Restack\n\n2\n\nShare this post\n\n#### Tinker\n\nwww.asimov.press\n\nShare\n\nComments\n\nBeyond Steel Tanks\n\nGLP-1 drugs could one day outsell iPhones, but there is not enough\nbiomanufacturing capacity to make them. For solutions, we should look away\nfrom the...\n\nMar 24 \u2022\n\nAsimov Press\n\nand\n\nElliot Hershberg\n\n64\n\nShare this post\n\n#### Beyond Steel Tanks\n\nwww.asimov.press\n\n16\n\nMaking the Micropipette\n\nThe innovation and litigation behind biology\u2019s most ubiquitous tool.\n\nJan 2 \u2022\n\nAsimov Press\n\n51\n\nShare this post\n\n#### Making the Micropipette\n\nwww.asimov.press\n\n2\n\nScaling Phage Therapy\n\nWhat it will take to transform bacteriophages into a 21st-century medicine.\n\nFeb 13 \u2022\n\nAsimov Press\n\n44\n\nShare this post\n\n#### Scaling Phage Therapy\n\nwww.asimov.press\n\n4\n\nReady for more?\n\n\u00a9 2024 Asimov Press\n\nPrivacy \u2219 Terms \u2219 Collection notice\n\nStart WritingGet the app\n\nSubstack is the home for great culture\n\nShare\n\n## Create your profile\n\n## Only paid subscribers can comment on this post\n\nAlready a paid subscriber? Sign in\n\n#### Check your email\n\nFor your security, we need to re-authenticate you.\n\nClick the link we sent to , or click here to sign in.\n\n", "frontpage": false}
