{"aid": "40044861", "title": "Sniping at web applications to discover input-handling vulnerabilities", "url": "https://link.springer.com/article/10.1007/s11416-024-00518-0", "domain": "springer.com", "votes": 1, "user": "gx1", "posted_at": "2024-04-15 19:49:21", "comments": 0, "source_title": "Sniping at web applications to discover input-handling vulnerabilities - Journal of Computer Virology and Hacking Techniques", "source_text": "Sniping at web applications to discover input-handling vulnerabilities | Journal of Computer Virology and Hacking Techniques\n\nLoading [MathJax]/jax/output/HTML-CSS/config.js\n\nSkip to main content\n\nLog in\n\n## Search\n\n## Navigation\n\n  * Find a journal\n  * Publish with us\n  * Track your research\n\n# Sniping at web applications to discover input-handling vulnerabilities\n\n  * Original Paper\n  * Open access\n  * Published: 12 April 2024\n\n  * (2024)\n  * Cite this article\n\nDownload PDF\n\nYou have full access to this open access article\n\nJournal of Computer Virology and Hacking Techniques Aims and scope Submit\nmanuscript\n\nSniping at web applications to discover input-handling vulnerabilities\n\nDownload PDF\n\n  * Ciro Brandi^1,\n  * Gaetano Perrone^1 &\n  * Simon Pietro Romano ORCID: orcid.org/0000-0002-5876-0382^1\n\n  * 143 Accesses\n\n  * 3 Altmetric\n\n  * Explore all metrics\n\n## Abstract\n\nWeb applications play a crucial role in modern businesses, offering various\nservices and often exposing sensitive data that can be enticing to attackers.\nAs a result, there is a growing interest in finding innovative approaches for\ndiscovering vulnerabilities in web applications. In the evolving landscape of\nweb security, the realm of fuzz testing has garnered substantial attention for\nits effectiveness in identifying vulnerabilities. However, existing literature\nhas often underemphasized the nuances of web-centric fuzzing methodologies.\nThis article presents a comprehensive exploration of fuzzing techniques\nspecifically tailored to web applications, addressing the gap in the current\nresearch. Our work presents a holistic perspective on web-centric fuzzing,\nintroduces a modular architecture that improves fuzzing effectiveness,\ndemonstrates the reusability of certain fuzzing steps, and offers an open-\nsource software package for the broader security community. By addressing\nthese key contributions, we aim to facilitate advancements in web application\nsecurity, empower researchers to explore new fuzzing techniques, and\nultimately enhance the overall cybersecurity landscape.\n\nUse our pre-submission checklist\n\nAvoid common mistakes on your manuscript.\n\n## 1 Introduction\n\nWeb applications are the cornerstone of modern digital interactions,\nfacilitating e-commerce, social networking, and information sharing. However,\ntheir widespread use also makes them prime targets for malicious actors\nseeking to exploit vulnerabilities. The rapid evolution of web technologies\nand the proliferation of complex web applications have rendered the task of\nsecuring them increasingly challenging. One potent weapon in the arsenal of\ncybersecurity professionals is fuzz testing, commonly known as fuzzing.\nFuzzing involves the automated injection of various inputs, often malformed or\nunexpected, into a target system to uncover vulnerabilities. While fuzzing has\nproven remarkably effective in identifying security weaknesses, its\napplication to web environments presents unique challenges that have been\nunderrepresented in the existing literature. This article aims to bridge this\ngap by presenting a comprehensive exploration of fuzzing techniques tailored\nexplicitly to web applications. We delve into the intricacies of web-centric\nfuzzing, highlighting the idiosyncrasies that set it apart from traditional\nfuzzing approaches. Our research investigates the complexities of web\ntechnologies, including input validation, session management, and client-side\ninteractions, shedding light on the specific challenges faced by security\npractitioners in this domain.\n\nFurthermore, we introduce a modular architecture for fuzzing that not only\nenhances vulnerability discovery but also replicates the decision-making\nprocesses of security experts. This architecture allows for the customization\nof fuzzing strategies, enabling users to select and deploy specific modules\naccording to their testing objectives. A Man In The Middle Proxy module stores\ninteractions performed to analyze the target application. A Repeater module\nconverts the collected interactions into template-based requests that can be\nused to initiate a fuzzing session. At the end of the fuzzing session, an\nAnalyzer module converts the fuzzing results into a formal representation of\nthe conditions observed by a security expert and is used to verify the\npresence of a vulnerability. We formalize these conditions by introducing the\n\u201canalyzer observations\u201d concept and show that it is possible to incorporate\nthem into a knowledge base developed using logic programming. To this purpose,\nwe leverage the declarative semantic of Prolog to implement a vulnerability\nknowledge base that converts the analyzer observations into input-handling\nvulnerabilities.\n\nWe demonstrate that our proposed architecture facilitates the reuse of certain\nfuzzing steps, significantly streamlining the security testing process. By\nreducing redundancy, our approach conserves computational resources and\naccelerates the identification of vulnerabilities.\n\nTo foster collaboration and innovation within the security community, we\nprovide an open-source implementation of our software.^Footnote 1 This open\napproach encourages researchers to explore new techniques by modifying\nindividual modules, ultimately promoting advancements in web application\nsecurity.\n\nIn conclusion, our work addresses the unique challenges of web-centric\nfuzzing, introduces an adaptable modular architecture, showcases the\nreusability of specific fuzzing steps, and offers an open-source software\npackage. Through these contributions, we aim to enhance the security of web\napplications, empower researchers to innovate in the field of fuzz testing,\nand fortify the cybersecurity landscape as a whole.\n\nThe remainder of this paper is organized into eight sections. Section 2\nintroduces the security testing process and describes the input-handling\nvulnerabilities analyzed in this work. To understand the inner details of the\nproposed solution, Sect. 3 provides an introduction to Prolog and logic\nprogramming. Section 4 analyzes the state of the art with respect to Fuzzing.\nIn Sect. 5, we show the design and implementation of our rule-based fuzzer by\ndelving into the details of its main building modules. The proposed approach\nis evaluated in Sect. 6 by means of a comparative analysis with the renowned\nZed Attack Proxy open-source web application scanner. In Sect. 7, we discuss a\nfew interesting optimization strategies aimed at enhancing the performance of\nour fuzzer. Section 8 summarizes the obtained results and gives an insight\ninto the prospective evolution of the proposed work.\n\n## 2 Vulnerabilities in web applications\n\nThere is some debate in the literature about the \u201cboundaries\u201d between web\nsecurity testing and web application penetration testing. Some authors define\nsecurity testing as the application of automatic approaches to discover\nvulnerabilities instead of manual methods called penetration tests [1]. A\nrecent study [2] provides a systematic mapping of security testing approaches\nin the literature by extending the scope to penetration testing methodologies,\nsuch as OWASP [3]. According to the semantics proposed by the study, even\nsecurity testing processes can use automatic, manual, and semi-automatic\ntechniques.\n\nIn the business world, the distinction between security testing and\npenetration testing lies in their respective execution stages. Security\ntesting is employed throughout various phases of the development process,\nensuring that security measures are incorporated from the early stages. On the\nother hand, penetration testing is specifically conducted in the production\nphase, allowing for testing in a real environment with real-world\nconfigurations. Throughout the rest of this paper, we will use the term Web\nApplication Penetration Testing (WAPT) to refer to an \u201copaque-box semi-\nautomatic security testing methodology\u201d. This approach combines both automated\nand manual techniques to comprehensively identify vulnerabilities in web\napplications.\n\nIn this section, we describe a Web application penetration testing process by\nalso briefly illustrating three input-handling vulnerabilities studied in this\nwork, namely, SQL injection, path traversal, and cross-site scripting.\n\n### 2.1 Web application penetration testing process\n\nWeb Application Penetration Testing (WAPT) is an \u201copaque-box\u201d process that\nenables companies to discover vulnerabilities in web applications by\nsimulating hacker activities. Here, the \u201copaque-box\u201d refers to the tester\nhaving very little prior knowledge about the target system. The process is\ncomposed of two main phases, as described in the following:\n\n  1. 1.\n\nThe penetration tester attempts to create a \u201cfootprint\u201d of the web\napplication. This includes:\n\n     * Gathering its visible content by exploring public resources as well as discovering information that seems to be hidden.\n\n     * Analyzing the application and identifying its core functions, especially those for which the web application was designed. The purpose is to have a map of all the possible Data Entry Points that the application exposes, which are the main potential flaws that a hacker recognizes in the target application.\n\n  2. 2.\n\nIn the second phase, the penetration tester knows which path to take and\nwhether to focus on the way the application handles inputs or on probable\nflaws in its inner logic. To have a comprehensive understanding of all\npossible application \u201choles\u201d it is, however, important to explore each of the\nfollowing areas:\n\n     * Focusing on the application logic means studying the client-side controls to find a way to bypass them.\n\n     * The stage that involves analyzing how the application handles access to private functionality might be the one to focus on immediately because authentication and session management techniques usually suffer from a number of design and implementation flaws.\n\n     * Input Handling attacking techniques are definitely the most widely deployed since important categories of vulnerabilities are triggered by unexpected user input. The application can be probed by fuzzing the parameters passed in a request. See the next section for insights on this topic.\n\n     * The website can represent an entry point that allows the attacker to have a complete understanding of the target network infrastructure. Defects and oversights within an application\u2019s architecture can often enable the tester to escalate an attack, moving from one component to another to eventually compromise the entire application.\n\n### 2.2 SQL injection\n\nAn SQL injection (SQLi) vulnerability allows a malicious user to manipulate\nthe queries issued by a web application to a back-end database. Exploiting the\nvulnerability exposes sensitive data that cannot be retrieved when the\napplication is used in an ordinary way. In many cases, a malicious user can\nmodify or delete data, thus causing permanent changes to the application and\naltering its behavior. In some extreme situations, an attacker can also\nescalate privileges and compromise the underlying server or other back-end\ninfrastructures.\n\n### 2.3 Union attack\n\nWhen SQL query results are returned as part of application responses, a\nmalicious user can retrieve data from database tables. This technique makes\nuse of an operator called UNION. The UNION operator allows the joining of\nseveral data types through two SELECT statements. It is used to perform an\nadditional SQL query that will show illegitimate data.\n\n### 2.4 Blind SQL\n\nBlind SQL is a type of SQL Injection attack that injects either true or false\nconditions into the underlying database to verify the presence of a\nvulnerability through the observation of differences in the application\u2019s\nresponses. The attack is often used when the web application is vulnerable to\nSQL injection. Still, it only displays generic error messages, and it is not\npossible to confirm the vulnerability by simply observing the response\ncontent. Blind vulnerabilities can be used to access unauthorized data through\nchallenging exploitation techniques.\n\n### 2.5 Path traversal\n\nPath Traversal (PT) is a web application vulnerability that allows attackers\nto access files and directories outside the web application\u2019s root directory.\nThe attack is performed by injecting payloads that allow access to files\nthrough directory traversal sequences using special characters (e.g., \u201c../\u201d)\nor by injecting absolute file paths. By exploiting this vulnerability, it is\npossible to access arbitrary files and directories in the file system of the\nserver, including application source code and critical system configuration\nfiles. This attack is also known as \u201cpoint-blank\u201d, \u201cdirectory traversal\u201d,\n\u201cdirectory climbing\u201d, and \u201cbacktracking\u201d [4].\n\n### 2.6 Cross-site scripting\n\nA Cross-site scripting (XSS) vulnerability may occur when a web application\ndoes not validate a client-side code in the input sent through an attribute of\nan HTTP request. When the input is received and bounced back to the browser as\npart of the response, the client-side code is executed. Typically, attackers\nexploit such a vulnerability to send malicious URLs containing malicious\nclient-side code to a target user. Since the target user could already be\nauthenticated in the vulnerable web application context, the injected\nmalicious code might potentially read their cookies, session token, or other\nsensitive information. Cross-site scripting vulnerabilities can indeed be\neither reflected or stored. As already anticipated, with reflected cross-site\nscripting, the web application bounces back the injected payload in the\ncontent of the HTTP response. The vulnerability is usually found in search\npages, error messages, and whenever the web application needs to send back\ninformation obtained in the received request. With stored XSS, instead, the\ninjected code is stored in the web application itself. When the users visit a\nspecific page of the vulnerable application, the maliciously stored content is\nretrieved, and the code is executed, thus potentially impacting all the users\nof the application.\n\n## 3 Prolog and first-order logic programming\n\nOur rule-based sniper discovers vulnerabilities by leveraging a vulnerability\nknowledge base written in Prolog. In this section, we give a high-level\noverview of logic, with particular reference to first-order programming logic.\nLogic programming was invented in 1974 by Kowalski [5]. He proved that first-\norder logic can be considered a useful and practical programming language with\ntheoretical foundations. This is due to the Horn clauses studied by the\nlogician Alfred Horn (1951) [6]. Horn clauses are the basis of logic\nprogramming as they help implement the so-called \u201clinear resolution with\nselection\u201d function (SL-resolution) [7]. SL-resolution starts with a query and\nresolves subsequently with rules and facts until a negation of the query is\nproved.\n\nProlog was designed as a programming language and has been extensively used in\nseveral research areas, such as molecular biology, design of VLSI (Very Large\nScale Integration) systems, legislation, and options trading [8]. It can be\nused in every domain that can be represented as facts and rules. The language\nallows us to have a logical representation of the context and to easily\ngenerate a program able to solve a problem. Prolog differs from Logic\nProgramming in several aspects. It can be considered a specialization or\nrefinement of a program in Horn Clause Logic, where the selection rule and the\nsearch strategy are fixed [9]. A quick introduction to the Prolog syntax can\nbe found in Matuszek\u2019s course [10].\n\n### 3.1 Logic programming and security\n\nLogic programming finds extensive utility within the security domain. Notably,\nin network security, logic programming serves as a valuable tool for\nevaluating the security level of communication protocols [11, 12].\nFurthermore, Barker and Steve (2000) [13] have adeptly employed logic\nprogramming in the formulation of Role-Based Access Control (RBAC) security\nmodels. Zech et al. (2013) [14] have demonstrated the potential of logic\nprogramming in automating the risk analysis process, thereby enhancing\npenetration testing endeavors with critical risk insights. Moreover, within\nthe realm of software testing, logic programming emerges as a prevalent\napproach for generating test cases, as evidenced by numerous studies\n[15,16,17].\n\nIn another paper published in 2019 [18], Zech et al. expand the realm of test-\ncase generation into the domain of security. The authors showcase the\nadeptness of a knowledge-based system in uncovering vulnerabilities within web\napplications. Their approach involves the introduction of a security problem\nconcept, which serves to model the System Under Test (SUT) using a Domain\nSpecific Language. The concept of a security risk profile is then introduced\nto identify the most critical vulnerability to which the SUT is susceptible.\nTo achieve this, they devise an expert system encompassing both the security\nrisk model and a grammar-based test data generator. To substantiate their\nfindings, the authors engineer a state machine that can effectively identify\nSQL injection (SQLi) and Cross-Site Scripting (XSS) vulnerabilities. Their\ninnovative formulation of the SUT using Specification and Description Language\n(SDL) proves intriguing, displaying several points of intersection with our\nown work. Their framework establishes a correlation between the most salient\nvulnerability based on security risks and the attack payloads directed at the\nsystem. Our approach follows a similar strategy, albeit with notable\ndistinctions. We employ a knowledge-based system to facilitate the oracle\nmodule within the fuzzing process. In contrast, the cited authors employ the\nknowledge base to map threats to the test-data generation procedure.\nAdditionally, our approach takes a semi-automatic route to generate template-\nbased HTTP requests for use in the fuzzing process. This varies from their\napproach of utilizing a web spider, which can be susceptible to issues like\nforbidden errors for authenticated pages and missing links generated\ndynamically before the HTML page rendering phase.\n\n## 4 Fuzzing: state of the art\n\nFuzzing, also known as fuzz testing, is an automated software testing\ntechnique designed to uncover irregularities in a target application by\nfeeding it with invalid, unexpected, or random input data. Throughout the\nexecution of fuzzing, the target application is closely monitored to unveil\nsoftware errors that might indicate the presence of a security vulnerability.\nAlthough the notion of employing random data to trigger anomalies may appear\nsimplistic, it has yielded impressive outcomes by identifying errors in\nvarious software applications.\n\nOver the past two decades, fuzzing has demonstrated its exceptional efficacy\nin unearthing vulnerabilities that often go unnoticed by static software\nanalysis and manual code reviews. This technique has seamlessly integrated\ninto software development practices and is now considered an indispensable\ncomponent for assuring software security.\n\nIn the literature, fuzzing techniques are categorized into three distinct\ntypes: opaque-box, grey-box, and clear-box. The classification hinges on the\nextent of information available about the System Under Test (SUT) before the\nfuzzer\u2019s execution.\n\nOpaque-box fuzzing techniques operate without access to any internal\ninformation about the target system, such as its source code or system\ndocumentation. These methods solely rely on input and output data. Often\nreferred to as data-driven or input/output-driven testing, this approach keeps\nthe intricacies of the system hidden. The testing process revolves around\nobserving the outcomes of the system and pinpointing deviations from specified\nbehavior. Complex fuzzers employ generative or mutational input strategies for\nthis purpose [19].\n\nConversely, clear-box fuzzing techniques differ from opaque-box methods. In a\nclear-box approach, the tester can tap into supplementary information like\nsource code or design specifications, offering insights into the system\u2019s\nbehavior. With this knowledge, it\u2019s feasible to enhance the coverage of the\nsystem during testing, thereby boosting its overall effectiveness.\n\nA grey-box fuzzer occupies a middle ground, adopting a \u201clightweight\u201d approach\nto gather insights about the System Under Test (SUT). It gathers data through\nstatistical analysis, sourced either from a static evaluation of the system or\ndynamic data extracted, for example, from a debugger (McNally et al. [20]).\nThis information might not be exact, but it enables faster execution and\nexploration of input possibilities. Grey-box techniques initiate mutational\ninput exploration with a seed input. When intriguing pathways are activated,\nthe mutated input is scrutinized, prompting further mutations to explore the\nremaining input spectrum. The objective is to enhance behavioral coverage and\nunearth vulnerabilities in the system based on information gleaned from the\nfuzzer [21].\n\nOur work aligns with the opaque-box fuzzing category. It pertains to emulating\nthe actions of a security expert who manually sets up the fuzzing attack. This\nmethod becomes essential when access to the source code is unavailable.\nConsequently, adopting an opaque-box approach offers a viable chance of\ndetecting vulnerabilities without any prior familiarity with the application\nunder examination. This not only enhances the efficiency of vulnerability\ndetection but also diminishes the time needed for a manual security\nassessment.\n\n### 4.1 A generic fuzzing algorithm\n\nValentin J.M. Man et al. (2019) [22] introduced a versatile fuzzing algorithm\ncapable of accommodating the various types of fuzzing methods described above.\n\nAlgorithm 1\n\nGeneric Algorithm of a fuzzer\n\nFull size image\n\nThe algorithm accepts a set of fuzz configurations \\\\(\\mathbb {C}\\\\) and a\ntimeout parameter \\\\(t_{limit}\\\\) as input, producing a set of vulnerabilities\n\\\\(\\mathbb {B}\\\\) as output. It is structured into two phases: a pre-\nprocessing phase, which is executed at the start of a fuzzing campaign, and a\nsubsequent fuzzing process. Notably, some fuzzers may not implement all the\nfunctions within the fuzzing process.\n\nThe pre-processing step involves working with a set of configurations and\nresults in a modified set of configurations tailored to the requirements of\nthe specific fuzzing algorithm being used. During each iteration, the chosen\nconfiguration file is updated using the schedule function, considering both\nthe time that has passed and the preconfigured time limit. This stage tackles\na challenge known as Exploration vs Exploitation [23], which revolves around\nthe allocation of computational resources. It entails finding the right\nequilibrium between exploring new configurations to maximize fuzzer\nperformance and refining the existing configuration to approach the optimal\nsolution.\n\nSubsequently, the scheduling function endeavors to enhance system coverage and\noptimize performance, potentially altering the set of selected configurations.\nFollowing the scheduling phase, the next step is to determine the test input.\nThe inputgen function defines, mutates, or generates a series of test cases to\nbe executed against the program. It assesses whether these executions result\nin security policy violations. Vulnerability detection is achieved by\nconsulting an oracle. Finally, the configuration parameters are adjusted based\non the discovered vulnerabilities, using the information acquired during the\ncurrent iteration through the confUpdate function. The algorithm concludes\nonce the specified number of iterations is completed.\n\n### 4.2 Related works and fuzzing problems\n\nOne of the challenges in fuzzing is selecting the configuration that best\noptimizes performance. Fuzzer designers must analyze the available information\nand make a choice that leads to an ideal outcome, such as discovering the\nmaximum number of vulnerabilities in the shortest possible time. This\nchallenge is often framed as the Exploration vs Exploitation problem, where\nthe decision revolves around whether to explore new configurations or maximize\nthe performance of a chosen configuration. Woo et al. (2013) [24] referred to\nthis challenge as Fuzz Configuration Scheduling. Berry and Fristedt (1985)\n[25] delved into the problem of searching for the optimal configuration. They\nallocated a finite set of resources to various configurations and aimed to\nmaximize the achievable gain. They found that a configuration minimizing the\nconsumption of temporal resources tended to discover a greater number of\nvulnerabilities. Householder et al. (2012) [26] took advantage of information\nfrom a mutational black-box fuzzing execution to define an ideal configuration\nthat maximized vulnerability discovery. They achieved this by modifying the\nCERT Basic Fuzzing Framework (BFF) algorithm, resulting in an 85% increase in\ncrashes.\n\nTable 1 Inspirational contributions from relevant works focused on software\ntesting and their impact on the design of our solution\n\nFull size table\n\nDiscovering vulnerabilities in a web application requires a fuzzer to\neffectively differentiate between expected (normal) behavior and unexpected\n(buggy) behavior. However, distinguishing a bug from a feature can be\nchallenging, leading to what is known as the Oracle Test Problem [27]. An\noracle test is employed to determine whether a test is successful or not,\ninvolving a comparison of the system\u2019s outputs, given a specific input, with\nthe expected outputs during the application\u2019s normal execution.\n\nAlthough these prior studies were focused on defining fuzzing solutions for\nthe software testing domain, they significantly influenced the design of our\nrule-based fuzzer. We incorporate an initial configuration exploration\nstrategy and a detection approach that combines an Oracle test with an expert\nsystem. The Oracle test is designed to identify discrepancies between valid\ninteractions with the application and those of an attacker. These differences\nserve as observations used to query a vulnerability knowledge base containing\na set of rules derived from existing security expertise. This knowledge base\nis constructed by inferring an application\u2019s behavior in the context of\ninjection-type vulnerabilities. Table 1 summarizes the contributions of the\nrelated works mentioned above, which have significantly influenced the design\nof our solution.\n\nOther works deepen the research of fuzzing solutions to discover\nvulnerabilities in web applications. Duchene et al. (2014) [28] used a black-\nbox fuzzer to detect Cross-Site Scripting (XSS) vulnerabilities with the\nassistance of an Oracle test. Their approach utilized a genetic algorithm to\ngenerate fuzz inputs and then compared the Document Object Model (DOM)\nobtained after a cross-site scripting injection with established behavioral\npatterns. They represented the DOM injection as a taint tree. Similarly,\nAppelt et al. (2014) [29] adopted a comparable approach to identify SQL\ninjection vulnerabilities. They employed a mutational approach that\nintercepted communications between the web application and the underlying\ndatabase to uncover these vulnerabilities.\n\nKhalid et al. (2018) [30] employed a predictive white-box fuzzer approach to\ndetect vulnerabilities. In their work, the authors analyzed the input\nvalidation and sanitization processes within an application to predict the\npresence of vulnerabilities. This predictive method combines insights from\nboth static and dynamic code analysis, enhancing the accuracy of vulnerability\nprediction.\n\nCurrent research endeavors are focused on adapting artificial intelligence\nmodels to grey-box fuzzing for the detection of cross-site scripting\nvulnerabilities [31, 32]. However, these approaches are often tailored to\nspecific programming languages and designed to identify a single type of\nvulnerability. Additionally, the grey-box approach can be challenging to\nimplement, especially when the source code of the assessed application is not\navailable.\n\nWhen selecting a web application scanner, it is critical to thoroughly assess\nits features. Each scanner has its unique characteristics and capabilities.\nAltulaihan et al. (2023) [33] have delved into this area, offering\nrecommendations for the best scanners and proposing tool combinations to\nachieve optimal performance. We have drawn inspiration from their guidance in\ndeveloping our fuzzer, which capitalizes on the knowledge provided by security\nexperts to enhance its capabilities. As stated in Sect. 8, there does not\nexist a formalized and comprehensive test suite useful to obtain an objective\nperformance evaluation. Furthermore, the related works do not publicly provide\nthe source code, making a comparison challenging. However, it is helpful to\nsummarize the strengths and limitations of each approach compared to ours.\nTables 2 and 3 show such a comparison. Related works typically integrate\nwhite-box or grey-box approaches with innovative algorithms or AI-based\ntechniques to enhance the discovery of specific classes of vulnerabilities.\nHowever, they often struggle to cover various programming languages or\nvulnerabilities comprehensively. In future endeavors, we aspire to develop a\nformalized benchmark to replicate experiments and facilitate a quantitative\ncomparison with related works.\n\nTable 2 Benchmarks and performance metrics used in related works compared with\nour approach\n\nFull size table\n\nTable 3 Qualitative comparison with related works focused on web applications\n\nFull size table\n\nAs evident from the tables, each work employs different benchmarks and\nperformance metrics. Hence, in Sect. 6, we endeavor to compare our approach\nwith a prominent open-source scanner. Moving forward, we plan to address this\nchallenge by developing a comprehensive benchmark, thereby enhancing the\nevaluation process. Section 8 delves into this issue, offering suggestions to\nrefine the proposed work.\n\nIn summary, our work employs a black-box, rule-based methodology to detect\ninput-handling vulnerabilities. We have devised specific vulnerability rules\ntargeting SQL injection, reflected XSS, and Path Traversal. The vulnerability\ndiscovery process entails validating the integrity of data retrieved from the\napplication and subsequently consulting the expert system to identify any\nbreaches of security policies.\n\n## 5 A rule-based fuzzer to discover input handling vulnerabilities\n\nThe methodology described in Sect. 2 is designed to uncover vulnerabilities in\nvarious areas, including authentication, client-side functionality, and input\nhandling. As demonstrated in [35], fuzzing can effectively identify input\nhandling flaws in web applications. The process of discovering input-handling\nvulnerabilities involves a series of sequential steps. Initially, the tester\nidentifies the entry points of the application. Then, they inject malicious\npayloads to provoke abnormal behaviors. Finally, they conduct further\ninvestigation to confirm the presence of a vulnerability. Fuzzing is a\nvaluable technique for generating anomalies that can reveal the existence of\nflaws and vulnerabilities. However, selecting appropriate input for fuzzing is\na complex task, as the input must be semantically correct to avoid rejection\nby the application. In the context of web applications, input primarily\nconsists of HTTP requests. Nevertheless, a single web application can expose\nmultiple input pages, and each HTML page may contain numerous parameters that\ncan be subjected to fuzzing. Security testers often manually instrument the\ninput by interacting with the tested web application and developing tools that\nreplicate these interactions. In our platform, we have two modules that\naddress this challenge.\n\nThe Interceptor Module is responsible for capturing and storing web\ninteractions that occur during the initial interaction between the security\ntester and the target website. This functionality enables the system to gain\ninsights into how to effectively engage with the target. Meanwhile, the\nRepeater Module replaces the values of request parameters with placeholders\nand generates a generic repeater file that can be employed in the fuzzing\nphase. In the realm of web application penetration testing, testers often lack\ncomprehensive knowledge of the inner workings of the target system.\nConsequently, opaque-box fuzzing becomes the most suitable technique.\nFollowing the methodology for testing web applications, testers devise\nmalicious payloads aimed at uncovering various types of vulnerabilities. For\ninstance, when seeking to identify an SQL Injection vulnerability, testers\ncreate a list of payloads that have the potential to trigger SQL errors. They\nthen initiate the fuzzing process using these payloads and examine the\noutcomes. If errors are encountered, anomalies are scrutinized to validate the\npresence of a vulnerability. This final step can be systematically modeled by\nemploying a rule-based system that incorporates the criteria established by\nsecurity experts to confirm the existence of vulnerabilities.\n\nAs previously mentioned, our primary focus revolves around three distinct\ntypes of injection vulnerabilities: Cross-Site Scripting, SQL Injection, and\nPath Traversal. Nevertheless, it is important to note that our designed\nsolution boasts a high degree of customization and can be readily extended to\ndetect other categories of vulnerabilities. The vulnerability discovery phase\nrelies on expert knowledge that has been formally represented in the form of a\nknowledge base. This approach enhances the efficiency of detection. The\nknowledge base contains information concerning the correlations between\nanomalous responses from the target and potential vulnerabilities, as\ndiscerned by experienced security testers. Essentially, it operates on the\nsame principles as an actual security expert who launches attacks against a\ntarget and assesses for vulnerabilities when anomalous behaviors stemming from\nerrors manifest.\n\nIn the following sections, we will delve into the algorithm, the system\narchitecture, and the interactions that transpire throughout the vulnerability\ndiscovery process.\n\nAlgorithm 2\n\nRule-Based Fuzzer Algorithm\n\nFull size image\n\n### 5.1 Initialization and fuzz-run phases\n\nThe Rule-Based fuzzer algorithm takes a set of configuration files \\\\(\\mathbb\n{C}\\\\) as input:\n\n  * Proxy configuration: a configuration file containing the information needed to setup the MITM (Man In The Middle) proxy module, used to capture HTTP interactions between the user and the target web application;\n\n  * Payload configuration: a configuration file containing information related to the attack strings used during fuzzing to trigger errors in the target web application. In our tests, we define SQLi, PT, and XSS payloads;\n\n  * Observations configuration: a configuration file that specifies the types of observations to be processed by the vulnerability knowledge base. In subsequent sections, we will provide a detailed explanation of these observations when we describe the functionality of the analyzer module in our platform;\n\n  * Relevant strings configuration: a configuration file containing keywords that will be searched for inside the content of an HTTP response. We will explain the role of relevant strings further in the paper.\n\nWhen the algorithm execution is completed, the system outputs a set of\nvulnerabilities \\\\(\\mathbb {B}\\\\).\n\nThe preprocess() function processes the information contained in the\nconfiguration files to define the execution environment. During this phase,\nthe system is instrumented by configuring the proxy parameters, specifying the\npayloads, and defining relevant strings. The schedule() function defines the\nconfiguration to be used within the fuzz-run phase. In our approach, the\nmentioned schedule function does not make any optimal configuration choice in\nthe configuration space. It rather uses the same configuration as the one\ndefined in the preprocessing phase, essentially acting as an identity\nfunction.\n\nThe intrusion and vulnerability detection phases follow the configuration\nphase. The inputgen() function generates test cases using the previously\ndefined payloads. In the context of web applications, these test cases are\nHTTP requests with malicious payloads inserted at specific points within the\nrequest. The intrusion phase occurs during the execution of the intruder()\nfunction, which processes the test cases and attacks the target application\nusing a \u201cSniper attack\u201d, as will be explained in more detail later in the\npaper. As a result of the intruder phase, an output file containing\ninformation about the results of the fuzzing attack is generated.\n\nThe detection phase involves evaluating the results to identify\nvulnerabilities using the analyzer() and eval() functions. The analyzer\nfunction compares the observations obtained from a valid HTTP interaction with\nthose generated during the fuzzing of the web application. An oracle module\nthen checks these observations based on its knowledge base. The Oracle test is\na critical component in this phase as it verifies the existence of specific\nconditions that indicate the presence of a vulnerability. The oracle checks\nfor vulnerabilities by querying a knowledge base containing a set of rules and\nassertions that formalize anomalous observations. The eval() function\nretrieves information from the Oracle tests and determines whether a security\npolicy is being violated. A policy violation occurs when the system concludes\nthat the observed anomalies are indeed indicative of a vulnerability.\n\nFigure 1 presents a high-level overview of the Rule-Based Fuzzer. As shown,\nthe system consists of several independent modules. This modular design\nenables security testers to execute each fuzzing phase individually.\nAdditionally, we employed the system to configure the files using the process\noutlined in Algorithm 3. Another advantage of this modular approach is the\nability to repeat experiments consistently.\n\nFor instance, it is possible to modify configuration files, optimize the\npayload set, enhance the knowledge base, and perform the fuzzing step without\nhaving to redo the previous ones.\n\nFig. 1\n\nRule-Based Fuzzer - High-Level Architecture\n\nFull size image\n\n### 5.2 Proxy\n\nThe Proxy module intercepts and records HTTP interactions generated during the\nuser\u2019s navigation. This acquisition is accomplished using a man-in-the-middle\nproxy positioned between the client and the server. As illustrated in Fig. 2,\nwe have implemented this proxy using Mitmproxy [36], an interactive proxy that\nsupports SSL/TLS for HTTP/1, HTTP/2, and WebSocket. The module is configured\nas a transparent proxy (see Fig. 3), also known as an intercepting proxy,\ninline proxy, or forced proxy. In this mode, it intercepts communications at\nthe network layer (layer 3 of the ISO/OSI stack) without requiring any client-\nside configuration [37]. This approach allows us to intercept HTTP session\nrequests without modifying them. The creation of a custom module that\nincorporates transparent proxy functionality was necessary to select the\ninjection points of interest in an HTTP request. Listing 1 presents a simple\nHTTP POST request, which serves as a typical example of input for one\nadditional component of our architecture, namely the Repeater module.\n\n### 5.3 Repeater\n\nBecause the system operates as an opaque-box fuzzer, it cannot automatically\ndeduce the schema of executed requests. The repeater module enhances\nMitmproxy\u2019s capabilities by inspecting the sent HTTP requests and substituting\nplaceholders for injection points. An injection point might represent any\npertinent attribute of an HTTP request, such as a parameter or a header value.\nThe outcome of the interceptor module is a collection of HTTP requests with\nplaceholders.\n\n### Definition 1\n\n(Placeholder HTTP request) Let:\n\n  * \\\\(\\mathbb{H}\\mathbb{R}\\\\) the set of HTTP requests intercepted by the MITM proxy component;\n\n  * \\\\(\\mathbb{I}\\mathbb{P}\\\\) a set of injection points of \\\\(hr \\in \\mathbb{H}\\mathbb{R}\\\\) defined in the Proxy configuration file;\n\n  * placeholder symbol a fixed-size string, like, e.g., $placeholder$.\n\nA Placeholder HTTP Request is the output of a function that replaces all of\nthe injection points with the placeholder symbol:\n\n$$\\begin{aligned} SetupPlaceholder(hr, \\mathbb{I}\\mathbb{P}), hr \\in\n\\mathbb{H}\\mathbb{R} \\end{aligned}$$\n\nAs highlighted in Fig. 4, the Repeater module outputs a set of data in the\nfollowing form:\n\n  * idFuzz: session fuzzing identifier;\n\n  * Request: a valid HTTP request;\n\n  * Response: a valid HTTP response;\n\n  * Placeholder HTTP request: a valid HTTP request containing placeholders at selected parameter places.\n\nAn example of how the HTTP request in Listing 1 might look after having been\nprocessed by the Repeater is reported in Listing 2.\n\nFig. 2\n\nHigh-Level Architecture - Mitmproxy Interceptor\n\nFull size image\n\nFig. 3\n\nTransparent Proxy\n\nFull size image\n\nThe output of the repeater module will be used in the fuzzing phase.\n\nFig. 4\n\nHigh-Level Architecture - Repeater\n\nFull size image\n\n### 5.4 Intruder\n\nFig. 5\n\nHigh-Level Architecture - Intruder\n\nFull size image\n\nThe Intruder module, whose high-level architecture is described in Fig. 5,\nperforms a fuzzing against the target application with several attack payloads\nthat specialize the placeholder HTTP requests generated by the Repeater\nmodule. The fuzzing session produces a set of HTTP interactions, together with\nadditional information (see Listing 3):\n\n  * idFuzz: session fuzzing identifier;\n\n  * Request: the HTTP request used for fuzzing;\n\n  * Response: the related HTTP response;\n\n  * TypePayload: the type of payload used during the fuzzing session, e.g., SQli, XSS, and PT;\n\n  * Payload the payload string used during the fuzzing session.\n\nThe attack on the system is carried out using one or more payload lists that\ntrigger the aforementioned vulnerabilities. Payloads were selected from\nseveral public lists, as well as by deeply investigating the behavior of\nseveral web application scanners. Collected payloads were fine-tuned during\nthe vulnerability knowledge base generation process defined in Algoritm 3. We\ndecided to retain:\n\n  * 59 PT payloads;\n\n  * 67 SQLi payloads;\n\n  * 15 XSS payloads.\n\nIn Sect. 6, we show that the accuracy depends on both the type and the number\nof payloads used during the fuzzing process. In this work, we use an empirical\napproach to choose the payloads by simply collecting those that generate the\nmost interesting analyzer observations. Payload set optimization is discussed\nin more detail in Sect. 7. To fuzz the application, the Sniper Attack is used.\n\n### Definition 2\n\n(Sniper Attack) Let phr a placeholder HTTP request composed of n injection\npoints ip, and \\\\(\\mathbb{F}\\mathbb{S}\\\\) a set of m fuzzing strings defined\nin the payload configuration file.\n\nFor each ip, generate m requests by replacing the ip placeholder with all m\nfuzzing strings in \\\\(\\mathbb{F}\\mathbb{S}\\\\) while configuring the other\ninjection points with valid values. The sniper attack is performed by sending\nto the target web application the \\\\(n \\times m \\\\) HTTP requests obtained in\nsuch a way.\n\nThis type of attack is beneficial when you want to test several request\nparameters individually for the same type of vulnerability. It is also useful\nto evaluate efficiency, as it is possible to compute the number of performed\nHTTP requests by multiplying the number of placeholders by the number of used\npayloads.\n\n### 5.5 Analyzer\n\nFig. 6\n\nHigh-Level Architecture - Analyzer\n\nFull size image\n\nThe high-level structure of the Analyzer module is reported in Fig. 6. The aim\nof such a component is to convert the collected fuzzing interactions into\ninformation understandable by the Oracle knowledge base. The hearth of the\nvulnerability discovery phase is indeed the observations\u2019 analysis. To confirm\na vulnerability, a security expert typically observes the differences between\na valid HTTP response and the collected fuzzing responses. We replicate the\nsame approach by comparing the performed HTTP interactions. As said, the\nRepeater module extracts the result of a valid HTTP interaction, whereas the\nIntruder module collects the information generated by the fuzzing phase.\n\nAfter collecting the HTTP responses, the Intruder parses them to extract\nrelevant information and check for the presence of potential vulnerabilities.\nWe call such information HTTP interaction features.\n\n### Definition 3\n\n(HTTP interaction features) Let:\n\n  * \\\\( \\mathbb{H}\\mathbb{I} \\\\) the set of HTTP interactions obtained through a fuzzing attack;\n\n  * \\\\(i\\in \\mathbb{H}\\mathbb{I}\\\\) a single HTTP interaction;\n\n  * \\\\(p(i), i \\in \\mathbb{H}\\mathbb{I} \\\\) a parser function of the intruder module.\n\nWe define HTTP interaction features as the set of values returned by\n\\\\(p(i)\\\\).\n\nWe identify the following useful HTTP interaction features:\n\n  * Payload Type: the categorized payload, namely SQLi, XSS, PT, or Normal (for valid HTTP interactions;\n\n  * Payload String: the payload string utilized during the fuzzing attack;\n\n  * HTTP status code: the HTTP response code associated with the sent HTTP request;\n\n  * Content-Length: length in bytes of the obtained HTTP response;\n\n  * Time-elapsed: elapsed time in microseconds from the time the HTTP request is sent to the time the associated response arrives;\n\n  * Body response: the content of the HTTP response.\n\nPayloads are categorized by the searched vulnerability. Categorization is\nperformed to decrease the number of false positives. Such an approach mimics\nthe methodology adopted by security experts to exclude false positives during\ntheir tests. After fuzzing the web application, obtaining the responses, and\nanalyzing a potential vulnerability, they verify if the used payloads may\ntrigger it. During the fuzzing attack, we collect the payload types. The\noracle will select a subset of rules from the knowledge base depending on the\nused payload. The Analyzer compares valid and fuzzing feature interactions and\ngenerates what we define Analyzer Observations.\n\n### Definition 4\n\n(Analyzer Observations) Let:\n\n  * \\\\( \\mathbb{H}\\mathbb{I} \\\\) the set of HTTP interactions obtained through a fuzzing attack;\n\n  * \\\\(i_{fuzz} \\in \\mathbb{H}\\mathbb{I}\\\\) a single HTTP interaction of the fuzzing attack;\n\n  * \\\\(i_{valid}\\\\) a valid HTTP interaction;\n\n  * \\\\( f_{valid} \\\\) the HTTP interaction features of \\\\(i_{valid}\\\\);\n\n  * \\\\(f_{i_{fuzz}}\\\\) the HTTP interaction features of \\\\(i_{fuzz}\\\\).\n\nThen, Analyzer Observations is a set of observations returned by a function of\n\\\\( f_{valid} \\\\) and \\\\(f_{i_{fuzz}}\\\\): \\\\(analyze(f_{valid}, f_{i_{fuzz}})\n\\\\)\n\nThe analyzer observations are the relevant observations commonly investigated\nby a security expert to verify the presence of a vulnerability. They are\ngenerated by comparing the HTTP interaction features of a valid HTTP\ninteraction with those captured during a fuzzing attack.\n\nDuring a security testing activity, security experts look for several strings\nwithin the response to confirm the presence of a vulnerability. For example,\nwhen performing an SQLi attack, they might be checking for the existence of\nstring sequences like the following one: \u201cYou have an error in your SQL\nsyntax\u201d.\n\nAnother relevant analysis is about content length and elapsed time anomalies,\nwhich typically unveil the presence of blind-based vulnerabilities.\n\nBased on the above considerations, the analyzer observations are essentially a\nformalized representation of the security expert\u2019s analysis:\n\n  * Relevant strings: tokens of interest extracted from the body of \\\\(f_{i_{fuzz}}\\\\);\n\n  * Payload string: the payload field of \\\\(f_{i_{fuzz}}\\\\);\n\n  * Payload Type: the payload type field of \\\\(f_{i_{fuzz}}\\\\);\n\n  * HTTP status code: the return code field of \\\\(f_{i_{fuzz}}\\\\);\n\n  * Anomalous Content-Length: a boolean value configured based on the difference between the content-length field values associated, respectively, with \\\\(f_{valid}\\\\) and \\\\(f_{i_{fuzz}}\\\\). If such a difference is above a predefined threshold value, Anmalous Content-Length is set to true, false otherwise;\n\n  * Anomalous response time delay a boolean value configured based on the difference between the elapsed time field values associated, respectively, with \\\\(f_{valid}\\\\) and \\\\(f_{i_{fuzz}}\\\\). If the difference is above a predefined threshold value, Anomalous response time delay is set to true, false otherwise.\n\n### 5.6 Oracle\n\nFig. 7\n\nHigh-Level Architecture - Oracle\n\nFull size image\n\nThe vulnerability discovery phase is accomplished through the Oracle module,\nwhich interacts with a vulnerability knowledge base to verify if the detected\nanomalies can be ascribed to the presence of vulnerabilities.\n\nHence, the vulnerability knowledge base leverages the logic programming\ndeclarative semantics to define a relationship between analyzer observations\nand vulnerabilities. The knowledge base is composed of terms, predicates, and\nrules categorized by the vulnerability type. Figure 7 shows the Oracle\narchitecture. The Oracle module receives the analyzer observations and, based\non the rules contained in the knowledge base, determines whether\nvulnerabilities exist in the tested web application. When considering the\npayload type, a specific subset of rules is activated. This means that the\nknowledge base rules are only triggered if they pertain to a particular\nvulnerability, as verified by checking the payload type used during the\nattack. To formalize these considerations, we introduce the concept of the\nvulnerability knowledge base.\n\n### Definition 5\n\n(Vulnerability Knowledge Base)\n\nLet:\n\n  * \\\\(\\mathbb {T}\\\\) the finite set of vulnerability types (SQL injection, XSS, etc.);\n\n  * \\\\(t \\in \\mathbb {T}\\\\) a vulnerability type; If \\\\(t,q \\in \\mathbb {T}\\\\), then \\\\(t \\ne q\\\\).\n\n  * \\\\(\\mathbb {P} = \\mathbb {T}\\\\) the finite set of payload types;\n\n  * \\\\(\\mathbb{A}\\mathbb{O}\\\\) a set of analyzer observations obtained from the execution of the analyze() function, with:\n\n    * s the status code of \\\\(\\mathbb{A}\\mathbb{O}\\\\);\n\n    * \\\\(r\\\\_s\\\\) the relevant strings of \\\\(\\mathbb{A}\\mathbb{O}\\\\);\n\n    * \\\\(pt \\in \\mathbb {P}\\\\) the payload type of \\\\(\\mathbb{A}\\mathbb{O}\\\\);\n\n    * \\\\(a_{content\\\\_length}\\\\) the anomalous content-length value of \\\\(\\mathbb{A}\\mathbb{O}\\\\);\n\n    * \\\\(a_{response\\\\_time}\\\\) the anomalous response time delay of \\\\(\\mathbb{A}\\mathbb{O}\\\\).\n\nThen:\n\n  * analyzer observations can be modeled as Prolog arguments. We call them vulnerability observations. We define \\\\(\\mathbb{V}\\mathbb{O}\\\\) as the set of vulnerability observations;\n\n  * s, \\\\(r\\\\_s\\\\), pt, \\\\(a_{content\\\\_length}\\\\), \\\\(a_{response\\\\_time}\\\\) values can be modeled as Prolog facts;\n\n  * \\\\(\\mathbb {VKB}\\\\) is a set of vr vulnerability rules;\n\n  * \\\\(\\mathbb {VKB}_{t}\\\\) is a subset of \\\\(\\mathbb {VKB}\\\\) containing all the \\\\(vr_{t}\\\\) vulnerability rules for a given vulnerability type t;\n\n  * \\\\(\\mathbb {VKB}_{t}\\\\) forms a \\\\(\\mathbb {VKB}\\\\) partition^Footnote 2 ;\n\n  * A vulnerability rule vr is a conjunction of a subset of vulnerability observations:\n\n$$\\begin{aligned} vr = vo_{1} \\wedge vo_{2} ... \\wedge vo_{m} \\end{aligned}$$\n\nwhere \\\\( 0 \\le m \\le \\\\) \\\\( \\textbf{card}(AO); vo_{i} \\in\n\\mathbb{V}\\mathbb{O} \\\\);\n\n  * The application is vulnerable to t when a \\\\(vr_{t} \\in \\mathbb {VKB}_{t} \\\\) is true;\n\n  * An application is vulnerable to v when at least one vulnerability rule is true. This property can be modeled as a disjunction of vulnerability rules:\n\n$$\\begin{aligned} is\\\\_vulnerable= vr_{1} \\vee vr_{2} ... \\vee vr_{m}\n\\end{aligned}$$\n\nOracle output (for which a sample excerpt is reported in Listing 4) is\ncomposed of the following fields:\n\n  * idFuzz: session\u2019s fuzzing identifier;\n\n  * Request: the HTTP request used for fuzzing;\n\n  * Response: the HTTP response generated by the fuzzing attack;\n\n  * TypePayload: the payload used during the fuzzing attack;\n\n  * Payload: the specific payload string tested during the fuzzing attack;\n\n  * Observation: an observation reported by the Analyzer;\n\n  * Oracle: the Oracle response in terms of vulnerability rules fired as true.\n\nAs mentioned previously, we have developed a process called the vulnerability\nknowledge base generation process to construct the vulnerability knowledge\nbase. This involved analyzing abnormal behaviors observed during penetration\ntests conducted in academic labs that focused on input-handling\nvulnerabilities. In particular, we solved various PortSwigger labs,^Footnote 3\nwhich are known for their emphasis on input-handling vulnerabilities. Through\nthis process, we continuously refined our knowledge base. Additionally, during\nour training sessions, we further enhanced the attack payloads by fine-tuning\nthem. Algorithm 3 shows the vulnerability knowledge base generation process.\n\nAlgorithm 3\n\nVulnerability knowledge base generation process\n\nFull size image\n\nWe used the Sniper Attack with a predefined set of attack payloads. Failure in\nsolving the lab might occur for two reasons:\n\n  * the absence of a payload capable of triggering the vulnerability;\n\n  * the absence of relevant analyzer observations that reveal the vulnerability.\n\nHence, after a lab failure, we repeated it by tuning both payloads and\nanalyzer observations. At the end of the process, we performed a further\noptimization step to reduce the number of payloads and knowledge base rules as\nmuch as possible. We basically applied the well-known logic minimization [38]\nprocess to obtain the minimal number of rules.\n\nAlso, the payloads were fine-tuned by selecting the minimal set that triggers\nthe vulnerabilities contained within the chosen labs.\n\n## 6 Evaluation\n\nIn this section, we will present the performance metrics that we have defined\nto evaluate the effectiveness of our rule-based fuzzer. These metrics are used\nto assess the performance of our approach on a benchmark consisting of both\nvulnerable and non-vulnerable test cases. We will discuss the limitations and\nstrengths of our approach and compare its performance with a well-known\ninjection vulnerability scanner commonly used in the scientific community to\ndetect vulnerabilities in web applications.\n\n### 6.1 Benchmark and performance metrics\n\nOur rule-based fuzzer was tested on a benchmark containing a set of test cases\nderived from the well-known WAVSEP (Web Application Vulnerability Scanner\nEvaluation Project) benchmark [39]. WAVSEP is a web application vulnerability\nbenchmark designed to help assess the performance of web vulnerability\nscanners. We selected several input-handling test cases from the WAVSEP\nbenchmark, namely:\n\n  * 125 test cases vulnerable to SQLi;\n\n  * 117 test cases vulnerable to PT;\n\n  * 55 test cases vulnerable to XSS.\n\nTo evaluate the fuzzer performance in the absence of vulnerabilities, we added\nadditional test cases to the WAVSEP benchmark. In particular, additional 103\nnon-vulnerable test cases were added. To this purpose, we selected the latest\nversion (version 5.6 at the time of writing) of WordPress, which is a widely\nused Content Management System for building web applications. We designated\ncertain test cases as non-vulnerable because there were no known input-\nhandling vulnerabilities identified for the specific version of WordPress used\nin our testing sessions.\n\nFuzz results are classified into:\n\n  * True Positives \u2014 TP: vulnerable test cases that the rule-based fuzzer properly detects;\n\n  * True Negatives \u2014 TN: non-vulnerable test cases that the rule-based fuzzer rightfully ignores;\n\n  * False Positives \u2014 FP: non-vulnerable test cases that the rule-based fuzzer erroneously classifies as vulnerable;\n\n  * False Negatives \u2014 FN: vulnerable test cases that the rule-based fuzzer erroneously classifies as non-vulnerable.\n\nWe define the following performance metrics:\n\n  * Accuracy \\\\( = \\frac{TP+TN}{TP+TN+FP+FN}\\\\)\n\n  * Precision \\\\( =\\frac{TP}{TP+FP}\\\\)\n\n  * Recall \\\\( = \\frac{TP}{TP+FN}\\\\)\n\nAccuracy, precision, and recall can be defined as follows:\n\n  * Accuracy: is the proportion of the total number of vulnerable and non-vulnerable test cases to the total number of test cases;\n\n  * Precision: is the proportion of vulnerable test cases correctly detected as vulnerable to the total number of test cases detected as vulnerable;\n\n  * Recall: is the proportion of vulnerable test cases correctly detected as vulnerable to the total number of vulnerable test cases.\n\nAccuracy is the most intuitive performance metric and is simply a ratio of\ncorrectly detected test cases to the total number of test cases. A good\nperformance is not a direct consequence of high accuracy. The metric becomes\nrelevant with a uniform set of false positive and false negative tests. High\nprecision refers to a low false-positive rate. Finally, recall, also referred\nto as true rate, defines how many vulnerable test cases were labeled as\nvulnerable among all the vulnerable cases. A high recall value implies a high\nvulnerability detection capability.\n\nIn addition to the performance metrics mentioned earlier, we have also defined\na few additional support metrics to provide a more comprehensive evaluation of\nthe rule-based fuzzer\u2019s behavior. One such metric is efficiency, which\nmeasures the number of HTTP requests required to complete a fuzzing campaign.\nThis metric helps us assess how efficiently the fuzzer performs in finding\nvulnerabilities. Another metric we use is the Number of Fuzzing Payloads\n(NFP), which indicates the level of optimization achieved by the fuzzer during\nthe fuzzing session. This metric allows us to evaluate how effectively the\nfuzzer generates and uses different fuzzing payloads to maximize code coverage\nand vulnerability detection.\n\n### 6.2 Rule-based fuzzer performance\n\nThe rule-based fuzzer was tested on the benchmark, and performance metrics\nwere collected. Table 4 shows the metrics for each vulnerability type. It is\npossible to observe that the fuzzer has high accuracy on SQLi and PT\nvulnerabilities. The number of false positives is higher for XSS. The\nexplanation for this behavior is that the rule-based fuzzer checks if an XSS\nstring payload is reflected without analyzing the so-called \u201creflection\ncontext\u201d [40].\n\nTable 4 Rule-based fuzzer performance for specific vulnerabilities\n\nFull size table\n\nWe address this problem by adding a vulnerability rule that leverages a\nheadless browser to check for the actual execution of JavaScript code, hence\nimproving the discovery of cross-site scripting vulnerabilities, in the same\nway as indicated in [41]. Results with the additional rule are shown in Table\n5.\n\nTable 5 Enhanced rule-based fuzzer performance metrics for XSS vulnerabilities\n\nFull size table\n\n### 6.3 Zed attack proxy comparison\n\nOWASP Zed Attack Proxy (ZAP) [42] is a popular open-source web application\nsecurity scanner that is widely used by professional penetration testers to\nidentify vulnerabilities in web applications. It is considered to be a\nreliable alternative to commercial solutions, as it offers comparable\nperformance. This has been confirmed in Chen\u2019s dynamic application security\ntesting solutions comparison [43], which recognized ZAP as an effective tool\nin this domain.\n\nFigure 8 shows a comparison between the Rule-Based Fuzzer and Zed Attack Proxy\nperformance metrics for each vulnerability type.\n\nFig. 8\n\nComparison between Rule-Based Fuzzer and ZAP\n\nFull size image\n\nThe results show a similar accuracy for SQL injection vulnerabilities and\nbetter accuracy of the rule-based fuzzer for both XSS and PT vulnerabilities,\nalthough with lower precision. Concerning recall, the rule-based fuzzer\nperforms better than ZAP in the case of SQL injection. Figures are comparable\nfor the XSS case. We perform worse than ZAP when it comes to PT\nvulnerabilities. With reference to the above results, we observe that\nprecision and recall could both be improved by tuning the vulnerability rules.\nThe high level of accuracy, on the other hand, proves the effectiveness of the\nchosen payloads, as it would not be possible to trigger the vulnerabilities\nwithout proper payloads. With the current settings, the rule-based fuzzer\noutperforms ZAP in the case of cross-site scripting vulnerabilities. Tables 6\nand 7 show the results in more detail.\n\nTable 6 Comparison between rule-based fuzzer and ZAP: detailed figures\n\nFull size table\n\nTable 7 Accuracy, precision and recall of rule-based fuzzer and ZAP\n\nFull size table\n\n### 6.4 Strengths and limitations of the rule-based fuzzer\n\nThe performance of the rule-based fuzzer depends on the number and type of\npayloads used during fuzzing. A high number of payloads increases the\nvulnerability discovery rate but reduces efficiency. Table 8 shows that\nperformance depends on the number of used payloads (NFP parameter in the\ntable).\n\nTable 8 Rule-Based Fuzzer performance improvements for a greater number of\npayloads\n\nFull size table\n\nThe table shows that performance metrics improve as the number of payloads\nincreases. A vulnerability is discovered when a payload alters the normal\nprocess of the target application by revealing a security flaw. Hence, the\nfuzzer has a better chance of triggering abnormal application behaviors if\nmore payloads are used. However, more payloads lead to a decrease in\nefficiency.\n\nFig. 9\n\nAccuracy-Request Trend\n\nFull size image\n\nTable 9 Accuracy and number of requests versus the number of Payloads\n\nFull size table\n\nFigure 9 and Table 9 show the relative trend between the number of payloads,\nthe accuracy of the scanner, and the number of requests. For example, accuracy\nslightly increases if the number of payloads is equal to 143, but the number\nof executed requests doubles compared to when we leverage just 77 payloads.\nThe increase in accuracy is 6% in such a case. The extensibility of the rule-\nbased fuzzer allows changing the number of payloads by simply adding attack\nstrings in the configuration file of the Analyzer module. The identification\nof the most suitable trade-off between the number of payloads and the\nscanner\u2019s accuracy does indeed represent an integral part of the rule-based\nfuzzer\u2019s instrumentation process.\n\nGenerally, it is important to underline a few intrinsic limitations of a rule-\nbased approach. The vulnerability knowledge base has been implemented through\nthe process described in Sect. 5. It is clearly possible to extend the process\nto other training scenarios and increase the system\u2019s performance by adding\nnew rules and assertions. The approach depends on the effectiveness of the\nknowledge base and requires a continuous update. As new attacks are devised on\na daily basis, keeping the pace of required updates to the knowledge base can\nbecome very challenging. Another problem associated with the quality of the\ninformation contained in the knowledge base concerns the selection of \u201cgood-\nenough\u201d fuzzing strings for the triggering of the entire set of target\nvulnerabilities. With reference to the points raised above, in the conclusions\nsection, we will propose some ideas to overcome the \u201cstatic nature\u201d of the\nrule-based approach.\n\n## 7 Optimization strategies\n\nThe previous section analyzes the performance of the rule-based fuzzer and\nhighlights the limitations of this approach. In this section, we will first\ndiscuss a couple of general performance enhancement criteria. Then, our focus\nwill shift to the specific topic of payload optimization. Finally, we will\nintroduce a few additional performance metrics.\n\n### 7.1 Performance enhancements\n\nAs observed in Sect. 6.4, the performance metrics improve when a larger number\nof payloads are used. This behavior can be attributed to the use of more\n\u201cfeature classes\u201d when a greater number of payloads are employed. We explain\nthe concept of \u201cfeature class\u201d in Sect. 7.2. However, increasing the number of\npayloads does not necessarily increase the precision metric, as it is\ninfluenced by the number of false positives. To mitigate the occurrence of\nfalse positives and reduce incorrect results, it is possible to implement\nsimple criteria that effectively decrease their number.\n\nPerformance Enhancement Criterion 1:\n\nCompare the observations obtained by sending a valid payload with those\nobtained with the fuzzed payload.\n\nThis criterion aims to reduce the number of false positives by analyzing the\nresponse content, particularly for rules involving the analysis of strings.\nFor example, consider a Path Traversal (PT) rule that aims to detect the\npresence of the \u201croot\u201d string in the response content, which would indicate\nthat the web server may have executed the \u201c/etc/passwd\u201d operating system\ncommand. However, it is important to note that the \u201croot\u201d string may also\nappear in the response content, especially within code comments, as\nillustrated in Listing 5.\n\nIf the knowledge base rules check for the presence of certain words in the\nobserved valid response, it can help reduce the number of false positives.\nHowever, it would be more effective if the observations did not contain the\nwords used to trigger a vulnerability in the first place. Continuing with the\nprevious example, the \u201cpasswd\u201d file often contains the word \u201csbin\u201d in its\ncontent. This word is very specific and less likely to be found in source code\ncomments or other non-relevant areas. This forms the basis for the second\ncriterion.\n\nPerformance Enhancement Criterion 2:\n\nWhen selecting a string presence in the content of an HTTP response to trigger\na rule and there\u2019s a choice between two different words, opt for the more\ndiscriminant word, i.e., the one that minimizes the chances of being found in\nthe content of a valid request.\n\nIn Sect. 5.5, we emphasize the importance of considering the payload type to\ntrigger the appropriate rules. This approach can also be extended to analyze\nthe type of payloads being used and prevent the activation of irrelevant\nrules. For instance, when employing SQL payloads to detect SQL injection\nvulnerabilities, it is beneficial to distinguish between error-based and time-\nbased payloads.\n\nThe rules within the oracle can evaluate conditions depending on the specific\nsubcategory. For instance, the \u201canomalous time\u201d check would not be triggered\nunless a \u201ctime-based\u201d SQL payload is used.\n\nPerformance Enhancement Criterion 3:\n\nWhenever possible, employ more fine-grained payload types and minimize the\nnumber of activated rules.\n\nBy implementing these straightforward rules, it becomes possible to enhance\nthe performance of the rule-based fuzzer. In the benchmark detailed in Sect.\n6, we can observe the improved statistics in Table 10, clearly illustrating\nthe desired performance enhancement.\n\nTable 10 Increased rule-based fuzzer performance with optimization criteria\n\nFull size table\n\n### 7.2 Payloads optimizations\n\nAs outlined in Sect. 6.4, augmenting the number of payloads correlates with\nimproved performance metrics. This relationship stems from the payloads\u2019\ncapacity to elicit the observations necessary for identifying vulnerabilities.\nNevertheless, it is feasible to establish optimization techniques aimed at\nreducing the number of payloads while maintaining the same level of\nvulnerability detection performance. In this section, we expound on these\nstrategies, which can be categorized into two groups:\n\n  1. 1.\n\nPayload Feature Optimization: these optimizations focus on enhancing the\neffectiveness of individual payloads by refining their features.\n\n  2. 2.\n\nOptimization through Web Application Enumeration: these optimizations revolve\naround the process of enumerating a web application to pinpoint\nvulnerabilities more efficiently.\n\n#### 7.2.1 Payload feature optimization\n\nIn a broader context, this category of optimization strategies can be\nsubdivided into the following more specific subcategories:\n\n  * Semantic Payload Generation: instead of generating a large number of random payloads, focus on generating payloads with specific semantic features that are more likely to trigger vulnerabilities. For instance, for SQL injection testing, you can create payloads that contain common SQL keywords or syntax patterns known to be effective in revealing vulnerabilities;\n\n  * Dynamic Payload Generation: develop a dynamic payload generation mechanism that adapts payloads based on the application\u2019s responses. Start with simple payloads and gradually increase complexity as you gather more information about the application\u2019s behavior. This can help reduce the number of payloads needed while maintaining effective testing;\n\n  * Payload Mutation Strategies: implement strategies for mutating existing payloads intelligently. Rather than creating entirely new payloads, modify existing ones by changing specific parts of the payload. This can lead to a more efficient use of payloads.\n\nThe best payload set aims to minimize the number of payloads having different\nfeatures. Through the proven experience of web application penetration\ntesters, we try to define several features for each vulnerability. However,\nthis approach is generic and can be further extended to any type of feature\nclassification and improved by adding further features.\n\nThe general observation is that each payload \u201cstimulates\u201d the web application\nin various ways, and each payload possesses specific characteristics that can\nbe shared with other payloads. For instance, an SQL injection can be executed\nusing a \u2018sleep\u2019 instruction, a single quote character, or a double quote\ncharacter. Moreover, the payload might contain the valid payload string or\nreplace it. Each condition can be considered a feature or characteristic of a\npayload, and these features can have categorical values. As a result, it is\nreasonable to expect that two payloads with identical characteristics will\ntrigger the same anomalous observations. We can, hence, define features for\npayloads that allow us to categorize them into similar classes. A \u2018feature\nclass\u2019 consists of payloads that share the same set of features.\n\nPayload Optimization Criterion 1:\n\nFor each feature class, extract a single payload. The optimal payload set\nshould include one payload from each feature class. The goal of the best\npayload set is to minimize the number of payloads with different features.\n\nWe draw from the extensive experience of web application penetration testers\nto define numerous features for each vulnerability. However, this approach is\nversatile and can be expanded to encompass any feature classification while\nalso benefiting from the addition of further features.\n\nXSS features For cross-site scripting vulnerabilities, it is possible to\nidentify several features. To trigger the vulnerability, the injected payload\nmust be reflected within an executable JavaScript code without causing errors.\nThe valid payload is contingent upon the reflection context, which is the\nlocation where the input is reflected. The payload that \u2018triggers\u2019 the\nvulnerability is also influenced by the input sanitization methods employed by\nthe web application to mitigate web attacks. In fact, for each defensive\nmechanism, it is possible to modify the payload to circumvent it [44]. Table\n11 presents the list of analyzed features.\n\nTable 11 XSS features\n\nFull size table\n\nSQLi features\n\nFor SQL injection vulnerabilities, it is essential to categorize the input\ntype. For instance, error-based or blind-based SQL injection attacks aim to\ngenerate an SQL syntax error, which would either display an SQL error message\nor return an error status code. In contrast, a time-based SQL injection attack\nattempts to execute an SQL \u2018sleep\u2019 command, creating a time delay in the web\napplication that can be analyzed. The oracle can be fine-tuned by\ndistinguishing between payload types (SQL or time-based SQL). Specifically, if\nthe payload used is not time-based, a response delay will not be evaluated as\nan SQL injection condition.\n\nAnother critical feature to consider is the type of quotation mark used in the\nbackend SQL queries. When a web application is vulnerable to SQL injection,\nthe injected payload is inserted into an SQL query that retrieves data from\nthe database. Typically, the input is injected into a \u2018WHERE\u2019 clause and can\nbe enclosed in quotation marks, such as single or double quotes. As previously\nmentioned, an error-based SQL injection vulnerability can be exposed by\ncausing an SQL syntax error in the web application. Therefore, if the injected\npayload is enclosed in double quotes, a payload that triggers an SQL syntax\nerror must contain a double quote^Footnote 4.\n\nAs this information is not usually deducible through information\ngathering,^Footnote 5 a comprehensive payload set should encompass all\npossible quotation marks. Additionally, an important feature to consider is\nthe underlying Database Management System (DBMS) in use. Each DBMS employs a\nspecific SQL dialect with different instructions. For instance, PostgreSQL\nuses the pg_sleep() instruction to induce sleep, while MySQL uses SELECT\nSLEEP(). For each dialect, it is possible to define multiple payloads. If N\nrepresents the number of dialects under examination, the number of payloads\nrequired to cover all possible databases is \\\\(M \\times N\\\\). Payload\nreduction can be achieved by employing enumeration techniques as described in\nSect. 7.2.2.\n\nTable 12 SQLi features\n\nFull size table\n\nAs with XSS vulnerabilities, the analysis of the sanitization techniques\nemployed by the web application is crucial for SQL injection vulnerabilities\n[45]. Table 12 provides a summary of valuable features for SQL injection\nvulnerabilities.\n\nPT features\n\nTo trigger a path traversal vulnerability, the injected payload must point to\na readable and existing file. This type of vulnerability occurs due to flaws\nin a web application that allow arbitrary file reading. The specific file that\nconstitutes a valid payload depends on the underlying operating system and web\nserver interpreter. For instance, a valid file for PHP might be login.php,\nwhereas for a Windows system, it could be c:\\Windows\\System32\\Drivers\\etc\n\\hosts. In the context of path traversal vulnerabilities, the presence or\nabsence of sanitization techniques that block special characters such as dots,\nbackslashes, and slashes are noteworthy features. These features can\nsignificantly influence the variety of payloads needed to comprehensively test\nfor this vulnerability [46]. Table 13 provides an overview of the analyzed\nfeatures.\n\nTable 13 PT features\n\nFull size table\n\n#### 7.2.2 Optimization through web application enumeration\n\nPayload reduction through Web Application Enumeration can happen in different\nways:\n\n  * Application Profiling: before conducting the fuzzing process, profile the target application thoroughly to identify its attack surface, input vectors, and potential vulnerabilities. This information can guide the selection of payloads, reducing the need for a large number of generic payloads;\n\n  * Input Enumeration: enumerate and catalog all possible input points in the web application, including input fields, URLs, and headers. Prioritize testing these inputs over less critical ones;\n\n  * Response Analysis: analyze the application\u2019s responses to initial requests to gain insights into its behavior. Identify patterns or anomalies that can help you craft more targeted payloads.\n\nBy incorporating these strategies into the fuzzing process, one can\nsignificantly reduce the number of payloads required while maintaining a high\nlevel of effectiveness in detecting vulnerabilities. This optimization can\nlead to more efficient and focused testing, especially in situations where\ngenerating a massive number of payloads is resource-intensive or time-\nconsuming.\n\nThe effectiveness of payloads is also contingent upon the specific web\napplication environment. For example, a path traversal payload designed for a\nWindows-based system would be ineffective against a Linux-based one. This\nunderscores the importance of tailoring payloads to suit the target\nenvironment. It is worth noting that web application scanners are generally\nnot optimized for environment analysis and payload selection. Instead, they\ntypically employ a predefined set of payloads intended to trigger known\nvulnerabilities. Recent works [47, 48] have demonstrated the feasibility of\nformalizing penetration testing activities in terms of hacking goals. We have\nintroduced an algorithm and a generic framework for integrating various\nactions and attacks, enabling the discovery of web vulnerabilities through an\noffensive approach. An effective method for improving web application security\nis to include enumeration actions before the fuzzing phase, utilizing the\nmethodology we have proposed.\n\nThrough this approach, it is possible to infer the web application environment\nand consequently reduce the number of payloads. Once the enumeration phase is\ncompleted, attacks can be launched against the web application.\n\nXSS footprinting\n\nFor cross-site scripting vulnerabilities, it is crucial to verify two\nconditions:\n\n  1. 1.\n\nthe sent payload is reflected.\n\n  2. 2.\n\nthe \u201creflection context\u201d is an executable one.\n\nIn [40], we introduced the concept of the reflection context and demonstrated\nits utility in optimizing the number of requests. A similar approach can be\napplied in this work to identify cross-site scripting vulnerabilities. By\nanalyzing where the input is reflected when a request is sent, it becomes\npossible to reduce the number of payloads to just a single one. However,\nseveral requests may need to be sent to properly understand the reflection\ncontext.\n\nSQL and PT footprinting\n\nIf the used DBMS is discovered, it is possible to divide the number of\npayloads by excluding those that target other SQL dialects. Information\nConflict of interest vulnerabilities can reveal the used DBMS and facilitate\nthis optimization. For example, the presence of a PHPinfo file can disclose\nessential information like the underlying DBMS and the operating system in\nuse. Therefore, by detecting the used DBMS, it is possible to exclude\nunnecessary payloads.\n\nIn the case of path traversal vulnerabilities, it is crucial to identify\nunderlying systems, such as the operating system or the webserver running the\nweb application. This knowledge allows for the exclusion of payloads designed\nto exploit vulnerabilities in different systems. For instance, if the\nunderlying operating system is Linux, a payload attempting to exploit a path\ntraversal vulnerability by reading the content of\nC:\\Windows\\System32\\drivers\\etc\\hosts is ineffective. Therefore, discovering\nthe underlying system permits payload optimization without compromising\nperformance metrics.\n\n#### 7.2.3 Payload optimization: summary results\n\nTables 14, 15, and 16 display the quantity of payloads employed by ZAP, as\nwell as by our rule-based fuzzer, both with and without the payload number\nreduction strategies detailed in Sect. 7.2.2.\n\nTable 14 Number of payloads for XSS detection\n\nFull size table\n\nTable 15 Number of payloads for SQL injection detection\n\nFull size table\n\nTable 16 Number of payloads for PT detection\n\nFull size table\n\nTable 17 Comparison table: language support and vulnerability coverage\n\nFull size table\n\nAs can be observed, the optimization strategies resulted in a 13% reduction in\nthe number of payloads used for XSS detection, a 56% reduction in the number\nof payloads used for SQL injection detection, and a remarkable 91% reduction\nfor path traversal detection. This indicates that the non-optimized rule-based\nfuzzer\u2019s input payload set contained numerous inputs with similar features, as\ndiscussed in Sect. 7.2.\n\n### 7.3 Additional performance metrics\n\nIn order to evaluate the effectiveness of the optimization strategies and\nenhance the assessment of scanners, it is possible to include additional\nperformance metrics. As noted earlier, improving a scanner\u2019s performance\ninvolves reducing the number of payloads while maintaining or even enhancing\nits effectiveness. Two additional vital metrics are the scanner\u2019s\nvulnerability coverage and its compatibility with various source code\nlanguages. Notably, many scanners in the literature are specialized for a\nsingle vulnerability type or can only be applied to web applications developed\nin specific programming languages [49].\n\nIn summary, two new performance metrics can be defined:\n\n  * Vulnerability Coverage (VC): VC measures the number of vulnerabilities that a scanner is capable of identifying;\n\n  * Language Support (LS): LS assesses the range of programming languages supported by the scanner.\n\nWe conducted an analysis of vulnerability coverage and language support based\non 38 studies included in Zhang et al.\u2019s 2021 survey [49]. The results are\npresented in Table 17.\n\nFigure 10 illustrates that 57,9% of the scanners mentioned in the survey\u2019s\nreferences primarily focus on a single vulnerability, while only 18,4% of them\ncover all three different types of vulnerabilities. Among these, only two also\nhappen to be language-independent (see Fig. 11).\n\nFig. 10\n\nScanners and vulnerability coverage\n\nFull size image\n\nFig. 11\n\nVulnerability coverage per language\n\nFull size image\n\nUnfortunately, many works do not make their source code publicly available,\nmaking it challenging to replicate experiments or build upon the authors\u2019\napproaches. In contrast, we have made our source code publicly accessible to\nfacilitate improvement and extension of our approach Table 18 presents a\nsummary of the key performance metrics, including Accuracy, Precision, Recall,\nand Number of Payloads, attained by our fuzzer before and after implementing\nthe discussed optimizations.^Footnote 6\n\nTable 18 Comparison after the application of optimization steps\n\nFull size table\n\n## 8 Conclusions\n\nIn this work, we have introduced a rule-based fuzzer designed to detect web\napplication injection vulnerabilities, with a particular focus on cross-site\nscripting, SQL Injection, and Path Traversal. It is worth noting that our\nproposed approach can be adapted to identify various types of input-based\nvulnerabilities. We conducted a comprehensive performance analysis comparing\nour system with Zed Attack Proxy (ZAP), one of the most widely used web\napplication security scanners. Our results demonstrate that the rule-based\napproach can yield comparable outcomes to those achieved with ZAP.\nAdditionally, we discussed how optimizing payloads, observations, and rules\ncan enhance the performance of our rule-based fuzzer.\n\nWe have identified the number of used payloads as a crucial performance metric\nand have shown that it can be reduced through specific optimization criteria.\nNevertheless, there is room for further payload reduction through the\nexploration of alternative approaches.\n\nAs part of our future work, we intend to concentrate on devising a payload\noptimization strategy to maximize vulnerability coverage while minimizing the\nnumber of payloads used. Drawing from past experience [40], we believe that an\napproach rooted in Reinforcement Learning (RL) can be instrumental in\nachieving this objective. With RL, we can train an agent to detect all\nvulnerabilities within a predefined environment while minimizing payload\nusage.\n\nIn recent years, Large Language Models (LLMs) have made substantial\ncontributions to natural language processing (NLP) [87]. Several contemporary\napproaches are investigating the security implications of generating source\ncode using such models [88, 89]. They aim to determine whether these models\ncan be leveraged to identify and rectify security vulnerabilities in source\ncode [90,91,92]. While these approaches typically operate in a white-box\ncontext, there is potential for our work to benefit from the adoption of LLMs.\nAs elaborated in Sect. 7, optimization criteria encompass tasks such as\napplication profiling, input enumeration, and response analysis. These are\nareas where Natural Language Processing techniques could be applied\neffectively. In future research, we plan to explore this avenue to enhance our\nwork.\n\nThe results we have presented in this work are contingent on the test suite we\nused. Although some research endeavors have attempted to gauge the\neffectiveness of test suites for evaluating web scanners, no formalized test\nsuite that guarantees complete vulnerability coverage of a web scanner [49]\ncurrently exists. In future endeavors, we intend to delve into this issue,\naiming to formalize performance metrics for web application vulnerability test\nsuites. This will involve delineating the limitations of existing platforms\nand proposing enhanced alternatives.\n\nOur forthcoming efforts will also be directed toward enhancing the Oracle\nmodule by introducing assertions and vulnerability observations. In this\ncontext, an evolved rule-based approach may harness a reinforcement learning\nmodel to automatically generate rules. The modular architecture of our\nplatform facilitates the collection of requests from a training environment,\nreplication within a specific test environment, and performance evaluation.\n\nWe are optimistic that our work will serve as a valuable resource for security\nresearchers seeking to explore novel fuzzing approaches for identifying input-\nhandling vulnerabilities.\n\n## Notes\n\n  1. https://github.com/NS-unina/Rule-Based-Fuzzer.\n\n  2. https://en.wikipedia.org/wiki/Partition_of_a_set.\n\n  3. https://portswigger.net/.\n\n  4. For example, if the <username> payload is used to create the following SQL query: SELECT * from USER WHERE name = \"<username>\", sending test\" as the payload would generate an SQL query with three double quotes, causing a syntax error that can be analyzed to discover the vulnerability.\n\n  5. The quotation mark can be ascertained by inspecting the backend source code. This information may be obtained through a vulnerability that exposes the source code or by employing a grey-box fuzzing approach. In such cases, the payload set can be streamlined by exclusively utilizing the correct quotation mark.\n\n  6. https://github.com/NS-unina/Rule-Based-Fuzzer/tree/master.\n\n## References\n\n  1. Singh, N., Meherhomji, V., Chandavarkar, B.: Automated versus manual approach of web application penetration testing. In: 2020 11th International Conference on Computing, Communication and Networking Technologies (ICCCNT). IEEE, pp. 1\u20136 (2020)\n\n  2. Aydos, M., Aldan, \u00c7., Co\u015fkun, E., Soydan, A.: Security testing of web applications: a systematic mapping of the literature. J. King Saud Univ. Comput. Inf. Sci. 34, 6775\u20136792 (2021)\n\nGoogle Scholar\n\n  3. The owasp testing guide (2022). [Online]. Available: https://owasp.org/www-project-web-security-testing-guide/. Accessed 01 Feb 2021\n\n  4. Path traversal. https://owasp.org/www-community/attacks/Path_Traversal. Accessed 12 Feb 2022\n\n  5. Kowalski, R.: Predicate logic as programming language. In: IFIP Congress, vol. 74, pp. 569\u2013544 (1974)\n\n  6. Horn, A.: On sentences which are true of direct unions of algebras. J. Symbol. Log. 16(1), 14\u201321 (1951)\n\nArticle MathSciNet Google Scholar\n\n  7. Kowalski, R., Kuehner, D.: Linear resolution with selection function. Artif. Intell. 2(3\u20134), 227\u2013260 (1971)\n\nArticle MathSciNet Google Scholar\n\n  8. Apt, K.R.: Logic programming. In: Handbook of Theoretical Computer Science, Volume B: Formal Models and Sematics (B), vol. 1990, pp. 493\u2013574 (1990)\n\n  9. Kok, J.N.: Specialization in logic programming: from horn clause logic to prolog and concurrent prolog. In: Workshop/School/Symposium of the REX Project (Research and Education in Concurrent Systems). Springer, pp. 401\u2013413 (1989)\n\n  10. A concise introduction to prolog. https://www.cis.upenn.edu/matuszek/Concise. Accessed 03 Mar 2021\n\n  11. Carlucci Aiello, L., Massacci, F.: Verifying security protocols as planning in logic programming. ACM Trans. Comput. Log. 2(4), 542\u2013580 (2001)\n\nArticle MathSciNet Google Scholar\n\n  12. Alberti, M., Chesani, F., Gavanelli, M., Lamma, E., Mello, P., Torroni, P.: Security protocols verification in abductive logic programming: a case study. In: International Workshop on Engineering Societies in the Agents World. Springer, pp. 106\u2013124 (2005)\n\n  13. Barker, S.: Data protection by logic programming. In: International Conference on Computational Logic. Springer, pp. 1300\u20131314 (2000)\n\n  14. Zech, P., Felderer, M., Breu, R.: Security risk analysis by logic programming. In: International Workshop on Risk Assessment and Risk-Driven Testing. Springer, pp. 38\u201348 (2013)\n\n  15. Vemuri, R., Kalyanaraman, R.: Generation of design verification tests from behavioral VHDL programs using path enumeration and constraint programming. IEEE Trans. Very Large Scale Integr. Syst. 3(2), 201\u2013214 (1995)\n\nArticle Google Scholar\n\n  16. Denney, R.: Test-case generation from prolog-based specifications. IEEE Softw. 8(2), 49\u201357 (1991)\n\nArticle Google Scholar\n\n  17. G\u00f3mez-Zamalloa, M., Albert, E., Puebla, G.: Test case generation for object-oriented imperative languages in CLP. Theory Pract. Logic Program. 10(4\u20136), 659\u2013674 (2010)\n\nArticle MathSciNet Google Scholar\n\n  18. Zech, P., Felderer, M., Breu, R.: Knowledge-based security testing of web applications by logic programming. Int. J. Softw. Tools Technol. Transf. 21(2), 221\u2013246 (2019)\n\nArticle Google Scholar\n\n  19. Boehme, M., Cadar, C., Roychoudhury, A.: Fuzzing: challenges and reflections. IEEE Softw. 38(3), 79\u201386 (2021)\n\nArticle Google Scholar\n\n  20. McNally, R., Yiu, K., Grove, D., Gerhardy, D.: Fuzzing: the state of the art. Defence Science and Technology Organisation Edinburgh (Australia), Technical report (2012)\n\n  21. Pham, V.-T., Bohme, M., Santosa, A., Caciulescu, A., Roychoudhury, A.: Smart greybox fuzzing. IEEE Trans. Softw. Eng. 47(9), 1980\u20131997 (2021)\n\n  22. Man\u00e8s, V.J.M., Han, H., Han, C., Cha, S.K., Egele, M., Schwartz, E.J., Woo, M.: The art, science, and engineering of fuzzing: a survey. IEEE Trans. Softw. Eng. 47, 2312\u20132331 (2019)\n\nArticle Google Scholar\n\n  23. Sinha, S.: The exploration-exploitation dilemma: a review in the context of managing growth of new ventures. Vikalpa 40(3), 313\u2013323 (2015)\n\nArticle Google Scholar\n\n  24. Woo, M., Cha, S. K., Gottlieb, S., Brumley, D.: Scheduling black-box mutational fuzzing. In: Proceedings of the 2013 ACM SIGSAC Conference on Computer & Communications Security, pp. 511\u2013522 (2013)\n\n  25. Berry, D.A., Fristedt, B.: Bandit Problems: Sequential Allocation of Experiments. In: Monographs on Statistics and Applied Probability. Chapman and Hall, London, vol. 5, no. 71-87, pp. 7\u20137 (1985)\n\n  26. Householder, A.D., Foote, J.M.: Probability-based parameter selection for black-box fuzz testing. Carnegie-Mellon Univ Pittsburgh PA Software Engineering Inst, Technical report (2012)\n\n  27. Barr, E.T., Harman, M., McMinn, P., Shahbaz, M., Yoo, S.: The oracle problem in software testing: a survey. IEEE Trans. Softw. Eng. 41(5), 507\u2013525 (2015)\n\nArticle Google Scholar\n\n  28. Duchene, F., Rawat, S., Richier, J.-L., Groz, R.: Kameleonfuzz: evolutionary fuzzing for black-box xss detection. In: Proceedings of the 4th ACM Conference on Data and Application Security and Privacy, pp. 37\u201348 (2014)\n\n  29. Appelt, D., Nguyen, C. D., Briand, L. C., Alshahwan, N.: Automated testing for SQL injection vulnerabilities: an input mutation approach. In: Proceedings of the 2014 International Symposium on Software Testing and Analysis, pp. 259\u2013269 (2014)\n\n  30. Khalid, M.N., Farooq, H., Iqbal, M., Alam, M. T., Rasheed, K.: Predicting web vulnerabilities in web applications based on machine learning. In: International Conference on Intelligent Technologies and Applications. Springer, pp. 473\u2013484 (2018)\n\n  31. Liu, Z., Fang, Y., Huang, C., Xu, Y.: Mfxss: an effective xss vulnerability detection method in JavaScript based on multi-feature model. Comput. Secur. 124, 103015 (2023)\n\nArticle Google Scholar\n\n  32. Song, X., Zhang, R., Dong, Q., Cui, B.: Grey-box fuzzing based on reinforcement learning for xss vulnerabilities. Appl. Sci. 13(4), 2482 (2023)\n\nArticle Google Scholar\n\n  33. Altulaihan, E.A., Alismail, A., Frikha, M.: A survey on web application penetration testing. Electronics 12(5), 1229 (2023)\n\nArticle Google Scholar\n\n  34. Walden, J., Stuckman, J., Scandariato, R.: Predicting vulnerable components: software metrics vs text mining. In: 2014 IEEE 25th International Symposium on Software Reliability Engineering, pp. 23\u201333 (2014)\n\n  35. Li, L., Dong, Q., Liu, D., Zhu, L.: The application of fuzzing in web software security vulnerabilities test. In: 2013 International Conference on Information Technology and Applications. IEEE, pp. 130\u2013133 (2013)\n\n  36. Mitmproxy is a free and open source interactive https proxy. https://mitmproxy.org/. Accessed 12 Feb 2022\n\n  37. What is a transparent proxy: client vs. server side use cases: Imperva (2020). https://www.imperva.com/learn/ddos/transparent-proxy/. Accessed 10 Jan 2022\n\n  38. Armstrong, J.: Computer Logic, Testing and Verification. By Paul Roth, vol. 90. Taylor & Francis, London (1983)\n\nGoogle Scholar\n\n  39. Sectooladdict, Sectooladdict/wavsep: the web application vulnerability scanner evaluation project. https://github.com/sectooladdict/wavsep. Accessed 01 June 2021\n\n  40. Caturano, F., Perrone, G., Romano, S.: Discovering reflected cross-site scripting vulnerabilities using a multiobjective reinforcement learning environment\u2019\u2019. Comput. Secur. 103, 102204 (2021)\n\nArticle Google Scholar\n\n  41. Lv, C., Zhang, L., Zeng, F., Zhang, J.: Adaptive random testing for xss vulnerability. In: Proceedings\u2014Asia-Pacific Software Engineering Conference, APSEC, vol. 2019\u2013December, pp. 63\u201369 (2019)\n\n  42. Bennetts, S.: Owasp zed attack proxy (2013). https://owasp.org/www-project-zap/. Accessed 01 Mar 2021\n\n  43. Chen, S.: Wavsep 2017/2018\u2014evaluating dast against pt/sdl challenges (1970). http://sectooladdict.blogspot.com/2017/11/wavsep-2017-evaluating-dast-against.html. Accessed 03 Mar 2021\n\n  44. OWASP, Xss filter evasion cheat sheet (2008). https://cheatsheetseries.owasp.org/cheatsheets/XSS_Filter_Evasion_Cheat_Sheet.html. Online. Accessed 4 Sep 2023\n\n  45. Swigger, P.: SQL injection bypassing common filters (2023). https://portswigger.net/support/sql-injection-bypassing-common-filters. Online. Accessed 4 Sep 2023\n\n  46. Google, Advanced obfuscation path traversal (2023). https://code.google.com/archive/p/teenage-mutant-ninja-turtles/wikis/AdvancedObfuscationPathtraversal.wiki. Online. Accessed 4 Sep2023\n\n  47. Caturano, F., Perrone, G., Romano, S. P.: Hacking goals: a goal-centric attack classification framework. In: Testing Software and Systems: 32nd IFIP WG 6.1 International Conference, ICTSS 2020, Naples, Italy, December 9\u201311, 2020, Proceedings. Springer, Berlin, pp. 296-301 (2020). [Online]. https://doi.org/10.1007/978-3-030-64881-7_19\n\n  48. Auricchio, N., Cappuccio, A., Caturano, F., Perrone, G., Romano, S.P.: An automated approach to web offensive security. Comput. Commun. 195, 248\u2013261 (2022)\n\nArticle Google Scholar\n\n  49. Zhang, B., Li, J., Ren, J., Huang, G.: Efficiency and effectiveness of web application vulnerability detection approaches: a review. ACM Comput. Surv. (2021). https://doi.org/10.1145/3474553\n\nArticle Google Scholar\n\n  50. Thom\u00e9, J., Shar, L.K., Bianculli, D., Briand, L.: Security slicing for auditing common injection vulnerabilities. J. Syst. Softw. 137, 766\u2013783 (2018). https://doi.org/10.1016/j.jss.2017.02.040\n\nArticle Google Scholar\n\n  51. Medeiros, I., Neves, N., Correia, M.: Detecting and removing web application vulnerabilities with static analysis and data mining. IEEE Trans. Reliab. 65(1), 54\u201369 (2016). https://doi.org/10.1109/tr.2015.2457411\n\nArticle Google Scholar\n\n  52. Yan, X., Ma, H., Wang, Q.: A static backward taint data analysis method for detecting web application vulnerabilities. In: 2017 IEEE 9th International Conference on Communication Software and Networks (ICCSN). IEEE (may 2017) [Online]. https://doi.org/10.1109/iccsn.2017.8230288\n\n  53. Noman, M., Iqbal, M., Talha, M., Jain, V., Mirza, H., Rasheed, K.: Web unique method (WUM): an open source blackbox scanner for detecting web vulnerabilities. IJACSA (2017). https://doi.org/10.14569/ijacsa.2017.081254\n\nArticle Google Scholar\n\n  54. Awang, N.F., Manaf, A.A.: Automated security testing framework for detecting SQL injection vulnerability in web application. In: Jahankhani, H., Carlile, A., Akhgar, B., Taal, A., Hessami, A.G., Hosseinian-Far, A. (eds.) Global Security, Safety and Sustainability: Tomorrow\u2019s Challenges of Cyber Security, pp. 160\u2013171. Springer, Cham (2015)\n\nChapter Google Scholar\n\n  55. Ciampa, A., Visaggio, C. A., Penta, M. D.: A heuristic-based approach for detecting SQL-injection vulnerabilities in web applications. In: Proceedings of the 2010 ICSE Workshop on Software Engineering for Secure Systems. ACM (may 2010) [Online]. https://doi.org/10.1145/1809100.1809107\n\n  56. Gupta, M.K., Govil, M. C., Singh, G., Sharma, P.: Xssdm: towards detection and mitigation of cross-site scripting vulnerabilities in web applications. In: 2015 International Conference on Advances in Computing, Communications and Informatics (ICACCI), pp. 2010\u20132015 (2015)\n\n  57. Akrout, R., Alata, E., Kaaniche, M., Nicomette, V.: An automated black box approach for web vulnerability identification and attack scenario generation. J. Braz. Comput. Soc. 20(1), 4 (2014). https://doi.org/10.1186/1678-4804-20-4\n\n  58. Medeiros, I., Neves, N., Correia, M.: Dekant: a static analysis tool that learns to detect web application vulnerabilities. In: Proceedings of the 25th International Symposium on Software Testing and Analysis, ser. ISSTA 2016. New York, NY, USA: Association for Computing Machinery (2016), pp. 1\u201311. [Online]. https://doi.org/10.1145/2931037.2931041\n\n  59. Jensen, T., Pedersen, H., Olesen, M.C., Hansen, R.R.: Thaps: automated vulnerability scanning of php applications. In: J\u00f8sang, A., Carlsson, B. (eds.) Secure IT Systems, pp. 31\u201346. Springer, Berlin (2012)\n\nChapter Google Scholar\n\n  60. Wassermann, G., Su, Z.: Static detection of cross-site scripting vulnerabilities. In: Proceedings of the 13th International Conference on Software Engineering\u2014ICSE \u201908. ACM Press (2008) [Online]. https://doi.org/10.1145/1368088.1368112\n\n  61. Shar, L.K., Briand, L.C., Tan, H.B.K.: Web application vulnerability prediction using hybrid program analysis and machine learning. Trans. Dependable Secure Comput. 12(6), 688\u2013707 (2015). https://doi.org/10.1109/tdsc.2014.2373377\n\nArticle Google Scholar\n\n  62. Gupta, S., Gupta, B.B.: XSS-SAFE: a server-side approach to detect and mitigate cross-site scripting (XSS) attacks in JavaScript code. Arab. J. Sci. Eng. 41(3), 897\u2013920 (2015). https://doi.org/10.1007/s13369-015-1891-7\n\nArticle Google Scholar\n\n  63. Shar, L.K., Tan, H.B.K.: Auditing the defense against cross site scripting in web applications. In: Proceedings of the International Conference on Security and Cryptography\u2014Volume 1: SECRYPT, (ICETE 2010), INSTICC. SciTePress, pp. 505\u2013511 (2010)\n\n  64. Bisht, P., Madhusudan, P., Venkatakrishnan, V.N.: Candid: dynamic candidate evaluations for automatic prevention of SQL injection attacks. ACM Trans. Inf. Syst. Secur. (2010). https://doi.org/10.1145/1698750.1698754\n\nArticle Google Scholar\n\n  65. Martin, M., Lam, M.S.: Automatic generation of XSS and SQL injection attacks with goal-directed model checking. In: Proceedings of the 17th Conference on Security Symposium, ser. SS\u201908. USA: USENIX Association, pp. 31\u201343 (2008)\n\n  66. Jang, Y.-S., Choi, J.-Y.: Detecting SQL injection attacks using query result size. Comput. Secur. 44, 104\u2013118 (2014)\n\nArticle Google Scholar\n\n  67. Lei, L., Jing, X., Minglei, L., Jufeng, Y.: A dynamic SQL injection vulnerability test case generation model based on the multiple phases detection approach. In: 2013 IEEE 37th Annual Computer Software and Applications Conference, pp. 256\u2013261 (2013)\n\n  68. Liu, L., Xu, J., Guo, C., Kang, J., Xu, S., Zhang, B.: Exposing SQL injection vulnerability through penetration test based on finite state machine. In: 2016 2nd IEEE International Conference on Computer and Communications (ICCC), pp. 1171\u20131175 (2016)\n\n  69. Liu, L., Xu, J., Yang, H., Guo, C., Kang, J., Xu, S., Zhang, B., Si, G.: An effective penetration test approach based on feature matrix for exposing SQL injection vulnerability. In: 2016 IEEE 40th Annual Computer Software and Applications Conference (COMPSAC). IEEE, [Online] (2016). https://doi.org/10.1109/compsac.2016.55\n\n  70. Ruse, M.E., Basu, S.: Detecting cross-site scripting vulnerability using concolic testing. In: 2013 10th International Conference on Information Technology: New Generations. IEEE [Online] (2013). https://doi.org/10.1109/itng.2013.97\n\n  71. Lee, I., Jeong, S., Yeo, S., Moon, J.: A novel method for SQL injection attack detection based on removing SQL query attribute values. Math. Comput. Model. 55(1), 58\u201368 (2012)\n\nArticle MathSciNet Google Scholar\n\n  72. Vithanage, N.M., Jeyamohan, N.: Webguardia: an integrated penetration testing system to detect web application vulnerabilities. In: 2016 International Conference on Wireless Communications, Signal Processing and Networking (WiSPNET), pp. 221\u2013227 (2016)\n\n  73. Avancini, A., Ceccato, M.: Comparison and integration of genetic algorithms and dynamic symbolic execution for security testing of cross-site scripting vulnerabilities. Inf. Softw. Technol. 55(12), 2209\u20132222 (2013). https://doi.org/10.1016/j.infsof.2013.08.001\n\n  74. Shar, L.K., Beng Kuan Tan, H., Briand, L.C.: Mining SQL injection and cross site scripting vulnerabilities using hybrid program analysis. In: 013 35th International Conference on Software Engineering (ICSE), pp. 642\u2013651 (2013)\n\n  75. Djuric, Z.: A black-box testing tool for detecting SQL injection vulnerabilities. In: 2013 Second International Conference on Informatics and Applications (ICIA). IEEE, [Online] (2013). https://doi.org/10.1109/icoia.2013.6650259\n\n  76. Kumar Singh, A., Roy, S.: A network based vulnerability scanner for detecting SQLI attacks in web applications. In: 2012 1st International Conference on Recent Advances in Information Technology (RAIT), pp. 585\u2013590 (2012)\n\n  77. Wu, H., Gao, G., Miao, C.: Test SQL injection vulnerabilities in web applications based on structure matching. In: Proceedings of 2011 International Conference on Computer Science and Network Technology, vol. 2, pp. 935\u2013938 (2011)\n\n  78. Li, N., Xie, T., Jin, M., Liu, C.: Perturbation-based user-input-validation testing of web applications. J. Syst. Softw. 83(11), 2263\u20132274 (2010). https://doi.org/10.1016/j.jss.2010.07.007\n\nArticle Google Scholar\n\n  79. Chen, J.-M., Wu, C.-L.: An automated vulnerability scanner for injection attack based on injection point. In: 2010 International Computer Symposium (ICS2010). IEEE, [Online] (2010). https://doi.org/10.1109/compsym.2010.5685537\n\n  80. Balzarotti, D., Cova, M., Felmetsger, V., Jovanovic, N., Kirda, E., Kruegel, C., Vigna, G.: Saner: composing static and dynamic analysis to validate sanitization in web applications. In: 2008 IEEE Symposium on Security and Privacy (sp 2008). IEEE [Online] (2008). https://doi.org/10.1109/sp.2008.22\n\n  81. Ahmed, M.A., Ali, F.: Multiple-path testing for cross site scripting using genetic algorithms. J. Syst. Archit. 64, 50\u201362 (2016). https://doi.org/10.1016/j.sysarc.2015.11.001\n\nArticle Google Scholar\n\n  82. Thome, J., Shar, L.K., Bianculli, D., Briand, L.: An integrated approach for effective injection vulnerability analysis of web applications through security slicing and hybrid constraint solving. IEEE Trans. Softw. Eng. Trans. Softw. Eng. 46(2), 163\u2013195 (2020). https://doi.org/10.1109/tse.2018.2844343\n\nArticle Google Scholar\n\n  83. Gupta, M.K., Govil, M.C., Singh, G.: Text-mining and pattern-matching based prediction models for detecting vulnerable files in web applications. J. Web Eng. 17(1\u20132), 028\u2013044 (2018)\n\nGoogle Scholar\n\n  84. Agosta, G., Barenghi, A., Parata, A., Pelosi, G.: Automated security analysis of dynamic web applications through symbolic code execution. In: 2012 Ninth International Conference on Information Technology\u2014New Generations. IEEE [Online] (2012). https://doi.org/10.1109/itng.2012.167\n\n  85. Ceccato, M., Nguyen, C.D., Appelt, D., Briand, L. C.: SOFIA: an automated security oracle for black-box testing of SQL-injection vulnerabilities. In: Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering. ACM [Online] (2016). https://doi.org/10.1145/2970276.2970343\n\n  86. Zhang, L., Zhang, D., Wang, C., Zhao, J., Zhang, Z.: Art4sqli: the art of SQL injection vulnerability discovery. IEEE Trans. Reliab. 68(4), 1470\u20131489 (2019)\n\nArticle Google Scholar\n\n  87. A survey of large language models (2023)\n\n  88. Charalambous, Y., Tihanyi, N., Jain, R., Sun, Y., Ferrag, M.A., Cordeiro, L.C.: A new era in software security: towards self-healing software via large language models and formal verification (2023)\n\n  89. He, J., Vechev, M.: Large language models for code: security hardening and adversarial testing (2023)\n\n  90. Yang, G., Dineen, S., Lin, Z., Liu, X.: Few-sample named entity recognition for security vulnerability reports by fine-tuning pre-trained language models. In: Wang, G., Ciptadi, A., Ahmadzadeh, A. (eds.) Deployable Machine Learning for Security Defense, pp. 55\u201378. Springer, Cham (2021)\n\n  91. Pearce, H., Tan, B., Ahmad, B., Karri, R., Dolan-Gavitt, B.: Examining zero-shot vulnerability repair with large language models. In: IEEE Symposium on Security and Privacy (SP), vol. 2023, pp. 2339\u20132356 (2023)\n\n  92. Noever, D.: Can large language models find and fix vulnerable software? (2023)\n\nDownload references\n\n## Funding\n\nOpen access funding provided by Universit\u00e1 degli Studi di Napoli Federico II\nwithin the CRUI-CARE Agreement.\n\n## Author information\n\n### Authors and Affiliations\n\n  1. Department of Electrical Engineering and Information Technology, University of Napoli Federico II, Naples, Campania, Italy\n\nCiro Brandi, Gaetano Perrone & Simon Pietro Romano\n\nAuthors\n\n  1. Ciro Brandi\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  2. Gaetano Perrone\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  3. Simon Pietro Romano\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n### Corresponding author\n\nCorrespondence to Simon Pietro Romano.\n\n## Ethics declarations\n\n### Conflict of interest Declaration\n\nThe authors declare that they have no known competing financial interests or\npersonal relationships that could have appeared to influence the work reported\nin this paper.\n\n## Additional information\n\n### Publisher's Note\n\nSpringer Nature remains neutral with regard to jurisdictional claims in\npublished maps and institutional affiliations.\n\n## Rights and permissions\n\nOpen Access This article is licensed under a Creative Commons Attribution 4.0\nInternational License, which permits use, sharing, adaptation, distribution\nand reproduction in any medium or format, as long as you give appropriate\ncredit to the original author(s) and the source, provide a link to the\nCreative Commons licence, and indicate if changes were made. The images or\nother third party material in this article are included in the article\u2019s\nCreative Commons licence, unless indicated otherwise in a credit line to the\nmaterial. If material is not included in the article\u2019s Creative Commons\nlicence and your intended use is not permitted by statutory regulation or\nexceeds the permitted use, you will need to obtain permission directly from\nthe copyright holder. To view a copy of this licence, visit\nhttp://creativecommons.org/licenses/by/4.0/.\n\nReprints and permissions\n\n## About this article\n\n### Cite this article\n\nBrandi, C., Perrone, G. & Romano, S.P. Sniping at web applications to discover\ninput-handling vulnerabilities. J Comput Virol Hack Tech (2024).\nhttps://doi.org/10.1007/s11416-024-00518-0\n\nDownload citation\n\n  * Received: 19 April 2023\n\n  * Accepted: 21 February 2024\n\n  * Published: 12 April 2024\n\n  * DOI: https://doi.org/10.1007/s11416-024-00518-0\n\n### Share this article\n\nAnyone you share the following link with will be able to read this content:\n\nSorry, a shareable link is not currently available for this article.\n\nProvided by the Springer Nature SharedIt content-sharing initiative\n\n### Keywords\n\n  * Fuzzing\n  * Web applications\n  * Web penetration testing\n  * Rule-based systems\n  * Web security testing\n\nUse our pre-submission checklist\n\nAvoid common mistakes on your manuscript.\n\nAdvertisement\n\n### Discover content\n\n  * Journals A-Z\n  * Books A-Z\n\n### Publish with us\n\n  * Publish your research\n  * Open access publishing\n\n### Products and services\n\n  * Our products\n  * Librarians\n  * Societies\n  * Partners and advertisers\n\n### Our imprints\n\n  * Springer\n  * Nature Portfolio\n  * BMC\n  * Palgrave Macmillan\n  * Apress\n\n128.140.102.183\n\nNot affiliated\n\n\u00a9 2024 Springer Nature\n\n", "frontpage": false}
