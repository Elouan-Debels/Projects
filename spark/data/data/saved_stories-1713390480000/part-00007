{"aid": "40066330", "title": "Network policies are not the right abstraction (for developers)", "url": "https://otterize.com/blog/network-policies-are-not-the-right-abstraction", "domain": "otterize.com", "votes": 1, "user": "mooreds", "posted_at": "2024-04-17 15:47:24", "comments": 0, "source_title": "Network policies are not the right abstraction (for developers)", "source_text": "Network policies are not the right abstraction (for developers)\n\nNetwork policies are not the right abstraction (for developers)\n\n\ud83d\udea8 KubeCon + CloudNativeCon Announcement: Otterize Unveils Support for\nMicrosoft Azure IAM, Google Cloud IAM, and Introduces Blast Radius &\nCompliance Mapping \u2192\n\nBlog\n\n  * Network Policy\n\n  * Kubernetes\n\n  * Zero-trust\n\n# Network policies are not the right abstraction (for developers)\n\nWe explore the limitations of relying solely on Kubernetes network policies as\na solution for achieving zero-trust between pods, identifying multiple flaws\nthat hinder their effectiveness in meeting the demands of real-world use\ncases, particularly when prioritizing developer experience in a Kubernetes-\nbased platform.\n\n###### Written By\n\nOri Shoshan\n\n###### Published Date\n\nFeb 12 2024\n\n###### Read Time\n\n10 minutes\n\nShare article\n\n###### In this Article\n\n  * Zero-trust means preventing access from unidentified or unauthorized sources\n  * Let\u2019s talk about network policies\n  * Friction when using network policies\n  * Enabling access between two pods\n  * What if you need to roll back the server?\n  * How do you know the policy is correct?\n  * How do you refer to pods within network policies?\n  * Network policies are effectively owned by multiple teams\n  * Is everyone in your organization proficient with how network policies work?\n  * What would a good abstraction look like?\n  * Enter client intents\n\nYou\u2019re a platform engineer working on building a Kubernetes-based platform\nthat achieves zero-trust between pods. Developers have to be able to get work\ndone quickly, which means you\u2019re putting a high priority on developer\nexperience alongside zero-trust.\n\nAre Kubernetes network policies good enough? I think there are multiple flaws\nthat prevent network policies, on their own, from being an effective solution\nfor a real-world use case.\n\nBefore pointing out the problems, I\u2019d like to walk you through what I mean\nwhen I say zero-trust, as well as a couple of details about how network\npolicies work.\n\n## Zero-trust means preventing access from unidentified or unauthorized\nsources\n\nNetwork policies can prevent incoming traffic to a destination (a server), or\nprevent outgoing traffic from a source (a client).\n\nZero trust inherently means you don\u2019t trust any of the sources just because\nthey\u2019re in your network perimeter, so the only blocking relevant for achieving\nzero-trust is blocking incoming traffic (\u201cingress\u201d) from unauthorized sources.\n\n    \n    \n    apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: my-policy spec: ingress: - {} # ingress rules policyTypes: - Ingress # policy refers to ingress, but it could also have egress\n\n## Let\u2019s talk about network policies\n\n### They\u2019re namespaced resources and refer to pods by label\n\nNetwork policies are namespaced resources, and refer to pods by label.\nLogically, they must live alongside the pods they apply to \u2013 in our case,\nsince we\u2019re using ingress policies, that means alongside the servers they\nprotect.\n\nThey don\u2019t refer directly to specific pods, of course, because pods are\nephemeral, but they refer logically to pods by label. This is common in\nKubernetes, but introduces problems for network policies. Keep this detail in\nmind as we\u2019ll get back to it later.\n\n    \n    \n    apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: protect-backend spec: podSelector: matchLabels: app: my-backend # policy will apply to pods labeled app=my-backend, in the same namespace as the policy ingress: - from: - podSelector: matchLabels: app: my-client # and allow ingress access from pods labeled app=my-client policyTypes: - Ingress\n\n### They hold information about multiple sets of pods\n\nThe contents of the network policies are effectively an allowlist specifying\nwhich other pods can access the pods which the policy protects. But there\u2019s\none big problem there: while the network policy must live with the protected\npods, and is updated as part of the protected pods\u2019 lifecycle, it won\u2019t\nnaturally be updated as part of the lifecycle of the client pods accessing the\nprotected pods.\n\n## Friction when using network policies\n\n## Enabling access between two pods\n\nWhenever a developer for a client pod needs access to a server, they need to\nget their client pod into the server\u2019s network policy so it\u2019s allowed to call\nthe server. The developer often cannot manage that network policy themselves,\nas it usually exists in a namespace they are not responsible for, and deployed\nwith a service they don\u2019t own.\n\nThe result is that the client developer is dependent on the server team for\naccess that should have been self-service, and the server team is now\ndistracted enabling a client developer even though nothing has really changed\nfrom the point of view of the server team \u2013 a new client is connecting to\ntheir server, that\u2019s it! There should not be server-side changes required to\nsimply enable another client.\n\n## What if you need to roll back the server?\n\nThere are also a myriad of second-order problems, which the team at Monzo had\nlearned about through solving for this problem. (It\u2019s a super well-written\nblog post; I recommend having a read), such as that rolling back the server\nwould affect whether clients could connect, since it rolled back its network\npolicy.\n\nWhen a server is rolled back due to an unrelated problem, its network policy\nmay also be rolled back if it is part of the same deployment (e.g. part of the\nsame Helm chart), and break the clients that relied on that version of the\nnetwork policy! It\u2019s a reflection of the unhealthy dependency between the\nclient and server teams: while it would make sense that a server-side change\nthat breaks functionality would affect the client, it does not make sense that\nan unrelated and functionally-non-breaking rollback of the server would affect\nthe client.\n\n## How do you know the policy is correct?\n\nBecause network policies refer to pod labels, they are difficult to validate\nstatically. Pods are generally not created directly, but instead created by\nother resources, such as Deployments.\n\nCan you tell whether a network policy will allow access for your service\nwithout deploying and trying it out? In fact, just asking the question \u201cwhich\nservices have effective access to service A?\u201d becomes super hard.\n\nDevelopers don\u2019t think of services as pod labels, but they tend to have a\ndeveloper-friendly name they use. For example, checkoutservice is a friendly\nname, whereas checkoutservice-dj3847-e120 is not. This may in fact be the\nvalue of some label, but there\u2019s no standard way to discover this name.\n\nSo then, how do you take the concept of a service, with its developer-friendly\nname, and map that to its labels that are referred to by the network policies\nand, say, its Deployment, to be able to check if it will have access once its\nnew labels are deployed? You could manually do that, as a developer in a\nsingle team that understands all the moving parts. However, this is very\nerror-prone, and of course, doesn\u2019t apply to a solution a platform engineer\ncould deploy: as a platform engineer, you\u2019d need something automated you could\nmake available to every developer in your organization.\n\nThis problem is one that the team at Monzo worked hard at. I recommend giving\nthat blog a read as it is very well-written and also covers other factors of\nthe problem.\n\n## How do you refer to pods within network policies?\n\nEarlier, I mentioned that network policies don\u2019t refer to pods directly, as\nthey\u2019re ephemeral, but refer to them by labels. This is common practice in\nKubernetes. However, network policies are unique in that they use labels to\nrefer to two (or more) sets of pods that are often owned by different teams in\nthe organization.\n\nThis presents unique challenges because, for the network policy to function,\nthe labels referenced by the policy and the labels attached to the pods must\nbe kept in sync, with destructive consequences if you fail to do so \u2013\ncommunication will be blocked! The pod labels for the client pods are managed\nby the client team, while the network policy that refers to them is managed by\nthe server team, so you can see where things can get out of sync.\n\n## Network policies are effectively owned by multiple teams\n\nThis means that you need coordination between the teams, not only when the\nnetwork policy is first deployed, but also over time as clients and servers\nevolve.\n\nWhat if you have a network policy that allows multiple clients to connect to\none server? Now you\u2019ve got the server team coordinating with 2 teams.\n\nFor each change a client team proposes, the server team needs to not only\nchange network policy rules referring to that client, but also make sure they\ndon\u2019t inadvertently affect other clients. This can be a cognitively difficult\ntask, as the server team members normally don\u2019t refer to pod labels belonging\nto other teams, so it may not immediately be clear which labels belong to\nwhich team.\n\nThis reduces the ability for teams to set internal standards and work\nindependently, and slows down development. If you don\u2019t get this right, there\ncan be painful points in the development cycle where changes are fragile and\ntheir pace slows to a crawl. The pain may lead to bikeshedding and inter-team\npolitics, as teams argue over how things should be done, and growing\nfrustration as client deployments are delayed as a result of server network\npolicies not yet being updated.\n\n## Is everyone in your organization proficient with how network policies work?\n\nIn many organizations, this is not the case. Network policies are already\nerror-prone, with destructive consequences for even small mistakes. Asking\nevery developer whose service calls another service to be familiar with\nnetwork policies may be a tall order, with potential for failed deployments or\nfailed calls that are hard to debug.\n\n## What would a good abstraction look like?\n\nA good solution for zero trust should be optimized for that specific outcome,\nwhereas network policies are a bit of a swiss army knife: they aren\u2019t just for\npod-to-pod traffic, so they\u2019re not optimized for this use case.\n\nThe following 3 attributes are key for a good zero-trust abstraction that\nactually gets adopted:\n\n  1. Single team ownership: Each resource should only be managed by one team so that client teams can get access independently, and server teams don\u2019t need to be involved if no changes are required on their end.\n\n  2. Static analysis should be possible: It should be possible to statically check if a service will have access without first deploying it.\n\n  3. Universal service identities: Services should be referred to using a standard name that is close to or identical to their developer-friendly names, rather than pod labels.\n\n## Enter client intents\n\nAt Otterize, we believe that client intents satisfy these requirements. Let me\nexplain briefly what they are, and then examine whether they satisfy the above\nattributes.\n\nA client intents file is simply a list of calls to servers which a given\nclient intends to make. Coupled with a mechanism for resolving service names,\nthe list of client intents can be translated to different authorization\nmechanisms, such as network policies.\n\nIn other words, developers declare what their service intends to access, and\nthat can then be converted to a network policy and the associated set of pod\nlabels.\n\nHere\u2019s an example of a client intents file (as a Kubernetes custom resource\nYAML) for a service named client calling another service named server:\n\n    \n    \n    apiVersion: k8s.otterize.com/v1alpha2 kind: ClientIntents metadata: name: client-intents spec: service: name: client calls: - name: server\n\n### Let\u2019s see if this is a good abstraction\n\nNow let\u2019s go back and review our criteria for a good zero-trust abstraction:\n\nDoes a team own all of, and only, the resources it should be managing?\n\nClient intents files are deployed and managed together with the client, so\nonly the client team owns them. You would deploy the ClientIntents for this\nclient along with the client, e.g. alongside its Deployment resource.\n\nCan access be checked statically?\n\nSince services are first-class identities in client intents (rather than\nindirectly represented by pod labels), it is trivially possible to query which\nclients have access to a server, and whether a specific client has access to a\nserver. As an added bonus, all the information for a single client is\ncollected in a single resource in one namespace, instead of being split up\nacross multiple namespaces where the servers are deployed.\n\nAre service identities universal and natural?\n\nService names are resolved in the same manner across the entire organization,\nmaking it easy to reason about whether a specific service has a specific name.\n\n### How would a Kubernetes operator that manages these intents work?\n\nWhen intents are created for a client, the intents operator should\nautomatically create, update and delete network policies, and automatically\nlabel client and server pods, to reflect precisely the client-to-server calls\ndeclared in client intents files. A single network policy is created per\nserver, and pod labels are dynamically updated for clients when their intents\nupdate.\n\nService names are resolved by recursively getting the owner of a pod until the\noriginal owner is found, usually a Deployment, StatefulSet, or other such\nresource. The name of that resource is used, unless the pod has a service-name\nannotation which overrides the name, in which case the value of that\nannotation is used instead.\n\n### Try out the intents operator!\n\nIt won\u2019t surprise you that we in fact built such an open source\nimplementation, and it\u2019s called the Otterize intents operator. Give it a shot\nand see if it makes managing network policies easier for you.\n\n###### Like this article?\n\nSign up for newsletter updates\n\nBy subscribing you agree to with our Privacy Policy and to receive updates\nfrom us.\n\nShare article\n\n###### In this Article\n\n  * Zero-trust means preventing access from unidentified or unauthorized sources\n  * Let\u2019s talk about network policies\n  * Friction when using network policies\n  * Enabling access between two pods\n  * What if you need to roll back the server?\n  * How do you know the policy is correct?\n  * How do you refer to pods within network policies?\n  * Network policies are effectively owned by multiple teams\n  * Is everyone in your organization proficient with how network policies work?\n  * What would a good abstraction look like?\n  * Enter client intents\n\n###### Like this article?\n\nSign up for newsletter updates\n\nBy subscribing you agree to with our Privacy Policy and to receive updates\nfrom us.\n\n##### Visit the Otter\u2019s Den\n\nYour go-to hub for Kubernetes security and tech know-how\n\n  * Kubernetes\n\n  * Zero-trust\n\n  * IBAC\n\n  * Dropbox\n\n  * Automation\n\n  * Startups\n\n  * Podcasts\n\nBlog\n\nApr 15 2024\n\n###### First Person Platform Episode 2 - Andrew Moore on Uber Workload\nIdentity and Authorization\n\nThe second episode of First Person Platform, a podcast: platform engineers\nnerd out with Ori Shoshan on access controls, Kubernetes, and platform\nengineering.\n\n  * Network Policy\n\n  * Kubernetes\n\n  * Zero-trust\n\nBlog\n\nFeb 12 2024\n\n###### Network policies are not the right abstraction (for developers)\n\nWe explore the limitations of relying solely on Kubernetes network policies as\na solution for achieving zero-trust between pods, identifying multiple flaws\nthat hinder their effectiveness in meeting the demands of real-world use\ncases, particularly when prioritizing developer experience in a Kubernetes-\nbased platform.\n\nJoin our newsletter to stay up to date on features and releases.\n\nBy subscribing you agree to with our Privacy Policy and to receive updates\nfrom us.\n\nPlatform\n\nOpen Source\n\nOtterize Cloud\n\nresources\n\nDocumentationResource HubAbout usPricing\n\nFollow Us\n\n\u00a9 2024 Otterize, Inc. All rights reserved.\n\nService StatusPrivacy PolicyTerms of Service\n\n", "frontpage": false}
