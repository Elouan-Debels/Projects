{"aid": "40067380", "title": "AI Eval Feedback Loops", "url": "https://www.braintrustdata.com/blog/eval-feedback-loops", "domain": "braintrustdata.com", "votes": 1, "user": "robertguss", "posted_at": "2024-04-17 17:01:27", "comments": 0, "source_title": "Eval feedback loops", "source_text": "Eval feedback loops \u2014 Braintrust\n\nCTRL K\n\n# Eval feedback loops\n\nAnkur Goyal\n\n17 April 2024\n\nIn AI engineering, it can be tough to understand how a change will impact end\nusers. The solution is to test AI products on a set of real-world examples,\naka \u201cevals\u201d.\n\nOnce you establish an initial set of evals, however, a few problems typically\nemerge:\n\n  * It\u2019s hard to find great eval data\n  * It\u2019s hard to identify interesting cases / failures in production\n  * It\u2019s hard to tie user feedback to your evals\n\nThe solution to these problems is to connect your real-world log data to your\nevals, so that as you encounter new and interesting cases in the wild, you can\neval them, improve, and avoid regressing in the future. In this post, we\u2019ll\nwork through how to set this up in the ideal way.\n\nSpecifically, we\u2019ll cover:\n\n  1. How to structure your evals and when to run them\n  2. How to flow production logs into your eval data\n  3. How Braintrust makes this easy\n\n## Structuring your evals\n\nWell-implemented evals serve as the foundation of the AI engineering dev loop.\nIf you can run them \"offline\" from your production application, evals free you\nto experiment with changes to code, prompts, data, etc. without affecting your\nusers.\n\nFundamentally, an eval is a function of some (a) data, (b) prompts/code (we\u2019ll\ncall this a task), and (c) scoring functions. Updating the (b) task impacts\nproduct behavior, while updating the (a) data or (c) scorers improves the\nfidelity of the evaluation. Consequently, each time any of these change, you\nshould run a new eval and assess how your performance has changed.\n\nBraintrust\u2019s Eval function makes this workflow very clear, by literally having\n3 inputs:\n\n    \n    \n    Eval(\"project name\", { data: <your data>, task: <your task>, scores: [Factuality, Levenshtein, ...], })\n\nIn the next section of this post, we\u2019re going to focus in on the \u201cdata\u201d part.\nAnd in particular, how to build and continually improve your datasets, by\ncapturing real-world data the right way.\n\n## Capturing and utilizing logs\n\nThe key to evaluating with good data is to use real-world examples. When we\u2019re\nplaying with AI products, we often discover good test cases while interacting\nwith them. For example, someone may struggle to get your chatbot to produce a\nmarkdown table. When this happens, it\u2019s the perfect moment to capture the\ncontext into a dataset you evaluate on.\n\nBecause logs and evals function similarly, the data from logs can easily be\nused to power your evals.\n\n### Initial steps\n\nWe highly recommend tracing and logging your projects from inception. In the\nearly days of developing a new product or feature, each time someone\nencounters a bug, you can immediately find the trace corresponding to their\ninteraction in the UI. You can also scan through most or all traces to look\nfor patterns. This is a benefit of small scale \ud83d\ude42.\n\nBraintrust makes this really easy. To log user interactions, you instrument\nyour application\u2019s code to trace relevant bits of context. Traces appear on\nthe logs page, where you can view them in detail.\n\nAs you find interesting examples in your logs, you can easily add them to a\ndataset. You can also use tags to organize different kinds of issues, e.g.\ntoxic-input, but you can also place different categories of logs into separate\ndatasets.\n\nOnce you set up a dataset, you can run evals on it by referencing it as the\ndata parameter in the Eval function:\n\n    \n    \n    Eval(\"your project\", { data: init_dataset(\"your project\", \"your dataset\"), ... })\n\nAs you add new cases to your dataset, your Eval will automatically test them.\nYou can also pin an Eval to a particular version of a dataset, and then\nexplicitly change the version when you\u2019re ready to.\n\n### Scaling\n\nAs you scale, it becomes critical to filter the logs to only consider the\ninteresting ones. There are three ways to accomplish this:\n\n  * Use filters, e.g. on metadata or tags, to help you track down interesting cases. For example, you might know that your evals are missing a certain type of request, filter down to them, scan a few, and add 1-2 to your eval dataset.\n  * Track user feedback, and review rated examples. For example, you may routinely capture \ud83d\udc4d\ud83c\udffd cases as positive cases not to regress, or carefully review recent \ud83d\udc4e\ud83c\udffd cases, and add them to a dataset for further improvement.\n  * Run online scores, ideally the same scores you do offline evals on, and find low scoring examples. This is a fantastic and very direct way to uncover test cases that you know need improvement by your own metrics.\n\n## Putting this to practice\n\nImplementing a good eval feedback loop is incredibly rewarding, but can get\nunwieldy, even at relatively small scale. Many teams we meet start with json\ndata in their git repo or source code. But they quickly find that it\u2019s a lot\nof manual effort to keep these files up-to-date with real-world examples,\ncollaborate on them, and visualize the data.\n\nBraintrust is specifically built and designed for this purpose:\n\n  * You can instrument your code once and reuse it across both logging and evaluation. For example, if you implement a score while evaluating (e.g. ensuring the length of an output is at most 2 paragraphs), you can automatically compute the score in production, discover examples where it\u2019s low, and add them to a dataset.\n  * The shared code across logging and evals also naturally unifies the UI. This is one of the most powerful ideas in Braintrust \u2014 you can use exactly the same code and UI to explore your logs and your evals. The trace is an incredibly powerful data structure that contains information about every LLM call plus additional data you log. You can even reproduce the LLM calls your users saw, tweak the prompts, and test them directly in the UI.\n  * All of your datasets, logs, evals, and user data can be stored in your cloud environment. We know how sensitive and valuable your AI data is, so we\u2019ve supported self-hosting from day one.\n\nIf you think Braintrust can help, then feel free to sign up or get in touch.\nWe\u2019re also hiring!\n\nLast updated on April 17, 2024\n\nBraintrust selected to be in the Enterprise Tech 30\n\n", "frontpage": false}
