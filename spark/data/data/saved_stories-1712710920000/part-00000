{"aid": "39982901", "title": "How Do AI Software Engineers Like Devin Compare to Humans?", "url": "https://stepchange-blog.ghost.io/why-do-ai-software-engineers-like-devin-struggle-to-fix-bugs/", "domain": "stepchange-blog.ghost.io", "votes": 7, "user": "htormey", "posted_at": "2024-04-09 18:59:59", "comments": 2, "source_title": "How Do AI Software Engineers Really Compare To Humans?", "source_text": "How Do AI Software Engineers Really Compare To Humans?\n\n# How Do AI Software Engineers Really Compare To Humans?\n\n#### Harry Tormey\n\nApr 9, 2024 \u2014 9 min read\n\nCurrent AI evaluation metrics fail to represent many aspects of real-world\nsoftware engineering tasks.\n\n## Introduction\n\nAI software engineers like Devin and SWE-agent are frequently compared to\nhuman software engineers. However SWE-bench, the benchmark upon which this\ncomparison is made, only applies to Python tasks, most of which involve making\nsingle-file changes of 15 lines or less and relies solely on unit tests to\nevaluate their correctness. My aim is to give you a framework to assess if\nAI's progress against this benchmark is relevant to your organization's work.\n\nIf you\u2019re an organization building mobile applications or work with Java, Go,\nSwift or Typescript then progress against SWE-bench by AI software engineers\nor models has limited use to you.\n\nAdditionally, changes made by AI software engineers often disregard the use of\nthird-party libraries and coding conventions, which is particularly relevant\nfor organizations managing larger scale codebases.\n\nTo enhance AI's ability to address bugs and features across non-Python\nengineering domains, we need to expand datasets and evaluation tools like SWE-\nbench horizontally to cover more languages and technologies. Furthermore, we\nmust also expand vertically, benchmarking against larger tasks, such as\nfeatures spanning multiple pull requests, requiring integration testing on\nreal hardware for mobile applications or against databases for performance-\nrelated issues. At StepChange, we are actively tackling this challenge. If you\nare interested in collaborating, please reach out.\n\nIn this article, I'll highlight important information from the SWE-bench paper\nand explore the dataset's tasks, employing AI to summarize and categorize\nthem. I'll explain both the tasks AI fails at and those it resolves\nsuccessfully. I will also perform this analysis for the tasks Devin was\nevaluated on.\n\n## What\u2019s SWE-bench?\n\nSWE-bench is a dataset and evaluation tool, offering 2,294 tasks based on\nactual GitHub issues and pull requests drawn from open source Python projects.\n\nA list of the GitHub sources where the tasks in SWE-bench are taken from\n\n## How Big Are The Tasks In SWE-bench?\n\nThe paper accompanying SWE-bench contains some high level information about\nthe size of tasks present in the dataset which I have summarized below:\n\n  * Problem Description Size: The median task in the SWE-bench dataset gives a problem description for a task of 140 words.\n  * Codebase Size: The median task involves a codebase with close to 1,900 files and 400,000 lines of code.\n  * Reference Task Solutions: The reference solution provided usually modifies a single function within one file changing \u223c15 lines.\n  * Task Evaluation: For each task, there is at least one fail-to-pass test which was used to test the reference solution, and 40% of tasks have at least two fail-to-pass tests. These tests evaluate whether the model addressed the problem in the issue. In addition, a median of 51 additional tests run to check whether prior functionality is properly maintained.\n\nLooking through the tasks and their original PRs shows they're quite small,\nsomething a skilled engineer could handle quickly, in at most a couple of\ndays. Here is an example of one feature Devin successfully completed to give\nyou a flavor for what they are like:\n\n  * Feature Summary: The issue is a feature request to allow overriding of deletion widget in formsets, similar to how ordering_widget and get_ordering_widget() were introduced in Django 3.0.\n\n    * Fixes: Original PR, Devin\u2019s PR\n\nMore are highlighted in my report on Devin's successes and failures. These\ntasks don't really show if an AI can take on bigger projects that stretch over\nseveral sprints, which is a key part of a software engineer's job.\n\n## How Did Claude 2 and GPT 4 Perform Against SWE-bench?\n\nAt the time the paper was published, Claude 2 was the best performing model\nand it only resolved around 4.8% of issues with oracle retrieval. Oracle\nretrieval gives the LLM files known to resolve an issue in addition to the\nproblem statement; this is less realistic because engineers don't usually know\nwhich files will need edits in advance. Without oracle retrieval Claude 2 was\nonly able to resolve 1.96% of these issues.\n\nLeaderboard taken from swebench.com on April 8 2024\n\n## Understanding How Model Limitations Apply To SWE-bench\n\nThe SWE-bench paper spotlighted a number of tasks and used them to highlight\nthemes the researchers saw while analyzing the evaluation results:\n\n  * Models struggle with multi-line and multi-file changes: Models are more adept when the required fix is relatively short, and need help with understanding the codebase in an efficient manner.\n  * Models ignore an organization's working style: Code produced often didn't follow the project's style. One example highlighted using its own Python solution over using built in functions for handling things like string formatting. This would likely not be well-received by a human reviewer due to its inconsistency with the rest of the codebase.\n  * Use Of Libraries: Significant use of additional modules from other parts of the codebase itself and third party libraries were noted as cause of model failure. This came up for a task from the sphinx-doc/sphinx repository, where a model is asked to write logic to fix a case where the title is incorrectly being rendered. This example highlights the importance and potential for training language models and designing inference procedures that allow for the automated discovery of such information.\n  * Contextual Awareness: The model struggled with tasks that needed an understanding of how changes in one part of the code affect the rest. For example, in scikit-learn, the model had to correct an issue where outputs were flipped incorrectly. It failed because, beyond fixing the immediate problem, it didn't consider how other parts of the code relied on the function it was changing. This highlights the challenge of ensuring a model understands the wider code context, which is crucial but difficult.\n  * Tasks with images: Models struggled with problems containing images due to a lack of multimodal capabilities and or integration with external tools to process images. Debugging real software issues often involve digesting images. Beyond the SWE-bench tasks, additional problems in software engineering, such as interpreting system design diagrams, user interface specifications, or user feedback might be challenging.\n\nAn important difference to note between models being tested with SWE-bench and\nAI software engineers like Devin and SWE-agent is that the latter are\niterative. Devin can execute multi-step plans to receive feedback from the\nenvironment, i.e they can iterate on errors like human software engineers\nwhereas the models just submit their solution and don\u2019t get a chance to\nincorporate feedback.\n\n## What Are Examples Of These Tasks?\n\nAt StepChange one of things we specialize in is Django web application\nperformance, Django is also the biggest group of tasks in the dataset. As such\nI decided to write some scripts using GPT4 and Langchain to categorize, tag\nand summarize all of the Django related tasks in the dataset. You can find\nthem here as well as a report summarizing all of the Django tasks Devin failed\nand passed here. It contains links to the original PR that fixed the issue,\nthe diff generated by Devin and stats about diff sizes.\n\nReviewing the tasks tagged by GPT4 based on their summaries, we see that 41%\nof the tasks are tagged as being to do with databases with a large number of\nthe other tags, such as models, model validation, SQL and SQLLite being\nrelated topics.\n\n## How Did Devin The AI Software Engineer Perform Against SWE-bench?\n\nThe team behind Devin published a technical report where they linked to a\ngithub repo containing the results of their evaluation against SWE-bench.\nDevin successfully completed 13.86% of these tasks, this outperforms the\nrecently released open source SWE-agent which completes 12.29% of issues on\nthe full test set. Going from 1.96% to 13.86% in six months is impressive\nprogress.\n\nAn interesting point brought up by the Cognition Labs team in their technical\nreport was when they provided Devin with the final unit test(s) along with the\nproblem statement the successful pass rate increased to 23% out of 100 sampled\ntests. While in most cases it\u2019s not realistic to have unit tests before code\nis written, in scenarios such as migrating from one language to another (e.g\nPHP to Next.js) unit tests could be generated from the legacy codebase.\n\nOne thing to note is that Cognition Labs, the team behind Devin and the team\nbehind SWE-bench tested with a randomly chosen 25% of the SWE-bench test set.\nIn the case of Devin this was done to reduce the time it takes for the\nbenchmark to finish. The team behind the SWE-bench paper did the same thing\nfor GPT-4; they used a 25% random subset of SWE-bench in the \u201coracle\u201d and BM25\n27K retriever settings. They claimed this was due to budget constraints.\n\nRunning the full SWE-bench evaluation takes hours because each task involves\ninstalling and deleting the tested repo and running multiple unit tests. If\nyou want to do this without spending money on GPT-4 you can download the\ngenerated results from Claude and the GPTs here.\n\n## How Did Devin The AI Software Engineer Perform Against Django Tasks?\n\nDevin was able to complete 19.19% of the Django tasks it attempted. The tasks\nin this subset were from between October 2015 to July 2023 with the majority\nof them being bugs.\n\nDevin's performance on SWE-bench tasks reveals a pattern of struggle with\ncomplex changes, particularly those requiring alterations across multiple\nfiles or exceeding 15 lines of code, mirroring challenges faced by models in\nthe SWE-bench study.\n\nIn tasks that it failed, 95 had more than 1 file changed and 230 had more than\n15 lines changed. In contrast to the tasks it successfully completed 11 had\nmore than 1 file changed and 18 had more than 15 lines changed. Implying that\nlike the models tested by SWE-bench, Devin struggled with larger changes\nspanning multiple files. Here are some examples of tasks Devin successfully\ncompleted to give you a flavor for what they are like:\n\n  * Bug Summary: The issue is about the TruncDate and TruncTime functions in Django not correctly handling timezone information passed to them. The functions are supposed to use the passed timezone info object, but they are instead using the return value from get_current_timezone_name() unconditionally.\n\n    * Fixes: Original PR, Devin\u2019s PR\n  * Bug Summary: The class preparation for lazy() is not being cached correctly. This makes functions like gettext_lazy, format_lazy and reverse_lazy slower than they should be.\n\n    * Fixes: Original PR, Devin\u2019s PR\n\nAt the time of writing this article the authors of SWE-bench had just released\nan open source competitor to Devin, SWE-agent which achieved a 12.29% resolve\nrate on the full test set. They will be releasing a paper on Apr 10, 2024, if\nyou are interested in me doing a similar deep dive into their results please\nlet me know by liking and sharing my article on X/twitter (my handle is\nhtormey).\n\n## Conclusion\n\nAI Software Engineers like Devin and SWE-Agent are getting a lot of attention\nright now, with huge potential and some very impressive demos. However it is\ncritical to understand the benchmarks they are working against to evaluate\nthem in real world scenarios and what their limitations are.\n\nAs an example, AI models like GPT-4 often struggle to keep up with API changes\nin third party libraries that update frequently. From my own experience,\ntrying to use AI to generate code leveraging fast moving projects like\nLangChain or Next.js can be frustrating because the suggestions you get are\noften out of date. Hence the need for better benchmarks that are updated\nfrequently.\n\nIt\u2019s still early days for AI Software Engineers. I think within the next 2-3\nyears, AI will be able to handle debugging and multi-line code changes\neffectively across large codebases. This advancement will transform software\nengineers' roles from focusing on detailed coding tasks to prioritizing\noversight and orchestration.\n\nI believe this transition will be more significant than others I have made in\nmy career such as from low-level to high-level programming languages. I don\u2019t\nbelieve it will eliminate the need for human software expertise.\n\nEngineers will still require a solid technical foundation to clearly define\nproject requirements, strong logical reasoning skills to navigate complex\nissues, and the capacity to correct AI errors.\n\nThey will also play a crucial role in the iterative processes of building,\ntesting, and debugging, now with AI as a tool in their arsenal. Furthermore,\nthe development landscape will adapt, with new frameworks and tools being\nintroduced to streamline the collaboration between engineers and AI.\n\nThanks to Niall O'Higgins, Gergely Orosz, Carl Cortright, Greg Kamradt, Nader\nDabit for reviewing and providing feedback\n\n\ud83d\udca1\n\nStepChange is a consultancy that develops AI co-pilots to help automate\nmodernization of apps and databases. We believe AI is better with humans in\nthe loop. We help you improve performance, reduce cost and improve developer\nproductivity. If you are interested in a high quality assessment of your apps\n& infrastructure, contact us.\n\n## Read more\n\n### Why are people migrating from dbt Cloud to dbt Core?\n\nStepChange is a consultancy that works with you to modernize your data\ninfrastructure. Often this involves optimizing cloud spend. Recently there has\nbeen a lot of conversation about dbt. This article hopes to clarify what it is\nand how people are dealing with their price increases. What is DBT? Data\n\nBy Harry Tormey Feb 22, 2024\n\n### Migrating from Apache Oozie to Airflow\n\nThe following blog post was inspired by Astronomer's webinar on how to migrate\nfrom Oozie to Airflow by Ben Spivey and Dylan Storey. Many enterprises are\nlooking to migrate from Apache Oozie to Airflow. For those who may not know,\nApache Oozie is an established open-source workflow scheduler\n\nBy Harry Tormey Feb 6, 2024\n\nPowered by Ghost\n\n", "frontpage": false}
