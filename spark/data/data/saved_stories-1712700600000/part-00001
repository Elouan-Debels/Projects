{"aid": "39980895", "title": "Intel details Guadi 3 at Vision 2024", "url": "https://www.tomshardware.com/pc-components/cpus/intel-details-guadi-3-at-vision-2024-new-ai-accelerator-sampling-to-partners-now-volume-production-in-q3", "domain": "tomshardware.com", "votes": 1, "user": "bentaber", "posted_at": "2024-04-09 16:08:18", "comments": 0, "source_title": "Intel details Gaudi 3 at Vision 2024 \u2014 new AI accelerator sampling to partners now, volume production in Q3", "source_text": "Intel details Gaudi 3 at Vision 2024 \u2014 new AI accelerator sampling to partners now, volume production in Q3 | Tom's Hardware\n\nSkip to main content\n\nWhen you purchase through links on our site, we may earn an affiliate\ncommission. Here\u2019s how it works.\n\n# Intel details Gaudi 3 at Vision 2024 \u2014 new AI accelerator sampling to\npartners now, volume production in Q3\n\nNews\n\nBy Paul Alcorn\n\npublished 1 hour ago\n\nGPUs fall the wayside as Intel goes all-in on Gaudi for AI.\n\n(Image credit: Intel)\n\nIntel made a slew of announcements during its Vision 2024 event today,\nincluding deep-dive details of its new Gaudi 3 AI processors, which it claims\noffer up to 1.7X the training performance, 50% better inference, and 40%\nbetter efficiency than Nvidia\u2019s market-leading H100 processors, but for\nsignificantly less money. Intel also announced new branding for its data\ncenter CPU portfolio, with the Granite Rapids and Sierra Forest chips now\nbranded as the new \u2018Xeon 6\u2019 family. Those chips are on track to come to market\nthis year and add support for the new performance-boosting standardized MXFP4\ndata format, which you can read about here.\n\nIntel also announced that it is developing an AI NIC ASIC for Ultra Ethernet\nConsortium-compliant networking and an AI NIC chiplet that will be used in its\nfuture XPU and Guadi 3 processors while also being made available to external\ncustomers through Intel Foundry, but it didn\u2019t share more details on these\nnetworking products.\n\nNvidia\u2019s dominance in AI infrastructure and software is undisputed. Still,\nIntel, like AMD, is looking to carve out a position as the premier alternative\nto Nvidia as the industry continues to struggle with Nvidia's crushing AI GPU\nshortages. To that end, Intel also outlined the full breadth of its AI\nenablement programs, which stretch from hardware to software, as it looks to\ngain traction in the booming AI market that Nvidia and AMD currently dominate.\nIntel\u2019s efforts focus on developing its partner ecosystem to deliver complete\nGaudi 3 systems while also working to build an open enterprise software stack\nto serve as an alternative to Nvidia\u2019s proprietary CUDA.\n\nIntel provided deep-dive details of the Gaudi 3 architecture along with plenty\nof convincing benchmarks against Nvidia\u2019s existing H100 GPUs (data for the\nupcoming Blackwell systems isn\u2019t available yet). First, let\u2019s take a closer\nlook at the Gaudi 3 architecture.\n\n## Intel Gaudi 3 Specifications\n\nIntel\u2019s Gaudi 3 marks the third generation of the Gaudi accelerator, which was\nthe fruit of Intel\u2019s $2 billion acquisition of Habana Labs in 2019. The Gaudi\naccelerators will enter high-volume production and general availability in Q3\nof 2024 in OEM systems. Intel will also make Gaudi 3 systems in its Developer\nCloud, thus providing a fast on-ramp for prospective customers to test the\nchips.\n\nImage 1 of 8\n\n(Image credit: Intel)\n\n(Image credit: Intel)\n\n(Image credit: Intel)\n\n(Image credit: Intel)\n\n(Image credit: Intel)\n\n(Image credit: Intel)\n\n(Image credit: Intel)\n\n(Image credit: Intel)\n\nGaudi comes in two form factors, with the OAM (OCP Accelerator Module) HL-325L\nbeing the common mezzanine form factor found in high-performance GPU-based\nsystems. This accelerator has 128GB of HBM2e (not HBM3E), providing 3.7 TB/s\nof bandwidth. It also has twenty-four 200 Gbps Ethernet RDMA NICs. The HL-325L\nOAM module has a 900W TDP (higher TDPs are possible, ostensibly with liquid\ncooling) and is rated for 1,835 TFLOPS of FP8 performance. The OAMs are\ndeployed in groups of eight per server node and can then scale up to 1,024\nnodes.\n\nIntel claims Gaudi 3 provides twice the FP8 and four times more BF16\nperformance than the prior generation, along with twice the network bandwidth\nand 1.5X the memory bandwidth.\n\n## Stay on the Cutting Edge\n\nJoin the experts who read Tom's Hardware for the inside track on enthusiast PC\ntech news \u2014 and have for over 25 years. We'll send breaking news and in-depth\nreviews of CPUs, GPUs, AI, maker hardware and more straight to your inbox.\n\nBy submitting your information you agree to the Terms & Conditions and Privacy\nPolicy and are aged 16 or over.\n\nThe OAMs drop into a universal baseboard that houses eight OAMs. Intel has\nalready shipped OAMs and baseboards to its partners as it prepares for general\navailability later this year. Scaling to eight OAMs on the HLB-325 baseboard\nbrings performance to 14.6 PFLOPS of FP8, while all other metrics, such as\nmemory capacity and bandwidth, scale linearly.\n\nIntel also has a Gaudi 3 PCIe dual-slot add-in card with a 600W TDP. This card\nalso has 128GB of HBMeE and twenty-four 200 Gbps Ethernet NICs \u2014 Intel says\ndual 400 Gbps NICs are used for scale-out. Intel says the PCIe card has the\nsame peak 1,835 TFLOPS of FP8 performance as the OAM, which is interesting\ngiven its 300W lower TDP (this likely won't hold in long-duration workloads).\nHowever, scaling is more limited inside the box, as it is designed to work in\ngroups of four. Intel says this card can also scale out to create larger\nclusters but didn\u2019t provide details.\n\nDell, HPE, Lenovo, and Supermicro will provide systems for the Gaudi 3 launch.\nGaudi air-cooled models have already been sampled, with sampling of liquid-\ncooled models to follow in Q2. These will be generally available (mass\nproduction) in Q3 and Q4 of 2024, respectively. The PCIe card will also be\navailable in Q4.\n\nImage 1 of 4\n\n(Image credit: Intel)\n\n(Image credit: Intel)\n\n(Image credit: Intel)\n\n(Image credit: Intel)\n\nGaudi 3 leverages the same architecture and underlying fundamental principles\nas its predecessor but comes with a more advanced TSMC 5nm process than the\nTSMC 7nm node Intel uses for the Gaudi 2 accelerator.\n\nThe OAM design has two central 5nm dies with 96MB of SRAM split between the\ntwo, providing 12.8 TB/s of bandwidth. The dies are flanked by eight HBM2E\npackages, totaling 128GB, that deliver up to 3.7 TB/s of bandwidth. A high-\nbandwidth interconnect between the two dies provides access to all memory\npresent on both dies, thus allowing it to look and act as a single device (at\nleast as far as the software is concerned - latency might vary). Gaudi 3 also\nhas a x16 PCIe 5.0 controller for communication with the host processor (CPU)\nand different ratios of CPUs and Gaudi accelerators can be employed.\n\nCompute is handled by 64 fifth-gen Tensor Processing Cores (TPC) and eight\nmatrix math engines (MME), with workloads orchestrated between the two engines\nby a graph compiler and the software stack. The Gaudi 3 chip package also\nincludes twenty-four 200 Gbps RoCE Ethernet controllers that provide both\nscale-up (in-box) and scale-out (node-to-node) connectivity, doubling the 100\nGbps connections on Gaudi 2.\n\n## Gaudi 3 Scalability\n\nImage 1 of 5\n\n(Image credit: Intel)\n\n(Image credit: Intel)\n\n(Image credit: Intel)\n\n(Image credit: Intel)\n\n(Image credit: Intel)\n\nAt the end of the day, the key to dominating today\u2019s AI training and inference\nworkloads resides in the ability to scale accelerators out into larger\nclusters. Intel\u2019s Gaudi takes a different approach than Nvidia\u2019s looming B200\nNVL72 systems, using fast 200 Gbps Ethernet connections between the Gaudi 3\naccelerators and pairing the servers with leaf and spine switches to create\nclusters.\n\nNvidia\u2019s system-level architecture utilizes NVLink over the PCIe interface for\nboth in-box connectivity between GPUs and stretching out to connect entire\nracks with passive copper cabling via its NVLink switches. AMD also has its\nown approach of using the PCIe interface and its Infinity Fabric protocol\nbetween GPUs residing in the server while using external NICs for\ncommunication with other nodes, but this adds more networking cost and\ncomplexity than Intel\u2019s approach of having networking NICs built right into\nthe chip.\n\nThanks to the doubled network bandwidth, Gaudi 3 scales from a single node\nwith 8 OAM Gaudis to clusters with up to 1,024 nodes (servers) housing 8,192\nOAM devices.\n\nEach server is comprised of eight Gaudi 3 accelerators communicating with each\nother via twenty-one 200 Gbps Ethernet connections apiece. The remaining three\nEthernet ports on each device are used for external communication with the\ncluster via a leaf switch. The switch aggregates these connections into six\n800 Gbps Ethernet ports with OFSP connectors to facilitate communication with\nother nodes.\n\nEach rack typically contains four nodes, but this can vary based on rack power\nlimits and the cluster size. Up to 16 nodes form a single sub-cluster with\nthree Ethernet leaf switches, which then connect to spine switches, typically\nwith 64 ports, to form even larger clusters. Half of the 64 ports on the 800\nGbps leaf switch are connected to the 16 nodes, while the remaining half\nconnect to the spine switches.\n\nVarying numbers of spine switches are used based on the size of the cluster,\nwith Intel providing an example of three spine switches used for 32 sub-\nclusters comprised of 512 nodes (4,096 Gaudi\u2019s). Intel says this configuration\nprovides equal bandwidth for all server-to-server connections (non-blocking\nall-to-all). Adding another layer of Ethernet switches can support up to tens\nof thousands of accelerators.\n\n## Gaudi 3 performance vs Nvidia H100\n\nImage 1 of 10\n\n(Image credit: Intel)\n\n(Image credit: Intel)\n\n(Image credit: Intel)\n\n(Image credit: Intel)\n\n(Image credit: Intel)\n\n(Image credit: Intel)\n\n(Image credit: Intel)\n\n(Image credit: Intel)\n\n(Image credit: Intel)\n\n(Image credit: Intel)\n\nIntel shared performance projections for Gaudi 3, but as with all vendor-\nprovided benchmarks, we should take these with a grain of salt. As you\u2019ll see\nin the last image in the above album, Intel is now simply providing a QR code\nfor information about its benchmarks instead of the line-by-line details of\ntest configurations that it has provided in the past. This code doesn\u2019t\nprovide us with any meaningful way to look at the closer details of these test\nresults and configurations, so add a shovelful of salt to any of these\nbenchmark claims.\n\nIntel compared to publicly available benchmarks for H100 systems but didn\u2019t\ncompare to Nvidia\u2019s upcoming Blackwell B200 due to a lack of real-world\ncomparative data. The company also didn\u2019t provide comparisons to AMD\u2019s\npromising Instinct MI300 GPUs, but that's impossible because AMD has continued\nto avoid publishing public performance data in the industry-accepted MLPerf\nbenchmarks.\n\nIntel provided plenty of comparisons in both training and inference workloads\ncompared to the H100 with similar cluster sizes, but the key takeaway is that\nIntel claims Gaudi is from 1.5X to 1.7X faster in training workloads.\nComparisons include LLAMA2-7B (7 billion parameters) and LLAMA2-13B models\nwith 8 and 16 Gaudi\u2019s, respectively, and a GPT 3-175B model tested with 8,192\nGaudi accelerators, all using FP8. Interestingly, Intel didn\u2019t compare to\nNvidia\u2019s H200 here, which has 76% more memory capacity and 43% more memory\nbandwidth than the H100.\n\nIntel did compare to the H200 for its inference comparisons but stuck to\nperformance with a single card as opposed to comparing scale-out performance\nwith clusters. Here, we can see a mixed bag, with five of the LLAMA2-7B/70B\nworkloads coming in 10 to 20% below the H100 GPUs, while two match and one\nslightly exceeds the H200. Intel claims Gaudi's performance scales better with\nlarger output sequences, with Gaudi delivering up to 3.8 times the performance\nwith the Falcon 180 billion parameter model with a 2048-length output.\n\nIntel also claims up to a 2.6X advantage in power consumption for inference\nworkloads, a critical consideration considering restrictive power limits in\ndata centers, but it didn\u2019t provide similar benchmarks for training workloads.\nFor these workloads, Intel tested a single H100 in a public instance and\nlogged the H100\u2019s power consumption (as reported by the H100) but didn\u2019t\nprovide examples of inference with a single node or larger clusters. With\nlarger output sequences, Intel again claims better performance and, thus,\nefficiency.\n\n## Gaudi 3 software ecosystem\n\nImage 1 of 5\n\n(Image credit: Intel)\n\n(Image credit: Intel)\n\n(Image credit: Intel)\n\n(Image credit: Intel)\n\n(Image credit: Intel)\n\nAs Nvidia\u2019s dominance with CUDA has illustrated, the software ecosystem is\njust as critical of a consideration as the hardware. Intel touts its end-to-\nend software stack and says that \u201cmost\u201d of its engineers are currently working\non bolstering support. Intel\u2019s current focus is on supporting multi-modal\ntraining and inference models and RAG (retrieval augmented generation).\n\nHugging Face has over 600,000 AI model checkpoints available, and Intel says\nits work with Hugging Face, PyTorch, DeepSpeed, and Mosaic has eased the\nsoftware porting process to enable faster turnaround times for deploying Gaudi\n3 systems. Intel says most programmers are programming at the framework level\nor above (i.e., simply using PyTorch and scripting with Python) and that low-\nlevel programming with CUDA isn\u2019t as common as perceived.\n\nIntel\u2019s tools are designed to ease the porting process while abstracting away\nthe underlying complexity, with OneAPI serving as the underlying kernel and\ncommunication libraries. These libraries adhere to the specifications outlined\nby the Unified Accelerator Foundation (UXL), and industry consortium that\nincludes Arm, Intel, Qualcomm, and Samsung, among others, that's intended to\nprovide an alternative to CUDA. PyTorch 2.0 is optimized for using OneAPI for\ninference and training with Intel CPUs and GPUs. Intel says its OpenVino also\ncontinues to enjoy rapid adoption, with over one million downloads so far this\nyear.\n\n## Thoughts\n\nAs we covered above, Intel, Nvidia, and AMD are all taking different paths to\nproviding the enhanced cluster scalability that is key to performance in both\nAI training and inference workloads. Each approach has its own respective\nstrengths, but Nvidia's proprietary NVLink is the most mature and well-\nestablished solution, and its extension to rack-scale architectures is a\nsignificant advantage. That said, Intel's approach with Ethernet-based\nnetworking brings an open solution that affords plenty of customization\noptions by supporting networking switches from multiple vendors, and its\nbuilt-in NICs also offer cost advantages over AMD's competing Instinct MI300\nseries.\n\nHowever, both Nvidia's Grace-based products and AMD's MI300A offer\nsophisticated merged CPU+GPU packages that will be hard to beat in some\nworkloads, while Intel continues to rely on separate CPU and accelerator\ncomponents due to its cancellation of the merged CPU+GPU version of Falcon\nShores. There have been reports of Nvidia's new GB200 CPU+GPU servers\ncomprising the bulk of the company's Blackwell orders, highlighting the\nindustry's voracious appetite for these types of tightly-coupled products.\n\nIntel's future Falcon Shores product will arrive as an AI accelerator-only\ndesign, so it will still be able to compete with GPU-only Nvidia and AMD\nclusters. We also see room for a refresh generation of Gaudi 3 that moves from\nHBM2E to HBM3/E\u2014both AMD and Nvidia employ the faster memory in their AI\nproducts. Although it hasn't shared hard data, Intel says it also plans to\ncompete aggressively on pricing, which might be a powerful recipe as Nvidia\ncontinues to grapple with shortages due to crushing demand for its GPUs.\n\nFalcon Shores will also be compatible with code optimized for Gaudi, providing\nforward compatibility. Intel also cites a 3X improvement to its Gaudi 2\nplatform over the last several quarters as an example of increased adoption of\nits platform.\n\nNotably, Intel didn't promote its Ponte Vecchio GPUs for the event, which\nisn't surprising given its cancellation of the next-gen Rialto Bridge GPUs, so\nwe expect the company's AI efforts to coalesce solely on Gaudi 3 as it\nprepares Falcon Shores for launch next year.\n\nThe air-cooled Gaudi 3 models are already sampling to partners, with general\navailability in Q3. Liquid-cooled models will follow in Q4. We're watching the\nIntel Vision webcast for further details and will update as neccesary.\n\nPaul Alcorn\n\nManaging Editor: News and Emerging Tech\n\nPaul Alcorn is the Managing Editor: News and Emerging Tech for Tom's Hardware\nUS. He also writes news and reviews on CPUs, storage, and enterprise hardware.\n\nSee more CPUs News\n\nMore about cpus\n\nIntel unveils new Xeon 6 branding for Granite Rapids and Sierra Forest\nprocessors \u2014 efficiency core models launch this quarter; performance-core\nmodels come soon after\n\nMicrosoft confident Snapdragon X Elite will defeat M3 MacBook Air laptops \u2014\nprepares demos to showcase this and will put the chip into Surface laptops\n\nLatest\n\nIntel reportedly plans to launch Arc 'Battlemage' GPUs before the holidays \u2014\nSecond-gen Arc prepares for takeoff this fall\n\nSee more latest \u25ba\n\nNo comments yet Comment from the forums\n\n##### Most Popular\n\nMemory makers reportedly stop publishing contract DRAM prices following Taiwan\nearthquake \u2014 further price hikes are expected\n\nBy Anton ShilovApril 08, 2024\n\nGamerTech's Magma Glove is a heated compression glove targeted at competitive\ngamers\n\nBy Christopher HarperApril 08, 2024\n\nUS CHIPS and Science program puts R&D funding on hold \u2014 highlighting intense\ndemand that far exceeded initial expectations\n\nBy Anton ShilovApril 08, 2024\n\nRaspberry Pi RP2040 keeps track of who's winning and losing with this wireless\nLED scoreboard\n\nBy Ash HillApril 08, 2024\n\nSeagate BarraCuda 530 SSD breaks cover with 7,400 MB/s speeds \u2014 Still PCIe\n4.0, but looks like a significant upgrade over the previous 520\n\nBy Zhiye LiuApril 08, 2024\n\nTSMC gets $6.6 billion in cash and $5 billion in loans from CHIPS Act, plans\nthird US fab\n\nBy Anton ShilovApril 08, 2024\n\nAcer's overclocked RX 7800 XT cards arrive \u2014 Shadow Knight in China, Nitro in\nthe US\n\nBy Christopher HarperApril 07, 2024\n\nBattery-powered AIO workstation forged from Framework laptop and 3D printed\ncomponents \u2014 the Flying Lotus takes off\n\nBy Christopher HarperApril 07, 2024\n\nFloppy disk-reliant San Francisco train control system spurs concerns of\n'catastrophic failure' \u2014 and it won't be replaced for at least another decade\n\nBy Mark TysonApril 07, 2024\n\nBest Buy Geek Squad agents raise specter of mass layoffs in unofficial\ncommunity forums \u2014 some may have been displaced by AI\n\nBy Roshan Ashraf ShaikhApril 07, 2024\n\nChinese gov't pushes Huawei's HarmonyOS hard, sets adoption targets to beat\nWindows, Android, and iOS\n\nBy Anton ShilovApril 07, 2024\n\nTom's Hardware is part of Future US Inc, an international media group and\nleading digital publisher. Visit our corporate site.\n\n\u00a9 Future US, Inc. Full 7th Floor, 130 West 42nd Street, New York, NY 10036.\n\n", "frontpage": false}
