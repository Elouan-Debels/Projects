{"aid": "40005005", "title": "Using ClickHouse to scale an events engine", "url": "https://github.com/getlago/lago/wiki/Using-Clickhouse-to-scale-an-events-engine", "domain": "github.com/getlago", "votes": 38, "user": "wyndham", "posted_at": "2024-04-11 18:02:52", "comments": 13, "source_title": "Using Clickhouse to scale an events engine", "source_text": "Using Clickhouse to scale an events engine \u00b7 getlago/lago Wiki \u00b7 GitHub\n\nSkip to content\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\ngetlago / lago Public\n\n  * Notifications\n  * Fork 244\n  * Star 5.9k\n\n# Using Clickhouse to scale an events engine\n\nJump to bottom\n\nAnh-Tho Chuong edited this page Feb 24, 2024 \u00b7 1 revision\n\nLike many companies, we had to change our database stack midway while scaling\nour core product Lago, an open-source usage-based billing platform. As we grew\nmore popular, we began ingesting millions of events every minute. And our\nrudimentary Postgres-only stack wasn\u2019t cutting it. We were suffering heavy\nload times, impacting our entire app\u2019s performance.\n\nAfter some exploration, we decided to use a distributed ClickHouse instance\nstrictly for our streamed events. Our analytics services were now able to\ndirectly query ClickHouse, an OLAP database. For all other data needs, we kept\nPostgres.\n\nThe strategy was successful. Since the refactor, we haven\u2019t looked back.\n\nToday, we\u2019re going to explore that decision for a hybrid database stack, and\nmore specifically, why we decided to go with ClickHouse.\n\n## OLTP versus OLAP databases\n\nMost developers, including junior developers, have experience using OLTP\n(online transactional processing) databases such as Postgres. As the name\nimplies, OLTP databases are designed for processing transactions. A\ntransaction is one of many different types of instructions that software might\ninvoke to a database. The most common are: (i) read, (ii) insert, (iii) update\nand (iv) delete.\n\nOLTP databases are typically general-purpose databases. Because they support\nevery type of data processing, they could be used for any data problem within\nlimits. And, even at a large scale, they are fantastic for software that\nrequire:\n\n  * atomic transactions, where a set of grouped transactions either all occur or don\u2019t occur at all\n  * consistency, where queries in-between writes and updates are deterministic and predictable\n\nFor most problems, these are important qualities. For some, they are crucial.\nA banking application can\u2019t have discrepancies whenever money is transferred\nbetween accounts. For those problems, an OLTP database is needed for cents-\nlevel accuracy. Today, we still use Postgres as our primary database,\nconfigured [via our database.yml file](https://github.com/getlago/lago-\napi/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/config/database.yml#L5). And\ngiven that we use Ruby on Rail\u2019s, [our Postgres\nschema](https://github.com/getlago/lago-\napi/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/db/schema.rb) is\nautomatically generated by Rail\u2019s [Active\nRecord](https://guides.rubyonrails.org/active_record_basics.html), an ORM that\nmanages our various models such as [charges](https://github.com/getlago/lago-\napi/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/spec/models/charge_spec.rb),\n[credit notes](https://github.com/getlago/lago-\napi/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/spec/models/credit_note_spec.rb),\n[invoices](https://github.com/getlago/lago-\napi/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/spec/models/invoice_spec.rb),\n[invites](https://github.com/getlago/lago-\napi/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/spec/models/invite_spec.rb),\n[fees](https://github.com/getlago/lago-\napi/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/spec/models/fee_spec.rb),\n[coupons](https://github.com/getlago/lago-\napi/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/spec/models/coupon_spec.rb),\nand [much, much more](https://github.com/getlago/lago-\napi/tree/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/spec/models). We write some\ncustom queries given [the performance limits of the\nORM](https://github.com/getlago/lago/wiki/Is-ORM-still-an-%27anti-\npattern%27%3F), but otherwise lean heavily on Active Record for most\ntransactions.\n\nSo where do OLAP (online analytical processing) databases like ClickHouse come\nin? Well, Postgres was designed to be strictly atomic and consistent; two\nproperties that require for data to be fully ingested before any query that\nmight process them is run. This creates a problem for tables where entries are\ningested in the millions per minutes (e.g. billable events, especially those\nfor infrastructure services like managed servers). Specifically, the issue\nisn\u2019t ingesting data, but rather simultaneously handling expensive analytical\nqueries without locking up the queue. These data-summarizing problems are\nwhere OLAP databases like ClickHouse shine.\n\nOLAP databases are designed for two primary problems\u2014(i) efficiently answering\ncomplex read queries with approximate accuracy and (ii) batch processing a\nlarge number of write queries. However, OLAP databases are terrible for\nmutating data (where the entire database often needs to be re-written) or\ndeleting data.\n\nDifferent OLAP solutions (e.g. ClickHouse, QuestDB, Druid) have different\nstrengths, and we\u2019ll dive into the specific strain of traits that made\nClickHouse a winning solution in the next section. But all OLAP solutions\nshare a common quality\u2014data is stored in an inverted layout relative to OLTP\ndatabases like Postgres.\n\nNow, from the user\u2019s standpoint, the table\u2019s columns and rows are still just\ncolumns and rows. But, physically in memory, data is scanned column-by-column,\nnot row-by-row. This makes aggregations\u2014such as adding every value in a\ncertain field\u2014very, very fast, as the relevant data is read sequentially.\n\n## Enter ClickHouse, our chosen OLAP solution\n\n[ClickHouse](https://clickhouse.com) is an open-source tool spun out from a\nclosed-source algorithm used by Yandex\u2019s website analytics product. Today,\nClickHouse is shepherded by [ClickHouse\nInc](https://clickhouse.com/company/our-story) with notable contributions by\n[Altinity](https://altinity.com). To date, it is one of the most successful\nOLAP databases, both commercially and qualitatively.\n\nClickHouse has three notable features that make it an analytics powerhouse\u2014(i)\ndynamic materialized views, (ii) specialized engines, and (iii) vectorized\nquery execution.\n\nTo summarize each:\n\n  * Dynamic Materialized Views. Materialized Views are query-able views that are generated from raw data in underlying tables. While many databases do support materialized views, including Postgres, ClickHouse\u2019s materialized views are dynamic, efficiently refreshing content whenever new content is ingested. These contrasts with ordinary materialized views which are just snapshots of a specific point of time, and are very expensive to refresh.\n  * Specialized Engines. Many databases have a single engine for utilizing hardware to process queries / transactions. ClickHouse, however, has dedicated engines for specific mathematical functions, such as summing or averaging numbers.\n  * Vectorized Query Execution. ClickHouse\u2019s specialized engines leverage vectorized query execution, where the hardware uses multiple units in parallel to achieve a communal result (known as SIMD\u2014Single Instruction, Multiple Data).\n\nCombined with its columnar storage, these traits allow ClickHouse to easily\nsum, average, or generally aggregate database values.\n\nAs a caveat, Postgres isn\u2019t entirely incapable of achieving similar results,\nbut only via a bastion of optimizations. For instance, there is a third-party\n[vectorized\nexecutor](https://github.com/citusdata/postgres_vectorization_test) designed\nfor Postgres that imitates ClickHouse\u2019s native support. There is also [a Fast\nRefresh Module](https://aws.amazon.com/blogs/database/building-fast-refresh-\ncapability-in-amazon-rds-for-postgresql/) that uses Postgres\u2019s log to\ndynamically update materialized views. Coupled with Postgres triggers,\ndevelopers could create a ClickHouse-like set-up. But all of these techniques\nrequire significant set-up work and additional columns to reach any efficiency\nthat is even comparable to ClickHouse\u2019s.\n\nA relevant meme from my Postgres vs Clickhouse guide for PostHog\n\nRecently, the most interesting rift in the Postgres vs OLAP space is\n[Hydra](https://www.hydra.so), an open-source, column-oriented distribution of\nPostgres that was very recently launched (after our migration to ClickHouse).\nHad Hydra been available during our decision-making time period, we might\u2019ve\nmade a different choice. However, ClickHouse remains an incredible pick, given\nits mature product, large community, hardware optimizations, and ease of use\nside-by-side with Postgres.\n\nOf course, migrating analytics processes to ClickHouse is only half the\nbattle. The next is actually deploying ClickHouse to production\u2014where a few\nstrategies exist.\n\n## How we utilize ClickHouse\n\nWhen discussing our ClickHouse implementation, there are fundamentally two\ndifferent topics\u2014what we use ClickHouse for, and how our ClickHouse instance\nis deployed and maintained.\n\n### What we query ClickHouse for\n\nOur ClickHouse instance [ingests raw billable\nevents](https://github.com/getlago/lago-\napi/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/app/models/clickhouse/events_raw.rb#L3)\ndispatched by our users. While we don\u2019t write our own ClickHouse schema (as it\nis auto-generated by ActiveRecord), it is written to a file, [available in our\nopen-source repository](https://github.com/getlago/lago-\napi/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/db/clickhouse_schema.rb#L4).\nOur ClickHouse instance only has two tables\u2014raw_events and\nraw_events_queue\u2014alongside one materialized view, events_raw_mv . That\u2019s it.\nWe don\u2019t store any of the other \u201cbusiness-critical\u201d data on ClickHouse because\nthey aren\u2019t analytical queries.\n\nIn detail, our [raw_events_queue](https://github.com/getlago/lago-\napi/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/db/clickhouse_migrate/20231026124912_create_events_raw_queue.rb)\nis where events are initially streamed to via [Apache\nKafka](https://kafka.apache.org), open-source event streaming software. From\nit, the [events_raw_mv](https://github.com/getlago/lago-\napi/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/db/clickhouse_migrate/20231030163703_create_events_raw_mv.rb)\nis generated with ClickHouse\u2019s [cast()](https://clickhouse.com/docs/en/sql-\nreference/functions/type-conversion-functions) function, which maps the\nevent\u2019s metadata from a JSON blob to a string array. Finally, this\nmaterialized view pushes data to the\n[raw_events](https://github.com/getlago/lago-\napi/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/db/clickhouse_migrate/20231024084411_create_events_raw.rb)\ntable. This is a [MergeTree](https://clickhouse.com/docs/en/engines/table-\nengines/mergetree-family/mergetree) table that is apt for a large number of\nwrites.\n\nraw_events is what Lago\u2019s general codebase interfaces with via our\n[ClickHouseStores](https://github.com/getlago/lago-\napi/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/app/services/events/stores/clickhouse_store.rb#L14)\nclass, which is tapped when [aggregating billable\nmetrics](https://github.com/getlago/lago-\napi/blob/e0da0a0b136577bffe5a1b8dac8747c913f7cdf1/app/services/billable_metrics/aggregation_factory.rb#L18).\nraw_events uses a tuple of organization_id, external_subscription_id, code,\nand a timestamp as primary keys; given ClickHouse\u2019s [sophisticated support for\nprimary key tuples](https://medium.com/datadenys/how-clickhouse-primary-key-\nworks-and-how-to-choose-it-4aaf3bf4a8b9), this helps ClickHouse locate rows\nvery quickly.\n\n### How we deploy ClickHouse\n\nBecause ClickHouse is an open-source database, it could be self-hosted on any\nordinary Linux server. However, many companies trust managed database\nsolutions because they (i) often reduce overall costs, (ii) make scaling\ndatabases easier, and (iii) take care of safe replication/backups.\n\nOne of the most popular options is ClickHouse Inc\u2019s ClickHouse Cloud offering,\nwhich offers a serverless ClickHouse instance with decoupled compute and\nstorage.\n\nHowever, we instead opted for Altinity Operator, which deploys and manages\nClickHouse in a Kubernetes cluster in our existing cloud offering. We\npreferred this approach given more flexibility due to custom definitions,\nefficiency on cost, and ease of maintenance.\n\n## Other notable open-source projects that use ClickHouse\n\nWe aren\u2019t the only open-source project that uses ClickHouse; in fact, we\naren\u2019t even the only open-source project that migrated from Postgres to\nClickHouse. A notable example is [PostHog](https://posthog.com), an open-\nsource analytics suite that switched from [Postgres to\nClickHouse](https://posthog.com/blog/clickhouse-announcement) given the sheer\namount of web events they were processing per second.\n\nAnother great example is Gitlab, which used ClickHouse to store data of\nstreamed events [in their observability\nsuite](https://docs.gitlab.com/ee/architecture/blueprints/clickhouse_usage/).\nIn general, it\u2019s common for open-source companies (and closed-source projects\nalike) to find their general-purpose database like Postgres or mySQL ill-\nsuited as they start to scale.\n\nEven some closed-source solutions, like the HTTP data-streaming product\nTinyBird, have made [open-source contributions to\nClickHouse](https://www.tinybird.co/blog-posts/we-launched-an-open-source-\nclickhouse-knowledge-base) given their dependence on it. Slowly, ClickHouse is\nbuilding the same level of success in the OLAP world as Postgres is achieving\nin the OLTP space.\n\n### Closing Thoughts\n\nDue to the hardware optimizations of inverting table layouts, there is no one-\nsize-fits-all database as applications scale. We ran into that problem fairly\nearly in our journey given the event-heavy nature of our product. However,\nthat doesn\u2019t meant that every team needs to start with an OLTP + OLAP\nstack\u2014just to be ready for it when the moment arrives.\n\n##### Clone this wiki locally\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": true}
