{"aid": "39985137", "title": "Recent Advances on Foundation Models", "url": "https://cs.uwaterloo.ca/~wenhuche/teaching/cs886/", "domain": "uwaterloo.ca", "votes": 1, "user": "oj2828", "posted_at": "2024-04-09 22:44:20", "comments": 0, "source_title": "CS 886: Recent Advances on Foundation Models", "source_text": "CS 886: Recent Advances on Foundation Models\n\n# CS 886: Recent Advances on Foundation Models\n\n## Winter 2024\n\n## People\n\n  * Instructor:\n\n    * Dr. Wenhu Chen, (wenhuchen [at] uwaterloo [dot] ca)\n  * TAs:\n\n    * Cong Wei (cong.wei [at] uwaterloo [dot] ca)\n\n## Timetable\n\nLectures will take place once per week as follows\n\n  * Davis Center 2585 every Wednesday from 12:00 PM - 2:50 PM\n  * Session 1: 12:00PM - 1:20PM on Wednesday\n  * Session 2: 1:30PM - 2:50PM on Wednesday\n  * The Jan 10th will be instructed by me. The following lectures will be student presentation and project\n\n## Deliverables\n\n  * [1] In-class Presentation. Two students will pair up to (1) create slides, (2) co-present it to the class, (3) leading the discussion period. The presentation is 80 minutes.\n  * [2] Reading Notes. Student needs to submit one reading note regarding the recent papers of their interest. The paper could be anything in the area of deep learning and foundation models.\n  * [3] Course Project. The group size is up to 2. The student needs to submit a high-quality 8-page report plus a final in-class presentation.\n  * Submit presentation material & reading notes & project to LEARN .\n\n## Assessment\n\n  * [1] In-class Presentation: 40%. [First submission deadline on Feb 7th. The second deadline on April 1st.]\n  * [2] Reading Notes: 10%. [deadline on April 1st.]\n  * [3] Course project: 50%. [1-page proposal deadline on Feb 28st. 8-page final report deadline on April 10th.]\n\n## Communication\n\n  * All communication should take place using the Piazza discussion board.\n  * We do not upload materials or assignments to Piazza, instead, these materials will appear on LEARN\n  * Sign up for Piazza (if you're not already) here.\n  * Public Piazza posts (can be anonymous) are the preferred method for questions about course material, etc. Students can then help each other and instructors can read/reply.\n  * Private Piazza posts (to instructors only) can be used for any posts that contain solution snippets or private questions.\n  * Only in exceptional cases where you need to contact only the instructor should you use the personal email above.\n\n## Project\n\n  * The project needs to be related to foundation models like text or vision or multimodal models.\n  * The project needs to contain the following sections: introduction, problem definition, algorithm design, experiments, evaluation, conclusion.\n  * Please use Latex template as a template to write the final report.\n  * Please make sure the code are not copied from public repository. Any violation will be seen as plagiarism with serious consequences.\n\n## Lectures\n\nLecture Number| Topic| Date| Slides| References  \n---|---|---|---|---  \n  \n#### Introduction to Foundation Models  \n  \n1| Foundation Models and its Applications| Jan 10th| Slides|\n\n  * On the Opportunities and Risks of Foundation Models\n  * Multimodal Foundation Models: From Specialists to General-Purpose Assistants\n  * Large Multimodal Models: Notes on CVPR 2023 Tutorial\n  * Towards Generalist Biomedical AI\n  * A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT\n  * Interactive Natural Language Processing\n  * Towards Reasoning in Large Language Models: A Survey\n\n  \n2| Course Logistics| Jan 10th|\n\n  * Building Teams\n  * Slides Requirements\n  * Project Requirements\n\n  \n3| RNN & CNN| Jan 17th| Slides Video|\n\n  * Recurrent Neural Networks (RNNs): A gentle Introduction and Overview\n  * Highway Networks\n  * Long Short-Term Memory\n  * Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling\n  * Effective Approaches to Attention-based Neural Machine Translation\n  * An Introduction to Convolutional Neural Networks\n  * ImageNet Classification with Deep Convolutional Neural Networks\n  * U-Net: Convolutional Networks for Biomedical Image Segmentation\n  * Deep Residual Learning for Image Recognition\n  * Densely Connected Convolutional Networks\n  * Aggregated Residual Transformations for Deep Neural Networks\n  * A ConvNet for the 2020s\n\n  \n4| NLP & CV| Jan 17th| Slides|\n\n  * Sequence to Sequence Learning with Neural Networks\n  * Thumbs up? Sentiment Classification using Machine Learning Techniques\n  * A survey of named entity recognition and classification\n  * Teaching Machines to Read and Comprehend\n  * Deep neural networks for acoustic modeling in speech recognition\n  * A Neural Attention Model for Sentence Summarization\n  * Microsoft COCO: Common Objects in Context\n  * Rich feature hierarchies for accurate object detection and semantic segmentation\n  * Fully Convolutional Networks for Semantic Segmentation\n  * DeepFace: Closing the Gap to Human-Level Performance in Face Verification\n  * DeepPose: Human Pose Estimation via Deep Neural Networks\n  * Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks\n\n  \n  \n#### Transformer Architecture  \n  \n5| Self Attention & Transformers| Jan 24th| Slides Video|\n\n  * Neural Machine Translation by Jointly Learning to Align and Translate\n  * Show, Attend and Tell: Neural Image Caption Generation with Visual Attention\n  * Attention Is All You Need\n  * Annotated Transformer\n  * Image Transformer\n  * An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\n\n  \n6| Efficient Transformers| Jan 24th| Slides Video1 Video2|\n\n  * Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention\n  * Perceiver: General Perception with Iterative Attention\n  * Random Feature Attention\n  * Longformer: The Long-Document Transformer\n  * Generating Long Sequences with Sparse Transformers\n  * Linformer: Self-Attention with Linear Complexity\n  * Efficiently Modeling Long Sequences with Structured State Spaces\n\n  \n7| Parameter-efficient Tuning| Jan 31st| Slides Video|\n\n  * Parameter-Efficient Transfer Learning for NLP\n  * LoRA: Low-Rank Adaptation of Large Language Models\n  * The Power of Scale for Parameter-Efficient Prompt Tuning\n  * It's Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners\n  * Making Pre-trained Language Models Better Few-shot Learners\n  * Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning\n  * Towards a Unified View of Parameter-Efficient Transfer Learning\n\n  \n8| Language Model Pretraining| Jan 31th| Slides Video1 Video2|\n\n  * Deep contextualized word representations\n  * BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n  * Unified Language Model Pre-training for Natural Language Understanding and Generation\n  * ALBERT: A Lite BERT for Self-supervised Learning of Language Representations\n  * RoBERTa: A Robustly Optimized BERT Pretraining Approach\n  * ERNIE: Enhanced Language Representation with Informative Entities\n  * ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators\n\n  \n  \n#### Large Language Models  \n  \n9| Large Language Models| Feb 7th| Slides Video|\n\n  * Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\n  * Language Models are Few-Shot Learners\n  * LaMDA: Language Models for Dialog Applications\n  * Language Models are Unsupervised Multitask Learners\n  * Evaluating Large Language Models Trained on Code\n  * PaLM: Scaling Language Modeling with Pathways\n  * Llama 2: Open Foundation and Fine-Tuned Chat Models\n  * Mixtral of Experts\n\n  \n10| Scaling Law| Feb 7th| Slides Video|\n\n  * Scaling Laws for Neural Language Models\n  * Scaling Laws for Transfer\n  * Emergent Abilities of Large Language Models\n  * Training Compute-Optimal Large Language Models\n  * Transcending Scaling Laws with 0.1% Extra Compute\n  * Inverse scaling can become U-shaped\n  * Are Emergent Abilities of Large Language Models a Mirage?\n\n  \n11| Instruction Tuning & RLHF| Feb 14th| Slides Video1 Video2 Video3|\n\n  * Training language models to follow instructions with human feedback\n  * Finetuned Language Models Are Zero-Shot Learners\n  * Multitask Prompted Training Enables Zero-Shot Task Generalization\n  * LIMA: Less Is More for Alignment\n  * Direct Preference Optimization: Your Language Model is Secretly a Reward Model\n  * Zephyr: Direct Distillation of LM Alignment\n\n  \n12| Efficient LLM Training| Feb 14st| Slides Video1 Video2|\n\n  * Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\n  * ZeRO: Memory Optimizations Toward Training Trillion Parameter Models\n  * Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation\n  * RoFormer: Enhanced Transformer with Rotary Position Embedding\n  * Fast Transformer Decoding: One Write-Head is All You Need\n  * GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints\n  * FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\n\n  \n13| Efficient LLM Inference| Feb 28th| Slides Video1 Video2|\n\n  * BERT Loses Patience: Fast and Robust Inference with Early Exit\n  * Confident Adaptive Language Modeling\n  * Fast Inference from Transformers via Speculative Decoding\n  * Efficient Memory Management for Large Language Model Serving with PagedAttention\n  * Flash-Decoding for long-context inference\n  * Breaking the Sequential Dependency of LLM Inference Using Lookahead Decoding\n\n  \n14| Compress and Sparsify LLM| Feb 28th| Slides Video|\n\n  * Efficient Large Scale Language Modeling with Mixtures of Experts\n  * GShard: Scaling Giant Models with Conditional Computation and Automatic Sharding\n  * CoLT5: Faster Long-Range Transformers with Conditional Computation\n  * LLM.int8(): 8-bit Matrix Multiplication for Transformers at Scale\n  * 8-bit Optimizers via Block-wise Quantization\n  * QLoRA: Efficient Finetuning of Quantized LLMs\n  * BitNet: Scaling 1-bit Transformers for Large Language Models\n\n  \n15| LLM Prompting| Mar 6th| Slides Video|\n\n  * Chain-of-Thought Prompting Elicits Reasoning in Large Language Models\n  * Self-Consistency Improves Chain of Thought Reasoning in Language Models\n  * Tree of Thoughts: Deliberate Problem Solving with Large Language Models\n  * Program of Thoughts Prompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks\n  * Least-to-Most Prompting Enables Complex Reasoning in Large Language Models\n  * Measuring and Narrowing the Compositionality Gap in Language Models\n  * ReAct: Synergizing Reasoning and Acting in Language Models\n  * Self-Refine: Iterative Refinement with Self-Feedback\n\n  \n  \n#### (Large) Multimodal Models  \n  \n16| Vision Transformers| Mar 6th| Slides Video1 Video2|\n\n  * An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\n  * Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n  * Training data-efficient image transformers & distillation through attention\n  * Emerging Properties in Self-Supervised Vision Transformers\n  * SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications\n  * MetaFormer Is Actually What You Need for Vision\n  * Masked Autoencoders Are Scalable Vision Learners\n\n  \n17| Diffusion Models| Mar 13th| Slides Video|\n\n  * Maximum Likelihood Training of Score-Based Diffusion Models\n  * Score-Based Generative Modeling through Stochastic Differential Equations\n  * Denoising Diffusion Implicit Models\n  * Denoising Diffusion Probabilistic Models\n  * DPM-Solver: A Fast ODE Solver for Diffusion Probabilistic Model Sampling in Around 10 Steps\n  * Consistency Models\n\n  \n18| Image Generation| Mar 13th| Slides Video|\n\n  * High-Resolution Image Synthesis with Latent Diffusion Models\n  * Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding\n  * Scaling Autoregressive Models for Content-Rich Text-to-Image Generation\n  * Hierarchical Text-Conditional Image Generation with CLIP Latents\n  * PIXART-\u03b1: Fast Training of Diffusion Transformer for Photorealistic Text-to-Image Synthesis\n  * Adversarial Diffusion Distillation\n\n  \n19| Multimodal Model Pre-training| Mar 20th| Slides Video|\n\n  * Learning Transferable Visual Models From Natural Language Supervision\n  * BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation\n  * CoCa: Contrastive Captioners are Image-Text Foundation Models\n  * Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks\n  * VinVL: Revisiting Visual Representations in Vision-Language Models\n  * Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision\n  * Sigmoid Loss for Language Image Pre-Training\n\n  \n20| Large Multimodal Models| Mar 20th| Slides Video1 Video2|\n\n  * Flamingo: a Visual Language Model for Few-Shot Learning\n  * Visual Instruction Tuning\n  * InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning\n  * PaLI: A Jointly-Scaled Multilingual Language-Image Model\n  * PaLI-3 Vision Language Models: Smaller, Faster, Stronger\n  * Generative Multimodal Models are In-Context Learners\n  * InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks\n  * Gemini: A Family of Highly Capable Multimodal Models\n\n  \n  \n#### Augmenting Foundation Models  \n  \n21| Tool Augmentation| Mar 27th| Slides Video1 Video2|\n\n  * Toolformer: Language Models Can Teach Themselves to Use Tools\n  * ART: Automatic multi-step reasoning and tool-use for large language models\n  * ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs\n  * ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings\n  * AgentBench: Evaluating LLMs as Agents\n  * CogAgent: A Visual Language Model for GUI Agents\n  * WebArena: A Realistic Web Environment for Building Autonomous Agents\n\n  \n22| Retrieval Augmentation| Mar 27th| Slides Video1|\n\n  * REALM: Retrieval-Augmented Language Model Pre-Training\n  * Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n  * Improving language models by retrieving from trillions of tokens\n  * Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection\n  * REPLUG: Retrieval-Augmented Black-Box Language Models\n\n  \n  \n#### Project Presentation  \n  \n23| Final Presentation| Apr 3rd|\n\n  * Group 1 - 10: Presenting their final project. Each with 12 minutes.\n\n  \n24| Final Presentation| Apr 3rd|\n\n  * Group 11 - 20: Presenting their final project. Each with 12 minutes.\n\n  \n  \n## University of Waterloo Academic Integrity Policy\n\nThe University of Waterloo Senate Undergraduate Council has also approved the\nfollowing message outlining University of Waterloo policy on academic\nintegrity and associated policies.\n\n### Academic Integrity\n\nIn order to maintain a culture of academic integrity, members of the\nUniversity of Waterloo community are expected to promote honesty, trust,\nfairness, respect and responsibility. Check the Office of Academic Integrity's\nwebsite for more information. All members of the UW community are expected to\nhold to the highest standard of academic integrity in their studies, teaching,\nand research. This site explains why academic integrity is important and how\nstudents can avoid academic misconduct. It also identifies resources available\non campus for students and faculty to help achieve academic integrity in, and\nour, of the classroom.\n\n### Grievance\n\nA student who believes that a decision affecting some aspect of his/her\nuniversity life has been unfair or unreasonable may have grounds for\ninitiating a grievance. Read Policy 70 - Student Petitions and Grievances,\nSection 4. When in doubt please be certain to contact the department's\nadministrative assistant who will provide further assistance.\n\n### Discipline\n\nA student is expected to know what constitutes academic integrity, to avoid\ncommitting academic offenses, and to take responsibility for his/her actions.\nA student who is unsure whether an action constitutes an offense, or who needs\nhelp in learning how to avoid offenses (e.g., plagiarism, cheating) or about\n\u201crules\u201d for group work/collaboration should seek guidance from the course\nprofessor, academic advisor, or the Undergraduate Associate Dean. For\ninformation on categories of offenses and types of penalties, students should\nrefer to Policy 71-Student Discipline. For typical penalties check Guidelines\nfor the Assessment of Penalties.\n\n### Avoiding Academic Offenses\n\nMost students are unaware of the line between acceptable and unacceptable\nacademic behaviour, especially when discussing assignments with classmates and\nusing the work of other students. For information on commonly misunderstood\nacademic offenses and how to avoid them, students should refer to the Faculty\nof Mathematics Cheating and Student Academic Discipline Policy.\n\n### Appeals\n\nA decision made or a penalty imposed under Policy 70, Student Petitions and\nGrievances (other than a petition) or Policy 71, Student Discipline may be\nappealed if there is a ground. A student who believes he/she has a ground for\nan appeal should refer to Policy 72 - Student Appeals.\n\n### Note for students with disabilities\n\nThe AccessAbility Services Office (AAS), located in Needles Hall, Room 1401,\ncollaborates with all academic departments to arrange appropriate\naccommodations for students with disabilities without compromising the\nacademic integrity of the curriculum. If you require academic accommodations\nto lessen the impact of your disability, please register with the AAS at the\nbeginning of each academic term.\n\n", "frontpage": false}
