{"aid": "39990664", "title": "Postgres Extension to Calculate the Maximal Information Coefficient (MIC)", "url": "https://github.com/Florents-Tselai/vasco", "domain": "github.com/florents-tselai", "votes": 1, "user": "fforflo", "posted_at": "2024-04-10 13:43:42", "comments": 0, "source_title": "GitHub - Florents-Tselai/vasco: vasco: MIC & MINE statistics for Postgres", "source_text": "GitHub - Florents-Tselai/vasco: vasco: MIC & MINE statistics for Postgres\n\nSkip to content\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nFlorents-Tselai / vasco Public\n\n  * Notifications\n  * Fork 1\n  * Star 18\n\nvasco: MIC & MINE statistics for Postgres\n\n### License\n\nGPL-3.0 license\n\n18 stars 1 fork Branches Tags Activity\n\nStar\n\nNotifications\n\n# Florents-Tselai/vasco\n\nThis commit does not belong to any branch on this repository, and may belong\nto a fork outside of the repository.\n\n5 Branches\n\n1 Tags\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\nFlorents-TselaiUpdate README.md596d712 \u00b7\n\n## History\n\n32 Commits  \n  \n### .github/workflows\n\n|\n\n### .github/workflows\n\n| Don't bother with postgres 11  \n  \n### demo\n\n|\n\n### demo\n\n| Add better example in README  \n  \n### docs\n\n|\n\n### docs\n\n| Add rtd boilerplate  \n  \n### scripts\n\n|\n\n### scripts\n\n| Add requirements for py scripts  \n  \n### sql\n\n|\n\n### sql\n\n| Add vasco_corr_matrix(regclass, text) to generate the correlation mat...  \n  \n### src\n\n|\n\n### src\n\n| Awful way to bypass infinity  \n  \n### test\n\n|\n\n### test\n\n| Add tests  \n  \n### .gitignore\n\n|\n\n### .gitignore\n\n| Ignore .png files in topdir  \n  \n### .readthedocs.yaml\n\n|\n\n### .readthedocs.yaml\n\n| Add rtd boilerplate  \n  \n### LICENSE\n\n|\n\n### LICENSE\n\n| Initial commit  \n  \n### Makefile\n\n|\n\n### Makefile\n\n| Add tests  \n  \n### README.md\n\n|\n\n### README.md\n\n| Update README.md  \n  \n### vasco.control\n\n|\n\n### vasco.control\n\n| Inch towards v0.2.0  \n  \n## Repository files navigation\n\n# vasco: MIC & MINE statistics for Postgres\n\nvasco is a Postgres extension that helps you discover hidden correlations in\nyour data. It is based on the MIC and the MINE family of statistics.\n\n### Exploring a table\n\nThe generic approach is the following.\n\n    \n    \n    SELECT * FROM vasco_explore('my_schema.my_table')\n\nThis will explore the relationships between all possible column pairs in the\ntable and return a detailed table of the results. Including all MINE\nstatistics and additional metadata.\n\nComing up: an option to reduce the set of columns to consider.\n\n### Exploring association strength\n\nThe Maximal Information Coefficient (MIC) measures how strong is the\nassociation.\n\n    \n    \n    SELECT mic(rand_x, rand_y), -- 0.13 approaching to 0 as sample grows (random independent variables) mic(x, ident), -- 1 identity function hence easy to estimate even with a small sample size mic(x, cubic), -- 0.999 approaching to 1 as sample grows mic(x, periodic) -- 1. FROM vasco_data;\n\n### Exploring the nature of the association\n\nNo algorithm can magically detect the function of the relationship between two\nvariables, but MINE statistics can shed some light into the nature of that\nrelationship.\n\nThe Maximum Asymmetry Score (MAS) measures how much the relationship deviates\nfrom monotonicity.\n\n    \n    \n    SELECT mas(X, Y)\n\nThe Maximum Edge Value (MEV) measures the degree to which the dataset appears\nto be sampled from a continuous function.\n\n    \n    \n    SELECT mev(X, Y)\n\nThe Minimum Cell Number (MCN) measures the complexity of the association.\n\n    \n    \n    SET vasco.mine_mcn_eps = 0.0 -- default SELECT mcn(X, Y)\n\nThe Minimum Cell Number General (MCNG) returns the MCN with eps = 1 - MIC .\n\n    \n    \n    SELECT mcn_general(X, Y)\n\nThe Total Information Coefficient (TIC) .\n\n    \n    \n    SET vasco.mine_tic_norm = true -- normalized or not (default = true) SELECT tic(X, Y)\n\nThe Generalized Mean Information Coefficient (GMIC) , a generalization of MIC\nwhich incorporates a tuning parameter that can be used to modify the\ncomplexity of the association favored by the measure [Luedtke2013]{.citation}\n.\n\n    \n    \n    SET vasco.mine_gmic_p = 0.0 SELECT gmic(X, Y)\n\nUsing the Automobile dataset found in demo/data as an example.\n\n    \n    \n    SELECT vasco_corr_matrix('vasco_demo.\"Automobile_data\"', 'auto_corr_matrix')\n\nvasco will explore the table Automobile_data for correlations between its\ncolumns pairs. A symmetric matrix of these correlations will be stored in the\ntable auto_corr_matrix. You can use that table for BI and analytics.\n\nYou can also use the utility script below to plot a heatmap of that matrix.\n\n    \n    \n    ./scripts/plot_corr_matrix.py 'public.auto_corr_matrix'\n\nThe main workhorse behind vasco is the MIC [Reshef2011]{.citation}: an\ninformation theory-based measure of association that can capture a wide range\nof functional and non-functional relationships between variables.\n\nMIC(X,Y) is symmetric and normalized score into a range [0, 1]. A high MIC\nvalue suggests a dependency between the investigated variables, whereas MIC=0\ndescribes the relationship between two independent variables.\n\n## Installation\n\n    \n    \n    cd /tmp git clone git@github.com:Florents-Tselai/vasco.git cd vasco make all # WITH_PGVECTOR=1 to enable pgvector support make install # may need sudo\n\nThen in a Postgres session run\n\n    \n    \n    CREATE EXTENSION vasco\n\n## Usage\n\nvasco exposes a set of Postgres functions to compute MINE statistics between\ntwo series (X,Y) . In Postgres terms X and X can be arrays, vectors or\ncolumns.\n\nThus, each score function is available in three flavors: using Postgres arrays\nas argument f(float8[], float8[]), , pgvector vectors f(vector, vector) or\ncolumns (hence f is an aggregate function). Necessary MINE parameters can be\nset as GUC , (prefixed as vasco.*)\n\nLet's discuss the supported statistics and their interpretation. Start by\ncreating a sample dataset\n\n    \n    \n    SET extra_float_digits = 0; CREATE TABLE vasco_data AS (SELECT RANDOM() AS rand_x, RANDOM() AS rand_y, x AS x, x AS ident, 4 * pow(x, 3) + pow(x, 2) - 4 * x AS cubic, COS(12 * PI() + x * (1 + x)) AS periodic FROM GENERATE_SERIES(0, 1, 0.001) x);\n\n### Choosing an estimator\n\nThere have been proposed a number of algorithms to estimate the MIC. Currently\nin vasco you can choose between ApproxMIC from [Reshef2011]{.citation} or\nMIC_e from [Reshef2016]{.citation} .\n\n    \n    \n    SET vasco.mic_estimator = ApproxMIC SET vasco.mic_estimator = MIC_e\n\n### pgvector support\n\nvasco can be build with pgvector support .\n\nIn that case all MINE statistics can be computed between vector types too.\n\n    \n    \n    SELECT mic( ARRAY [0,1.3,2,0,1.3,20,1.3,20,1.3,20,1.3,20,1.3,2]::float4[]::vector, ARRAY [0,1.3,2,0,1.3,20,1.3,20,1.3,20,1.3,20,1.3,2]::float4[]::vector )\n\n### Configuration parameters\n\nThe following MINE parameters can be set via GUC.\n\n  * vasco.mine_c\n  * vasco.mine_alpha\n  * vasco.mic_estimator\n  * vasco.mine_mcn_eps\n  * vasco.mine_tic_norm\n  * vasco.mine_gmic_p\n\n## How it works\n\nAs described in [Reshef2011]{.citation} :\n\n> The maximal information coefficient (MIC) is a measure of two-variable\n> dependence designed specifically for rapid exploration of many-dimensional\n> data sets. MIC is part of a larger family of maximal information-based\n> nonparametric exploration (MINE) statistics, which can be used not only to\n> identify important relationships in data sets but also to characterize them.\n>\n> Intuitively, MIC is based on the idea that if a relationship exists between\n> two variables, then a grid can be drawn on the scatterplot of the two\n> variables that partitions the data to encapsulate that relationship.\n>\n> Thus, to calculate the MIC of a set of two-variable data, we explore all\n> grids up to a maximal grid resolution, dependent on the sample size\n> computing for every pair of integers (x,y) the largest possible mutual\n> information achievable by any x-by-y grid applied to the data. We then\n> normalize these mutual information values to ensure a fair comparison\n> between grids of different dimensions and to obtain modified values between\n> 0 and 1.\n>\n> These different combination of grids form the so-called characteristic\n> matrix M(x,y) of the data. Each element (x,y) of M stores the highest\n> normalized mutual information achieved by any x-by-y grid. Computing M is\n> the core of the algorithmic process and is computationally expensive. The\n> maximum of M is the MIC and the rest of MINE statistics are derived from\n> that matrix as well.\n\nTL;DR: Computing the Characteristic Matrix is the big deal; Once that is done,\ncomputing the statistics is trivial.\n\n## Next Steps\n\n  * Try out ChiMIC [Chen2013]{.citation} and BackMIC [Cao2021]{.citation}:\n  * Currently M is re-computed every time a function score is called. That's a huge waste of resources. Caching M or sharing it between runs should be the first optimization to be done.\n  * A potential next step would be continuously updating the CM as columns are updated (think a trigger or bgw process).\n  * Make an extension for SQLite and DuckDB as well\n  * Build convenience functions to create variable pairs and explore tables in one pass.\n\n## Thanks\n\nFor MINE statistics, vasco currently uses the implementation provided by\n[Albanese2013]{.citation} via the minepy package.\n\nAlternative implementations are coming up.\n\n## Resources\n\n::: {#citations}\n\n[Albanese2013]{#Albanese2013 .citation-label}\n\n: Albanese, D., Filosi, M., Visintainer, R., Riccadonna, S., Jurman, G., &\nFurlanello, C. (2013). Minerva and minepy: a C engine for the MINE suite and\nits R, Python and MATLAB wrappers. Bioinformatics, 29(3), 407-408.\n\n[Albanese2018]{#Albanese2018 .citation-label}\n\n: Davide Albanese, Samantha Riccadonna, Claudio Donati, Pietro Franceschi; A\npractical tool for Maximal Information Coefficient analysis, GigaScience,\ngiy032, https://doi.org/10.1093/gigascience/giy032\n\n[Cao2021]{#Cao2021 .citation-label}\n\n: Cao, D., Chen, Y., Chen, J., Zhang, H., & Yuan, Z. (2021). An improved\nalgorithm for the maximal information coefficient and its application. Royal\nSociety open science, 8(2), 201424. PDF GitHub\n\n[Chen2013]{#Chen2013 .citation-label}\n\n: Chen Y, Zeng Y, Luo F, Yuan Z. 2016 A new algorithm to optimize maximal\ninformation coefficient. PLoS ONE 11, e0157567. (doi:10.\n1371/journal.pone.0157567) GitHub\n\n[Ge2016]{#Ge2016 .citation-label}\n\n: Ge, R., Zhou, M., Luo, Y. et al. McTwo: a two-step feature selection\nalgorithm based on maximal information coefficient. BMC Bioinformatics 17, 142\n(2016). https://doi.org/10.1186/s12859-016-0990-0\n\n[Luedtke2013]{#Luedtke2013 .citation-label}\n\n: Luedtke A., Tran L. The Generalized Mean Information Coefficient\nhttps://doi.org/10.48550/arXiv.1308.5712\n\n[Matejka2017]{#Matejka2017 .citation-label}\n\n: J. Matejka and G. Fitzmaurice. Same Stats, Different Graphs: Generating\nDatasets with Varied Appearance and Identical Statistics through Simulated\nAnnealing. ACM SIGCHI Conference on Human Factors in Computing Systems, 2017.\n\n[Reshef2011]{#Reshef2011 .citation-label}\n\n: Reshef, D. N., Reshef, Y. A., Finucane, H. K., Grossman, S. R., McVean, G.,\nTurnbaugh, P. J., ... & Sabeti, P. C. (2011). Detecting novel associations in\nlarge data sets. science, 334(6062), 1518-1524.\n\n[Reshef2016]{#Reshef2016 .citation-label}\n\n: Yakir A. Reshef, David N. Reshef, Hilary K. Finucane and Pardis C. Sabeti\nand Michael Mitzenmacher. Measuring Dependence Powerfully and Equitably.\nJournal of Machine Learning Research, 2016. PDF\n\n[Shao2021]{#Shao2021 .citation-label}\n\n: Shao, F. & Liu, H. (2021). The Theoretical and Experimental Analysis of the\nMaximal Information Coefficient Approximate Algorithm. Journal of Systems\nScience and Information, 9(1), 95-104.\nhttps://doi.org/10.21078/JSSI-2021-095-10\n\n[Xu2016]{#Xu2016 .citation-label}\n\n: Xu, Z., Xuan, J., Liu, J., & Cui, X. (2016, March). MICHAC: Defect\nprediction via feature selection based on maximal information coefficient with\nhierarchical agglomerative clustering. In 2016 IEEE 23rd International\nConference on Software Analysis, Evolution, and Reengineering (SANER) (Vol. 1,\npp. 370-381). IEEE. http://cstar.whu.edu.cn/paper/saner_16.pdf\n\n[Zhang2014]{#Zhang2014 .citation-label}\n\n: Zhang Y, Jia S, Huang H, Qiu J, Zhou C. 2014 A novel algorithm for the\nprecise calculation of the maximal information coefficient. Sci. Rep.-UK 4,\n6662. (doi:10.1038/ srep06662) http://lxy.depart.hebust.edu.cn/SGMIC/SGMIC.htm\n:::\n\n## About\n\nvasco: MIC & MINE statistics for Postgres\n\n### Topics\n\npostgres similarity vectors postgresql-extension correlation-coefficient\nvector-similarity maximal-information-coefficient\n\n### Resources\n\nReadme\n\n### License\n\nGPL-3.0 license\n\nActivity\n\n### Stars\n\n18 stars\n\n### Watchers\n\n4 watching\n\n### Forks\n\n1 fork\n\nReport repository\n\n## Releases\n\n1 tags\n\n## Packages 0\n\nNo packages published\n\n## Languages\n\n  * C 74.2%\n  * PLpgSQL 21.8%\n  * Makefile 2.4%\n  * Python 1.6%\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": false}
