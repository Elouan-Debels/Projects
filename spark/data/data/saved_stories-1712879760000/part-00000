{"aid": "40004887", "title": "Storm: LLM system that researches a topic and generates full-length wiki article", "url": "https://github.com/stanford-oval/storm", "domain": "github.com/stanford-oval", "votes": 8, "user": "GavCo", "posted_at": "2024-04-11 17:53:38", "comments": 3, "source_title": "GitHub - stanford-oval/storm: An LLM-powered knolwedge curation system that researches a topic and generates a full-length report with citations.", "source_text": "GitHub - stanford-oval/storm: An LLM-powered knolwedge curation system that\nresearches a topic and generates a full-length report with citations.\n\nSkip to content\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nstanford-oval / storm Public\n\n  * Notifications\n  * Fork 2\n  * Star 40\n\nAn LLM-powered knolwedge curation system that researches a topic and generates\na full-length report with citations.\n\narxiv.org/abs/2402.14207\n\n### License\n\nMIT license\n\n40 stars 2 forks Branches Tags Activity\n\nStar\n\nNotifications\n\n# stanford-oval/storm\n\nThis commit does not belong to any branch on this repository, and may belong\nto a fork outside of the repository.\n\n1 Branch\n\n0 Tags\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\nYucheng-JiangUpdate README.md42f4d5b \u00b7\n\n## History\n\n3 Commits  \n  \n### FreshWiki\n\n|\n\n### FreshWiki\n\n| Init.  \n  \n### assets\n\n|\n\n### assets\n\n| Init.  \n  \n### eval\n\n|\n\n### eval\n\n| Init.  \n  \n### src\n\n|\n\n### src\n\n| Init.  \n  \n### .gitignore\n\n|\n\n### .gitignore\n\n| Init.  \n  \n### LICENSE\n\n|\n\n### LICENSE\n\n| Init.  \n  \n### README.md\n\n|\n\n### README.md\n\n| Update README.md  \n  \n### requirements.txt\n\n|\n\n### requirements.txt\n\n| Init.  \n  \n## Repository files navigation\n\n# STORM: Synthesis of Topic Outlines through Retrieval and Multi-perspective\nQuestion Asking\n\nThis repository contains the code for our NAACL 2024 paper Assisting in\nWriting Wikipedia-like Articles From Scratch with Large Language Models by\nYijia Shao, Yucheng Jiang, Theodore A. Kanell, Peter Xu, Omar Khattab, and\nMonica S. Lam.\n\n## Overview (Try STORM now!)\n\nSTORM is a LLM system that writes Wikipedia-like articles from scratch based\non Internet search.\n\nWhile the system cannot produce publication-ready articles that often require\na significant number of edits, experienced Wikipedia editors have found it\nhelpful in their pre-writing stage.\n\nTry out our live demo to see how STORM can help your knowledge exploration\njourney and please provide feedback to help us improve the system \ud83d\ude4f!\n\n## Research Before Writing\n\nSTORM breaks down generating long articles with citations into two steps:\n\n  1. Pre-writing stage: The system conducts Internet-based research to collect references and generates an outline.\n  2. Writing stage: The system uses the outline and references to generate the full-length article with citations.\n\nSTORM identifies the core of automating the research process as automatically\ncoming up with good questions to ask. Directly prompting the language model to\nask questions does not work well. To improve the depth and breadth of the\nquestions, STORM adopts two strategies:\n\n  1. Perspective-Guided Question Asking: Given the input topic, STORM discovers different perspectives by surveying existing articles from similar topics and uses them to control the question-asking process.\n  2. Simulated Conversation: STORM simulates a conversation between a Wikipedia writer and a topic expert grounded in Internet sources to enable the language model to update its understanding of the topic and ask follow-up questions.\n\nBased on the separation of the two stages, STORM is implemented in a highly\nmodular way (see engine.py) using dspy.\n\n## Setup\n\nWe view STORM as an example of automated knowledge curation. We are working on\nenhancing our codebase to increase its extensibility. Stay tuned!\n\nBelow, we provide a quick start guide to run STORM locally to reproduce our\nexperiments.\n\n  1. Install the required packages.\n    \n        conda create -n storm python=3.11 conda activate storm pip install -r requirements.txt\n\n  2. Set up OpenAI API key and You.com search API key. Create a file secrets.toml under the root directory and add the following content:\n    \n        # Set up OpenAI API key. OPENAI_API_KEY=<your_openai_api_key> # If you are using the API service provided by OpenAI, include the following line: OPENAI_API_TYPE=openai # If you are using the API service provided by Microsoft Azure, include the following lines: OPENAI_API_TYPE=azure AZURE_API_BASE=<your_azure_api_base_url> AZURE_API_VERSION=<your_azure_api_version> # Set up You.com search API key. YOU_API_KEY=<your_youcom_api_key>\n\n## Paper Experiments\n\nThe FreshWiki dataset used in our experiments can be found in ./FreshWiki.\n\nRun the following commands under ./src.\n\n### Pre-writing Stage\n\nFor batch experiment on FreshWiki dataset:\n\n    \n    \n    python -m scripts.run_prewriting --input-source file --input-path ../FreshWiki/topic_list.csv --engine gpt-4 --do-research --max-conv-turn 5 --max-perspective 5\n\n  * \\--engine (choices=[gpt-4, gpt-35-turbo]): the LLM engine used for generating the outline\n  * \\--do-research: if True, simulate conversation to research the topic; otherwise, load the results.\n  * \\--max-conv-turn: the maximum number of questions for each information-seeking conversation\n  * \\--max-perspective: the maximum number of perspectives to be considered, each perspective corresponds to an information-seeking conversation.\n\n    * STORM also uses a general conversation to collect basic information about the topic. So, the maximum number of QA pairs is max_turn * (max_perspective + 1). \ud83d\udca1 Reducing max_turn or max_perspective can speed up the process and reduce the cost but may result in less comprehensive outline.\n    * The parameter will not have any effect if --disable-perspective is set (the perspective-driven question asking is disabled).\n\nTo run the experiment on a single topic:\n\n    \n    \n    python -m scripts.run_prewriting --input-source console --engine gpt-4 --max-conv-turn 5 --max-perspective 5\n\n  * The script will ask you to enter the Topic and the Ground truth url that will be excluded. If you do not have any url to exclude, leave that field empty.\n\nThe generated outline will be saved in\n{output_dir}/{topic}/storm_gen_outline.txt and the collected references will\nbe saved in {output_dir}/{topic}/raw_search_results.json.\n\n### Writing Stage\n\nFor batch experiment on FreshWiki dataset:\n\n    \n    \n    python -m scripts.run_writing --input-source file --input-path ../FreshWiki/topic_list.csv --engine gpt-4 --do-polish-article --remove-duplicate\n\n  * \\--do-polish-article: if True, polish the article by adding a summarization section and removing duplicate content if --remove-duplicate is set True.\n\nTo run the experiment on a single topic:\n\n    \n    \n    python -m scripts.run_writing --input-source console --engine gpt-4 --do-polish-article --remove-duplicate\n\n  * The script will ask you to enter the Topic. Please enter the same topic as the one used in the pre-writing stage.\n\nThe generated article will be saved in\n{output_dir}/{topic}/storm_gen_article.txt and the references corresponding to\ncitation index will be saved in {output_dir}/{topic}/url_to_info.json. If\n--do-polish-article is set, the polished article will be saved in\n{output_dir}/{topic}/storm_gen_article_polished.txt.\n\n## Customize the STORM Configurations\n\nWe set up the default LLM configuration in LLMConfigs in src/modules/utils.py.\nYou can use set_conv_simulator_lm(),set_question_asker_lm(),\nset_outline_gen_lm(), set_article_gen_lm(), set_article_polish_lm() to\noverride the default configuration. These functions take in an instance from\ndspy.dsp.LM or dspy.dsp.HFModel.\n\n\ud83d\udca1 For a good practice,\n\n  * choose a cheaper/faster model for conv_simulator_lm which is used to split queries, synthesize answers in the conversation.\n  * if you need to conduct the actual writing step, choose a more powerful model for article_gen_lm. Based on our experiments, weak models are bad at generating text with citations.\n\n## Automatic Evaluation\n\nIn our paper, we break down the evaluation into two parts: outline quality and\nfull-length article quality.\n\n### Outline Quality\n\nWe introduce heading soft recall and heading entity recall to evaluate the\noutline quality. This makes it easier to prototype methods for pre-writing.\n\nRun the following command under ./eval to compute the metrics on FreshWiki\ndataset:\n\n    \n    \n    python eval_outline_quality.py --input-path ../FreshWiki/topic_list.csv --gt-dir ../FreshWiki --pred-dir ../results --pred-file-name storm_gen_outline.txt --result-output-path ../results/storm_outline_quality.csv\n\n### Full-length Article Quality\n\neval/eval_article_quality.py provides the entry point of evaluating full-\nlength article quality using ROUGE, entity recall, and rubric grading. Run the\nfollowing command under eval to compute the metrics:\n\n    \n    \n    python eval_article_quality.py --input-path ../FreshWiki/topic_list.csv --gt-dir ../FreshWiki --pred-dir ../results --gt-dir ../FreshWiki --output-dir ../results/storm_article_eval_results --pred-file-name storm_gen_article_polished.txt\n\n### Use the Metric Yourself\n\nThe similarity-based metrics (i.e., ROUGE, entity recall, and heading entity\nrecall) are implemented in eval/metrics.py.\n\nFor rubric grading, we use the prometheus-13b-v1.0 introduced in this paper.\neval/evaluation_prometheus.py provides the entry point of using the metric.\n\n## Contributions\n\nIf you have any questions or suggestions, please feel free to open an issue or\npull request. We welcome contributions to improve the system and the codebase!\n\nContact person: Yijia Shao and Yucheng Jiang\n\n## Citation\n\nPlease cite our paper if you use this code or part of it in your work:\n\n    \n    \n    @inproceedings{shao2024assisting, title={{Assisting in Writing Wikipedia-like Articles From Scratch with Large Language Models}}, author={Yijia Shao and Yucheng Jiang and Theodore A. Kanell and Peter Xu and Omar Khattab and Monica S. Lam}, year={2024}, booktitle={Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)} }\n\n## About\n\nAn LLM-powered knolwedge curation system that researches a topic and generates\na full-length report with citations.\n\narxiv.org/abs/2402.14207\n\n### Resources\n\nReadme\n\n### License\n\nMIT license\n\nActivity\n\nCustom properties\n\n### Stars\n\n40 stars\n\n### Watchers\n\n3 watching\n\n### Forks\n\n2 forks\n\nReport repository\n\n## Contributors 2\n\n  * shaoyijia Yijia Shao\n  * Yucheng-Jiang\n\n## Languages\n\n  * HTML 95.2%\n  * Python 4.8%\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": true}
