{"aid": "40066253", "title": "Flink SQL\u2013Misconfiguration, Misunderstanding, and Mishaps", "url": "https://www.decodable.co/blog/flink-sql-misconfiguration-misunderstanding-and-mishaps", "domain": "decodable.co", "votes": 1, "user": "mooreds", "posted_at": "2024-04-17 15:41:53", "comments": 0, "source_title": "Flink SQL\u2014Misconfiguration, Misunderstanding, and Mishaps", "source_text": "Flink SQL\u2014Misconfiguration, Misunderstanding, and Mishaps\n\nOpens in a new window Opens an external website Opens an external website in a\nnew window\n\nThis website utilizes technologies such as cookies to enable essential site\nfunctionality, as well as for analytics, personalization, and targeted\nadvertising purposes. You may change your settings at any time or accept the\ndefault settings. You may close this banner to continue with only essential\ncookies. Privacy Policy\n\nStorage Preferences\n\nJoin us May 8th at 9am PT/noon ET for a Multi-Stream Connector Tech Talk\n\nRegister Today!\n\nBack\n\nApril 15, 2024\n\n15\n\nmin read\n\n# Flink SQL\u2014Misconfiguration, Misunderstanding, and Mishaps\n\nBy\n\nRobin Moffatt\n\nShare this post\n\nI never meant to write this blog. I had a whole blog series about Flink SQL\nlined up...and then I started to write it and realised rapidly that one's\ninitial exposure to Flink and Flink SQL can be somewhat, shall we say,\ninteresting. Interesting, as in the curse, \"may you live in interesting\ntimes\". Because as wonderful and as powerful Flink is, it is not a simple\nbeast to run for yourself, even as a humble developer just trying to try out\nsome SQL.\n\nQuite a few of the problems are a subset of the broader challenge of getting\nthe right JAR in the right place at the right time\u2014a topic worth its own blog.\n\nLet's start off by looking at the JDBC connector. This provides its own\ncatalog, which I explored recently in another post. In that article I trod the\nhappy path; below are the potholes and pitfalls that befell me on the way \ud83e\uddcc.\n\n## The JDBC Catalog\n\nYou can learn more about the JDBC Catalog, and Flink SQL Catalogs in general,\nhere and here.\n\nWe'll start by provisioning the environment. Using this Docker Compose from\nthe Decodable examples repository we can spin up a Postgres container and a\nFlink cluster. In the Flink cluster I've added the Flink JDBC Connector and\nthe Postgres JDBC driver.\n\n    \n    \n    docker compose up\n\nOnce the containers are running you can inspect the pre-provisioned Postgres\ndatabase:\n\n    \n    \n    docker compose exec -it postgres psql -h localhost -d world-db -U world\n    \n    \n    world-db-# \\d List of relations Schema | Name | Type | Owner --------+------------------+-------+------- public | city | table | world public | country | table | world public | country_flag | table | world public | country_language | table | world (4 rows)\n\nNow we'll head into Flink SQL and try to create a JDBC catalog so that we can\nquery these Postgres tables:\n\n    \n    \n    docker compose exec -it jobmanager bash -c \"./bin/sql-client.sh\"\n    \n    \n    Flink SQL> CREATE CATALOG c_jdbc WITH ( 'type' = 'jdbc', 'base-url' = 'localhost:5432', 'default-database' = 'world-db', 'username' = 'world', 'password' = 'world123' ); [ERROR] Could not execute SQL statement. Reason: java.lang.IllegalArgumentException\n\nUh-oh. We're not off to a particularly illustrious start. What does Illegal\nArgument\u2014 the argument that we've specified for base-url is \"Illegal\" because\nit needs the fully qualified JDBC URL prefix. Let's try again:\n\n    \n    \n    Flink SQL> CREATE CATALOG c_jdbc WITH ( 'type' = 'jdbc', 'base-url' = 'jdbc:postgres://localhost:5432', 'default-database' = 'world-db', 'username' = 'world', 'password' = 'world123' ); [ERROR] Could not execute SQL statement. Reason: java.lang.IllegalStateException: Could not find any jdbc dialect factory that can handle url 'jdbc:postgres://localhost:5432' that implements 'org.apache.flink.connector.jdbc.dialect.JdbcDialectFactory' in the classpath. Available factories are: org.apache.flink.connector.jdbc.databases.derby.dialect.DerbyDialectFactory org.apache.flink.connector.jdbc.databases.mysql.dialect.MySqlDialectFactory org.apache.flink.connector.jdbc.databases.oracle.dialect.OracleDialectFactory org.apache.flink.connector.jdbc.databases.postgres.dialect.PostgresDialectFactory org.apache.flink.connector.jdbc.databases.sqlserver.dialect.SqlServerDialectFactory\n\nThis error is definitely more esoteric than the last one. At the heart of it\nis this:\n\n  *     Could not find any jdbc dialect factory that can handle url 'jdbc:postgres://localhost:5432'\n\nWe've told the JDBC connector that it's a JDBC dialect (jdbc:) but it can't\nwork out what dialect of JDBC it is. It tells us which \"factories\" it does\nunderstand:\n\n    \n    \n    Available factories are: `[...].jdbc.databases.derby.dialect.DerbyDialectFactory` `[...].jdbc.databases.mysql.dialect.MySqlDialectFactory` `[...].jdbc.databases.oracle.dialect.OracleDialectFactory` `[...].jdbc.databases.postgres.dialect.PostgresDialectFactory` `[...].jdbc.databases.sqlserver.dialect.SqlServerDialectFactory`\n\nSo what's up with this? Well if we look at the Postgres JDBC URL spec we'll\nsee that it should be jdbc:postgresql (note the ql suffix that we've missed\nabove). Let's try that:\n\n    \n    \n    Flink SQL> CREATE CATALOG c_jdbc WITH ( 'type' = 'jdbc', 'base-url' = 'jdbc:postgresql://localhost:5432', 'default-database' = 'world-db', 'username' = 'world', 'password' = 'world123' ); [ERROR] Could not execute SQL statement. Reason: java.net.ConnectException: Connection refused (Connection refused) Flink SQL>\n\nHey, another new error! That's because localhost in the context of the\ncontainer running sql-client is the Flink job manager container, so there is\nno Postgres on port 5432 to be found. Instead, we need to reference the\nPostgres container, conveniently called in our casepostgres.\n\n    \n    \n    Flink SQL> CREATE CATALOG c_jdbc WITH ( 'type' = 'jdbc', 'base-url' = 'jdbc:postgresql://postgres:5432', 'default-database' = 'world-db', 'username' = 'world', 'password' = 'world123' ); [INFO] Execute statement succeed. Flink SQL>\n\nPhew! \ud83d\ude05\n\nWhilst we're here with a Catalog created in Flink SQL, let's cover off another\nthing that kept tripping me up. Since we've created the catalog, we want to\ntell Flink SQL to now use it as the current catalog:\n\n    \n    \n    Flink SQL> USE c_jdbc; [ERROR] Could not execute SQL statement. Reason: org.apache.flink.table.catalog.exceptions.CatalogException: A database with name [c_jdbc] does not exist in the catalog: [default_catalog].\n\nThis comes about from an inconsistency (IMHO) in the syntax of Flink SQL here.\nUSE on its own in Flink SQL meansUSE DATABASE. So if you want to actually\nswitch the current catalog, the command isUSE CATALOG.\n\n    \n    \n    Flink SQL> USE CATALOG c_jdbc; [INFO] Execute statement succeed.\n\nOn the other hand, if you're getting this error and you actually did mean to\nswitch the current database then the error is what it says - the database\nyou've specified doesn't exist in the current catalog. Check the current\ncatalog withSHOW CURRENT CATALOG.\n\n## What's Running Where? (Fun with Java Versions)\n\nTo recount the next incident, I'll start by describing the environment. This\nwas before the JDBC connector for Flink 1.18 was released, so I was running\nFlink 1.17.1, and locally on my laptop.\n\n    \n    \n    # Unpack a fresh Flink 1.17.1 tarball tar xf flink-1.17.1-bin-scala_2.12.tgz && cd flink-1.17.1 # Install JDBC connector and Postgres JDBC driver cd lib curl https://repo.maven.apache.org/maven2/org/apache/flink/flink-connector-jdbc/3.1.2-1.17/flink-connector-jdbc-3.1.2-1.17.jar -O curl https://repo.maven.apache.org/maven2/org/postgresql/postgresql/42.7.3/postgresql-42.7.3.jar -O cd .. # Launch a Flink cluster and SQL Client $ ./bin/start-cluster.sh && ./bin/sql-client.sh Starting cluster. Starting standalonesession daemon on host asgard08. Starting taskexecutor daemon on host asgard08. [... cute ANSI Squirrel pic...] Flink SQL>\n\nI've also got the same Postgres Docker image as above running, but just as a\nstandalone container with port 5432 open for connections:\n\n    \n    \n    docker run --rm --detach \\ --name postgres \\ --publish 5432:5432 \\ ghusta/postgres-world-db:2.10\n\nNow we'll do the same as above\u2014create a JDBC catalog. Note that this time the\nbase-url is referencing localhost because Flink is running locally on the\nmachine and I\u2019ve exposed Postgres\u2019 port from its container, so localhost is\nthe right place to access it.\n\n    \n    \n    Flink SQL> CREATE CATALOG c_jdbc WITH ( 'type' = 'jdbc', 'base-url' = 'jdbc:postgresql://localhost:5432', 'default-database' = 'world-db', 'username' = 'world', 'password' = 'world123' ); [INFO] Execute statement succeed Flink SQL> USE `c_jdbc`.`world-db`; [INFO] Execute statement succeed. Flink SQL> SHOW TABLES; +-------------------------+ | table name | +-------------------------+ | public.city | | public.country | | public.country_flag | | public.country_language | +-------------------------+ 4 rows in set\n\nLet's try querying one of these tables:\n\n    \n    \n    Flink SQL> SELECT * FROM country_flag LIMIT 5; [ERROR] Could not execute SQL statement. Reason: java.lang.reflect.InaccessibleObjectException: Unable to make field private static final int java.lang.Class.ANNOTATION accessible: module java.base does not \"opens java.lang\" to unnamed module @52bf72b5\n\nHuh. After a bit of head scratching I found a reference on StackOverflow to\nthe version of Java, so I checked that. I'm using the excellent SDKMan which\nmakes this kind of thing very straightforward:\n\n    \n    \n    $ sdk current java Using java version 17.0.5-tem\n\nThat might be a problem. Support for Java 17 was only added in Flink 1.18 and\nis considered experimental at this time. I reverted my Java version to 11, and\nlaunched the SQL Client again:\n\n    \n    \n    $ sdk install java 11.0.21-tem $ sdk use java 11.0.21-tem Using java version 11.0.21-tem in this shell. $ ./bin/sql-client.sh\n\nI recreated the catalog, switched to the world-db database within it, and\ntried my SELECT again:\n\n    \n    \n    Flink SQL> CREATE CATALOG c_jdbc WITH ( > 'type' = 'jdbc', > 'base-url' = 'jdbc:postgresql://localhost:5432', > 'default-database' = 'world-db', > 'username' = 'world', > 'password' = 'world123' > ); [INFO] Execute statement succeed. Flink SQL> USE `c_jdbc`.`world-db`; [INFO] Execute statement succeed. Flink SQL> SELECT * FROM country_flag LIMIT 5;\n\nAt this point there's no error... but no nothing either. After a minute or so\nI get this:\n\n    \n    \n    [ERROR] Could not execute SQL statement. Reason: java.net.ConnectException: Connection refused\n\nThis puzzles me, because Postgres is up\u2014and we can verify the connectivity\nfrom SQL Client withSHOW TABLES:\n\n    \n    \n    Flink SQL> SHOW TABLES; +-------------------------+ | table name | +-------------------------+ | public.city | | public.country | | public.country_flag | | public.country_language | +-------------------------+ 4 rows in set\n\nSo where is the Connection that is beingrefused?\n\nLog files are generally a good place to go and look, particularly when one\nstarts troubleshooting things that are beyond the obvious. If we search the\nFlink log folder for the Connection refused error we get this:\n\n    \n    \n    $ grep -r \"Connection refused\" log log/flink-rmoff-sql-client-asgard08.log:java.util.concurrent.CompletionException: org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: localhost/127.0.0.1:8081 flink-rmoff-sql-client-asgard08.log:Caused by: org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: localhost/127.0.0.1:8081 flink-rmoff-sql-client-asgard08.log:Caused by: java.net.ConnectException: Connection refused [...]\n\nWhat's interesting here is the port number. 8081 is not the Postgres port\n(5432) but the Flink job manager.\n\nSo whilst the SQL Client itself is making the request to Postgres for a SHOW\nTABLES request, a SELECT gets run as a Flink job\u2014which the SQL Client will\nsend to the job manager in order for it to execute it on an available task\nmanager node.\n\nThe outstanding question though is why the job manager isn't available. We\nlaunched it already above when we did./bin/start-cluster.sh. Or did we? Let's\ncheck the running Java process list:\n\n    \n    \n    $ jps 23795 Jps 3990 SqlClient\n\nSo neither the job manager nor task manager processes are running. Were they\never?\n\n    \n    \n    $ head log/flink-rmoff-standalonesession-1-asgard08.log 2024-04-05 14:01:48,837 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - Shutting StandaloneSessionClusterEntrypoint down with application status FAILED. Diagnostics java.lang.IllegalAccessError: class org.apache.flink.util.NetUtils (in unnamed module @0x5b8dfcc1) cannot access class sun.net.util.IPAddressUtil (in module java.base) because module java.base does not export sun.net.util to unnamed module @0x5b8dfcc1 [...] $ head log/flink-rmoff-taskexecutor-2-asgard08.log 2024-04-05 14:02:11,255 ERROR org.apache.flink.runtime.taskexecutor.TaskManagerRunner [] - Terminating TaskManagerRunner with exit code 1. java.lang.IllegalAccessError: class org.apache.flink.util.NetUtils (in unnamed module @0x1a482e36) cannot access class sun.net.util.IPAddressUtil (in module java.base) because module java.base does not export sun.net.util to unnamed module @0x1a482e36\n\nMy guess is that the Java version caused these fatal errors. Regardless, in\neffect what I had running locally was this:\n\nNo wonder things didn't work! Realising the problem I'd had with SQL Client\nand the Java version, it was a fair guess that the same issue was true for the\nFlink cluster components.\n\nLet's try again, with Java 11, and use jps to verify that things are starting\nup correctly:\n\n    \n    \n    $ ./bin/start-cluster.sh Starting cluster. Starting standalonesession daemon on host asgard08. Starting taskexecutor daemon on host asgard08. $ jps 28774 StandaloneSessionClusterEntrypoint 29064 Jps 29049 TaskManagerRunner\n\nBetter! Now we'll launch SQL Client and see if things are now working as they\nshould:\n\n    \n    \n    Flink SQL> CREATE CATALOG c_jdbc WITH ( 'type' = 'jdbc', 'base-url' = 'jdbc:postgresql://localhost:5432', 'default-database' = 'world-db', 'username' = 'world', 'password' = 'world123' ); [INFO] Execute statement succeed. Flink SQL> USE `c_jdbc`.`world-db`; [INFO] Execute statement succeed. Flink SQL> SELECT * FROM country_flag LIMIT 5; +----+---------+-----------+--------------------+ | op | code2 | emoji | unicode | +----+---------+-----------+--------------------+ | +I | AD | \ud83c\udde6\ud83c\udde9 | U+1F1E6 U+1F1E9 | | +I | AE | \ud83c\udde6\ud83c\uddea | U+1F1E6 U+1F1EA | | +I | AF | \ud83c\udde6\ud83c\uddeb | U+1F1E6 U+1F1EB | | +I | AG | \ud83c\udde6\ud83c\uddec | U+1F1E6 U+1F1EC | | +I | AI | \ud83c\udde6\ud83c\uddee | U+1F1E6 U+1F1EE | +----+---------+-----------+--------------------+ Received a total of 5 rows\n\n\ud83c\udf89 that's more like it!\n\n## What's Running Where? (Fun with JAR dependencies)\n\nI wrote a whole article about JAR files because they are so central to the\nsuccessful operation of Flink. Let's look at an example of the kind of \"fun\"\nyou can get with them. Moving around the Flink versions, this incident was\nwith 1.18.1. Let's start with a clean slate and a fresh Flink install:\n\n    \n    \n    # Unpack a fresh Flink 1.18.1 tarball tar xf flink-1.18.1-bin-scala_2.12.tgz && cd flink-1.18.1\n\nI was exploring the Filesystem connector and wanted to see if I could write a\nfile in Parquet format.\n\n    \n    \n    Flink SQL> CREATE TABLE t_foo (c1 varchar, c2 int) WITH ( 'connector' = 'filesystem', 'path' = 'file:///tmp/t_foo', 'format' = 'parquet' ); [INFO] Execute statement succeed. Flink SQL> INSERT INTO t_foo VALUES ('bar',42); [ERROR] Could not execute SQL statement. Reason: org.apache.flink.table.api.ValidationException: Could not find any format factory for identifier 'parquet' in the classpath.\n\nThis error (Could not find any factory for identifier) usually means that\nthere's a JAR missing. From the Maven repository I grabbed flink-sql-\nparquet-1.18.1.jar\n\n    \n    \n    cd ./lib curl https://repo1.maven.org/maven2/org/apache/flink/flink-sql-parquet/1.18.1/flink-sql-parquet-1.18.1.jar -O\n\nAfter restarting the SQL Client I got a different error:\n\n    \n    \n    Flink SQL> INSERT INTO t_foo VALUES ('bar',42); [ERROR] Could not execute SQL statement. Reason: java.lang.ClassNotFoundException: org.apache.hadoop.conf.Configuration\n\nThis one I'd come across before - I needed the Hadoop JARs (discussed in more\ndetail here). I downloaded Hadoop (words I never thought I'd write in 2024 \ud83d\ude09)\nand set the environment variable:\n\n    \n    \n    export HADOOP_CLASSPATH=$(~/hadoop/hadoop-3.3.4/bin/hadoop classpath)\n\nI then restarted the SQL Client (with a small detour via this PR for a\nSqlClientException: Could not read from command line error):\n\n    \n    \n    Flink SQL> CREATE TABLE t_foo (c1 varchar, c2 int) WITH ( 'connector' = 'filesystem', 'path' = 'file:///tmp/t_foo', 'format' = 'parquet' ); [INFO] Execute statement succeed. Flink SQL> INSERT INTO t_foo VALUES ('bar',42); [INFO] Submitting SQL update statement to the cluster... [INFO] SQL update statement has been successfully submitted to the cluster: Job ID: c49df5c96e013007d6224b06e218533c\n\nSuccess! Or so I thought...\n\n    \n    \n    Flink SQL> SELECT * FROM t_foo; [ERROR] Could not execute SQL statement. Reason: java.lang.ClassNotFoundException: org.apache.flink.formats.parquet.ParquetColumnarRowInputFormat\n\nKinda confusing, since we only just installed the Parquet JAR (which this\nerror seems to be referencing).\n\nLet's go back to our log files and look at what's going on here. I'll search\nfor the error:\n\n    \n    \n    $ grep -r -l \"java.lang.ClassNotFoundException\" ./log ./log/flink-rmoff-taskexecutor-0-asgard08.log ./log/flink-rmoff-sql-client-asgard08.log ./log/flink-rmoff-standalonesession-0-asgard08.log\n\nSo it's being logged back the SQL Client (which we saw), but also taskexecutor\n(the Task Manager) and standalonesession (the Job Manager). Opening up the\nfiles and looking closely at the timestamps shows us that the SQL Client is\nsurfacing the above error from a call to the job manager (Could not start the\nJobMaster):\n\n    \n    \n    ERROR org.apache.flink.table.gateway.service.operation.OperationManager [] - Failed to execute the operation 072fdeef-76bc-4813-a2af-5444b7806737. org.apache.flink.table.api.TableException: Failed to execute sql Caused by: org.apache.flink.util.FlinkException: Failed to execute job 'collect'. Caused by: java.lang.RuntimeException: org.apache.flink.runtime.client.JobInitializationException: Could not start the JobMaster. [...] Caused by: java.lang.ClassNotFoundException: org.apache.flink.formats.parquet.ParquetColumnarRowInputFormat\n\nIn fact, if we open up the rather useful Flink Dashboard UI we can see a whole\nlotta red:\n\nNot only has the collect job (theSELECT ) failed\u2014but also theINSERT. Above, I\nsaw this:\n\n    \n    \n    Flink SQL> INSERT INTO t_foo VALUES ('bar',42); [INFO] Submitting SQL update statement to the cluster... [INFO] SQL update statement has been successfully submitted to the cluster: Job ID: c49df5c96e013007d6224b06e218533c\n\nand somewhat rashly assumed that successfully submitted meant that it had run\nsuccessfully. Oh, foolish and na\u00efve one am I! This is an asynchronous job\nsubmission. The only success was that the job was submitted to the job manager\nto execute. If we dig into the job in the UI (or indeed the log files for the\ntaskexecutor process and standalonesession process and scroll up from the\nother error), we'll see why the INSERT failed\u2014for a similar reason as\ntheSELECT:\n\n    \n    \n    java.lang.ClassNotFoundException: org.apache.flink.formats.parquet.ParquetWriterFactory\n\nSo what is happening here? Well we've happily made the Parquet JAR available\nto the SQL Client... yet the actual business of writing and reading data is\nnot done by the client, but the Flink task manager\u2014and we didn't put the JAR\nthere. Or rather we did (since we're running on the same local installation as\nour SQL Client is started from), but we didn't restart the cluster components.\nWe also need to remember to make sure that the HADOOP_CLASSPATH is in the\nenvironment variables when we do:\n\n    \n    \n    $ ./bin/stop-cluster.sh Stopping taskexecutor daemon (pid: 62910) on host asgard08. Stopping standalonesession daemon (pid: 62630) on host asgard08. # Make the Hadoop JARs available $ export HADOOP_CLASSPATH=$(~/hadoop/hadoop-3.3.4/bin/hadoop classpath) # Check that the Parquet JAR is there $ ls -l ./lib/*parquet* -rw-r--r--@ 1 rmoff staff 6740707 5 Apr 17:06 ./lib/flink-sql-parquet-1.18.1.jar # Start the cluster $ ./bin/start-cluster.sh Starting cluster. Starting standalonesession daemon on host asgard08. Starting taskexecutor daemon on host asgard08.\n\nNow when we run the INSERT and SELECT from the SQL Client, things work as they\nshould:\n\n    \n    \n    Flink SQL> INSERT INTO t_foo VALUES ('bar',42); [INFO] Submitting SQL update statement to the cluster... [INFO] SQL update statement has been successfully submitted to the cluster: Job ID: 850b4771b79845790e48d57c7172f203 Flink SQL> SELECT * FROM t_foo; +----+--------------------------------+-------------+ | op | c1 | c2 | +----+--------------------------------+-------------+ | +I | bar | 42 | +----+--------------------------------+-------------+ Received a total of 1 row\n\nThe Flink Dashboard shows that things are healthy too:\n\nAnd we can also double check from within the SQL Client itself that the INSERT\njob ran successfully:\n\n    \n    \n    Flink SQL> SHOW JOBS; +----------------------------------+----------------------------------------------------+----------+-------------------------+ | job id | job name | status | start time | +----------------------------------+----------------------------------------------------+----------+-------------------------+ | 850b4771b79845790e48d57c7172f203 | insert-into_default_catalog.default_database.t_foo | FINISHED | 2024-04-05T17:02:38.143 | +----------------------------------+----------------------------------------------------+----------+-------------------------+ 1 row in set\n\n## A JAR full of Trouble\n\nMost of the problems that I've encountered seem bewildering at first, but once\nsolved can be understood and reverse-engineered to see how I could, in theory,\nhave avoided the problem from better comprehension of the documentation or\nconcepts. This one though defies that. I fixed the error, but I still have no\nidea what causes it. Explanations welcome!\n\nMy original starting point was this Docker Compose file, which provides a\nFlink stack with support for writing to Apache Iceberg on S3-compatible MinIO.\nI used it as the basis for exploring catalogs provided by the Iceberg Flink\nconnector, including using JDBC as a backing store for the catalog. I added a\nPostgres container to the setup, as well as adapting the base Flink container\nto include the necessary Postgres driver.\n\nThe Iceberg JDBC catalog (org.apache.iceberg.jdbc.JdbcCatalog) uses a JDBC\ndatabase for storing the metadata of the catalog, similar to what the Hive\nmetastore would. When we create a Flink SQL catalog and objects (tables, etc)\nwithin it we'll get rows written to Postgres, and when we run DML to interact\nwith the Flink SQL table we'd expect to have queries run against Postgres to\nfetch the metadata for the table.\n\nI created an Iceberg catalog with JDBC metastore and a table within it:\n\n    \n    \n    CREATE CATALOG jdbc_catalog WITH ( 'type' = 'iceberg', 'io-impl' = 'org.apache.iceberg.aws.s3.S3FileIO', 'client.assume-role.region' = 'us-east-1', 'warehouse' = 's3://warehouse', 's3.endpoint' = 'http://storage:9000', 's3.path-style-access' = 'true', 'catalog-impl' = 'org.apache.iceberg.jdbc.JdbcCatalog', 'uri' ='jdbc:postgresql://postgres:5432/world-db?user=world&password=world123'); USE `jdbc_catalog`.`default`; CREATE TABLE t_foo (c1 varchar, c2 int);\n\nThen I added a row of data, and tried to read it back:\n\n    \n    \n    Flink SQL> INSERT INTO t_foo VALUES ('a',42); [INFO] Submitting SQL update statement to the cluster... [INFO] SQL update statement has been successfully submitted to the cluster: Job ID: 21803e3a205877e801536214c9f2d560 Flink SQL> SELECT * FROM t_foo; [ERROR] Could not execute SQL statement. Reason: java.sql.SQLException: No suitable driver found for jdbc:postgresql://postgres:5432/world-db?user=world&password=world123\n\nAs I'd learnt the hard way previously, an INSERT in Flink SQL is run\nasynchronously, so I now knew better than to think that it had succeeded\u2014the\nSELECT was just to see what would happen.\n\nLooking at the task manager log we can see that the INSERT failed with the\nsame error as theSELECT:\n\n    \n    \n    2024-04-09 16:15:13,773 WARN org.apache.flink.runtime.taskmanager.Task [] - IcebergFilesCommitter -> Sink: IcebergSink jdbc_catalog.default.t_foo (1/1) #0 (fa6353abff898aa3e4005455ff93a5cb_90bea66de1c231edf33913ecd54406c1_0_0) switched from INITIALIZING to FAILED with failure cause: org.apache.iceberg.jdbc.UncheckedSQLException: Failed to connect: jdbc:postgresql://postgres:5432/world-db?user=world&password=world123 at org.apache.iceberg.jdbc.JdbcClientPool.newClient(JdbcClientPool.java:57) at org.apache.iceberg.jdbc.JdbcClientPool.newClient(JdbcClientPool.java:30) at org.apache.iceberg.ClientPoolImpl.get(ClientPoolImpl.java:125) at org.apache.iceberg.ClientPoolImpl.run(ClientPoolImpl.java:56) at org.apache.iceberg.ClientPoolImpl.run(ClientPoolImpl.java:51) at org.apache.iceberg.jdbc.JdbcCatalog.initializeCatalogTables(JdbcCatalog.java:146)[...] Caused by: java.sql.SQLException: No suitable driver found for jdbc:postgresql://postgres:5432/world-db?user=world&password=world123 at java.sql/java.sql.DriverManager.getConnection(Unknown Source) at java.sql/java.sql.DriverManager.getConnection(Unknown Source) at org.apache.iceberg.jdbc.JdbcClientPool.newClient(JdbcClientPool.java:55) ... 22 more\n\nThe stack trace confirms that this error is when the connection to the\nPostgres database is attempted as part of the catalog access\n(org.apache.iceberg.jdbc.JdbcCatalog.initializeCatalogTables), and that the\nroot cause is that the Postgres JDBC driver can't be found.\n\nWhat was puzzling was that the JDBC driver was present on the Flink task\nmanager container, as well as the SQL client container. In fact, even more\npuzzling was that the catalog and table creation had worked\u2014so the connection\nto Postgres for those statements must have been ok.\n\nTo save you some of the pain of debugging this, I'll point out that the Docker\nCompose used a different image for SQL Client than the Flink task manager and\njob manager:\n\nI confirmed, re-confirmed, and then confirmed again once more that the\nPostgres JDBC driver was present on the Flink task manager\n\n    \n    \n    root@a3a8182bd227:/opt/flink# ls -l /opt/flink/lib/postgresql-42.7.1.jar -rw-r--r-- 1 root root 1084174 Jan 23 17:36 /opt/flink/lib/postgresql-42.7.1.jar\n\nI also checked that it was present in the Classpath too by looking at the\nstartup messages in the log file:\n\n    \n    \n    [...] org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - Classpath: /opt/flink/lib/bundle-2.20.18.jar:/opt/flink/lib/flink-cep-1.16.1.jar:/opt/flink/lib/flink-connector-files-1.16.1.jar:/opt/flink/lib/flink-csv-1.16.1.jar:/opt/flink/lib/flink-json-1.16.1.jar:/opt/flink/lib/flink-scala_2.12-1.16.1.jar:/opt/flink/lib/flink-shaded-hadoop-2-uber-2.8.3-10.0.jar:/opt/flink/lib/flink-shaded-zookeeper-3.5.9.jar:/opt/flink/lib/flink-sql-connector-hive-2.3.9_2.12-1.16.1.jar:/opt/flink/lib/flink-table-api-java-uber-1.16.1.jar:/opt/flink/lib/flink-table-planner-loader-1.16.1.jar:/opt/flink/lib/flink-table-runtime-1.16.1.jar:/opt/flink/lib/hadoop-common-2.8.3.jar:/opt/flink/lib/iceberg-flink-runtime-1.16-1.3.1.jar:/opt/flink/lib/log4j-1.2-api-2.17.1.jar:/opt/flink/lib/log4j-api-2.17.1.jar:/opt/flink/lib/log4j-core-2.17.1.jar:/opt/flink/lib/log4j-slf4j-impl-2.17.1.jar:/opt/flink/lib/postgresql-42.7.1.jar:/opt/flink/lib/flink-dist-1.16.1.jar:::: [...]\n\nOne thing that did stand out was the version of Flink - 1.16.1. No reason why\nthis should be an issue (and in the end it wasn't), but I decided to try and\nrebuild the environment using my own Docker Compose from scratch with Flink\n1.18.1. You can find this on GitHub here. As well as bumping the Flink\nversion, I switched to my previous deployment model of a single Flink image,\nand running the SQL Client within one of the containers:\n\nAll the JARs I kept the same as in the initial deployment, except pulling in\nthe correct version for Flink 1.18.1 where needed:\n\n  * bundle-2.20.18.jar\n  * flink-shaded-hadoop-2-uber-2.8.3-10.0.jar\n  * hadoop-common-2.8.3.jar\n  * iceberg-flink-runtime-1.18-1.5.0.jar\n  * postgresql-42.7.1.jar\n\nThis time I got the same error, but at a different time\u2014as soon as I tried to\ncreate the catalog:\n\n    \n    \n    Flink SQL> CREATE CATALOG jdbc_catalog WITH ( 'type' = 'iceberg', 'io-impl' = 'org.apache.iceberg.aws.s3.S3FileIO', 'client.assume-role.region' = 'us-east-1', 'warehouse' = 's3://warehouse', 's3.endpoint' = 'http://minio:9000', 's3.path-style-access' = 'true', 'catalog-impl' = 'org.apache.iceberg.jdbc.JdbcCatalog', 'uri' ='jdbc:postgresql://postgres:5432/world-db?user=world&password=world123'); [ERROR] Could not execute SQL statement. Reason: java.sql.SQLException: No suitable driver found for jdbc:postgresql://postgres:5432/world-db?user=world&password=world123\n\nThis makes sense when we realise that the SQL Client is running using the same\nimage as the Flink cluster\u2014where we saw the error above. So if there's a\nproblem with this environment, then it's going to manifest itself in SQL\nClient too.\n\nThis then prompted me to look at the difference between the original SQL\nClient image, and the Flink taskmanager. I knew that I'd added the Postgres\nJDBC driver to them, but I'd not looked more closely at their base\nconfiguration.\n\nIt turned out that the Flink Hive connector (flink-sql-connector-\nhive-2.3.9_2.12-1.16.1.jar) was present on the taskmanager image, but not the\nSQL Client.\n\nBack on my 1.18.1 environment, I removed this JAR, rebuilt the Docker image\nand retried the experiment. Things looked better straight away:\n\n    \n    \n    Flink SQL> CREATE CATALOG jdbc_catalog WITH ( > 'type' = 'iceberg', > 'io-impl' = 'org.apache.iceberg.aws.s3.S3FileIO', > 'client.assume-role.region' = 'us-east-1', > 'warehouse' = 's3://warehouse', > 's3.endpoint' = 'http://minio:9000', > 's3.path-style-access' = 'true', > 'catalog-impl' = 'org.apache.iceberg.jdbc.JdbcCatalog', > 'uri' ='jdbc:postgresql://postgres:5432/world-db?user=world&password=world123'); [INFO] Execute statement succeed.\n\nI successfully created a table:\n\n    \n    \n    Flink SQL> create database `jdbc_catalog`.`db01`; [INFO] Execute statement succeed. Flink SQL> use `jdbc_catalog`.`db01`; [INFO] Execute statement succeed. Flink SQL> CREATE TABLE t_foo (c1 varchar, c2 int); [INFO] Execute statement succeed.\n\nOver in Postgres I could see the catalog entries:\n\n    \n    \n    world-db=# select * from iceberg_namespace_properties ; catalog_name | namespace | property_key | property_value --------------+-----------+--------------+---------------- jdbc_catalog | db01 | exists | true (1 row) world-db=# select * from iceberg_tables; catalog_name | table_namespace | table_name | metadata_location | previous_metadata_location --------------+-----------------+------------+---------------------------------------------------------------------------------------------+---------------------------- jdbc_catalog | db01 | t_foo | s3://warehouse/db01/t_foo/metadata/00000-5073e16c-36c7-493e-8653-30122a9460e5.metadata.json | (1 row)\n\nNow for the crunch moment... writing data:\n\n    \n    \n    Flink SQL> INSERT INTO t_foo VALUES ('a',42); [INFO] Submitting SQL update statement to the cluster... [INFO] SQL update statement has been successfully submitted to the cluster: Job ID: 33d09054f65555ec08a96e1f9817f77d Flink SQL> SHOW JOBS ; +----------------------------------+-------------------------------------+----------+-------------------------+ | job id | job name | status | start time | +----------------------------------+-------------------------------------+----------+-------------------------+ | 33d09054f65555ec08a96e1f9817f77d | insert-into_jdbc_catalog.db01.t_foo | FINISHED | 2024-04-10T10:20:53.283 | +----------------------------------+-------------------------------------+----------+-------------------------+ 1 row in set\n\nIt's looking promising (the job showsFINISHED)...\n\n    \n    \n    Flink SQL> SELECT * FROM t_foo; +----+--------------------------------+-------------+ | op | c1 | c2 | +----+--------------------------------+-------------+ | +I | a | 42 | +----+--------------------------------+-------------+ Received a total of 1 row\n\nSuccess! But what is going on?\n\nWith the Flink Hive Connector JAR (flink-sql-connector-\nhive-2.3.9_2.12-1.18.1.jar ) present, Flink can't find the Postgres JDBC\nDriver:\n\n    \n    \n    java.sql.SQLException: No suitable driver found for jdbc:postgresql://postgres:5432/world-db?user=world&password=world123\n\nIf I remove the Hive connector JAR, then Flink finds the Postgres JDBC driver\nand things are just fine.\n\nWhen looking at the Hive connector JAR on Maven and peering at the digit-salad\nthat is the JAR file naming style I did notice that 2.3.9 is not the latest\nHive version:\n\nSo, in the interest of hacking around to learn stuff, I gave the most recent\nversion (3.1.3) of the JAR (flink-sql-connector-hive-3.1.3_2.12-1.18.1.jar) a\ntry. Same 1.18.1 environment as above when things didn't work, except with\nflink-sql-connector-hive-3.1.3_2.12-1.18.1.jar in the place of flink-sql-\nconnector-hive-2.3.9_2.12-1.18.1.jar and...it works.\n\nSo ultimately it seems that something to do with flink-sql-connector-\nhive-2.3.9_2.12-1.18.1.jar stops Flink from being able to find the Postgres\nJDBC Driver. I have no idea why. But at least I know now how to fix it \ud83d\ude42\n\n## Writing to S3 from Flink\n\nUnlike the previous problem, this one makes sense once you get it working and\nlook back over what was needed. Nonetheless, it took me a lot of iterating\n(a.k.a. changing random things) to get it to work.\n\nFollow along as I relive the journey...\n\nI've got Flink 1.18 and a standalone Hive Metastore container, as well as a\nMinIO container. MinIO is a S3-compatible object store that you can run\nlocally, making it perfect for this kind of playground.\n\nThe stack is running under Docker Compose (you'll find this on GitHub if you\nwant to try it).\n\nThe first step is creating the catalog (I'm using Hive) and table:\n\n    \n    \n    Flink SQL> CREATE CATALOG c_hive WITH ( 'type' = 'hive', 'hive-conf-dir' = './conf/'); [INFO] Execute statement succeed. Flink SQL> USE `c_hive`.`default`; [INFO] Execute statement succeed.\n\nNext up we define a table using the filesystem connector and an S3 path.\n\n    \n    \n    Flink SQL> CREATE TABLE t_foo_fs (c1 varchar, c2 int) WITH ( 'connector' = 'filesystem', 'path' = 's3://warehouse/t_foo_fs/', 'format' = 'csv' ); [INFO] Execute statement succeed.\n\nAnd then we try to add some data, which doesn't work (the job ends with FAILED\nstatus):\n\n    \n    \n    Flink SQL> INSERT INTO t_foo_fs VALUES ('a',42); [INFO] Submitting SQL update statement to the cluster... [INFO] SQL update statement has been successfully submitted to the cluster: Job ID: 76dd5a9044c437a2e576f29df86a3df4 Flink SQL> SHOW JOBS; +----------------------------------+-------------------------------------+--------+-------------------------+ | job id | job name | status | start time | +----------------------------------+-------------------------------------+--------+-------------------------+ | 76dd5a9044c437a2e576f29df86a3df4 | insert-into_c_hive.default.t_foo_fs | FAILED | 2024-04-10T14:42:12.597 | +----------------------------------+-------------------------------------+--------+-------------------------+\n\nOver to the cluster log files to see what the problem is. The jobmanager log\nshows us this:\n\n    \n    \n    2024-04-10 14:35:58,809 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Values[1] -> StreamingFileWriter -> Sink: end (1/1) (2c1d784ada116b60a9bcd63a9439410a_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from INITIALIZING to FAILED on 192.168.97.6:41975-23ccd7 @ 03-hive-parquet-taskmanager-1.zaphod (dataPort=40685). org.apache.flink.core.fs.UnsupportedFileSystemSchemeException: Could not find a file system implementation for scheme 's3'. The scheme is directly supported by Flink through the following plugin(s): flink-s3-fs-hadoop, flink-s3-fs-presto. Please ensure that each plugin resides within its own subfolder within the plugins directory. See https://nightlies.apache.org/flink/flink-docs-stable/docs/deployment/filesystems/plugins/ for more information. If you want to use a Hadoop file system for that scheme, please add the scheme to the configuration fs.allowed-fallback-filesystems. For a full list of supported file systems, please see https://nightlies.apache.org/flink/flink-docs-stable/ops/filesystems/. [...] 2024-04-10 14:35:58,836 INFO org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManager [] - Clearing resource requirements of job a67f542ab426485698c9db3face73c36 2024-04-10 14:35:58,845 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Job insert-into_c_hive.default.t_foo_fs (a67f542ab426485698c9db3face73c36) switched from state RUNNING to FAILING.\n\nThis is one of my favourite kinds of error message: descriptive, and overly\nhelpful. (My colleague Gunnar Morling also has some more words of wisdom to\nshare on this subject).\n\n  * What's the problem? Could not find a file system implementation for scheme 's3'\n  * How do I fix it? The scheme is directly supported by Flink through the following plugin(s): flink-s3-fs-hadoop, flink-s3-fs-presto\n  * How do I find out more? See https://nightlies.apache.org/flink/flink-docs-stable/docs/deployment/filesystems/plugins/ for more information\n\nThe S3 plugin actually ships as part of the Flink distribution; we just need\nto make it available at runtime by putting it in the plugins folder:\n\n    \n    \n    mkdir ./plugins/s3-fs-hadoop && \\ cp ./opt/flink-s3-fs-hadoop-1.18.1.jar ./plugins/s3-fs-hadoop/\n\nAfter bouncing the Flink cluster I got a different error from jobmanager when\ntrying the INSERT:\n\n    \n    \n    Job insert-into_c_hive.default.t_foo_fs (f1532244c3a97d3308d42c41ab79e092) switched from state FAILING to FAILED. [...] java.nio.file.AccessDeniedException: t_foo_fs/part-c4075636-891e-4233-8224-e09064c7c7eb-0-0: org.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by DynamicTemporaryAWSCredentialsProvider TemporaryAWSCredentialsProvider SimpleAWSCredentialsProvider EnvironmentVariable [...] com.amazonaws.SdkClientException: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY))\n\nThis makes sense\u2014we're trying to use S3 (well, MinIO) but we've not provided\nany S3 credentials (NoAuthWithAWSException: No AWS Credentials provided). The\ndocs for S3 offer one option\u2014adding them to flink.conf. We can pass this as a\nruntime option by setting it in the FLINK_PROPERTIES environment variable as\npart of the Docker Compose:\n\n    \n    \n    jobmanager: [...] environment: - | FLINK_PROPERTIES= s3.access.key: admin s3.secret.key: password [...]\n\nNow the error evolves...\n\n    \n    \n    Caused by: java.nio.file.AccessDeniedException: t_foo_fs/part-ddd5011f-9863-432c-9988-50dc1d2628b3-0-0: initiate MultiPartUpload on t_foo_fs/part-ddd5011f-9863-432c-9988-50dc1d2628b3-0-0: com.amazonaws.services.s3.model.AmazonS3Exception: The AWS Access Key Id you provided does not exist in our records. (Service: Amazon S3; Status Code: 403; Error Code: InvalidAccessKeyId; Request ID: P6ZZ5SJAVR9C38AA; S3 Extended Request ID: 7Nxqk2li47vlMAzllA57vfRmiePcFYFrv9/vHn6Aknv5+V5gwYyLzk9KIwGC9fE/biNzCWTzozI=; Proxy: null), S3 Extended Request ID: 7Nxqk2li47vlMAzllA57vfRmiePcFYFrv9/vHn6Aknv5+V5gwYyLzk9KIwGC9fE/biNzCWTzozI=:InvalidAccessKeyId\n\nThis is because we're trying to use the credentials that we've configured for\nMinIO (the super-secret admin/password \ud83d\ude09) against AWS S3 itself. Because we're\nusing MinIO we need to tell Flink where to direct its S3 call, and we do this\nwith s3.endpoint:\n\n    \n    \n    jobmanager: [...] environment: - | FLINK_PROPERTIES= s3.access.key: admin s3.secret.key: password s3.endpoint: http://minio:9000 [...]\n\nAt this point things slow down, because the INSERT job runs... and runs...\n\nAfter two minutes there's an INFO in the log:\n\n    \n    \n    org.apache.hadoop.fs.s3a.WriteOperationHelper [] - initiate MultiPartUpload on t_foo_fs/part-29594611-b313-4ff6-a0a0-86087ec6f262-0-0: Retried 0: org.apache.hadoop.fs.s3a.AWSClientIOException: initiate MultiPartUpload on t_foo_fs/part-29594611-b313-4ff6-a0a0-86087ec6f262-0-0: com.amazonaws.SdkClientException: Unable to execute HTTP request: warehouse.minio: Unable to execute HTTP request: warehouse.minio\n\nand five minutes after submitting the INSERT there's thisERROR:\n\n    \n    \n    org.apache.hadoop.fs.s3a.WriteOperationHelper [] - initiate MultiPartUpload on t_foo_fs/part-29594611-b313-4ff6-a0a0-86087ec6f262-0-0: Retried 1: org.apache.hadoop.fs.s3a.AWSClientIOException: initiate MultiPartUpload on t_foo_fs/part-29594611-b313-4ff6-a0a0-86087ec6f262-0-0: com.amazonaws.SdkClientException: Unable to execute HTTP request: warehouse.minio: Name or service not known: Unable to execute HTTP request: warehouse.minio: Name or service not known\n\nThe problem here looks like some kind of hostname issue. Previously we saw how\nreferencing localhost from a Docker container can be a misconfiguration, but\nthis is something different. warehouse comes from the CREATE TABLE\nconfiguration'path' = 's3://warehouse/t_foo_fs/', whilst minio is the\ns3.endpoint we just set.\n\nSo the S3 endpoint is being picked up, but somehow mangled together with the\npath of the table. Something I've learnt from working with MinIO before is\nthat using path style access can be important. I added this to\ntheFLINK_PROPERTIES:\n\n    \n    \n    [...] FLINK_PROPERTIES= s3.access.key: admin s3.secret.key: password s3.endpoint: http://minio:9000 s3.path.style.access: true\n\nand then got yet another different error from the INSERT job:\n\n    \n    \n    Caused by: java.util.concurrent.ExecutionException: java.io.IOException: Stream closed. at java.base/java.util.concurrent.FutureTask.report(Unknown Source) at java.base/java.util.concurrent.FutureTask.get(Unknown Source) at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.completeProcessing(SourceStreamTask.java:368) at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:340) Caused by: java.io.IOException: Stream closed. at org.apache.flink.core.fs.RefCountedFileWithStream.requireOpened(RefCountedFileWithStream.java:72) at org.apache.flink.core.fs.RefCountedFileWithStream.write(RefCountedFileWithStream.java:52) at org.apache.flink.core.fs.RefCountedBufferingFileStream.flush(RefCountedBufferingFileStream.java:104) at org.apache.flink.fs.s3.common.writer.S3RecoverableFsDataOutputStream.closeAndUploadPart(S3RecoverableFsDataOutputStream.java:209) at org.apache.flink.fs.s3.common.writer.S3RecoverableFsDataOutputStream.closeForCommit(S3RecoverableFsDataOutputStream.java:177) at org.apache.flink.streaming.api.functions.sink.filesystem.OutputStreamBasedPartFileWriter.closeForCommit(OutputStreamBasedPartFileWriter.java:75)\n\nThis looks like FLINK-33536, and so using the convenience afforded by just\nwriting a blog and not needing to use CSV as the format (which seems to be at\nthe root of the issue) I sidestepped the issue and switched the table to\nParquet. I also added the necessary JAR for Parquet in Flink and restarted the\nFlink cluster before changing the table:\n\n    \n    \n    CREATE TABLE t_foo_fs (c1 varchar, c2 int) WITH ( 'connector' = 'filesystem', 'path' = 's3://warehouse/t_foo_fs/', 'format' = 'parquet' );\n\nAfter which ...wait... what is this? Behold!\n\n    \n    \n    Flink SQL> SHOW JOBS; +----------------------------------+-------------------------------------+----------+-------------------------+ | job id | job name | status | start time | +----------------------------------+-------------------------------------+----------+-------------------------+ | a8ca5cd4c59060ac4d4f0996e426af17 | insert-into_c_hive.default.t_foo_fs | FINISHED | 2024-04-11T09:51:16.904 | +----------------------------------+-------------------------------------+----------+-------------------------+ 1 row in set\n\nSuccess! \ud83e\udd73\n\nQuerying the table proves it...\n\n    \n    \n    Flink SQL> SELECT * FROM t_foo_fs; +----+--------------------------------+-------------+ | op | c1 | c2 | +----+--------------------------------+-------------+ | +I | a | 42 | +----+--------------------------------+-------------+ Received a total of 1 row\n\nTo recap then, what was needed to write to S3 (MinIO) from Flink SQL was this:\n\n1\\. Add the S3 plugin to the Flink ./plugins folder:\n\n    \n    \n    mkdir ./plugins/s3-fs-hadoop && \\ cp ./opt/flink-s3-fs-hadoop-1.18.1.jar ./plugins/s3-fs-hadoop/\n\n2\\. Add S3 credentials to Flink configuration. This can be done as environment\nvariables, or added to flink-conf.yaml either directly or by adding to the\nFLINK_PROPERTIES environment variable, which is what I did\n\n3\\. For MinIO, set the S3 endpoint and enable path-style access. As with the\ncredentials, I set this as part of FLINK_PROPERTIES, which ended up as this:\n\n    \n    \n    services: jobmanager: [...] environment: - | FLINK_PROPERTIES= s3.access.key: admin s3.secret.key: password s3.endpoint: http://minio:9000 s3.path.style.access: true [...]\n\nOh - and the CSV bug that I hit is FLINK-33536 and I worked around it by just\nnot using CSV :)\n\n## What's Running Where? (Not So Much Fun with Hive MetaStore)\n\nIf this blog so far has been some gnarly but reasonable challenges, I'd like\nto round off with the big end-of-level boss. This brings together JARs, Flink\nconfiguration\u2014and the importance of understanding what is running where.\n\nTo set the scene: I was doing the same as the above section\u2014I was setting up\nFlink 1.18 writing files to MinIO (S3 compatible storage), using Hive\nMetastore for catalog persistence. But instead of Parquet or CSV format, I was\nwriting Iceberg files. Now, that may seem insignificant, but the impact was\ncrucial.\n\nMy environment was setup with Docker Compose as before (and available on\nGitHub):\n\nTo my base Flink image I'd added Parquet, S3, Hadoop, Hive, and Iceberg\ndependencies. Building on my lessons learnt from above, I've also added S3\nconfiguration to the FLINK_PROPERTIES environment variable for the Flink\ncluster containers:\n\n    \n    \n    services: jobmanager: [...] environment: - | FLINK_PROPERTIES= s3.access.key: admin s3.secret.key: password s3.endpoint: http://minio:9000 s3.path.style.access: true [...]\n\nAfter bringing the stack up, we'll start by creating the Iceberg catalog. We\nuse s3a rather than s3 per the Iceberg docs (since this is all done through\nthe Iceberg Flink support)\n\n    \n    \n    CREATE CATALOG c_iceberg_hive WITH ( 'type' = 'iceberg', 'warehouse' = 's3a://warehouse', 'catalog-type'='hive', 'uri'='thrift://localhost:9083');\n\nNow let's see if there's a default database provided by the catalog:\n\n    \n    \n    Flink SQL> USE CATALOG c_iceberg_hive; [INFO] Execute statement succeed. Flink SQL> SHOW DATABASES; [ERROR] Could not execute SQL statement. Reason: java.net.ConnectException: Connection refused (Connection refused)\n\nThe Connection refused here is coming from the fact that the SQL Client is\ntrying to reach the Hive MetaStore (HMS) usingthrift://localhost:9083 \u2014but\nlocalhost won't work as that's local to the SQL Client container.\n\nInstead we need to use the hostname of the HMS in the uri configuration:\n\n    \n    \n    Flink SQL> DROP CATALOG c_iceberg_hive; [ERROR] Could not execute SQL statement. Reason: org.apache.flink.table.catalog.exceptions.CatalogException: Cannot drop a catalog which is currently in use. Flink SQL> show catalogs; +-----------------+ | catalog name | +-----------------+ | c_iceberg_hive | | default_catalog | +-----------------+ 2 rows in set Flink SQL> use catalog default_catalog; [INFO] Execute statement succeed. Flink SQL> DROP CATALOG c_iceberg_hive; [INFO] Execute statement succeed. Flink SQL> CREATE CATALOG c_iceberg_hive WITH ( 'type' = 'iceberg', 'warehouse' = 's3a://warehouse', 'catalog-type'='hive', 'uri'='thrift://hms:9083'); [INFO] Execute statement succeed.\n\nNow we can see that there is one database, calleddefault:\n\n    \n    \n    Flink SQL> USE CATALOG c_iceberg_hive; [INFO] Execute statement succeed. Flink SQL> SHOW DATABASES; +---------------+ | database name | +---------------+ | default | +---------------+ 1 row in set\n\nLet's create a new one:\n\n    \n    \n    Flink SQL> CREATE DATABASE `c_iceberg_hive`.`db01`; [ERROR] Could not execute SQL statement. Reason: MetaException(message:java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found)\n\nFear not! We've seen this before, right? Based on my exploration of Flink SQL\nand JARs previously I thought I'd be well-equipped to deal with this\none.ClassNotFound? Piece of cake. Right?\n\nAll we need to do is make sure that the Hadoop AWS JAR\u2014that provides the\nS3AFileSystem class\u2014is present. But if we head over to our Flink containers,\nit looks like it already is:\n\n    \n    \n    flink@jobmanager:~$ tree /opt/flink/lib /opt/flink/lib \u251c\u2500\u2500 aws \u2502 \u251c\u2500\u2500 aws-java-sdk-bundle-1.12.648.jar \u2502 \u2514\u2500\u2500 hadoop-aws-3.3.4.jar [...] \u251c\u2500\u2500 hive \u2502 \u2514\u2500\u2500 flink-sql-connector-hive-3.1.3_2.12-1.18.1.jar \u251c\u2500\u2500 iceberg \u2502 \u2514\u2500\u2500 iceberg-flink-runtime-1.18-1.5.0.jar [...]\n\nUsing jinfo we can see that the Classpath for the SQL Client shows that the\nhadoop-aws JAR is present:\n\nWe can even double-check that this is indeed the JAR that we want by searching\nits contents for the class:\n\n    \n    \n    $ jar -tf /opt/flink/lib/aws/hadoop-aws-3.3.4.jar|grep S3AFileSystem.class org/apache/hadoop/fs/s3a/S3AFileSystem.class\n\nSo what next? Honestly, a looooot of hacking about. Various suggestions from\ncolleagues, Slack groups, Stack Overflow, chatGPT\u2014and of course Google, which\nincluded:\n\n  * Check Java version\n  * Check Hadoop dependency version\n  * RTFM \ud83d\udcd6\n  * Try different JAR version\n  * Install full Hadoop and set HADOOP_CLASSPATH\n  * Try a different version of Flink\n  * RTFM some more \ud83d\udcda\n  * Turn it off and on again \ud83e\udd1e\n  * Add iceberg-aws-bundle-1.5.0.jar\n  * Stand on one leg whilst singing La Marseillaise wearing a silver cross and holding a clove of garlic \ud83e\udd2a\n\nAll of these ended up with the same (or different) errors. Whatever I did,\nFlink just didn't seem to be able to find the S3 class.\n\nAnd then...the clouds parted. The sun shone, the angels sang, and one of them\nnamed Aleksandr Pilipenko stepped forth on the Apache Flink Slack group and\ndid thus proclaim:\n\nCould this actually originate from hive side? ThriftHiveMetastore seems to me\nlike something outside of Flink Caused by:\nMetaException(message:java.lang.RuntimeException:\njava.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem\nnot found) at\norg.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_database_result$create_database_resultStandardScheme.read(ThriftHiveMetastore.java:39343)\n\nReader, this fixed it.\n\nOr rather, it put me on the right lines. Because what was happening was that\nthe Hive MetaStore was throwing the error. SQL Client was simply surfacing the\nerror.\n\nWhen you create a database in Iceberg, not only is there metadata written to\nthe metastore (Hive, in this case), but also the warehouse on S3.\n\nWhen we created the catalog we told Iceberg where to find the Hive\nMetastore:'uri'='thrift://hms:9083'. The Hive Metastore then writes additional\nIceberg metadata to'warehouse' = 's3a://warehouse'.\n\nYou can actually see this if you look at the Hive Metastore log. First there's\nthe request from Flink's Iceberg implementation to create the database (note\nthe storage specified ats3a://warehouse/db01.db):\n\n    \n    \n    source:172.24.0.4 create_database: Database(name:db01, description:null, locationUri:s3a://warehouse/db01.db, parameters:{}, ownerName:flink, ownerType:USER, catalogName:hive)\n\nfollowed shortly after by\n\n    \n    \n    ERROR [pool-6-thread-1] metastore.RetryingHMSHandler: MetaException(message:java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newMetaException(HiveMetaStore.java:6937) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database(HiveMetaStore.java:1338) [...]\n\nThe fix? Add the Hadoop AWS JAR (which includes S3 support) to Hive Metastore\n(this is not the same as the Flink deployment, which also needs these JARs):\n\n    \n    \n    cd /opt/hive-metastore/lib && \\ curl https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar -O\n\nThis alone doesn't quite get us over the hill:\n\n    \n    \n    Flink SQL> CREATE DATABASE `c_iceberg_hive`.`db01`; [ERROR] Could not execute SQL statement. Reason: org.apache.thrift.transport.TTransportException\n\nAt least this error isn't such a red-herring; we can see it's a thrift error,\nand so nursing the fresh wounds of our S3 JAR escapades above we go straight\nto check the Hive Metastore log:\n\n    \n    \n    WARN [pool-6-thread-1] metastore.ObjectStore: Failed to get database hive.db01, returning NoSuchObjectException ERROR [pool-6-thread-1] metastore.RetryingHMSHandler: java.lang.NoClassDefFoundError: com/amazonaws/auth/AWSCredentialsProvider\n\nAWSCredentialsProvider is included with aws-java-sdk-bundle and after adding\nthat we're very nearly there:\n\n    \n    \n    Flink SQL> CREATE DATABASE `c_iceberg_hive`.`db01`; [ERROR] Could not execute SQL statement. Reason: MetaException(message:Got exception: java.nio.file.AccessDeniedException s3a://warehouse/db01.db: org.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by TemporaryAWSCredentialsProvider SimpleAWSCredentialsProvider EnvironmentVariableCredentialsProvider IAMInstanceCredentialsProvider : com.amazonaws.SdkClientException: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY)))\n\nBuilding on what we learnt about S3 access from Flink we know that now we just\nneed to add the S3 credentials and additional configuration needed for MinIO\nto Hive Metastore. We do this by adding it to the ./conf/hive-site.xml file on\nthe Hive Metastore:\n\n    \n    \n    <property> <name>fs.s3a.access.key</name> <value>admin</value> </property> <property> <name>fs.s3a.secret.key</name> <value>password</value> </property> <property> <name>fs.s3a.endpoint</name> <value>http://minio:9000</value> </property> <property> <name>fs.s3a.path.style.access</name> <value>true</value> </property>\n\nAnd with that...success.\n\n    \n    \n    Flink SQL> CREATE CATALOG c_iceberg_hive WITH ( > 'type' = 'iceberg', > 'warehouse' = 's3a://warehouse', > 'catalog-type'='hive', > 'uri'='thrift://hms:9083'); [INFO] Execute statement succeed. Flink SQL> CREATE DATABASE `c_iceberg_hive`.`db01`; [INFO] Execute statement succeed.\n\nIn MinIO we've got a object created for the database:\n\n    \n    \n    $ mc ls -r minio/warehouse/ [2024-04-11 16:59:06 UTC] 0B STANDARD db01.db/\n\nIf we create a table within this Iceberg catalog and add some data:\n\n    \n    \n    Flink SQL> CREATE TABLE t_foo_fs (c1 varchar, c2 int); [ERROR] Could not execute SQL statement. Reason: com.amazonaws.SdkClientException: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY))\n\nWhat?! Surely not again. Well not quite. This time the error is coming from\nthe SQL Client itself, as the log (under./log ) shows:\n\n    \n    \n    Caused by: org.apache.flink.table.api.TableException: Could not execute CreateTable in path `c_iceberg_hive`.`db01`.`t_foo_fs` at org.apache.flink.table.catalog.CatalogManager.execute(CatalogManager.java:1296) at org.apache.flink.table.catalog.CatalogManager.createTable(CatalogManager.java:946) [...] Caused by: org.apache.iceberg.exceptions.RuntimeIOException: Failed to create file: s3a://warehouse/db01.db/t_foo_fs/metadata/00000-cec79e1d-1039-45a6-be3a-00a29528ff72.metadata.json\n\nWe're pretty much on the home straight now. In this case, the SQL Client\nitself is writing some of the metadata for the table to S3 (MinIO). Other\nmetadata still goes to the Hive Metastore. I have dug into this in more detail\nin another article.\n\nWhilst we've set the S3 configuration for the jobmanager process as part of\nthe FLINK_PROPERTIES (which gets written to flink-conf.yaml at runtime), this\nconfiguration doesn't seem to be used by the SQL Client.\n\nTo simplify things, I'm going to move the S3 config away from FLINK_PROPERTIES\nand specify it in just one place, the ./conf/hive-site.xml on the Flink\ncontainers, where it should get used by both the jobmanager, taskmanager\u2014and\nSQL Client. It's the same as I added to the Hive Metastore above:\n\n    \n    \n    <?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?> <?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?> <configuration> <property> <name>fs.s3a.access.key</name> <value>admin</value> </property> <property> <name>fs.s3a.secret.key</name> <value>password</value> </property> <property> <name>fs.s3a.endpoint</name> <value>http://minio:9000</value> </property> <property> <name>fs.s3a.path.style.access</name> <value>true</value> </property> </configuration>\n\nFor this to be picked up I also needed to add hive-conf-dir as part of the\nIceberg catalog configuration:\n\n    \n    \n    CREATE CATALOG c_iceberg_hive WITH ( 'type' = 'iceberg', 'warehouse' = 's3a://warehouse', 'catalog-type'='hive', 'uri'='thrift://hms:9083', 'hive-conf-dir' = './conf');\n\nAnd with that\u2014we're done:\n\n    \n    \n    Flink SQL> CREATE TABLE t_foo_fs (c1 varchar, c2 int); [INFO] Execute statement succeed. Flink SQL> INSERT INTO t_foo_fs VALUES ('a',42); [INFO] Submitting SQL update statement to the cluster... [INFO] SQL update statement has been successfully submitted to the cluster: Job ID: da7c9c4fc427a0796729a7cf10d05b2b Flink SQL> SELECT * FROM t_foo_fs; +----+--------------------------------+-------------+ | op | c1 | c2 | +----+--------------------------------+-------------+ | +I | a | 42 | +----+--------------------------------+-------------+ Received a total of 1 row\n\nWe'll wrap up with one more little wrinkle to iron out that's worth\ndocumenting as part of this. As I was testing this, I experimented with a\ndifferent way of defining an Iceberg table. Instead of creating an Iceberg\ncatalog, and then within that a table, you can define a table and specify it\nto use the Iceberg connector. It looks like this:\n\n    \n    \n    CREATE TABLE iceberg_test WITH ( 'connector' = 'iceberg', 'catalog-type'='hive', 'catalog-name'='dev', 'warehouse' = 's3a://warehouse', 'uri'='thrift://hms:9083', 'hive-conf-dir' = './conf') AS SELECT * FROM (VALUES ('Never Gonna','Give You','Up'));\n\nThis works great:\n\n    \n    \n    Flink SQL> SELECT * FROM iceberg_test; +----+--------------------------------+--------------------------------+--------------------------------+ | op | EXPR$0 | EXPR$1 | EXPR$2 | +----+--------------------------------+--------------------------------+--------------------------------+ | +I | Never Gonna | Give You | Up | +----+--------------------------------+--------------------------------+--------------------------------+ Received a total of 1 row\n\nBut\u2014to get to this point I had to get past this:\n\n    \n    \n    Flink SQL> CREATE TABLE iceberg_test WITH ( 'connector' = 'iceberg', 'catalog-type'='hive', 'catalog-name'='dev', 'warehouse' = 's3a://warehouse', 'hive-conf-dir' = './conf') AS SELECT * FROM (VALUES ('Never Gonna','Give You','Up')); [ERROR] Could not execute SQL statement. Reason: java.lang.ClassNotFoundException: org.datanucleus.NucleusContext\n\nA ClassNotFoundException which is something we've dealt with before. But why\nwouldn't this work, if in the same environment things work fine if I create\nthe catalog first and then a table within it?\n\nThe answer comes down to how Flink is picking up the Hive configuration.\nWhilst we've defined in the table where to find the hive-site.xml\nconfiguration ('hive-conf-dir' = './conf' ), in that file itself it only has\nthe S3 configuration. What it doesn't have is a value forhive.metastore.uris.\nThe hive docs tell us that if hive.metastore.uris is not set then Flink\nassumes the metastore is local. For us that means local to the Flink\ncontainer, which it's not\u2014and is where the JAR problem comes in.\n\nThis didn't happen when we created the table as part of the catalog because\nthe CREATE CATALOG included 'uri'='thrift://hms:9083' and thus could find the\nHive Metastore. So the lesson here is that the uri must be specified\nsomewhere\u2014either in the DDL (the successful CREATE TABLE iceberg_test above\ndoes this), or by adding it to thehive-site.xml:\n\n    \n    \n    <?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?> <?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?> <configuration> <property> <name>hive.metastore.uris</name> <value>thrift://hms:9083</name> </property> <property> <name>fs.s3a.access.key</name> <value>admin</value> </property> [...]\n\nWith this added, the CREATE TABLE without a uri configuration also works:\n\n    \n    \n    Flink SQL> CREATE TABLE iceberg_test WITH ( 'connector' = 'iceberg', 'catalog-type'='hive', 'catalog-name'='dev', 'warehouse' = 's3a://warehouse', 'hive-conf-dir' = './conf') AS SELECT * FROM (VALUES ('Never Gonna','Give You','Up')); [INFO] Submitting SQL update statement to the cluster... [INFO] SQL update statement has been successfully submitted to the cluster: Job ID: b5588a1e34375f9c4d4d13a6e6f34d99 Flink SQL> SELECT * FROM iceberg_test; +----+--------------------------------+--------------------------------+--------------------------------+ | op | EXPR$0 | EXPR$1 | EXPR$2 | +----+--------------------------------+--------------------------------+--------------------------------+ | +I | Never Gonna | Give You | Up | +----+--------------------------------+--------------------------------+--------------------------------+ Received a total of 1 row\n\nComing back to theCREATE CATALOG, it can also omit the uri if we've specified\nit in thehive-site.xml, which slims it down to this:\n\n    \n    \n    CREATE CATALOG c_iceberg_hive WITH ( 'type' = 'iceberg', 'warehouse' = 's3a://warehouse', 'catalog-type'='hive', 'hive-conf-dir' = './conf');\n\n## \ud83d\ude32 Gosh. That's all rather confusing and down-in-the-weeds, isn't it?\n\nWell, yes. That is the joy of running a complex distributed system\u2014and one\nwith a venerable history dating back to the Apache Hadoop ecosystem\u2014for\nyourself.\n\nIf you want to spend your time solving your business problems instead of\ndebugging infrastructure, check our Decodable. Our fully-managed platform\ngives you access to Flink SQL and connectors (including Iceberg) and does all\nthe gnarly stuff for you. Not a JAR or Catalog to worry about in sight! (Of\ncourse, you can bring your own Flink JAR jobs if you want to run a custom\npipeline, but that's a point for a different blog post on a different day).\n\nDecodable has a free trial that doesn't require a credit card to use\u2014so give\nit a try today.\n\n### \ud83d\udceb Email signup \ud83d\udc47\n\n#### Did you enjoy this issue of Checkpoint Chronicle? Would you like the next\nedition delivered directly to your email to read from the comfort of your own\nhome?\n\nSimply enter your email address here and we'll send you the next issue as soon\nas it's published\u2014and nothing else, we promise!\n\n\ud83d\udc4d Got it!\n\nOops! Something went wrong while submitting the form.\n\nRobin Moffatt\n\nRobin is a Principal DevEx Engineer at Decodable. He has been speaking at\nconferences since 2009 including QCon, Devoxx, Strata, Kafka Summit, and\n\u00d8redev. You can find many of his talks online and his articles on the\nDecodable blog as well as his own blog.\n\nOutside of work, Robin enjoys running, drinking good beer, and eating fried\nbreakfasts\u2014although generally not at the same time.\n\n### Related Posts\n\nFebruary 16, 2024\n\n8\n\nmin read\n\nBlog\n\n### Catalogs in Flink SQL\u2014A Primer\n\nBy\n\nRobin Moffatt\n\nFebruary 19, 2024\n\n10\n\nmin read\n\nBlog\n\n### Catalogs in Flink SQL\u2014Hands On\n\nBy\n\nRobin Moffatt\n\nFebruary 27, 2024\n\n4\n\nmin read\n\nBlog\n\n### Flink SQL and the Joy of JARs\n\nBy\n\nRobin Moffatt\n\n### Table of contents\n\nThe JDBC Catalog\n\nWhat's Running Where? (Fun with Java Versions)\n\nWhat's Running Where? (Fun with JAR dependencies)\n\nA JAR full of Trouble\n\nWriting to S3 from Flink\n\nWhat's Running Where? (Not So Much Fun with Hive MetaStore)\n\n\ud83d\ude32 Gosh. That's all rather confusing and down-in-the-weeds, isn't it?\n\nLet's Get Decoding\n\nStart freeTalk To An Expert\n\nI never meant to write this blog. I had a whole blog series about Flink SQL\nlined up...and then I started to write it and realised rapidly that one's\ninitial exposure to Flink and Flink SQL can be somewhat, shall we say,\ninteresting. Interesting, as in the curse, \"may you live in interesting\ntimes\". Because as wonderful and as powerful Flink is, it is not a simple\nbeast to run for yourself, even as a humble developer just trying to try out\nsome SQL.\n\nQuite a few of the problems are a subset of the broader challenge of getting\nthe right JAR in the right place at the right time\u2014a topic worth its own blog.\n\nLet's start off by looking at the JDBC connector. This provides its own\ncatalog, which I explored recently in another post. In that article I trod the\nhappy path; below are the potholes and pitfalls that befell me on the way \ud83e\uddcc.\n\n## The JDBC Catalog\n\nYou can learn more about the JDBC Catalog, and Flink SQL Catalogs in general,\nhere and here.\n\nWe'll start by provisioning the environment. Using this Docker Compose from\nthe Decodable examples repository we can spin up a Postgres container and a\nFlink cluster. In the Flink cluster I've added the Flink JDBC Connector and\nthe Postgres JDBC driver.\n\n    \n    \n    docker compose up\n\nOnce the containers are running you can inspect the pre-provisioned Postgres\ndatabase:\n\n    \n    \n    docker compose exec -it postgres psql -h localhost -d world-db -U world\n    \n    \n    world-db-# \\d List of relations Schema | Name | Type | Owner --------+------------------+-------+------- public | city | table | world public | country | table | world public | country_flag | table | world public | country_language | table | world (4 rows)\n\nNow we'll head into Flink SQL and try to create a JDBC catalog so that we can\nquery these Postgres tables:\n\n    \n    \n    docker compose exec -it jobmanager bash -c \"./bin/sql-client.sh\"\n    \n    \n    Flink SQL> CREATE CATALOG c_jdbc WITH ( 'type' = 'jdbc', 'base-url' = 'localhost:5432', 'default-database' = 'world-db', 'username' = 'world', 'password' = 'world123' ); [ERROR] Could not execute SQL statement. Reason: java.lang.IllegalArgumentException\n\nUh-oh. We're not off to a particularly illustrious start. What does Illegal\nArgument\u2014 the argument that we've specified for base-url is \"Illegal\" because\nit needs the fully qualified JDBC URL prefix. Let's try again:\n\n    \n    \n    Flink SQL> CREATE CATALOG c_jdbc WITH ( 'type' = 'jdbc', 'base-url' = 'jdbc:postgres://localhost:5432', 'default-database' = 'world-db', 'username' = 'world', 'password' = 'world123' ); [ERROR] Could not execute SQL statement. Reason: java.lang.IllegalStateException: Could not find any jdbc dialect factory that can handle url 'jdbc:postgres://localhost:5432' that implements 'org.apache.flink.connector.jdbc.dialect.JdbcDialectFactory' in the classpath. Available factories are: org.apache.flink.connector.jdbc.databases.derby.dialect.DerbyDialectFactory org.apache.flink.connector.jdbc.databases.mysql.dialect.MySqlDialectFactory org.apache.flink.connector.jdbc.databases.oracle.dialect.OracleDialectFactory org.apache.flink.connector.jdbc.databases.postgres.dialect.PostgresDialectFactory org.apache.flink.connector.jdbc.databases.sqlserver.dialect.SqlServerDialectFactory\n\nThis error is definitely more esoteric than the last one. At the heart of it\nis this:\n\n  *     Could not find any jdbc dialect factory that can handle url 'jdbc:postgres://localhost:5432'\n\nWe've told the JDBC connector that it's a JDBC dialect (jdbc:) but it can't\nwork out what dialect of JDBC it is. It tells us which \"factories\" it does\nunderstand:\n\n    \n    \n    Available factories are: `[...].jdbc.databases.derby.dialect.DerbyDialectFactory` `[...].jdbc.databases.mysql.dialect.MySqlDialectFactory` `[...].jdbc.databases.oracle.dialect.OracleDialectFactory` `[...].jdbc.databases.postgres.dialect.PostgresDialectFactory` `[...].jdbc.databases.sqlserver.dialect.SqlServerDialectFactory`\n\nSo what's up with this? Well if we look at the Postgres JDBC URL spec we'll\nsee that it should be jdbc:postgresql (note the ql suffix that we've missed\nabove). Let's try that:\n\n    \n    \n    Flink SQL> CREATE CATALOG c_jdbc WITH ( 'type' = 'jdbc', 'base-url' = 'jdbc:postgresql://localhost:5432', 'default-database' = 'world-db', 'username' = 'world', 'password' = 'world123' ); [ERROR] Could not execute SQL statement. Reason: java.net.ConnectException: Connection refused (Connection refused) Flink SQL>\n\nHey, another new error! That's because localhost in the context of the\ncontainer running sql-client is the Flink job manager container, so there is\nno Postgres on port 5432 to be found. Instead, we need to reference the\nPostgres container, conveniently called in our casepostgres.\n\n    \n    \n    Flink SQL> CREATE CATALOG c_jdbc WITH ( 'type' = 'jdbc', 'base-url' = 'jdbc:postgresql://postgres:5432', 'default-database' = 'world-db', 'username' = 'world', 'password' = 'world123' ); [INFO] Execute statement succeed. Flink SQL>\n\nPhew! \ud83d\ude05\n\nWhilst we're here with a Catalog created in Flink SQL, let's cover off another\nthing that kept tripping me up. Since we've created the catalog, we want to\ntell Flink SQL to now use it as the current catalog:\n\n    \n    \n    Flink SQL> USE c_jdbc; [ERROR] Could not execute SQL statement. Reason: org.apache.flink.table.catalog.exceptions.CatalogException: A database with name [c_jdbc] does not exist in the catalog: [default_catalog].\n\nThis comes about from an inconsistency (IMHO) in the syntax of Flink SQL here.\nUSE on its own in Flink SQL meansUSE DATABASE. So if you want to actually\nswitch the current catalog, the command isUSE CATALOG.\n\n    \n    \n    Flink SQL> USE CATALOG c_jdbc; [INFO] Execute statement succeed.\n\nOn the other hand, if you're getting this error and you actually did mean to\nswitch the current database then the error is what it says - the database\nyou've specified doesn't exist in the current catalog. Check the current\ncatalog withSHOW CURRENT CATALOG.\n\n## What's Running Where? (Fun with Java Versions)\n\nTo recount the next incident, I'll start by describing the environment. This\nwas before the JDBC connector for Flink 1.18 was released, so I was running\nFlink 1.17.1, and locally on my laptop.\n\n    \n    \n    # Unpack a fresh Flink 1.17.1 tarball tar xf flink-1.17.1-bin-scala_2.12.tgz && cd flink-1.17.1 # Install JDBC connector and Postgres JDBC driver cd lib curl https://repo.maven.apache.org/maven2/org/apache/flink/flink-connector-jdbc/3.1.2-1.17/flink-connector-jdbc-3.1.2-1.17.jar -O curl https://repo.maven.apache.org/maven2/org/postgresql/postgresql/42.7.3/postgresql-42.7.3.jar -O cd .. # Launch a Flink cluster and SQL Client $ ./bin/start-cluster.sh && ./bin/sql-client.sh Starting cluster. Starting standalonesession daemon on host asgard08. Starting taskexecutor daemon on host asgard08. [... cute ANSI Squirrel pic...] Flink SQL>\n\nI've also got the same Postgres Docker image as above running, but just as a\nstandalone container with port 5432 open for connections:\n\n    \n    \n    docker run --rm --detach \\ --name postgres \\ --publish 5432:5432 \\ ghusta/postgres-world-db:2.10\n\nNow we'll do the same as above\u2014create a JDBC catalog. Note that this time the\nbase-url is referencing localhost because Flink is running locally on the\nmachine and I\u2019ve exposed Postgres\u2019 port from its container, so localhost is\nthe right place to access it.\n\n    \n    \n    Flink SQL> CREATE CATALOG c_jdbc WITH ( 'type' = 'jdbc', 'base-url' = 'jdbc:postgresql://localhost:5432', 'default-database' = 'world-db', 'username' = 'world', 'password' = 'world123' ); [INFO] Execute statement succeed Flink SQL> USE `c_jdbc`.`world-db`; [INFO] Execute statement succeed. Flink SQL> SHOW TABLES; +-------------------------+ | table name | +-------------------------+ | public.city | | public.country | | public.country_flag | | public.country_language | +-------------------------+ 4 rows in set\n\nLet's try querying one of these tables:\n\n    \n    \n    Flink SQL> SELECT * FROM country_flag LIMIT 5; [ERROR] Could not execute SQL statement. Reason: java.lang.reflect.InaccessibleObjectException: Unable to make field private static final int java.lang.Class.ANNOTATION accessible: module java.base does not \"opens java.lang\" to unnamed module @52bf72b5\n\nHuh. After a bit of head scratching I found a reference on StackOverflow to\nthe version of Java, so I checked that. I'm using the excellent SDKMan which\nmakes this kind of thing very straightforward:\n\n    \n    \n    $ sdk current java Using java version 17.0.5-tem\n\nThat might be a problem. Support for Java 17 was only added in Flink 1.18 and\nis considered experimental at this time. I reverted my Java version to 11, and\nlaunched the SQL Client again:\n\n    \n    \n    $ sdk install java 11.0.21-tem $ sdk use java 11.0.21-tem Using java version 11.0.21-tem in this shell. $ ./bin/sql-client.sh\n\nI recreated the catalog, switched to the world-db database within it, and\ntried my SELECT again:\n\n    \n    \n    Flink SQL> CREATE CATALOG c_jdbc WITH ( > 'type' = 'jdbc', > 'base-url' = 'jdbc:postgresql://localhost:5432', > 'default-database' = 'world-db', > 'username' = 'world', > 'password' = 'world123' > ); [INFO] Execute statement succeed. Flink SQL> USE `c_jdbc`.`world-db`; [INFO] Execute statement succeed. Flink SQL> SELECT * FROM country_flag LIMIT 5;\n\nAt this point there's no error... but no nothing either. After a minute or so\nI get this:\n\n    \n    \n    [ERROR] Could not execute SQL statement. Reason: java.net.ConnectException: Connection refused\n\nThis puzzles me, because Postgres is up\u2014and we can verify the connectivity\nfrom SQL Client withSHOW TABLES:\n\n    \n    \n    Flink SQL> SHOW TABLES; +-------------------------+ | table name | +-------------------------+ | public.city | | public.country | | public.country_flag | | public.country_language | +-------------------------+ 4 rows in set\n\nSo where is the Connection that is beingrefused?\n\nLog files are generally a good place to go and look, particularly when one\nstarts troubleshooting things that are beyond the obvious. If we search the\nFlink log folder for the Connection refused error we get this:\n\n    \n    \n    $ grep -r \"Connection refused\" log log/flink-rmoff-sql-client-asgard08.log:java.util.concurrent.CompletionException: org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: localhost/127.0.0.1:8081 flink-rmoff-sql-client-asgard08.log:Caused by: org.apache.flink.shaded.netty4.io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: localhost/127.0.0.1:8081 flink-rmoff-sql-client-asgard08.log:Caused by: java.net.ConnectException: Connection refused [...]\n\nWhat's interesting here is the port number. 8081 is not the Postgres port\n(5432) but the Flink job manager.\n\nSo whilst the SQL Client itself is making the request to Postgres for a SHOW\nTABLES request, a SELECT gets run as a Flink job\u2014which the SQL Client will\nsend to the job manager in order for it to execute it on an available task\nmanager node.\n\nThe outstanding question though is why the job manager isn't available. We\nlaunched it already above when we did./bin/start-cluster.sh. Or did we? Let's\ncheck the running Java process list:\n\n    \n    \n    $ jps 23795 Jps 3990 SqlClient\n\nSo neither the job manager nor task manager processes are running. Were they\never?\n\n    \n    \n    $ head log/flink-rmoff-standalonesession-1-asgard08.log 2024-04-05 14:01:48,837 INFO org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - Shutting StandaloneSessionClusterEntrypoint down with application status FAILED. Diagnostics java.lang.IllegalAccessError: class org.apache.flink.util.NetUtils (in unnamed module @0x5b8dfcc1) cannot access class sun.net.util.IPAddressUtil (in module java.base) because module java.base does not export sun.net.util to unnamed module @0x5b8dfcc1 [...] $ head log/flink-rmoff-taskexecutor-2-asgard08.log 2024-04-05 14:02:11,255 ERROR org.apache.flink.runtime.taskexecutor.TaskManagerRunner [] - Terminating TaskManagerRunner with exit code 1. java.lang.IllegalAccessError: class org.apache.flink.util.NetUtils (in unnamed module @0x1a482e36) cannot access class sun.net.util.IPAddressUtil (in module java.base) because module java.base does not export sun.net.util to unnamed module @0x1a482e36\n\nMy guess is that the Java version caused these fatal errors. Regardless, in\neffect what I had running locally was this:\n\nNo wonder things didn't work! Realising the problem I'd had with SQL Client\nand the Java version, it was a fair guess that the same issue was true for the\nFlink cluster components.\n\nLet's try again, with Java 11, and use jps to verify that things are starting\nup correctly:\n\n    \n    \n    $ ./bin/start-cluster.sh Starting cluster. Starting standalonesession daemon on host asgard08. Starting taskexecutor daemon on host asgard08. $ jps 28774 StandaloneSessionClusterEntrypoint 29064 Jps 29049 TaskManagerRunner\n\nBetter! Now we'll launch SQL Client and see if things are now working as they\nshould:\n\n    \n    \n    Flink SQL> CREATE CATALOG c_jdbc WITH ( 'type' = 'jdbc', 'base-url' = 'jdbc:postgresql://localhost:5432', 'default-database' = 'world-db', 'username' = 'world', 'password' = 'world123' ); [INFO] Execute statement succeed. Flink SQL> USE `c_jdbc`.`world-db`; [INFO] Execute statement succeed. Flink SQL> SELECT * FROM country_flag LIMIT 5; +----+---------+-----------+--------------------+ | op | code2 | emoji | unicode | +----+---------+-----------+--------------------+ | +I | AD | \ud83c\udde6\ud83c\udde9 | U+1F1E6 U+1F1E9 | | +I | AE | \ud83c\udde6\ud83c\uddea | U+1F1E6 U+1F1EA | | +I | AF | \ud83c\udde6\ud83c\uddeb | U+1F1E6 U+1F1EB | | +I | AG | \ud83c\udde6\ud83c\uddec | U+1F1E6 U+1F1EC | | +I | AI | \ud83c\udde6\ud83c\uddee | U+1F1E6 U+1F1EE | +----+---------+-----------+--------------------+ Received a total of 5 rows\n\n\ud83c\udf89 that's more like it!\n\n## What's Running Where? (Fun with JAR dependencies)\n\nI wrote a whole article about JAR files because they are so central to the\nsuccessful operation of Flink. Let's look at an example of the kind of \"fun\"\nyou can get with them. Moving around the Flink versions, this incident was\nwith 1.18.1. Let's start with a clean slate and a fresh Flink install:\n\n    \n    \n    # Unpack a fresh Flink 1.18.1 tarball tar xf flink-1.18.1-bin-scala_2.12.tgz && cd flink-1.18.1\n\nI was exploring the Filesystem connector and wanted to see if I could write a\nfile in Parquet format.\n\n    \n    \n    Flink SQL> CREATE TABLE t_foo (c1 varchar, c2 int) WITH ( 'connector' = 'filesystem', 'path' = 'file:///tmp/t_foo', 'format' = 'parquet' ); [INFO] Execute statement succeed. Flink SQL> INSERT INTO t_foo VALUES ('bar',42); [ERROR] Could not execute SQL statement. Reason: org.apache.flink.table.api.ValidationException: Could not find any format factory for identifier 'parquet' in the classpath.\n\nThis error (Could not find any factory for identifier) usually means that\nthere's a JAR missing. From the Maven repository I grabbed flink-sql-\nparquet-1.18.1.jar\n\n    \n    \n    cd ./lib curl https://repo1.maven.org/maven2/org/apache/flink/flink-sql-parquet/1.18.1/flink-sql-parquet-1.18.1.jar -O\n\nAfter restarting the SQL Client I got a different error:\n\n    \n    \n    Flink SQL> INSERT INTO t_foo VALUES ('bar',42); [ERROR] Could not execute SQL statement. Reason: java.lang.ClassNotFoundException: org.apache.hadoop.conf.Configuration\n\nThis one I'd come across before - I needed the Hadoop JARs (discussed in more\ndetail here). I downloaded Hadoop (words I never thought I'd write in 2024 \ud83d\ude09)\nand set the environment variable:\n\n    \n    \n    export HADOOP_CLASSPATH=$(~/hadoop/hadoop-3.3.4/bin/hadoop classpath)\n\nI then restarted the SQL Client (with a small detour via this PR for a\nSqlClientException: Could not read from command line error):\n\n    \n    \n    Flink SQL> CREATE TABLE t_foo (c1 varchar, c2 int) WITH ( 'connector' = 'filesystem', 'path' = 'file:///tmp/t_foo', 'format' = 'parquet' ); [INFO] Execute statement succeed. Flink SQL> INSERT INTO t_foo VALUES ('bar',42); [INFO] Submitting SQL update statement to the cluster... [INFO] SQL update statement has been successfully submitted to the cluster: Job ID: c49df5c96e013007d6224b06e218533c\n\nSuccess! Or so I thought...\n\n    \n    \n    Flink SQL> SELECT * FROM t_foo; [ERROR] Could not execute SQL statement. Reason: java.lang.ClassNotFoundException: org.apache.flink.formats.parquet.ParquetColumnarRowInputFormat\n\nKinda confusing, since we only just installed the Parquet JAR (which this\nerror seems to be referencing).\n\nLet's go back to our log files and look at what's going on here. I'll search\nfor the error:\n\n    \n    \n    $ grep -r -l \"java.lang.ClassNotFoundException\" ./log ./log/flink-rmoff-taskexecutor-0-asgard08.log ./log/flink-rmoff-sql-client-asgard08.log ./log/flink-rmoff-standalonesession-0-asgard08.log\n\nSo it's being logged back the SQL Client (which we saw), but also taskexecutor\n(the Task Manager) and standalonesession (the Job Manager). Opening up the\nfiles and looking closely at the timestamps shows us that the SQL Client is\nsurfacing the above error from a call to the job manager (Could not start the\nJobMaster):\n\n    \n    \n    ERROR org.apache.flink.table.gateway.service.operation.OperationManager [] - Failed to execute the operation 072fdeef-76bc-4813-a2af-5444b7806737. org.apache.flink.table.api.TableException: Failed to execute sql Caused by: org.apache.flink.util.FlinkException: Failed to execute job 'collect'. Caused by: java.lang.RuntimeException: org.apache.flink.runtime.client.JobInitializationException: Could not start the JobMaster. [...] Caused by: java.lang.ClassNotFoundException: org.apache.flink.formats.parquet.ParquetColumnarRowInputFormat\n\nIn fact, if we open up the rather useful Flink Dashboard UI we can see a whole\nlotta red:\n\nNot only has the collect job (theSELECT ) failed\u2014but also theINSERT. Above, I\nsaw this:\n\n    \n    \n    Flink SQL> INSERT INTO t_foo VALUES ('bar',42); [INFO] Submitting SQL update statement to the cluster... [INFO] SQL update statement has been successfully submitted to the cluster: Job ID: c49df5c96e013007d6224b06e218533c\n\nand somewhat rashly assumed that successfully submitted meant that it had run\nsuccessfully. Oh, foolish and na\u00efve one am I! This is an asynchronous job\nsubmission. The only success was that the job was submitted to the job manager\nto execute. If we dig into the job in the UI (or indeed the log files for the\ntaskexecutor process and standalonesession process and scroll up from the\nother error), we'll see why the INSERT failed\u2014for a similar reason as\ntheSELECT:\n\n    \n    \n    java.lang.ClassNotFoundException: org.apache.flink.formats.parquet.ParquetWriterFactory\n\nSo what is happening here? Well we've happily made the Parquet JAR available\nto the SQL Client... yet the actual business of writing and reading data is\nnot done by the client, but the Flink task manager\u2014and we didn't put the JAR\nthere. Or rather we did (since we're running on the same local installation as\nour SQL Client is started from), but we didn't restart the cluster components.\nWe also need to remember to make sure that the HADOOP_CLASSPATH is in the\nenvironment variables when we do:\n\n    \n    \n    $ ./bin/stop-cluster.sh Stopping taskexecutor daemon (pid: 62910) on host asgard08. Stopping standalonesession daemon (pid: 62630) on host asgard08. # Make the Hadoop JARs available $ export HADOOP_CLASSPATH=$(~/hadoop/hadoop-3.3.4/bin/hadoop classpath) # Check that the Parquet JAR is there $ ls -l ./lib/*parquet* -rw-r--r--@ 1 rmoff staff 6740707 5 Apr 17:06 ./lib/flink-sql-parquet-1.18.1.jar # Start the cluster $ ./bin/start-cluster.sh Starting cluster. Starting standalonesession daemon on host asgard08. Starting taskexecutor daemon on host asgard08.\n\nNow when we run the INSERT and SELECT from the SQL Client, things work as they\nshould:\n\n    \n    \n    Flink SQL> INSERT INTO t_foo VALUES ('bar',42); [INFO] Submitting SQL update statement to the cluster... [INFO] SQL update statement has been successfully submitted to the cluster: Job ID: 850b4771b79845790e48d57c7172f203 Flink SQL> SELECT * FROM t_foo; +----+--------------------------------+-------------+ | op | c1 | c2 | +----+--------------------------------+-------------+ | +I | bar | 42 | +----+--------------------------------+-------------+ Received a total of 1 row\n\nThe Flink Dashboard shows that things are healthy too:\n\nAnd we can also double check from within the SQL Client itself that the INSERT\njob ran successfully:\n\n    \n    \n    Flink SQL> SHOW JOBS; +----------------------------------+----------------------------------------------------+----------+-------------------------+ | job id | job name | status | start time | +----------------------------------+----------------------------------------------------+----------+-------------------------+ | 850b4771b79845790e48d57c7172f203 | insert-into_default_catalog.default_database.t_foo | FINISHED | 2024-04-05T17:02:38.143 | +----------------------------------+----------------------------------------------------+----------+-------------------------+ 1 row in set\n\n## A JAR full of Trouble\n\nMost of the problems that I've encountered seem bewildering at first, but once\nsolved can be understood and reverse-engineered to see how I could, in theory,\nhave avoided the problem from better comprehension of the documentation or\nconcepts. This one though defies that. I fixed the error, but I still have no\nidea what causes it. Explanations welcome!\n\nMy original starting point was this Docker Compose file, which provides a\nFlink stack with support for writing to Apache Iceberg on S3-compatible MinIO.\nI used it as the basis for exploring catalogs provided by the Iceberg Flink\nconnector, including using JDBC as a backing store for the catalog. I added a\nPostgres container to the setup, as well as adapting the base Flink container\nto include the necessary Postgres driver.\n\nThe Iceberg JDBC catalog (org.apache.iceberg.jdbc.JdbcCatalog) uses a JDBC\ndatabase for storing the metadata of the catalog, similar to what the Hive\nmetastore would. When we create a Flink SQL catalog and objects (tables, etc)\nwithin it we'll get rows written to Postgres, and when we run DML to interact\nwith the Flink SQL table we'd expect to have queries run against Postgres to\nfetch the metadata for the table.\n\nI created an Iceberg catalog with JDBC metastore and a table within it:\n\n    \n    \n    CREATE CATALOG jdbc_catalog WITH ( 'type' = 'iceberg', 'io-impl' = 'org.apache.iceberg.aws.s3.S3FileIO', 'client.assume-role.region' = 'us-east-1', 'warehouse' = 's3://warehouse', 's3.endpoint' = 'http://storage:9000', 's3.path-style-access' = 'true', 'catalog-impl' = 'org.apache.iceberg.jdbc.JdbcCatalog', 'uri' ='jdbc:postgresql://postgres:5432/world-db?user=world&password=world123'); USE `jdbc_catalog`.`default`; CREATE TABLE t_foo (c1 varchar, c2 int);\n\nThen I added a row of data, and tried to read it back:\n\n    \n    \n    Flink SQL> INSERT INTO t_foo VALUES ('a',42); [INFO] Submitting SQL update statement to the cluster... [INFO] SQL update statement has been successfully submitted to the cluster: Job ID: 21803e3a205877e801536214c9f2d560 Flink SQL> SELECT * FROM t_foo; [ERROR] Could not execute SQL statement. Reason: java.sql.SQLException: No suitable driver found for jdbc:postgresql://postgres:5432/world-db?user=world&password=world123\n\nAs I'd learnt the hard way previously, an INSERT in Flink SQL is run\nasynchronously, so I now knew better than to think that it had succeeded\u2014the\nSELECT was just to see what would happen.\n\nLooking at the task manager log we can see that the INSERT failed with the\nsame error as theSELECT:\n\n    \n    \n    2024-04-09 16:15:13,773 WARN org.apache.flink.runtime.taskmanager.Task [] - IcebergFilesCommitter -> Sink: IcebergSink jdbc_catalog.default.t_foo (1/1) #0 (fa6353abff898aa3e4005455ff93a5cb_90bea66de1c231edf33913ecd54406c1_0_0) switched from INITIALIZING to FAILED with failure cause: org.apache.iceberg.jdbc.UncheckedSQLException: Failed to connect: jdbc:postgresql://postgres:5432/world-db?user=world&password=world123 at org.apache.iceberg.jdbc.JdbcClientPool.newClient(JdbcClientPool.java:57) at org.apache.iceberg.jdbc.JdbcClientPool.newClient(JdbcClientPool.java:30) at org.apache.iceberg.ClientPoolImpl.get(ClientPoolImpl.java:125) at org.apache.iceberg.ClientPoolImpl.run(ClientPoolImpl.java:56) at org.apache.iceberg.ClientPoolImpl.run(ClientPoolImpl.java:51) at org.apache.iceberg.jdbc.JdbcCatalog.initializeCatalogTables(JdbcCatalog.java:146)[...] Caused by: java.sql.SQLException: No suitable driver found for jdbc:postgresql://postgres:5432/world-db?user=world&password=world123 at java.sql/java.sql.DriverManager.getConnection(Unknown Source) at java.sql/java.sql.DriverManager.getConnection(Unknown Source) at org.apache.iceberg.jdbc.JdbcClientPool.newClient(JdbcClientPool.java:55) ... 22 more\n\nThe stack trace confirms that this error is when the connection to the\nPostgres database is attempted as part of the catalog access\n(org.apache.iceberg.jdbc.JdbcCatalog.initializeCatalogTables), and that the\nroot cause is that the Postgres JDBC driver can't be found.\n\nWhat was puzzling was that the JDBC driver was present on the Flink task\nmanager container, as well as the SQL client container. In fact, even more\npuzzling was that the catalog and table creation had worked\u2014so the connection\nto Postgres for those statements must have been ok.\n\nTo save you some of the pain of debugging this, I'll point out that the Docker\nCompose used a different image for SQL Client than the Flink task manager and\njob manager:\n\nI confirmed, re-confirmed, and then confirmed again once more that the\nPostgres JDBC driver was present on the Flink task manager\n\n    \n    \n    root@a3a8182bd227:/opt/flink# ls -l /opt/flink/lib/postgresql-42.7.1.jar -rw-r--r-- 1 root root 1084174 Jan 23 17:36 /opt/flink/lib/postgresql-42.7.1.jar\n\nI also checked that it was present in the Classpath too by looking at the\nstartup messages in the log file:\n\n    \n    \n    [...] org.apache.flink.runtime.entrypoint.ClusterEntrypoint [] - Classpath: /opt/flink/lib/bundle-2.20.18.jar:/opt/flink/lib/flink-cep-1.16.1.jar:/opt/flink/lib/flink-connector-files-1.16.1.jar:/opt/flink/lib/flink-csv-1.16.1.jar:/opt/flink/lib/flink-json-1.16.1.jar:/opt/flink/lib/flink-scala_2.12-1.16.1.jar:/opt/flink/lib/flink-shaded-hadoop-2-uber-2.8.3-10.0.jar:/opt/flink/lib/flink-shaded-zookeeper-3.5.9.jar:/opt/flink/lib/flink-sql-connector-hive-2.3.9_2.12-1.16.1.jar:/opt/flink/lib/flink-table-api-java-uber-1.16.1.jar:/opt/flink/lib/flink-table-planner-loader-1.16.1.jar:/opt/flink/lib/flink-table-runtime-1.16.1.jar:/opt/flink/lib/hadoop-common-2.8.3.jar:/opt/flink/lib/iceberg-flink-runtime-1.16-1.3.1.jar:/opt/flink/lib/log4j-1.2-api-2.17.1.jar:/opt/flink/lib/log4j-api-2.17.1.jar:/opt/flink/lib/log4j-core-2.17.1.jar:/opt/flink/lib/log4j-slf4j-impl-2.17.1.jar:/opt/flink/lib/postgresql-42.7.1.jar:/opt/flink/lib/flink-dist-1.16.1.jar:::: [...]\n\nOne thing that did stand out was the version of Flink - 1.16.1. No reason why\nthis should be an issue (and in the end it wasn't), but I decided to try and\nrebuild the environment using my own Docker Compose from scratch with Flink\n1.18.1. You can find this on GitHub here. As well as bumping the Flink\nversion, I switched to my previous deployment model of a single Flink image,\nand running the SQL Client within one of the containers:\n\nAll the JARs I kept the same as in the initial deployment, except pulling in\nthe correct version for Flink 1.18.1 where needed:\n\n  * bundle-2.20.18.jar\n  * flink-shaded-hadoop-2-uber-2.8.3-10.0.jar\n  * hadoop-common-2.8.3.jar\n  * iceberg-flink-runtime-1.18-1.5.0.jar\n  * postgresql-42.7.1.jar\n\nThis time I got the same error, but at a different time\u2014as soon as I tried to\ncreate the catalog:\n\n    \n    \n    Flink SQL> CREATE CATALOG jdbc_catalog WITH ( 'type' = 'iceberg', 'io-impl' = 'org.apache.iceberg.aws.s3.S3FileIO', 'client.assume-role.region' = 'us-east-1', 'warehouse' = 's3://warehouse', 's3.endpoint' = 'http://minio:9000', 's3.path-style-access' = 'true', 'catalog-impl' = 'org.apache.iceberg.jdbc.JdbcCatalog', 'uri' ='jdbc:postgresql://postgres:5432/world-db?user=world&password=world123'); [ERROR] Could not execute SQL statement. Reason: java.sql.SQLException: No suitable driver found for jdbc:postgresql://postgres:5432/world-db?user=world&password=world123\n\nThis makes sense when we realise that the SQL Client is running using the same\nimage as the Flink cluster\u2014where we saw the error above. So if there's a\nproblem with this environment, then it's going to manifest itself in SQL\nClient too.\n\nThis then prompted me to look at the difference between the original SQL\nClient image, and the Flink taskmanager. I knew that I'd added the Postgres\nJDBC driver to them, but I'd not looked more closely at their base\nconfiguration.\n\nIt turned out that the Flink Hive connector (flink-sql-connector-\nhive-2.3.9_2.12-1.16.1.jar) was present on the taskmanager image, but not the\nSQL Client.\n\nBack on my 1.18.1 environment, I removed this JAR, rebuilt the Docker image\nand retried the experiment. Things looked better straight away:\n\n    \n    \n    Flink SQL> CREATE CATALOG jdbc_catalog WITH ( > 'type' = 'iceberg', > 'io-impl' = 'org.apache.iceberg.aws.s3.S3FileIO', > 'client.assume-role.region' = 'us-east-1', > 'warehouse' = 's3://warehouse', > 's3.endpoint' = 'http://minio:9000', > 's3.path-style-access' = 'true', > 'catalog-impl' = 'org.apache.iceberg.jdbc.JdbcCatalog', > 'uri' ='jdbc:postgresql://postgres:5432/world-db?user=world&password=world123'); [INFO] Execute statement succeed.\n\nI successfully created a table:\n\n    \n    \n    Flink SQL> create database `jdbc_catalog`.`db01`; [INFO] Execute statement succeed. Flink SQL> use `jdbc_catalog`.`db01`; [INFO] Execute statement succeed. Flink SQL> CREATE TABLE t_foo (c1 varchar, c2 int); [INFO] Execute statement succeed.\n\nOver in Postgres I could see the catalog entries:\n\n    \n    \n    world-db=# select * from iceberg_namespace_properties ; catalog_name | namespace | property_key | property_value --------------+-----------+--------------+---------------- jdbc_catalog | db01 | exists | true (1 row) world-db=# select * from iceberg_tables; catalog_name | table_namespace | table_name | metadata_location | previous_metadata_location --------------+-----------------+------------+---------------------------------------------------------------------------------------------+---------------------------- jdbc_catalog | db01 | t_foo | s3://warehouse/db01/t_foo/metadata/00000-5073e16c-36c7-493e-8653-30122a9460e5.metadata.json | (1 row)\n\nNow for the crunch moment... writing data:\n\n    \n    \n    Flink SQL> INSERT INTO t_foo VALUES ('a',42); [INFO] Submitting SQL update statement to the cluster... [INFO] SQL update statement has been successfully submitted to the cluster: Job ID: 33d09054f65555ec08a96e1f9817f77d Flink SQL> SHOW JOBS ; +----------------------------------+-------------------------------------+----------+-------------------------+ | job id | job name | status | start time | +----------------------------------+-------------------------------------+----------+-------------------------+ | 33d09054f65555ec08a96e1f9817f77d | insert-into_jdbc_catalog.db01.t_foo | FINISHED | 2024-04-10T10:20:53.283 | +----------------------------------+-------------------------------------+----------+-------------------------+ 1 row in set\n\nIt's looking promising (the job showsFINISHED)...\n\n    \n    \n    Flink SQL> SELECT * FROM t_foo; +----+--------------------------------+-------------+ | op | c1 | c2 | +----+--------------------------------+-------------+ | +I | a | 42 | +----+--------------------------------+-------------+ Received a total of 1 row\n\nSuccess! But what is going on?\n\nWith the Flink Hive Connector JAR (flink-sql-connector-\nhive-2.3.9_2.12-1.18.1.jar ) present, Flink can't find the Postgres JDBC\nDriver:\n\n    \n    \n    java.sql.SQLException: No suitable driver found for jdbc:postgresql://postgres:5432/world-db?user=world&password=world123\n\nIf I remove the Hive connector JAR, then Flink finds the Postgres JDBC driver\nand things are just fine.\n\nWhen looking at the Hive connector JAR on Maven and peering at the digit-salad\nthat is the JAR file naming style I did notice that 2.3.9 is not the latest\nHive version:\n\nSo, in the interest of hacking around to learn stuff, I gave the most recent\nversion (3.1.3) of the JAR (flink-sql-connector-hive-3.1.3_2.12-1.18.1.jar) a\ntry. Same 1.18.1 environment as above when things didn't work, except with\nflink-sql-connector-hive-3.1.3_2.12-1.18.1.jar in the place of flink-sql-\nconnector-hive-2.3.9_2.12-1.18.1.jar and...it works.\n\nSo ultimately it seems that something to do with flink-sql-connector-\nhive-2.3.9_2.12-1.18.1.jar stops Flink from being able to find the Postgres\nJDBC Driver. I have no idea why. But at least I know now how to fix it \ud83d\ude42\n\n## Writing to S3 from Flink\n\nUnlike the previous problem, this one makes sense once you get it working and\nlook back over what was needed. Nonetheless, it took me a lot of iterating\n(a.k.a. changing random things) to get it to work.\n\nFollow along as I relive the journey...\n\nI've got Flink 1.18 and a standalone Hive Metastore container, as well as a\nMinIO container. MinIO is a S3-compatible object store that you can run\nlocally, making it perfect for this kind of playground.\n\nThe stack is running under Docker Compose (you'll find this on GitHub if you\nwant to try it).\n\nThe first step is creating the catalog (I'm using Hive) and table:\n\n    \n    \n    Flink SQL> CREATE CATALOG c_hive WITH ( 'type' = 'hive', 'hive-conf-dir' = './conf/'); [INFO] Execute statement succeed. Flink SQL> USE `c_hive`.`default`; [INFO] Execute statement succeed.\n\nNext up we define a table using the filesystem connector and an S3 path.\n\n    \n    \n    Flink SQL> CREATE TABLE t_foo_fs (c1 varchar, c2 int) WITH ( 'connector' = 'filesystem', 'path' = 's3://warehouse/t_foo_fs/', 'format' = 'csv' ); [INFO] Execute statement succeed.\n\nAnd then we try to add some data, which doesn't work (the job ends with FAILED\nstatus):\n\n    \n    \n    Flink SQL> INSERT INTO t_foo_fs VALUES ('a',42); [INFO] Submitting SQL update statement to the cluster... [INFO] SQL update statement has been successfully submitted to the cluster: Job ID: 76dd5a9044c437a2e576f29df86a3df4 Flink SQL> SHOW JOBS; +----------------------------------+-------------------------------------+--------+-------------------------+ | job id | job name | status | start time | +----------------------------------+-------------------------------------+--------+-------------------------+ | 76dd5a9044c437a2e576f29df86a3df4 | insert-into_c_hive.default.t_foo_fs | FAILED | 2024-04-10T14:42:12.597 | +----------------------------------+-------------------------------------+--------+-------------------------+\n\nOver to the cluster log files to see what the problem is. The jobmanager log\nshows us this:\n\n    \n    \n    2024-04-10 14:35:58,809 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Source: Values[1] -> StreamingFileWriter -> Sink: end (1/1) (2c1d784ada116b60a9bcd63a9439410a_cbc357ccb763df2852fee8c4fc7d55f2_0_0) switched from INITIALIZING to FAILED on 192.168.97.6:41975-23ccd7 @ 03-hive-parquet-taskmanager-1.zaphod (dataPort=40685). org.apache.flink.core.fs.UnsupportedFileSystemSchemeException: Could not find a file system implementation for scheme 's3'. The scheme is directly supported by Flink through the following plugin(s): flink-s3-fs-hadoop, flink-s3-fs-presto. Please ensure that each plugin resides within its own subfolder within the plugins directory. See https://nightlies.apache.org/flink/flink-docs-stable/docs/deployment/filesystems/plugins/ for more information. If you want to use a Hadoop file system for that scheme, please add the scheme to the configuration fs.allowed-fallback-filesystems. For a full list of supported file systems, please see https://nightlies.apache.org/flink/flink-docs-stable/ops/filesystems/. [...] 2024-04-10 14:35:58,836 INFO org.apache.flink.runtime.resourcemanager.slotmanager.FineGrainedSlotManager [] - Clearing resource requirements of job a67f542ab426485698c9db3face73c36 2024-04-10 14:35:58,845 INFO org.apache.flink.runtime.executiongraph.ExecutionGraph [] - Job insert-into_c_hive.default.t_foo_fs (a67f542ab426485698c9db3face73c36) switched from state RUNNING to FAILING.\n\nThis is one of my favourite kinds of error message: descriptive, and overly\nhelpful. (My colleague Gunnar Morling also has some more words of wisdom to\nshare on this subject).\n\n  * What's the problem? Could not find a file system implementation for scheme 's3'\n  * How do I fix it? The scheme is directly supported by Flink through the following plugin(s): flink-s3-fs-hadoop, flink-s3-fs-presto\n  * How do I find out more? See https://nightlies.apache.org/flink/flink-docs-stable/docs/deployment/filesystems/plugins/ for more information\n\nThe S3 plugin actually ships as part of the Flink distribution; we just need\nto make it available at runtime by putting it in the plugins folder:\n\n    \n    \n    mkdir ./plugins/s3-fs-hadoop && \\ cp ./opt/flink-s3-fs-hadoop-1.18.1.jar ./plugins/s3-fs-hadoop/\n\nAfter bouncing the Flink cluster I got a different error from jobmanager when\ntrying the INSERT:\n\n    \n    \n    Job insert-into_c_hive.default.t_foo_fs (f1532244c3a97d3308d42c41ab79e092) switched from state FAILING to FAILED. [...] java.nio.file.AccessDeniedException: t_foo_fs/part-c4075636-891e-4233-8224-e09064c7c7eb-0-0: org.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by DynamicTemporaryAWSCredentialsProvider TemporaryAWSCredentialsProvider SimpleAWSCredentialsProvider EnvironmentVariable [...] com.amazonaws.SdkClientException: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY))\n\nThis makes sense\u2014we're trying to use S3 (well, MinIO) but we've not provided\nany S3 credentials (NoAuthWithAWSException: No AWS Credentials provided). The\ndocs for S3 offer one option\u2014adding them to flink.conf. We can pass this as a\nruntime option by setting it in the FLINK_PROPERTIES environment variable as\npart of the Docker Compose:\n\n    \n    \n    jobmanager: [...] environment: - | FLINK_PROPERTIES= s3.access.key: admin s3.secret.key: password [...]\n\nNow the error evolves...\n\n    \n    \n    Caused by: java.nio.file.AccessDeniedException: t_foo_fs/part-ddd5011f-9863-432c-9988-50dc1d2628b3-0-0: initiate MultiPartUpload on t_foo_fs/part-ddd5011f-9863-432c-9988-50dc1d2628b3-0-0: com.amazonaws.services.s3.model.AmazonS3Exception: The AWS Access Key Id you provided does not exist in our records. (Service: Amazon S3; Status Code: 403; Error Code: InvalidAccessKeyId; Request ID: P6ZZ5SJAVR9C38AA; S3 Extended Request ID: 7Nxqk2li47vlMAzllA57vfRmiePcFYFrv9/vHn6Aknv5+V5gwYyLzk9KIwGC9fE/biNzCWTzozI=; Proxy: null), S3 Extended Request ID: 7Nxqk2li47vlMAzllA57vfRmiePcFYFrv9/vHn6Aknv5+V5gwYyLzk9KIwGC9fE/biNzCWTzozI=:InvalidAccessKeyId\n\nThis is because we're trying to use the credentials that we've configured for\nMinIO (the super-secret admin/password \ud83d\ude09) against AWS S3 itself. Because we're\nusing MinIO we need to tell Flink where to direct its S3 call, and we do this\nwith s3.endpoint:\n\n    \n    \n    jobmanager: [...] environment: - | FLINK_PROPERTIES= s3.access.key: admin s3.secret.key: password s3.endpoint: http://minio:9000 [...]\n\nAt this point things slow down, because the INSERT job runs... and runs...\n\nAfter two minutes there's an INFO in the log:\n\n    \n    \n    org.apache.hadoop.fs.s3a.WriteOperationHelper [] - initiate MultiPartUpload on t_foo_fs/part-29594611-b313-4ff6-a0a0-86087ec6f262-0-0: Retried 0: org.apache.hadoop.fs.s3a.AWSClientIOException: initiate MultiPartUpload on t_foo_fs/part-29594611-b313-4ff6-a0a0-86087ec6f262-0-0: com.amazonaws.SdkClientException: Unable to execute HTTP request: warehouse.minio: Unable to execute HTTP request: warehouse.minio\n\nand five minutes after submitting the INSERT there's thisERROR:\n\n    \n    \n    org.apache.hadoop.fs.s3a.WriteOperationHelper [] - initiate MultiPartUpload on t_foo_fs/part-29594611-b313-4ff6-a0a0-86087ec6f262-0-0: Retried 1: org.apache.hadoop.fs.s3a.AWSClientIOException: initiate MultiPartUpload on t_foo_fs/part-29594611-b313-4ff6-a0a0-86087ec6f262-0-0: com.amazonaws.SdkClientException: Unable to execute HTTP request: warehouse.minio: Name or service not known: Unable to execute HTTP request: warehouse.minio: Name or service not known\n\nThe problem here looks like some kind of hostname issue. Previously we saw how\nreferencing localhost from a Docker container can be a misconfiguration, but\nthis is something different. warehouse comes from the CREATE TABLE\nconfiguration'path' = 's3://warehouse/t_foo_fs/', whilst minio is the\ns3.endpoint we just set.\n\nSo the S3 endpoint is being picked up, but somehow mangled together with the\npath of the table. Something I've learnt from working with MinIO before is\nthat using path style access can be important. I added this to\ntheFLINK_PROPERTIES:\n\n    \n    \n    [...] FLINK_PROPERTIES= s3.access.key: admin s3.secret.key: password s3.endpoint: http://minio:9000 s3.path.style.access: true\n\nand then got yet another different error from the INSERT job:\n\n    \n    \n    Caused by: java.util.concurrent.ExecutionException: java.io.IOException: Stream closed. at java.base/java.util.concurrent.FutureTask.report(Unknown Source) at java.base/java.util.concurrent.FutureTask.get(Unknown Source) at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.completeProcessing(SourceStreamTask.java:368) at org.apache.flink.streaming.runtime.tasks.SourceStreamTask$LegacySourceFunctionThread.run(SourceStreamTask.java:340) Caused by: java.io.IOException: Stream closed. at org.apache.flink.core.fs.RefCountedFileWithStream.requireOpened(RefCountedFileWithStream.java:72) at org.apache.flink.core.fs.RefCountedFileWithStream.write(RefCountedFileWithStream.java:52) at org.apache.flink.core.fs.RefCountedBufferingFileStream.flush(RefCountedBufferingFileStream.java:104) at org.apache.flink.fs.s3.common.writer.S3RecoverableFsDataOutputStream.closeAndUploadPart(S3RecoverableFsDataOutputStream.java:209) at org.apache.flink.fs.s3.common.writer.S3RecoverableFsDataOutputStream.closeForCommit(S3RecoverableFsDataOutputStream.java:177) at org.apache.flink.streaming.api.functions.sink.filesystem.OutputStreamBasedPartFileWriter.closeForCommit(OutputStreamBasedPartFileWriter.java:75)\n\nThis looks like FLINK-33536, and so using the convenience afforded by just\nwriting a blog and not needing to use CSV as the format (which seems to be at\nthe root of the issue) I sidestepped the issue and switched the table to\nParquet. I also added the necessary JAR for Parquet in Flink and restarted the\nFlink cluster before changing the table:\n\n    \n    \n    CREATE TABLE t_foo_fs (c1 varchar, c2 int) WITH ( 'connector' = 'filesystem', 'path' = 's3://warehouse/t_foo_fs/', 'format' = 'parquet' );\n\nAfter which ...wait... what is this? Behold!\n\n    \n    \n    Flink SQL> SHOW JOBS; +----------------------------------+-------------------------------------+----------+-------------------------+ | job id | job name | status | start time | +----------------------------------+-------------------------------------+----------+-------------------------+ | a8ca5cd4c59060ac4d4f0996e426af17 | insert-into_c_hive.default.t_foo_fs | FINISHED | 2024-04-11T09:51:16.904 | +----------------------------------+-------------------------------------+----------+-------------------------+ 1 row in set\n\nSuccess! \ud83e\udd73\n\nQuerying the table proves it...\n\n    \n    \n    Flink SQL> SELECT * FROM t_foo_fs; +----+--------------------------------+-------------+ | op | c1 | c2 | +----+--------------------------------+-------------+ | +I | a | 42 | +----+--------------------------------+-------------+ Received a total of 1 row\n\nTo recap then, what was needed to write to S3 (MinIO) from Flink SQL was this:\n\n1\\. Add the S3 plugin to the Flink ./plugins folder:\n\n    \n    \n    mkdir ./plugins/s3-fs-hadoop && \\ cp ./opt/flink-s3-fs-hadoop-1.18.1.jar ./plugins/s3-fs-hadoop/\n\n2\\. Add S3 credentials to Flink configuration. This can be done as environment\nvariables, or added to flink-conf.yaml either directly or by adding to the\nFLINK_PROPERTIES environment variable, which is what I did\n\n3\\. For MinIO, set the S3 endpoint and enable path-style access. As with the\ncredentials, I set this as part of FLINK_PROPERTIES, which ended up as this:\n\n    \n    \n    services: jobmanager: [...] environment: - | FLINK_PROPERTIES= s3.access.key: admin s3.secret.key: password s3.endpoint: http://minio:9000 s3.path.style.access: true [...]\n\nOh - and the CSV bug that I hit is FLINK-33536 and I worked around it by just\nnot using CSV :)\n\n## What's Running Where? (Not So Much Fun with Hive MetaStore)\n\nIf this blog so far has been some gnarly but reasonable challenges, I'd like\nto round off with the big end-of-level boss. This brings together JARs, Flink\nconfiguration\u2014and the importance of understanding what is running where.\n\nTo set the scene: I was doing the same as the above section\u2014I was setting up\nFlink 1.18 writing files to MinIO (S3 compatible storage), using Hive\nMetastore for catalog persistence. But instead of Parquet or CSV format, I was\nwriting Iceberg files. Now, that may seem insignificant, but the impact was\ncrucial.\n\nMy environment was setup with Docker Compose as before (and available on\nGitHub):\n\nTo my base Flink image I'd added Parquet, S3, Hadoop, Hive, and Iceberg\ndependencies. Building on my lessons learnt from above, I've also added S3\nconfiguration to the FLINK_PROPERTIES environment variable for the Flink\ncluster containers:\n\n    \n    \n    services: jobmanager: [...] environment: - | FLINK_PROPERTIES= s3.access.key: admin s3.secret.key: password s3.endpoint: http://minio:9000 s3.path.style.access: true [...]\n\nAfter bringing the stack up, we'll start by creating the Iceberg catalog. We\nuse s3a rather than s3 per the Iceberg docs (since this is all done through\nthe Iceberg Flink support)\n\n    \n    \n    CREATE CATALOG c_iceberg_hive WITH ( 'type' = 'iceberg', 'warehouse' = 's3a://warehouse', 'catalog-type'='hive', 'uri'='thrift://localhost:9083');\n\nNow let's see if there's a default database provided by the catalog:\n\n    \n    \n    Flink SQL> USE CATALOG c_iceberg_hive; [INFO] Execute statement succeed. Flink SQL> SHOW DATABASES; [ERROR] Could not execute SQL statement. Reason: java.net.ConnectException: Connection refused (Connection refused)\n\nThe Connection refused here is coming from the fact that the SQL Client is\ntrying to reach the Hive MetaStore (HMS) usingthrift://localhost:9083 \u2014but\nlocalhost won't work as that's local to the SQL Client container.\n\nInstead we need to use the hostname of the HMS in the uri configuration:\n\n    \n    \n    Flink SQL> DROP CATALOG c_iceberg_hive; [ERROR] Could not execute SQL statement. Reason: org.apache.flink.table.catalog.exceptions.CatalogException: Cannot drop a catalog which is currently in use. Flink SQL> show catalogs; +-----------------+ | catalog name | +-----------------+ | c_iceberg_hive | | default_catalog | +-----------------+ 2 rows in set Flink SQL> use catalog default_catalog; [INFO] Execute statement succeed. Flink SQL> DROP CATALOG c_iceberg_hive; [INFO] Execute statement succeed. Flink SQL> CREATE CATALOG c_iceberg_hive WITH ( 'type' = 'iceberg', 'warehouse' = 's3a://warehouse', 'catalog-type'='hive', 'uri'='thrift://hms:9083'); [INFO] Execute statement succeed.\n\nNow we can see that there is one database, calleddefault:\n\n    \n    \n    Flink SQL> USE CATALOG c_iceberg_hive; [INFO] Execute statement succeed. Flink SQL> SHOW DATABASES; +---------------+ | database name | +---------------+ | default | +---------------+ 1 row in set\n\nLet's create a new one:\n\n    \n    \n    Flink SQL> CREATE DATABASE `c_iceberg_hive`.`db01`; [ERROR] Could not execute SQL statement. Reason: MetaException(message:java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found)\n\nFear not! We've seen this before, right? Based on my exploration of Flink SQL\nand JARs previously I thought I'd be well-equipped to deal with this\none.ClassNotFound? Piece of cake. Right?\n\nAll we need to do is make sure that the Hadoop AWS JAR\u2014that provides the\nS3AFileSystem class\u2014is present. But if we head over to our Flink containers,\nit looks like it already is:\n\n    \n    \n    flink@jobmanager:~$ tree /opt/flink/lib /opt/flink/lib \u251c\u2500\u2500 aws \u2502 \u251c\u2500\u2500 aws-java-sdk-bundle-1.12.648.jar \u2502 \u2514\u2500\u2500 hadoop-aws-3.3.4.jar [...] \u251c\u2500\u2500 hive \u2502 \u2514\u2500\u2500 flink-sql-connector-hive-3.1.3_2.12-1.18.1.jar \u251c\u2500\u2500 iceberg \u2502 \u2514\u2500\u2500 iceberg-flink-runtime-1.18-1.5.0.jar [...]\n\nUsing jinfo we can see that the Classpath for the SQL Client shows that the\nhadoop-aws JAR is present:\n\nWe can even double-check that this is indeed the JAR that we want by searching\nits contents for the class:\n\n    \n    \n    $ jar -tf /opt/flink/lib/aws/hadoop-aws-3.3.4.jar|grep S3AFileSystem.class org/apache/hadoop/fs/s3a/S3AFileSystem.class\n\nSo what next? Honestly, a looooot of hacking about. Various suggestions from\ncolleagues, Slack groups, Stack Overflow, chatGPT\u2014and of course Google, which\nincluded:\n\n  * Check Java version\n  * Check Hadoop dependency version\n  * RTFM \ud83d\udcd6\n  * Try different JAR version\n  * Install full Hadoop and set HADOOP_CLASSPATH\n  * Try a different version of Flink\n  * RTFM some more \ud83d\udcda\n  * Turn it off and on again \ud83e\udd1e\n  * Add iceberg-aws-bundle-1.5.0.jar\n  * Stand on one leg whilst singing La Marseillaise wearing a silver cross and holding a clove of garlic \ud83e\udd2a\n\nAll of these ended up with the same (or different) errors. Whatever I did,\nFlink just didn't seem to be able to find the S3 class.\n\nAnd then...the clouds parted. The sun shone, the angels sang, and one of them\nnamed Aleksandr Pilipenko stepped forth on the Apache Flink Slack group and\ndid thus proclaim:\n\nCould this actually originate from hive side? ThriftHiveMetastore seems to me\nlike something outside of Flink Caused by:\nMetaException(message:java.lang.RuntimeException:\njava.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem\nnot found) at\norg.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_database_result$create_database_resultStandardScheme.read(ThriftHiveMetastore.java:39343)\n\nReader, this fixed it.\n\nOr rather, it put me on the right lines. Because what was happening was that\nthe Hive MetaStore was throwing the error. SQL Client was simply surfacing the\nerror.\n\nWhen you create a database in Iceberg, not only is there metadata written to\nthe metastore (Hive, in this case), but also the warehouse on S3.\n\nWhen we created the catalog we told Iceberg where to find the Hive\nMetastore:'uri'='thrift://hms:9083'. The Hive Metastore then writes additional\nIceberg metadata to'warehouse' = 's3a://warehouse'.\n\nYou can actually see this if you look at the Hive Metastore log. First there's\nthe request from Flink's Iceberg implementation to create the database (note\nthe storage specified ats3a://warehouse/db01.db):\n\n    \n    \n    source:172.24.0.4 create_database: Database(name:db01, description:null, locationUri:s3a://warehouse/db01.db, parameters:{}, ownerName:flink, ownerType:USER, catalogName:hive)\n\nfollowed shortly after by\n\n    \n    \n    ERROR [pool-6-thread-1] metastore.RetryingHMSHandler: MetaException(message:java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.fs.s3a.S3AFileSystem not found) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newMetaException(HiveMetaStore.java:6937) at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database(HiveMetaStore.java:1338) [...]\n\nThe fix? Add the Hadoop AWS JAR (which includes S3 support) to Hive Metastore\n(this is not the same as the Flink deployment, which also needs these JARs):\n\n    \n    \n    cd /opt/hive-metastore/lib && \\ curl https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar -O\n\nThis alone doesn't quite get us over the hill:\n\n    \n    \n    Flink SQL> CREATE DATABASE `c_iceberg_hive`.`db01`; [ERROR] Could not execute SQL statement. Reason: org.apache.thrift.transport.TTransportException\n\nAt least this error isn't such a red-herring; we can see it's a thrift error,\nand so nursing the fresh wounds of our S3 JAR escapades above we go straight\nto check the Hive Metastore log:\n\n    \n    \n    WARN [pool-6-thread-1] metastore.ObjectStore: Failed to get database hive.db01, returning NoSuchObjectException ERROR [pool-6-thread-1] metastore.RetryingHMSHandler: java.lang.NoClassDefFoundError: com/amazonaws/auth/AWSCredentialsProvider\n\nAWSCredentialsProvider is included with aws-java-sdk-bundle and after adding\nthat we're very nearly there:\n\n    \n    \n    Flink SQL> CREATE DATABASE `c_iceberg_hive`.`db01`; [ERROR] Could not execute SQL statement. Reason: MetaException(message:Got exception: java.nio.file.AccessDeniedException s3a://warehouse/db01.db: org.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by TemporaryAWSCredentialsProvider SimpleAWSCredentialsProvider EnvironmentVariableCredentialsProvider IAMInstanceCredentialsProvider : com.amazonaws.SdkClientException: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY)))\n\nBuilding on what we learnt about S3 access from Flink we know that now we just\nneed to add the S3 credentials and additional configuration needed for MinIO\nto Hive Metastore. We do this by adding it to the ./conf/hive-site.xml file on\nthe Hive Metastore:\n\n    \n    \n    <property> <name>fs.s3a.access.key</name> <value>admin</value> </property> <property> <name>fs.s3a.secret.key</name> <value>password</value> </property> <property> <name>fs.s3a.endpoint</name> <value>http://minio:9000</value> </property> <property> <name>fs.s3a.path.style.access</name> <value>true</value> </property>\n\nAnd with that...success.\n\n    \n    \n    Flink SQL> CREATE CATALOG c_iceberg_hive WITH ( > 'type' = 'iceberg', > 'warehouse' = 's3a://warehouse', > 'catalog-type'='hive', > 'uri'='thrift://hms:9083'); [INFO] Execute statement succeed. Flink SQL> CREATE DATABASE `c_iceberg_hive`.`db01`; [INFO] Execute statement succeed.\n\nIn MinIO we've got a object created for the database:\n\n    \n    \n    $ mc ls -r minio/warehouse/ [2024-04-11 16:59:06 UTC] 0B STANDARD db01.db/\n\nIf we create a table within this Iceberg catalog and add some data:\n\n    \n    \n    Flink SQL> CREATE TABLE t_foo_fs (c1 varchar, c2 int); [ERROR] Could not execute SQL statement. Reason: com.amazonaws.SdkClientException: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY))\n\nWhat?! Surely not again. Well not quite. This time the error is coming from\nthe SQL Client itself, as the log (under./log ) shows:\n\n    \n    \n    Caused by: org.apache.flink.table.api.TableException: Could not execute CreateTable in path `c_iceberg_hive`.`db01`.`t_foo_fs` at org.apache.flink.table.catalog.CatalogManager.execute(CatalogManager.java:1296) at org.apache.flink.table.catalog.CatalogManager.createTable(CatalogManager.java:946) [...] Caused by: org.apache.iceberg.exceptions.RuntimeIOException: Failed to create file: s3a://warehouse/db01.db/t_foo_fs/metadata/00000-cec79e1d-1039-45a6-be3a-00a29528ff72.metadata.json\n\nWe're pretty much on the home straight now. In this case, the SQL Client\nitself is writing some of the metadata for the table to S3 (MinIO). Other\nmetadata still goes to the Hive Metastore. I have dug into this in more detail\nin another article.\n\nWhilst we've set the S3 configuration for the jobmanager process as part of\nthe FLINK_PROPERTIES (which gets written to flink-conf.yaml at runtime), this\nconfiguration doesn't seem to be used by the SQL Client.\n\nTo simplify things, I'm going to move the S3 config away from FLINK_PROPERTIES\nand specify it in just one place, the ./conf/hive-site.xml on the Flink\ncontainers, where it should get used by both the jobmanager, taskmanager\u2014and\nSQL Client. It's the same as I added to the Hive Metastore above:\n\n    \n    \n    <?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?> <?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?> <configuration> <property> <name>fs.s3a.access.key</name> <value>admin</value> </property> <property> <name>fs.s3a.secret.key</name> <value>password</value> </property> <property> <name>fs.s3a.endpoint</name> <value>http://minio:9000</value> </property> <property> <name>fs.s3a.path.style.access</name> <value>true</value> </property> </configuration>\n\nFor this to be picked up I also needed to add hive-conf-dir as part of the\nIceberg catalog configuration:\n\n    \n    \n    CREATE CATALOG c_iceberg_hive WITH ( 'type' = 'iceberg', 'warehouse' = 's3a://warehouse', 'catalog-type'='hive', 'uri'='thrift://hms:9083', 'hive-conf-dir' = './conf');\n\nAnd with that\u2014we're done:\n\n    \n    \n    Flink SQL> CREATE TABLE t_foo_fs (c1 varchar, c2 int); [INFO] Execute statement succeed. Flink SQL> INSERT INTO t_foo_fs VALUES ('a',42); [INFO] Submitting SQL update statement to the cluster... [INFO] SQL update statement has been successfully submitted to the cluster: Job ID: da7c9c4fc427a0796729a7cf10d05b2b Flink SQL> SELECT * FROM t_foo_fs; +----+--------------------------------+-------------+ | op | c1 | c2 | +----+--------------------------------+-------------+ | +I | a | 42 | +----+--------------------------------+-------------+ Received a total of 1 row\n\nWe'll wrap up with one more little wrinkle to iron out that's worth\ndocumenting as part of this. As I was testing this, I experimented with a\ndifferent way of defining an Iceberg table. Instead of creating an Iceberg\ncatalog, and then within that a table, you can define a table and specify it\nto use the Iceberg connector. It looks like this:\n\n    \n    \n    CREATE TABLE iceberg_test WITH ( 'connector' = 'iceberg', 'catalog-type'='hive', 'catalog-name'='dev', 'warehouse' = 's3a://warehouse', 'uri'='thrift://hms:9083', 'hive-conf-dir' = './conf') AS SELECT * FROM (VALUES ('Never Gonna','Give You','Up'));\n\nThis works great:\n\n    \n    \n    Flink SQL> SELECT * FROM iceberg_test; +----+--------------------------------+--------------------------------+--------------------------------+ | op | EXPR$0 | EXPR$1 | EXPR$2 | +----+--------------------------------+--------------------------------+--------------------------------+ | +I | Never Gonna | Give You | Up | +----+--------------------------------+--------------------------------+--------------------------------+ Received a total of 1 row\n\nBut\u2014to get to this point I had to get past this:\n\n    \n    \n    Flink SQL> CREATE TABLE iceberg_test WITH ( 'connector' = 'iceberg', 'catalog-type'='hive', 'catalog-name'='dev', 'warehouse' = 's3a://warehouse', 'hive-conf-dir' = './conf') AS SELECT * FROM (VALUES ('Never Gonna','Give You','Up')); [ERROR] Could not execute SQL statement. Reason: java.lang.ClassNotFoundException: org.datanucleus.NucleusContext\n\nA ClassNotFoundException which is something we've dealt with before. But why\nwouldn't this work, if in the same environment things work fine if I create\nthe catalog first and then a table within it?\n\nThe answer comes down to how Flink is picking up the Hive configuration.\nWhilst we've defined in the table where to find the hive-site.xml\nconfiguration ('hive-conf-dir' = './conf' ), in that file itself it only has\nthe S3 configuration. What it doesn't have is a value forhive.metastore.uris.\nThe hive docs tell us that if hive.metastore.uris is not set then Flink\nassumes the metastore is local. For us that means local to the Flink\ncontainer, which it's not\u2014and is where the JAR problem comes in.\n\nThis didn't happen when we created the table as part of the catalog because\nthe CREATE CATALOG included 'uri'='thrift://hms:9083' and thus could find the\nHive Metastore. So the lesson here is that the uri must be specified\nsomewhere\u2014either in the DDL (the successful CREATE TABLE iceberg_test above\ndoes this), or by adding it to thehive-site.xml:\n\n    \n    \n    <?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?> <?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?> <configuration> <property> <name>hive.metastore.uris</name> <value>thrift://hms:9083</name> </property> <property> <name>fs.s3a.access.key</name> <value>admin</value> </property> [...]\n\nWith this added, the CREATE TABLE without a uri configuration also works:\n\n    \n    \n    Flink SQL> CREATE TABLE iceberg_test WITH ( 'connector' = 'iceberg', 'catalog-type'='hive', 'catalog-name'='dev', 'warehouse' = 's3a://warehouse', 'hive-conf-dir' = './conf') AS SELECT * FROM (VALUES ('Never Gonna','Give You','Up')); [INFO] Submitting SQL update statement to the cluster... [INFO] SQL update statement has been successfully submitted to the cluster: Job ID: b5588a1e34375f9c4d4d13a6e6f34d99 Flink SQL> SELECT * FROM iceberg_test; +----+--------------------------------+--------------------------------+--------------------------------+ | op | EXPR$0 | EXPR$1 | EXPR$2 | +----+--------------------------------+--------------------------------+--------------------------------+ | +I | Never Gonna | Give You | Up | +----+--------------------------------+--------------------------------+--------------------------------+ Received a total of 1 row\n\nComing back to theCREATE CATALOG, it can also omit the uri if we've specified\nit in thehive-site.xml, which slims it down to this:\n\n    \n    \n    CREATE CATALOG c_iceberg_hive WITH ( 'type' = 'iceberg', 'warehouse' = 's3a://warehouse', 'catalog-type'='hive', 'hive-conf-dir' = './conf');\n\n## \ud83d\ude32 Gosh. That's all rather confusing and down-in-the-weeds, isn't it?\n\nWell, yes. That is the joy of running a complex distributed system\u2014and one\nwith a venerable history dating back to the Apache Hadoop ecosystem\u2014for\nyourself.\n\nIf you want to spend your time solving your business problems instead of\ndebugging infrastructure, check our Decodable. Our fully-managed platform\ngives you access to Flink SQL and connectors (including Iceberg) and does all\nthe gnarly stuff for you. Not a JAR or Catalog to worry about in sight! (Of\ncourse, you can bring your own Flink JAR jobs if you want to run a custom\npipeline, but that's a point for a different blog post on a different day).\n\nDecodable has a free trial that doesn't require a credit card to use\u2014so give\nit a try today.\n\n### \ud83d\udceb Email signup \ud83d\udc47\n\n#### Did you enjoy this issue of Checkpoint Chronicle? Would you like the next\nedition delivered directly to your email to read from the comfort of your own\nhome?\n\nSimply enter your email address here and we'll send you the next issue as soon\nas it's published\u2014and nothing else, we promise!\n\n\ud83d\udc4d Got it!\n\nOops! Something went wrong while submitting the form.\n\nRobin Moffatt\n\nRobin is a Principal DevEx Engineer at Decodable. He has been speaking at\nconferences since 2009 including QCon, Devoxx, Strata, Kafka Summit, and\n\u00d8redev. You can find many of his talks online and his articles on the\nDecodable blog as well as his own blog.\n\nOutside of work, Robin enjoys running, drinking good beer, and eating fried\nbreakfasts\u2014although generally not at the same time.\n\n## Let's Get Decoding\n\nStart freeTalk To An Expert\n\n### Related Posts\n\nFebruary 16, 2024\n\n8\n\nmin read\n\nBlog\n\n### Catalogs in Flink SQL\u2014A Primer\n\nBy\n\nRobin Moffatt\n\nFebruary 19, 2024\n\n10\n\nmin read\n\nBlog\n\n### Catalogs in Flink SQL\u2014Hands On\n\nBy\n\nRobin Moffatt\n\nFebruary 27, 2024\n\n4\n\nmin read\n\nBlog\n\n### Flink SQL and the Joy of JARs\n\nBy\n\nRobin Moffatt\n\nJoin the community on Slack\n\nJoin the Channel\n\nBuild real-time data apps & services. Fast.\n\nProductSolutionsDocsPricing\n\nCompanyCareersResourcesSecurity\n\nContactCookie PreferencesPrivacy PolicyTerms of Services\n\nApache Flink, Flink\u00ae, Apache Kafka, Kafka\u00ae, Apache\u00ae, the Flink squirrel logo,\nthe Kafka logo, and the Apache feather logo are either registered trademarks\nor trademarks of The Apache Software Foundation.\n\nAll rights reserved \u00a9 2021-2024 Decodable\n\n", "frontpage": false}
