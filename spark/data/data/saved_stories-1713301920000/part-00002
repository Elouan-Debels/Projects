{"aid": "40052985", "title": "Compression Represents Intelligence Linearly", "url": "https://huggingface.co/papers/2404.09937", "domain": "huggingface.co", "votes": 2, "user": "ororm", "posted_at": "2024-04-16 15:10:20", "comments": 0, "source_title": "Paper page - Compression Represents Intelligence Linearly", "source_text": "Paper page - Compression Represents Intelligence Linearly\n\nHugging Face\n\nPapers\n\narxiv:2404.09937\n\n# Compression Represents Intelligence Linearly\n\nPublished on Apr 15\n\n\u00b7 Featured in Daily Papers on Apr 16\n\nUpvote\n\n16\n\nAuthors:\n\nYuzhen Huang ,\n\n,\n\n,\n\n## Abstract\n\nThere is a belief that learning to compress well will lead to intelligence.\nRecently, language modeling has been shown to be equivalent to compression,\nwhich offers a compelling rationale for the success of large language models\n(LLMs): the development of more advanced language models is essentially\nenhancing compression which facilitates intelligence. Despite such appealing\ndiscussions, little empirical evidence is present for the interplay between\ncompression and intelligence. In this work, we examine their relationship in\nthe context of LLMs, treating LLMs as data compressors. Given the abstract\nconcept of \"intelligence\", we adopt the average downstream benchmark scores as\na surrogate, specifically targeting intelligence related to knowledge and\ncommonsense, coding, and mathematical reasoning. Across 12 benchmarks, our\nstudy brings together 30 public LLMs that originate from diverse\norganizations. Remarkably, we find that LLMs' intelligence -- reflected by\naverage benchmark scores -- almost linearly correlates with their ability to\ncompress external text corpora. These results provide concrete evidence\nsupporting the belief that superior compression indicates greater\nintelligence. Furthermore, our findings suggest that compression efficiency,\nas an unsupervised metric derived from raw text corpora, serves as a reliable\nevaluation measure that is linearly associated with the model capabilities. We\nopen-source our compression datasets as well as our data collection pipelines\nto facilitate future researchers to assess compression properly.\n\nView arXiv page View PDF Add to collection\n\n### Community\n\n\u00b7 Sign up or log in to comment\n\nUpvote\n\n16\n\n## Models citing this paper 0\n\nNo model linking this paper\n\nCite arxiv.org/abs/2404.09937 in a model README.md to link it from this page.\n\n## Datasets citing this paper 1\n\n### Spaces citing this paper 0\n\nNo Space linking this paper\n\nCite arxiv.org/abs/2404.09937 in a Space README.md to link it from this page.\n\n## Collections including this paper 4\n\n", "frontpage": false}
