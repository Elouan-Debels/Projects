{"aid": "40040019", "title": "How to build a better RAG pipeline", "url": "https://vectorize.io/how-to-build-a-rag-pipeline/", "domain": "vectorize.io", "votes": 1, "user": "bytearray", "posted_at": "2024-04-15 13:02:34", "comments": 0, "source_title": "How to build a better RAG pipeline - Vectorize", "source_text": "How to build a better RAG pipeline - Vectorize\n\nWe use essential cookies to make our site work. With your consent, we may also\nuse non-essential cookies to improve user experience, personalize content, and\nanalyze website traffic. For these reasons, we may share your site usage data\nwith our analytics partners. By clicking \u201cAccept,\u201d you agree to our website's\ncookie use as described in our Cookie Policy. You can change your cookie\nsettings at any time by clicking \u201cPreferences.\u201d\n\n  * Use Cases\n\n    * Question Answering Systems\n    * AI Copilots\n    * Call Center Automation\n    * Content Automation\n    * Hyper-personalization\n  * Blog\n  * About\n  * Learn\n\n    * Prompt Engineering\n    * Retrieval Augmented Generation (RAG)\n    * Vector Database Guide\n  * Contact\n\nContact Us\n\n#### Be on of the first to try Vectorize!\n\nEdit Content\n\nGuides, RAG, RAG Pipelines\n\n# How to build a better RAG pipeline\n\nApril 15, 2024 Chris Latimer No comments yet\n\n## Introduction\n\nIn a short period of time, large language models (LLMs) and the applications\nthey power have become an integral part of the way we work. For those of us\nwho have embraced tools like ChatGPT and Claude to serve as our sounding\nboard, help us write code, and just generally get more done, it\u2019s becoming\nharder and harder to imagine going back to a world where we don\u2019t have these\ntools to rely on.\n\nHowever, as useful as these tools have been at enabling personal productivity,\nthey have yet to live up to the hype of transforming businesses and\nrevolutionizing society. At least for the moment, generative AI feels more\nlike a very useful new tool in our toolbox rather than a disruptive technology\nthat makes the toolbox obsolete all together.\n\n## Large language models do not know your data\n\nOne of the biggest barriers to using generative AI to completely automate\ncommon, repetitive tasks is its lack of access to relevant information. Out of\nthe box, LLMs are limited by the information contained in their training data.\nThey have no contextual understanding of domain specific knowledge which\nlimits how useful they can be on their own.\n\nFor instance, let\u2019s imagine that you notice your 2019 Toyota Camry is starting\nto make a weird noise. You can ask ChatGPT what this might mean and it will\nanswer questions to the best of its ability. It will base those answers on\ninformation included in its training data such as public car forum threads and\nthe car manual provided by Toyota.\n\nExamples of data that ChatGPT wouldn\u2019t know about include things like new\nmanufacturer recalls that have been announced since its last training cutoff.\nLikewise, it has no awareness of the work done recently on your particular\n2019 Toyota Camry. You can see that without this up-to-date information, it is\nvery difficult for the LLM to generate responses that can definitely make a\ndiagnosis as to the root cause of the annoying noise.\n\nThis same limitation exists for almost all potential LLM applications.\n\n## Retrieval augmented generation (RAG) overview\n\nA new approach called retrieval augmented generation (RAG) has emerged to\naddress this challenge. The idea behind RAG is to provide a bridge between the\nLLM and your proprietary information. This includes relevant context which may\nbe stored in external knowledge bases, file systems, large documents, SaaS\nplatforms and many other potential places.\n\nOn paper, the idea behind retrieval augmented generation is pretty simple as\nshown here:\n\nThe user starts by submitting a question or request to a RAG application. The\napplication then takes that user query and performs a similarity search,\nusually against a vector database. This allows the LLM application to identify\nchunks from the most relevant documents to then pass to the LLM. Using the\nuser query along with the retrieved data allows the LLM to provide more\ncontextually relevant responses that takes into account a more complete view\nof all available data.\n\nRetrieval augmented generation typically combines techniques of prompt\nengineering with information retrieval. A typical RAG workflow involves\ninjecting relevant chunks into a prompt template so that users get more\naccurate answer answers with fewer hallucinations from the language models.\n\n## The power and challenges of unstructured data\n\nCertainly, the need for relevant information is nothing new in application\ndevelopment. However, one of the unique aspects of solving this challenge\nwhile building generative AI features is the heavy reliance on unstructured\ndata.\n\n### Relevant information often comes from unstructured data sources\n\nIn order for the LLM to have the necessary contextual understanding to provide\naccurate responses, it often requires access to data that is not readily\naccessible using traditional information retrieval techniques. There is no SQL\ninterface to your Google Drive. There\u2019s no API to retrieve the 5 most similar\ncustomer service requests to the one a customer has right now.\n\nThis has led to the rapid adoption of vector databases to provide a work\naround to this problem. Using an embedding model, developers and AI engineers\ncan extract the relevant information from external knowledge bases and turn\nthat information into a set of high dimensional vectors. This lays the\nfoundation for very effective semantic retrieval, which is at the heart of any\nretrieval augmented generation system.\n\n### Data engineering for unstructured data is lagging\n\nYou might be wondering why this is a problem at all. After all, aren\u2019t we just\nloading data into a database? We already know how to do ETL, what\u2019s so special\nabout this one?\n\nThe biggest differences between a traditional data pipeline and a RAG pipeline\nare threefold. First, most traditional data engineering solutions are\noptimized to handle structured data integration. In these cases, you have a\nwell defined data structure at both ends of the pipeline. Here, you have a\ncompletely unstructured input on one end, and a numerical vector\nrepresentation at the other. It\u2019s not clear up front how many vectors a given\npiece of unstructured data should produce, or if the vectors that get\ngenerated are going to work well for your use case.\n\nSecond, the connector ecosystem for most data engineering platforms has poor\nsupport for both unstructured data sources as well as for vector databases\ndestinations. And finally, the process of transforming your unstructured\nsources into an optimized vector search index, is completely absent in most\ndata engineering solutions.\n\nAll of this has led to the need for a new type of pipeline, purpose built to\npower retrieval augmented generation (RAG) use cases. This type of pipeline is\nunimaginatively referred to as a RAG pipeline or sometimes a vector pipeline.\n\n## The key data requirement for LLM-powered apps: RAG pipelines\n\n### What is a RAG pipeline?\n\nAt least in this article, when we talk about a RAG pipeline, we specifically\nmean one where the source is some sort of unstructured data. That data could\nexist in the form of files on your Google Drive or Sharepoint, it should exist\nas notes in relational database table, it could be emails exchanged between\ncustomers and a support team, or many other possible options.\n\nThe destination for a RAG pipeline is a vector database. And along the way,\nthere are a number of transformation and document pre processing steps that\nhelp to achieve a scalable, robust RAG architecture.\n\n### Primary objective of a pipeline\n\nThe primary goal of a RAG pipeline is to create a reliable vector search\nindex, populated with factual knowledge and relevant context. When done\ncorrectly, you can ensure that the retrieved context provided by the vector\nstore also reflects up to date information.\n\nIn this way, you can ensure that anytime a user query requires information\nfrom an external knowledge source to generate an accurate response, your large\nlanguage model will have the necessary context to respond the user query\ncorrectly.\n\n### Retrieval augmented generation pipelines: A step by step breakdown\n\nA RAG pipeline typically follows a standard sequence of steps to convert\nunstructured into an optimized vector index in your vector database. Let\u2019s\nstart by looking at the end-to-end flow of a simple RAG pipeline:\n\n#### Ingestion\n\nTo build a RAG pipeline, you must first understand the sources of domain\nspecific, external knowledge that you want to ingest data from. This could be\na knowledge base, web pages, or custom datasets sourced from SaaS platforms.\nFor retrieval augmented generation use cases such as question answering tasks,\nit\u2019s important to identify the source documents that contain the most relevant\ninformation for your anticipated user queries.\n\n#### Extraction\n\nBecause many sources of unstructured data require some processing to retrieve\nthe natural language text data contained inside them, we need to include\nextraction logic in our RAG pipeline. Data extracted from data sources may or\nmay not be immediately useful.\n\nFor example, PDF documents in particular are notoriously tricky to convert\ninto useful text. Document pre processing with open source libraries work in\nsimple cases. However, for complex PDFs you may need to rely on something more\npurpose built for knowledge intensive NLP tasks to arrange the extracted\nnatural language into a form that more closely resembles the way a human would\nread the document. Other advanced options, such as AWS Textractor, rely on\nsolutions rooted in neural networks and other machine learning techniques to\npower this process. This approach has several advantages in terms of accuracy\nof the extraction, but comes with increased costs.\n\n#### Chunking/Embedding\n\nChunking and embedding are two independent steps but are very closely related\nto once another. Chunking refers to the process of taking the content\nextracted from the source data and converting it into a set of text chunks.\n\nThe chunking strategy used here is an important factor in retrieval augmented\ngeneration (RAG). That is because the text chunks you create in this step will\nbe used later by your RAG application to supply context to the LLM at runtime.\n\nThe embedding step is where we actually turn the text chunks into document\nembeddings that will ultimately get stored inside our vector database. These\nvectors are generated using an embedding model such as text-embedding-ada-0002\nor text-embedding-v3-large from OpenAI or of the many options from companies\nlike Mistral AI. Most embedding models are general purpose, although companies\nlike Voyage AI are experimenting with fine tuning these models for domain\nspecific use cases like finance or legal.\n\n#### Persistence\n\nThe vectors that are produced from an embedding model will usually have a\nfixed number of dimensions in each vector. When you create your search index\nin your vector database, you will typically define the number of dimensions\nfor the index, and any new data inserted into the index must have the\nspecified dimension length.\n\n#### Refreshing\n\nOnce your vector database is populated, you\u2019ll need to think about how to keep\nyour vector data synchronized with the source data which was used to populate\nit. Otherwise, you retrieval augmented generation use case will eventually\nencounter problems as the retrieved documents are out of date, and your\nlanguage models will generate incorrect responses to user queries.\n\n## Building an architecture for your RAG pipeline\n\nAnyone who has experience in data engineering can tell you that building\nreliable pipelines is more difficult than you might expect. Vector pipelines\nfor retrieval augmented generation are no exception as we\u2019ll examine in this\nsection.\n\n### Lessons learned at Vectorize\n\nBefore launching Vectorize, the founding team spent around three years\nbuilding out a cloud native event streaming platform together at DataStax. As\nyou can imagine, they learned a lot about data engineering and building\nreliable, mission critical data pipelines for many large companies.\n\nThey also learned a lot about retrieval augmented generation and vector\ndatabases since DataStax also offered a leading cloud native vector database.\nVectorize is basically the platform they kept wishing they had as they were\nworking with customers to help them get retrieval augmented generation apps\ninto production.\n\n### Think like a data engineer\n\nVectorize isn\u2019t focused on solving simple \u201cchat with a PDF\u201d type use cases.\nThe mission is to help developers and companies productionalize their data\ncapabilities around retrieval augmented generation (RAG). From a KPI\nperspective, this means maximizing retrieval accuracy and accelerating\napplication development for any generative AI feature or applications that\nrely on vector data.\n\n#### Real time vs batch\n\nInitially, you might find limited success with using a schedule-based\nsolution, such as cron, to update your vector database. However, as time goes\nby, it will become increasingly certain that you will encounter use cases that\nrequire immediate, real time updates to ensure your RAG applications have the\nlatest context.\n\n#### Avoiding accidental DDOS\n\nEven more important than having accurate, up to date information is that you\ncan actually respond to your user\u2019s query with retrieved information. This is\ndifficult to do if you inadvertently overwhelm your source system or vector\ndatabase with a denial of service attack because of a poorly designed RAG\npipeline.\n\nGenerally speaking, this is another reason why real time data architectures\nare preferable to batch ones. With an event-driven approach, you have an event\ntopic acting as a buffer between the source and destination. If you have a\nhuge burst of changes, you can throttle the updates to your vector database to\navoid service interruptions.\n\n### Errors are a fact of life\n\nNo matter how well constructed your RAG pipeline is, you will always be at the\nmercy of the upstream and downstream systems you\u2019re connecting to. Building a\nresilient system means accepting the fact that errors will happen and having a\nstrategy to deal with those errors when they occur.\n\n#### API failures\n\nAPI failures are extremely common. These could be APIs used to retrieve custom\ndata from a knowledge base. It could also be an API to retrieve a set of\ndocument embeddings from a service like OpenAI\u2019s embedding endpoint.\n\n#### Vector database failure scenarios\n\nJust like any piece of infrastructure, your vector database could experience a\ntemporary or sustained outage. These could range from simple timeouts that\nresolve on their own to more prolonged major outages.\n\n#### Tokenization & context window challenges\n\nTry as you might, it is sometimes tricky to know exactly how many tokens a\ngiven chunk will produce for your embedding model. You may encounter scenarios\nwhere your tokenized input exceeds the maximum context window for your\nembedding model.\n\n### Event streaming is the only solid foundation\n\nWhile we certainly could have used an approach that relies on batch jobs,\nschedulers and polling to solve the RAG pipeline pipeline problem, that would\nhave been a brittle and naive implementation. Instead, we opted to build on\ntop of an open source event streaming platform called Apache Pulsar.\n\nPulsar is basically Apache Kafka++. It has all the ultra-reliable event\nstreaming of Kafka plus multi-tenancy, more elastic scalability, and an\narchitecture that is made for Kubernetes. It also has protocol-level\ncompatibility with Kafka, so there\u2019s literally no downside.\n\n#### Integration with source systems\n\nOne of the advantage of using a streaming platform is that you get a connector\nframework for free. Why is that a big deal?\n\nLet\u2019s say you are relying on batch scripts and polling to perform your initial\ningestion or to process changes from some knowledge base. Now let\u2019s imagine\nyou have a lot of data and changes you need to process all at once. With\nconnector frameworks like Kafka Connect and Pulsar Functions, connector\ninstances are stateless and scalable. If a huge burst of changes come through,\nyou just scale the number of connector instances. On Kubernetes, you can even\nscale them automatically.\n\n#### Integration with vector databases\n\nLikewise, you get similar benefits on the destination side when it comes to\nwriting vector data into the vector database. One other big advantage of using\nstreaming is that you also have greater control over the throughput of writes\ninto your database.\n\nIf your database is healthy you can keep pushing up the ingestion rate.\nHowever, if your vector database is under load or you want to avoid costly\nscaling events (e.g. addding another database pod from your vector database\nprovider), you can let the change events back up in the topic while you\nprocess them at a more comfortable pace to reduce risk.\n\n#### Event stream processing\n\nThroughout the pipeline, you\u2019ll often want some level of lightweight event\nprocessing and at times more full featured event stream processing. Examples\nhere might include tasks like document pre processing to generate the\nappropriate metadata for your source data.\n\nIt could also include mediation and data scrubbing tasks to address data\nprivacy requirements.\n\n#### Retries and dead letter queues\n\nFinally, the key to a resilient pipeline is error handling. Errors will\nhappen. Your APIs will return with HTTP 503 and other error codes. Your vector\ndatabase will get into an unhealthy state periodically.\n\nA well designed vector pipeline will provide approaches to be resilient to\nthese situations. This means implementing retry logic to account for the case\nwhen the processing of a document fails. To avoid overwhelming the source or\ndestination system when retries are necessary, it means relying on techniques\nlike exponential backoffs to give the problem time to resolve.\n\nWhen things simply can\u2019t be fixed, it means providing dead letter queue\ncapability so problems aren\u2019t swallowed and there\u2019s a mechanism to complete\nprocessing once the system becomes healthy again.\n\n## Building a RAG pipeline using Vectorize\n\nVectorize is currently in private beta. If you\u2019d like to try it out, you can\nsign up for our waitlist. Or, if you have a production use case you can\ncontact us here; your message will go straight to the company founders.\n\n## Conclusion\n\nAs generative AI continues to be top of mind for developers and companies\nalike, having a solid strategy to ensure your LLMs always have access to the\nright data couldn\u2019t be more important. Choosing the wrong approach now is\nlikely to come back to haunt you later. By following the guiding principles in\nthis article, you can build a robust, reliable set of capabilities to ensure\nyour retrieval augmented generation applications provide innovative, accurate\nuser experiences.\n\n### Share this:\n\n  * Twitter\n  * LinkedIn\n  * Facebook\n  * Reddit\n  * Pinterest\n  * Threads\n  * X\n\n### Related\n\n### Leave a ReplyCancel reply\n\n#### Search\n\n#### Categories\n\n#### Recent posts\n\n  * How to build a better RAG pipeline\n\n  * How to Get More from Your Pinecone Vector Database\n\n  * Picking the best embedding model for RAG\n\n#### Tags\n\nRAG Retrieval Augmented Generation\n\nThe easiest, fastest way to connect your data to your LLMs.\n\n##### Resources\n\n  * Support center\n  * Documentation\n  * Community\n  * Hosting\n\n##### Company\n\n  * About us\n  * Latest news\n  * Contact us\n  * Resources\n\n\u00a9 Vectorize AI, Inc, All Rights Reserved.\n\n  * Terms & Conditions\n  * Privacy Policy\n\n## Discover more from Vectorize\n\nSubscribe now to keep reading and get access to the full archive.\n\nContinue reading\n\nLoading Comments...\n\n", "frontpage": false}
