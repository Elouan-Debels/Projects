{"aid": "39989602", "title": "Improving our Jest execution time by 300%", "url": "https://www.camggould.com/posts/Jest-With-RTL-Is-Slow/", "domain": "camggould.com", "votes": 2, "user": "thunderbong", "posted_at": "2024-04-10 11:45:25", "comments": 0, "source_title": "Improving our Jest execution time by 300%", "source_text": "Improving our Jest execution time by 300%\n\nHome About Blog #RED\n\n# Improving our Jest execution time by 300%\n\nby Cameron Gould on Tue Apr 09 2024\n\njest2024learning in publicoptimization\n\nOver the course of 2023, our large engineering team started working within a\nnew React codebase using TypeScript. At first, everything was really fun and\nblazingly fast. But things started to slow down rapidly once the number of\nunit tests got above a certain level. At one point, in some cases, our test\nsuite took over 500 seconds to run just a few hundred tests! Performance was\nabysmal. I took up the responsibility for doing some research to determine why\nwe were having these issues with our tests, and how we could solve the issues\nwithout rewriting a bunch of our tests.\n\n## Our Test Setup\n\nFor testing our React components, we used the following configuration:\n\n  * Jest testing framework\n  * Mock service worker (msw) for intercepting API calls\n  * React Testing Library\n  * ts-jest transformer/preprocessor\n\n## Investigation\n\nMy investigation started with identifying the test suites which ran the\nlongest. I quickly discovered that a handful of the test suites were running\nfor upwards of 25 seconds. Some of these suites only contained 3 or 4 tests.\nThis was immediately a red flag. Diving into these tests, I found that there\nwas significant usage of the getByRole query. To validate this as the root\ncause, I wrapped these getByRole calls with a timer to determine the duration\nof their execution. Of course, these seemed to be the culprit. Initially, it\nwas challenging to find others online experiencing similar issues. I was\nsearching for issues with slow Jest performance when using getByRole. However,\nI was finally able to find what I was looking for when I started searching for\nissues related to React Testing Library instead. I found this issue opened\nagainst RTL way back in 2020 which is still open, addressing this performance\nconcern. We found our first bottleneck, and confirmed it as a known issue.\n\nNext, I investigated the performance and memory consumption of our Jest tests.\nI read this useful article discussing Jest memory leaks, and decided to\ndiagnose our test suites to determine whether we had memory leaks too. The\ncommand from the article did the trick:\n\n    \n    \n    node --expose-gc ./node_modules/.bin/jest --runInBand --logHeapUsage --silent\n\nI found that we had massive memory leaks! Our heap grew to 3GB in size before\nthe runner ran out of memory. It was horrible. There\u2019s problem number two!\n\nFinally, I did some research on the transformer we were using, ts-jest. One\ninteresting thing about ts-jest is that part of the transformation process is\ncompiling TypeScript types before running tests. This added a lot of time to\nour tests. The ts-jest transformer has an isolatedModules option which, when\nset to true, disables type checking.\n\n## Results\n\n### GetByRole Optimization\n\nAs mentioned before, it would be a massive undertaking to remove all use of\ngetByRole from our test suites. We opted to accept our current tests as they\nare, but put out an advisory to no longer use getByRole queries and instead\nuse alternative methods for selecting DOM elements such as getByLabel,\ngetByText, or getByTestId. Label and text selectors are preferred, as\nretrieving by hidden attributes doesn\u2019t reflect the actual customer experience\nvery well. For this discovery, we\u2019re preventing the fire from spreading more\nthan anything.\n\n### Memory Leaks!\n\nMany tests were unknowingly creating async calls that were not being\nintercepted and handled by msw or were not being properly awaited. These async\noperations were leaving behind resources that were not being properly cleaned\nup, leading to significant memory leaks over time.\n\nThe effect of this was that our tests were starting to time out when being run\nin parallel, and our runners in our deployment pipeline began to run out of\nmemory before the tests could finish running, causing our deployment pipeline\nto get blocked.\n\nSimilar to the effort limits of the GetByRole problem, finding and fixing all\nof the memory leaks within all of our test suites would be far too much\neffort. The long-term solution is to resolve these memory leaks one by one.\nThe short-term solution was to cap the worker idle memory limit in our Jest\nconfig to 512mb:\n\n    \n    \n    workerIdleMemoryLimit: '512MB'\n\nThis setting checks the memory usage of a test after it has been completed,\nand kills/restarts the worker if it exceeds the specified limit. It prevents\nheap utilization from growing out of control, but it\u2019s really only a bandaid\nsolution. In addition to this, we reconfigured the runners in our deployment\npipeline to run tests in band (not parallel) to guarantee that each test has\nenough memory to execute. This approach decelerates our testing in the\ndeployment pipeline; however, developers can still execute tests in parallel\non local machines, leveraging the superior memory capacities of our MacBook\nPros.\n\n### Running Jest Without Type Checking\n\nThis change, by far, had the most significant impact on our test\u2019s execution\ntime. The justification for this change was that we were type checking\neverywhere around our tests, so there\u2019s no reason to also have type checking\nenabled for our tests:\n\n  * Build command is transpiling TypeScript, it\u2019s already doing type checking.\n  * Dev IDEs/editors will detect type mismatches and highlight them.\n  * Pre-commit hook builds the package, so it\u2019s type checking as a guard before committing anything.\n  * We have a code review automation which runs a release build script to check that the code being checked in will build properly. If it fails, it blocks the dev from merging the code.\n  * If that isn\u2019t enough, the same release build is part of our deployment pipeline. A build failure will block the pipeline and never make it to prod.\n\nSo we disabled type checking. The result. A 300% improvement in test execution\ntime. The test suite that once took over 500 seconds to run all tests, was now\nrunning in 150 seconds!\n\nBased on a likely scenario of 5 developers running the test suite twice a day\nwe estimated that over the course of a year, these changes will save\napproximately 253 developer hours, or approximately 31.625 work days (8 hours\nper day). This put over 3 sprints of time back in the hands of the developers!\n\n    \n    \n    350 * 2 * 5 = 3500 seconds per day 3500 * 5 * 52 = 910,000 seconds per year (work week only) 910,000 / 60 / 60 = 252.7 hours per year 252.7 / 8 = 31.58 work days per year\n\n## Learnings\n\nSome things I took away from this:\n\n  1. It\u2019s worth investing in learning your libaries from the start. Writing tests slightly incorrectly will have a snowball effect later on, which can significantly slow down your ability to iterate.\n  2. Profile your test suite frequently enough to catch memory leaks early on! It\u2019s a lot easier to fix a handful of tests, but nearly impossible to prioritize fixing hundreds!\n  3. Don\u2019t type check your tests if everything around your tests is already doing type checking.\n  4. Small inefficiencies add up. A test suite running for 500 seconds instead of 150 seconds is significant over the course of a year. It wastes a TON of time. Yes, you could just pivot to another task. But context switching is extremely expensive and kills productivity. This was worth the optimization.\n\nGitHub LinkedIn\n\n", "frontpage": false}
