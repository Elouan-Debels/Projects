{"aid": "40045007", "title": "Bayesian Optimization, Part 1: Key Ideas, Gaussian Processes", "url": "https://blog.quipu-strands.com/bayesopt_1_key_ideas_GPs", "domain": "quipu-strands.com", "votes": 2, "user": "abhgh", "posted_at": "2024-04-15 20:07:53", "comments": 0, "source_title": "Bayesian Optimization, Part 1: Key Ideas, Gaussian Processes", "source_text": "Bayesian Optimization, Part 1: Key Ideas, Gaussian Processes\n\nProcessing math: 100%\n\nA Not-So Primordial Soup\n\n# Bayesian Optimization, Part 1: Key Ideas, Gaussian Processes\n\nCreated: Nov 18, 2023. Last major update: Jan 15, 2024.\n\nThe real reason I like Bayesian Optimization: lots of pretty pictures!\n\nIf I wanted to sell you on the idea of Bayesian Optimization (BayesOpt), I\u2019d\njust list some of its applications:\n\n  * Hyperparameter Optimization (HPO) (Turner et al., 2021).\n  * Neural Architecture Search (NAS) (White et al., 2021).\n  * Molecule discovery (G\u00f3mez-Bombarelli et al., 2018).\n  * Liquid chromatography (Boelrijk et al., 2023).\n  * Creating low-carbon concrete (Ament et al., 2023).\n  * Plasma control in nuclear fusion (Mehta et al., 2022).\n  * Parameter tuning for lasers (Kirschner et al., 2019-11)\n\nMy own usage has been tamer, but also varied: I\u2019ve used it to (a) learn\ndistributions (Ghose & Ravindran, 2020; Ghose & Ravindran, 2019), and (b) tune\nSHAP model explanations to identify informative training instances (Nguyen &\nGhose, 2023).\n\nWhy is BayesOpt everywhere? Because the problems it solves are everywhere. It\nprovides tools to optimize an noisy and expensive black-box function: given an\ninput, you can only query your function for an output, and you don\u2019t have any\nother information such as its gradients. Sometimes you mightn\u2019t even have a\nmathematical form for it. Since its expensive, you can\u2019t query it a lot. Plus,\nfunction evaluations might be noisy, i.e., the same input might produce\nslightly different outputs. The lack of function information is in contrast to\nthe plethora of Gradient Descent variants that seem to be fueling the Deep\nLearning revolution. But if you think about it, this is a very practical\nproblem. For example, you want to optimize various small and large processes\nin a factory to increase throughput - what does a gradient even mean here?\n\nI encountered the area in 2018, when, as part of my Ph.D., I\u2019d run out of ways\nto maximize a certain function. And the area has only grown since then. If\nyou\u2019re interested, I\u2019ve an introductory write-up of BayesOpt in my\ndissertation (Section 2.1) too, and Section 2.3 there talks about the general\nproblem I wanted to solve. Much of the material here though comes from a talk\nI gave a while ago (slides).\n\nOK, let\u2019s get started. This is part-1 of a two-part series, where I intend to\nprovide a detailed introduction to BayesOpt. I also introduce Gaussian\nProcesses, and if you\u2019re here just for that you can jump straight to this\nsection. Here\u2019s a layout for this part:\n\n  * The Problem\n  * Key Ideas\n  * Comparison to Gradient Descent (GD)\n  * Gaussian Processes (GP)\n\n    * Intuition\n    * Deep Dive\n    * A Different Perspective\n    * Minimal Code for GP\n    * Summary and Challenges\n  * References\n\nIn part-2, I\u2019ll talk about acquisition functions, learning resources, etc. As\nyou can probably tell, this is going to be a long-ish series; settle in with\nample amounts of coffee!\n\n## The Problem\n\nAt its heart the family of BayesOpt techniques still strive to solve an\noptimization problem:\n\nargmaxx\u2208Xf(x)\n\nHere, we are maximizing (or minimizing) wrt x whose values range over a domain\nX. Reiterating what we said before, BayesOpt is useful when f(x) possesses one\nor more of these properties:\n\n  1. Its mathematical expression is unavailable to us or it is not differentiable.\n  2. Evaluations are noisy, i.e., for the same input x you might obtain slightly different values of f(x).\n  3. It is expensive to evaluate.\n\nThese are very practical challenges. To ground them, I\u2019ll use the (personally\nappealing) example of baking a perfect chocolate chip cookie (Solnik et al.,\n2017).\n\nThis is a good chocolate chip cookie! Barefoot, Campbell, CA if you're in the\narea!\n\nThe problem is: find a recipe, i.e., amount of flour, sugar, baking\ntemperature and time, etc., for baking cookies that is optimal wrt taste.\nTaste assessment is made by multiple human raters. The objective function f is\nthe assessment (averaged, if there are multiple human raters) given the input\nvector x of various cookie ingredients, e.g., quantity of flour, amount of\nchocolate chips, and cooking conditions, e.g., oven temperature, time to bake.\nIn other words, x encodes the recipe.\n\nWhy is this a good use case for BayesOpt?\n\n  1. f(x) is not available for us to inspect (forget differentiability) - it exists in the heads of the human raters. But it can be invoked: ask people to rate cookies.\n  2. Does a person rate the same cookie in the same way in a blind test? Mostly yes, but sometimes, may be not. Also, external factors such as humidity might affect the taste for the same x. Safe to assume f(x) is noisy.\n  3. To evaluate f(x), you need cookies baked according to the recipe x. Baking a batch of cookies and distributing them for assessment consumes time (order of hours or days) and effort, making it expensive to evaluate f(x). The only upside is an unending supply of cookies would probably make your raters happy!\n\nOne way to optimize f(x) is to create the most promising recipe based on a\nthorough examination of the history of recipes and assessments. Loosely\nspeaking, you tease out parts of past recipes that worked or didn\u2019t, and use\nthat knowledge to assemble a new recipe. Bake a batch of cookies based on it\nand have it evaluated by the human raters. Add the current recipe and\nassessment to your history, and repeat. BayesOpt formalizes this idea.\n\nOf course, as you might suspect, baking cookies is not all this is good for\n;-).\n\nHyperparameter Optimization (HPO) is probably the flagship use-case of\nBayesOpt (Turner et al., 2021). We want to increase the predictive accuracy of\na model f, by tuning its hyperparams. Note the apt prerequisites:\n\n  1. f is effectively black-box if we want our solution to be generic, i.e., it should be usable as much for Decision Trees as for Neural Networks.\n  2. It is computationally expensive to evaluate (since evaluating f for a hyperparam setting would require us to retrain the model).\n  3. It can also be noisy, e.g., randomly initialized learning like with gradient descent.\n\nHyperopt is a popular library that implements this approach, using a specific\nform of BayesOpt known as the Tree-structured Parzen Estimator (TPE) (Bergstra\net al., 2011). There are others such as BoTorch, Ray Tune and\nBayesianOptimization.\n\nThese kinds of problems are ubiquitous - and the preamble to this article\nlisted diverse settings where BayesOpt crops up. Applications aside, BayesOpt\nitself is evolving, e.g., InfoBAX (Neiswanger et al., 2021) uses BayesOpt-like\nideas to compute arbitrary function properties (as opposed to just extrema) in\na sample-efficient manner, grey-box optimization (Astudillo & Frazier, 2022)\nrelaxes the black-box setting to utilize additional information like\ngradients.\n\n## Key Ideas\n\nSo what makes BayesOpt tick? Recall that BayesOpt is an iterative process,\ni.e., at each iteration a specific x is proposed and f(x) is evaluated.\nEssentially, BayesOpt primarily performs these two steps:\n\n  1. At a given iteration t, you\u2019ve the function evaluations f(x1),f(x2),...,f(xt\u22121). You can fit a regression model to this history H={(xi,f(xi))}i=t\u22121i=1 of t\u22121 evaluated pairs (xi,f(xi)), to approximate the actual objective function over the entire input space x\u2208X. We will refer to this approximation as our surrogate model \u02c6f. Would any regression model do, e.g., Decision Trees, Neural Networks? Almost - but there is an additional property we require of our regressor, as we\u2019ll shortly see.\n  2. Use \u02c6f as the function to optimize (instead of f) and propose a minima xt. Evaluate f(xt), and if it isn\u2019t satisfactory or you haven\u2019t exhausted your optimization budget, you can add f(xt) to the history H, refit \u02c6f on these t evaluations, and repeat these steps.\n\nThis is a simplified picture, and there are at least two important nuances:\n\n  * First, the surrogate model needs to have the property that for x\u2208X where the exact value is not known, i.e., f(x) is not available in history H, we need to be aware of how uncertain is our guess \u02c6f(x).\n  * And second, in Step 2 above, we don\u2019t always guess the minima based on the current \u02c6f. Sometimes, we might choose to propose an xt in the interest of exploration, i.e., in a region of X where we haven\u2019t evaluated f enough. This is the famous explore-exploit tradeoff you might\u2019ve heard in the context of Reinforcement Learning; it applies to BayesOpt too. Given the current surrogate model \u02c6f, that was fit on history H, the next point xt on which we wish to acquire f(xt) is provided by an acquisition function. In fact, its the acquisition function that needs to know how uncertain \u02c6f is at various inputs, so it can judiciously make the explore-exploit trade-off.\n\nLet\u2019s visualize these ideas for maximization of a univariate function, i.e., X\nis a line.\n\nAt an iteration of BayesOpt. The bottom plot shows the acquisition function\nvalues over the input space.\n\nThe image above show two plots stacked vertically. Start with the top plot,\nand note the legend there. This shows what things look like with BayesOpt at\nits 4th iteration. f is shown with a black solid line and red dots mark the 3\nprior evaluations of f. A surrogate model \u02c6f fitted to these dots is shown as\na blue dashed line. Outside of the red dots, we\u2019re uncertain of the precise\nvalues, and this is shown as an envelope of somewhat likely values, i.e.,\nthose that fall within one standard deviation (denoted as 1\u03c3) of our mean\nguess (the dashed blue line). We\u2019ll discuss how to obtain these estimates in\nthe next section, using Gaussian Processes. At the red dots, the envelopes\ndisappear because there is no guesswork involved. The envelope balloons up as\nwe move into regions away from the red dots. The current discovered max. is\nshown by the horizontal red line.\n\nThe bottom plot shows what the acquisition function looks like over X. We\u2019ll\nsee how its exactly calculated later (spoiler: there are many ways), but the\nred dot in this plot denotes the greatest acquisition value.\n\nLets move onto the 5th iteration, as shown below. Now we have evaluated f on x\nthe acquisition function had suggested, and this recent evaluation is shown\nwith a square box. This leads to a higher estimated maxima - horizontal red\nline. The other steps are repeated: \u02c6f is refit on this data with 4 points,\nwhich means its uncertainty envelopes are recomputed as well. Note how the\nenvelope nearly disappears in the interval \u223c[11.5,12.25] because of two\nevaluations, i.e., red dots, in close proximity. We also re-calculate the\nacquisition values over X. Note that this time though, the acquisition\nfunction has a single hump - it seems reasonably sure about where the maxima\nlies.\n\nThe next iteration. The square box shows the new evaluation based on the\nprevious iteration.\n\nOK, now lets look at iterations 4, 5, 6 and 7 together - shown below.\n\nMultiple iterations. Note the multiple high-density regions in the final\nacquisition value plot.\n\nNotice something interesting? In the last iteration the acquisition function\nhas grown less sure about where valuable points are located. This \u201cjumping\naround\u201d is characteristic of BayesOpt: as newer evaluations become available,\nthe acquisition function revises its notion of valuable locations. In fact,\nthe amount of \u201cfat\u201d envelopes might increase as well, when regions of\nsurprising behavior are discovered. Uncertainty decreases asymptotically but\noccasional fluctuations are to be expected.\n\nFor this example, I\u2019d argue this is a good thing because we still haven\u2019t\ndiscovered the actual global maxima. More exploration is great if we have the\nbudget for it.\n\nUnfortunately, this also makes it challenging to have a simple stopping\ncriteria for BayesOpt (there are some, e.g., (Makarova et al., 2022)),\nrelative to, say, Gradient Descent (GD), where you might stop when the changes\nin the extrema become vanishingly small. Typically, you decide on an\noptimization budget, e.g., number of iterations T, run it all the way through,\nand pick the best value (again, this is different from GD, where we might pick\nthe last value).\n\nHere\u2019s the high-level BayesOpt algo summarized:\n\nBayesOpt pseudocode.\n\nThe first point, i.e., at t=1, is picked via random sampling here: U(X)\ndenotes uniform distribution over the domain X. You can begin with known prior\nevaluations too - this is helpful if you already know something about the\nfunction, e.g., where the maxima is likely to occur. Even prior known\nevaluations at arbitrary points help, as they save some exploration.\n\nLook at Line 7 - we need to maximize for the acquisition value, and that also\nwould require an optimizer. Here\u2019s a question you might ask: did we just\ntranslate one optimization problem into another? What did we gain?\n\nThe answer is yes, there is another optimization problem we need to solve per\niteration of BayesOpt, using what we refer to as the auxiliary optimizer. What\nwe gain is this optimizer works with a tractable function - the surrogate\nmodel - instead of the original real-life function, which may or may not have\na known form (like in the cookie example). The auxiliary optimization can be\nperformed way quicker, relatively speaking, because the surrogate model exists\non your computer, and you\u2019re free to make as many calls to it as you like. It\nstill needs to be fast, since this optimization needs to occur per iteration\nof BayesOpt.\n\nCommon choices for this optimizer are DIRECT and CMA-ES. We won\u2019t go into\ndetails here, but this StackOverflow answer provides some information. There\nhas been work to get rid of the auxiliary optimization (Wang et al., 2014).\n\nNOTE:\n\nThis gives us another perspective on BayesOpt. There already exist black-box\noptimizers like DIRECT and CMA-ES. This aspect is not new. What\u2019s different\nhere is evaluating the objective function is expensive, and hence, inputs must\nbe strategically picked so as to discover the optima in the minimum number of\nevaluations. In this view, BayesOpt provides a way to make other black-box\noptimizers sample efficient.\n\n## Comparison to Gradient Descent (GD)\n\nSpeaking of GD, let\u2019s compare with it to better conceptually situate BayesOpt.\nOn a minimization problem this time.\n\nIn the plots below, the top row shows BayesOpt and vanilla GD minimizing the\nsame function, with the same starting point of 0.3, with a budget of T=100\niterations. Points recently visited are both larger and darker. We use up all\nof T for BayesOpt but exit early for GD when the minima value doesn\u2019t change\nappreciably.\n\nThe lower row shows Kernel Density Plots (KDE) visualizing where each\noptimizer (in the same column) spent its time.\n\nBayesOpt compared with Gradient Descent.\n\nWe notice some interesting trends:\n\n  * GD (right column) spends most of its time getting to the closest local minima. The KDE plot shows this focus. The minima found is f(\u22120.04)=\u22120.1. All recent points visited are concentrated around this minima, and the \u201cshape\u201d of the path is like a comet\u2019s tail.\n  * BayesOpt (left column) is more \u201cglobal\u201d because of its exploration, and manages to find the global minima f(\u22122)=\u22129.81. It doesn\u2019t always find the global minima - just that here it does. The KDE plot shows that while BayesOpt also focuses on the global minima, its attention is fairly diffuse (look at the y-axes of both KDE plots). You can also see this from the scattered placement of the large red circles.\n  * As a follow-up to the previous point, the fact that BayesOpt spends much of its function evaluation budget closer to the minima, is a characteristic behavior of these methods. That\u2019s to say, while it tries to model the whole input space, it allocates a larger budget to regions closer to the minima. This is the region that\u2019s important to us. This is what you would expect from a smart search algorithm.\n\nTo be fair, you wouldn\u2019t typically employ GD in this naive fashion - you would\nprobably use momentum, use some kind of adaptive learning rate (this was fixed\nhere) and perform multiple starts, but this example is more illustrative than\npractical.\n\nAt this point we have a view of BayesOpt at the, uh, cocktail party level.\nHopefully, all of this has whetted your appetite to learn more about the two\ncritical ingredients of BayesOpt: surrogate models and acquisition functions.\nWe begin by looking at an example of the former.\n\n## Gaussian Processes (GP)\n\nRecall we said that a good surrogate model needs to be able to quantify\nuncertainty well - in addition to being a good regressor. In my opinion,\nGaussian Processes (GP) are almost the gold standard here. This is the reason\nwhy you tend to encounter GPs in conjunction with BayesOpt a lot. You could do\nBayesOpt without GPs, e.g., you can use Decision Trees (Kim & Choi, 2022) or\nNeural Networks (Snoek et al., 2015), but GPs and BayesOpt are a match made in\nheaven.\n\nThe topic of GPs is interesting and vast enough to merit its own blog post,\nbut we\u2019ll try to cover the relevant bits here.\n\n### Intuition\n\nOK, let\u2019s consider the regression angle. You have a bunch of points\n(xi,yi),i=1...N where yi\u2208R, to which you want to fit a function. Normally, you\nwould posit a function f(x;\u03b8) with parameters \u03b8, e.g., coefficients in\nLogistic Regression or weights in a Neural Network, and learn \u03b8 that best\nexplains the data. This is a parametric model, since f\u2019s behavior depends on\nthe parameter \u03b8. Once you have learned \u03b8, you can discard your training data\n{(xi,yi)}Ni=1.\n\nGPs though, ask you to assume a certain smoothness in the function; which,\nroughly speaking, is how similar do you want yi and yj to be given similar xi\nand xj. So instead of parameterizing the shape of the overall function, you\ntry to define how input similarities dictate output similarities. This is a\nvery different philosophy (something that Support Vector Machines also use),\nbut quite natural. Consider the following function values:\n\n  * f(1)=10\n  * f(2)=40\n  * f(3)=90\n  * f(4)=160\n\nIf I now wonder about the value at 3.1, wouldn\u2019t it make sense to guess that\nf(3.1)\u224890? This is our natural sense of smoothness at work. In a way, GPs\nassemble the overall function from smaller function pieces where each piece\nhonors some notion of local similarity that we define.\n\nHow do we operationalize this? Let\u2019s step back for a bit, recall that we also\nwant good uncertainties, and see if we can get both these properties in one\nshot. This might seem out of the left field but consider the Gaussian\ndistribution N(\u03bc,\u03a3) for a while. As a refresher, for a distribution over two\nvariables, for the following covariance matrix:\n\n\u03a3=[\u03a311\u03a312\u03a321\u03a322]\n\nwe have:\n\n  * \u03a311 and \u03a322 are the individual variances of the random variables (which I\u2019ll refer to as Y1 and Y2 - for reasons to be clear soon).\n  * \u03a312 is the covariance between the two variables and signifies what values we might expect of one given the other. A positive value indicates high Y1 values signal high Y2 values; higher the \u03a312, greater is this correlation. And vice-versa, for a negative value. This is a symmetric quantity, i.e., \u03a312=\u03a321.\n\nThe plots below show some examples, with differing covariances (shown in the\nlegend).\n\nEffect of the covariance matrix.\n\nConsider the first plot. The \u201cforward slant\u201d of the Gaussian exists because\n\u03a312=\u03a321=2, and thus Y1 and Y2 are positively correlated. As possible values\nfor Y1 increase (going from left to right), possible values Y2 also increase\n(goes from bottom to top).\n\nNow, here\u2019s an outlandish idea: what if we let the Gaussian random variable Yi\nrepresent all possible outputs for xi? And the covariance matrix is defined by\ninput similarities? Specifically, if k(xi,xj) denotes the similarity between\nxi and xj, we let \u03a3ij=k(xi,xj). This gets us at least the following benefits:\n\n  * The values Yi can take for a given Yj are constrained by \u03a3ij, which here is the similarity in the inputs xi and xj. This is a good way to enforce smoothness.\n  * Since Yi is a random variable, we can infer some sort of a distribution over the values it can take, based on our assumption that the joint distribution is Normal. Which is to say, we may think of our current knowledge of the true outputs corresponding to a given xi as following some distribution, i.e., we believe some values are very likely (the mean of the distribution), some not so much, etc. This can be translated into uncertainty; informally, flatter distributions indicate greater uncertainty. This is also something we wanted.\n\nEffectively, we are saying that the function f we want to fit has its outputs\njointly distributed as a Gaussian, where the inputs decide their similarity\nvia the covariance matrix. Yes, this is unusual ... and awesome. Let\u2019s tie all\nthese pieces together in the next sections.\n\nIMPORTANT: we aren\u2019t saying the outputs form an univariate Gaussian.\nMathematically, this is what we ARE NOT saying:\n\nY\u223cN(\u03bc,\u03c32)\n\nHere, Y is a random variable that takes on all possible output values across\ninputs, where we assume exactly one \u201cbest-guess\u201d output for an input, i.e.,\nY\u2208{yi}. Note that here I am assigning one specific yi, however we may arrive\nat it, for an xi. This is known as a point estimate. This is how regression\nmodels are commonly used: you plug in an input and receive an output. Instead\nof a covariance matrix \u03a3, I am using the standard notation for variance \u03c32,\nsince this is a 1D Gaussian.\n\nSo, what are we saying? The Gaussian distribution we\u2019re interested in is\nmultivariate and has one random variable dedicated to possible outputs for one\ngiven input. The distribution of the output describes our current knowledge of\nwhat the output is. This is a bit like saying \u201cI think the output is likely\n90, but there is also a fair chance it is 91. It is highly unlikely it would\nbe greater than 92 though.\u201d - but only that we\u2019re saying it mathematically, by\ncondensing the verbal description into an appropriate distribution. The\ndistribution of one output is related to the distribution of another output -\nand the extent of this coupling is determined by the similarity in their\ninputs. Mathematically, this is what we ARE saying:\n\np(Y1,Y2,Y3,...)\u223cN(\u03bc,\u03a3)\n\nThe precise coupling between Yi and Yj is enforced by the similarity in the\nrespective inputs, \u03a3ij=k(xi,xj).\n\n### Deep Dive\n\nLet\u2019s formalize all of this stuff. But fear not - there will be pictures!\n\nWe\u2019ll talk about the following sets of points:\n\n  1. Training data (xi,yi)Ni=1.\n  2. Test data or inputs where we want to evaluate the learned function: x\u2217i,i=1...M.\n  3. All other points in the input space \u0303xk.\n\nThe corresponding uppercase letters would indicate a set of points, e.g., X,\nX\u2217, \u0303X. This notation would be overloaded to imply matrices at times, e.g.,\nX\u2208RN\u00d7d where d is the number of dimensions of the data. The number of elements\nin \u0303X is potentially infinite. Also, as mentioned earlier, we want to talk\nabout a range of outcomes for a particular input, and hence the outcomes would\nbe denoted by random variables Yi, Y\u2217i and \u0303Yi. We can obviously talk about\nspecific values these random variables can take, e.g., Yi=yi.\n\nThe model we\u2019re proposing above is:\n\np(Y1,...,YN,Y\u22171,...,Y\u2217M, \u0303Y1,...)\u223cN(\u03bc,\u03a3)\n\nwhere \u03a3ij=k(zi,zj), and each z comes from one of X, X\u2217 or \u0303X, based on the\npositions i and j.\n\nNOTE:\n\n  * While the uppercase letters for inputs, e.g., X, denote collections, uppercase indexed letters for the output denote individual random variables, e.g., Yi. Non-indexed uppercase letters denote a collection of random variables (analogous to the input), e.g., Y, Y\u2217, \u0303Y.\n  * Many references explicitly mention the input as conditioning variables: p(Y1,...,YN,Y\u22171,...,Y\u2217M, \u0303Y1,...|X,X\u2217, \u0303X). But I\u2019ll avoid it here to avoid clutter.\n\nAt this point, I can almost see this conversation between us:\n\n  * You: Whoa! Infinite points in \u0303X? How do we compute an infinite-dimensional distribution?\n  * Me: We are only going to be dealing with X (training data) and X\u2217 (test data) which are finite. I mentioned \u0303X to write down the overall model.\n\n  * You: Hmm, and what distribution do the finite parts follow?\n  * Me: Gaussian. Here\u2019s why. Gaussian distributions have this interesting property that if you were to focus on only a subset of the random variables from the original Gaussian, then you\u2019d see they also follow a Gaussian distribution (known as marginalization). The parameters for this Gaussian is easily obtained by only retaining parts of the original parameters that involve the current subset. For example, if the random variables t1,t2,t3,t4 follow a Gaussian distribution, then [t1,t3] is a Gaussian with the parameters in this color.\n\n[t1,t2,t3,t4]\u223cN([\u03bc1,\u03bc2,\u03bc3,\u03bc4],[\u03a311\u03a312\u03a313\u03a314\u03a321\u03a322\u03a323\u03a324\u03a331\u03a332\u03a333\u03a334\u03a341\u03a342\u03a343\u03a344])\n\n  * You: This helps us how?\n  * Me: We can talk only about the train and test data as coming from a Gaussian. Yes, there is a \u201clarger\u201d Gaussian that conceptually exists and deals with the infinite points in the input space, but we can talk only about our marginalization of interest without breaking this conceptual model. So, what we\u2019ll really focus on is this distribution: p(Y1,...,YN,Y\u22171,...,Y\u2217M)\u223cN(\u03bc,\u03a3). You don\u2019t see the inputs in this expression, but remember they\u2019re hiding within the \u03a3.\n\n  * You: I see. So we worry only about the train and test data. But how do derive the test outcomes given the training data? This regression problem is why we\u2019re here after all.\n  * Me: If you look at the expression above, we\u2019re asking for the conditional distribution given training outcomes, i.e., p(Y\u22171,...,Y\u2217M|Y1=y1,Y2=y2,...,YN=yN). Guess what this conditional distribution is ... yes, also a Gaussian. This has nothing to do with GPs; its a property of the Gaussian distribution that its conditionals are Gaussian as well. And its parameters are expressible in terms of X, Y, X\u2217, \u03bc and \u03a3 . This is it - this is the big reveal. The random variables corresponding to our test outputs are jointly Gaussian, whose parameters are calculable in closed form.\n\nYou can also calculate the distribution of just one Y\u2217i. No points for\nguessing what this distribution is: by the marginalization property this is a\n(univariate) Gaussian since p(Y\u22171,...,Y\u2217M) is a Gaussian. With a distribution\nfor Y\u2217i we can now do the following:\n\n    * Calculate the mean value for Yi - this serves as the regression value for input xi.\n    * Assign an uncertainty score to Yi, based on the flatness of its Gaussian.\n\nHopefully, it starting to look like things are coming together. A few more\ndetails should seal the deal:\n\n  1. We set \u03bc=0. This is a convenient assumption but works well in practice because (we shall see soon) the conditional distribution we want is strongly influenced by the actual training data. If you\u2019re interested, this discussion on StackExchange discusses the validity of such an assumption.\n  2. Lets start referring to the covariance matrix as the kernel matrix K to emphasize that it consists of similarity values. k() itself is known as the kernel function. To rephrase something we saw earlier, Kij=k(zi,zj), where z comes from one of X, X\u2217 or \u0303X, based on the positions i and j.\n  3. What is an example of a kernel function? A very common example is the Radial Basis Function (RBF) kernel k(zi,zj)=e\u2212||zi\u2212zj||2/(2L2). If zi and zj are close, say, ||zi\u2212zj||\u22480, then we\u2019ve k(zi,zj)\u22481 (for a given L). On the other hand, if ||zi\u2212zj||\u2192\u221e, then k(zi,zj)\u21920. This is how we would expect similarities to behave. Here L is known as the length scale, and we\u2019ll explain its role soon. Of course, there are many other kernels, but we\u2019ll use the RBF kernel in our examples here. For others, this is a good reference.\n\nCan any notion of similarity be written up as a kernel function? Unfortunately\nnot - the matrix K needs to be positive semi-definite. This is a property we\nwon\u2019t delve into here. This is not particularly restrictive for these reasons:\n(a) there are many useful similarity functions that follow this property, and\n(b) you can use such valid kernel functions to compose new ones in certain\nways that guarantee that the new kernel also possesses this property.\n\n  4. We\u2019ll think of K as being partitioned into four sections based on the data among which similarities are computed. For example, the partition KXX\u2217 implies this submatrix contains similarities between instances in X and X\u2217. This is shown below.\n\nPartitioning of the kernel matrix.\n\nNow we\u2019re ready to write down the exact form of the test distributions:\np(Y\u22171,...,Y\u2217M|Y1,...,YN)=N(KX\u2217XK\u22121XXy,KX\u2217X\u2217\u2212KX\u2217XK\u22121XXKXX\u2217)\n\nHere,\n\n  * The above expression is just the standard formula for the conditional distribution for a Gaussian.\n  * y is the vector of training outputs [y1,y2,...,yN].\n  * The new mean is KX\u2217XK\u22121XXy. This is a M-dimensional vector, where the ith value is the mean value for Yi. The different matrix sizes are visualized below.\n\n  * The new covariance matrix is KX\u2217X\u2217\u2212KX\u2217XK\u22121XXKXX\u2217.\n\nThese look complicated, but the important thing here is p(Y) can be computed\nin closed form! With just that equation. Think of what we\u2019re getting - an\nentire joint distribution over outputs Y1,Y2,...,YN.\n\nIf we now want to visualize this, we realize that the standard ways don\u2019t work\nbecause we don\u2019t have test outputs, but an entire joint distribution. A simple\nway out is to sample from the distribution p(Y\u22171,...,Y\u2217M|Y1,...,YN) for one\nrealization of the learned function. You can sample multiple times to obtain\ndifferent such realizations.\n\nThe image below shows this (zooming in recommended), given some training data\nand test input locations. It shows three realizations of p(Y). It also shows,\nin gory detail, what the kernel matrix looks like - note that its color coded\nto show interactions between X and X\u2217.\n\nThe big picture: given evaluated (red) points and points where we want to plot\n(blue), what the overall matrix K looks like. Note the color coding in K:\nvalues created using the red and blue values are in purple.\n\nSince we\u2019re effectively sampling realizations of the learned function, we\nconventionally say we\u2019re \u201csampling functions\u201d, and that the GP provides a\ndistribution over functions. There is nothing stopping us from having a lot of\nclosely-spaced test inputs, plotting against which actually lends credence to\nthat terminology. The image below shows this - we\u2019ve a lot of test inputs in\n[0,10], which are not explicitly marked to avoid cluttering. The training data\nis shown in red dots. This time I have also plotted the mean output values for\neach Yi and connected them with a dashed line - this acts like a standard\nregression curve.\n\nMultiple sampled functions. Evaluated locations act like constrictions.\n\nNOTE:\n\nObserve how the training data acts as \u201cconstrictions\u201d, i.e., it clamps down on\nvariability around it. Do you see why this is a good surrogate model for\nBayesOpt? This is the behavior we need!\n\nCan we control for how smooth we want the functions to be? One way to do it\nwould be pick an appropriate kernel. But even with an RBF kernel, you can\nexercise control using the length scale parameter L. Think about the role it\nplays in k(zi,zj)=e\u2212||zi\u2212zj||2/(2L2). A large value suppresses the difference\n||zi\u2212zj||2. This is to say, points that are both close and somewhat farther\napart, would end up with similarity scores \u22481. Since this means the\ncorresponding outputs should also be similar, such functions would seem less\n\u201cbendy\u201d, i.e., we won\u2019t see drastic changes over short spans. The following\nimages show the effect of changing the length scale (see the titles).\n\nEffect of the length scale - shown increasing from top to bottom. Lower length\nscales lead to functions that can 'bend' more.\n\nWe had briefly mentioned that the distribution for a single output random\nvariable Y\u2217i is also a Gaussian. Now that we can plot multiple values for an\ninput xi we can see that for ourselves - by constructing a KDE plot sideways\non its values. We show this KDE plot for a couple of locations below.\n\nThe empirical distribution of y shown as a KDE plot.\n\nSince we know the parameters of these Gaussians, an alternative way to\nvisualize them is to mark the points that are one standard deviation away from\nthe mean, and connect these for different Y\u2217i. If we denote the distribution\nparameters at a given x\u2217i as \u03bcx\u2217i and \u03c3x\u2217i, then we\u2019re drawing curves through\n(1) \u03bcx\u2217i, (2) \u03bcx\u2217i+\u03c3x\u2217i and (3) \u03bcx\u2217i\u2212\u03c3x\u2217i, as shown below. Indeed, this is the\nstandard way of visualizing these distributions.\n\nHow the bands are drawn.\n\nOf course, you can draw these bands for other multipliers of the standard\ndeviation, e.g., \u03bcx\u2217i+2\u03c3x\u2217i and \u03bcx\u2217i\u22122\u03c3x\u2217i. This is shown below.\n\nBands representing values at different standard deviations are shown.\n\nThis wraps up the overview of GPs. We\u2019ll dwell over it just a bit more before\nwe move on.\n\n### A Different Perspective\n\nIn the previous section we motivated GPs by tying in our requirements with\nproperties of the multivariate Gaussian. That might\u2019ve seemed a bit out of the\nleft field. But there is another way to look at GPs that is closer to familiar\nterritory.\n\nConsider Linear Regression for the case that we want to learn model parameters\nin a Bayesian fashion. As before, let test data be X\u2217\u2208RM\u00d7d, Y\u2217\u2208RM\u00d71. Let train\ndata be denoted by D. We assume a linear model where y\u2217i\u223cN(W[1...d]x\u2217i+W0,\u03c32).\nThis denotes that y\u2217i is almost linear in xi, i.e., y\u2217i=W[1...d]x\u2217i, but\nexhibits some perturbation or noise in the real world. This noisy linear\nrelationship is concisely represented using a Normal distribution with a mean\nthat linearly depends on x\u2217 with variance \u03c32. Effectively, every y\u2217 gets its\nown distribution, but the level of noise - captured by \u03c3 - is assumed fixed\nfor the data. We also assume the prior over weights p(W) is Gaussian. Here W\nis all weights, i.e., W=[W0,W1,...,Wd].\n\nIf we write down the expression for p(Y\u2217|X\u2217,D), we\u2019ll notice that it involves\na bunch of Gaussians that combine together to finally simplify into a\nGaussian. This is shown below.\n\np(Y\u2217|X\u2217,D)=\u222bWp(Y\u2217|X\u2217,W)p(W|D)dW=\u222bWprod.ofGauss.\u23dep(Y\u2217|X\u2217,W)Gaussian\u2190prod.ofGauss.\u23deprod.ofGauss.\u2190i.i.d.\u23dep(D|W)\u00d7Gaussianprior\u23dep(W)ZdW\u23dfGaussian\u2190marginalizationofaGaussian\n\nHere:\n\n  * p(D|W) is a product of Gaussians because we assume the data to be independent and identically distributed (iid). Which is to say p(D|W)=\u220fNi=1p(yi|xi,W). This results in a joint Gaussian distribution over Y1,Y2,...,YN. For the same reason, p(Y\u2217|X\u2217,W) is a Gaussian.\n\n  * Z is a normalizing factor. We don\u2019t require its exact value here.\n\nWhat we\u2019re eventually saying above is that the output distribution is jointly\nGaussian, much like the GP. And very similar to an earlier plot, we can\n\u201cvisually\u201d test this. In the following plot, I have drawn a bunch of lines,\nwhose parameters - slope m(=W1) and intercept c(=W0) - are drawn from a\nGaussian. For each of these lines, if I now determine the value of y\u2217i at\nx\u2217i=0, as per the linear model y\u2217i\u223cN(mx\u2217i+c,\u03c32) with a fixed \u03c3, and show their\nspread using a KDE plot (again, shown sideways) this is what we get:\n\nDistribution of outputs at 0 using various lines. The slopes and intercepts\ncome from a bivariate Gaussian - shown in the inset image.\n\nSo, really, this output joint being a Gaussian is not new. We just mightn\u2019t\nhave thought of Linear Regression (of all things!) in this manner. I think all\nof this nicely segues into GPs if you consider the following arguments:\n\n  * The output joint is a Gaussian - why not a directly model it?\n  * The effect of W here is to constrain the covariance of this distribution in certain ways, so that the model is linear. But if we\u2019re directly modeling the output joint distribution, we don\u2019t need these restrictions - lets just use a covariance/kernel matrix that faithfully expresses the similarity in inputs we want!\n\nIn a way, GPs \u201ccut out the middleman\u201d from the linear model, by replacing its\ncovariances with an arbitrarily expressive kernel matrix.\n\n### Minimal Code for GP\n\nGPs are conceptually unusual, but getting some code up and running is\nsurprisingly easy. This is because the output distribution comes from just one\nequation. I\u2019ll use the RBF kernel implementation in\nsklearn.metrics.pairwise.rbf_kernel. If we don\u2019t worry about learning the\nlength scale, and use the default, the following code is all you need for a\nfunctional GP.\n\n    \n    \n    import numpy as np from sklearn.metrics.pairwise import rbf_kernel as rbf from scipy.stats import multivariate_normal as mvn def gp_predict(X_train, y_train, X_test, num_sample_functions=10, return_params=False): # calculate the various partitions K_x_x = rbf(X_train, X_train) K_x_x_star = rbf(X_train, X_test) K_x_star_x = K_x_x_star.T K_x_star_x_star = rbf(X_test, X_test) # calculate the mean and cov. for the joint output distribution mean = K_x_star_x @ np.linalg.inv(K_x_x) @ y_train.reshape((-1, 1)) mean = mean.flatten() cov = K_x_star_x_star - K_x_star_x @ np.linalg.inv(K_x_x) @ K_x_x_star # sample some functions # y_star is an array of size num_sample_functions x M y_star = mvn.rvs(mean=mean, cov=cov, size=num_sample_functions) # That's it folks! if return_params: return y_star, mean, cov else: return y_star\n\nThis is it. Really. The above code is training, prediction and prediction\nprobabilities all rolled into one.\n\nSomething to note here is that because of practical issues with numerical\nstability, such as those that might arise in calculating the covariance\nmatrix, an alternative is to sample from the standard Normal (one with zero\nmean and unit variance) and adjust the output later (for an example, see how\nthe Cholesky decomposition is used here).\n\nDo we want to take it out for a spin? Of course we do! Let\u2019s generate some\ndata on a sine curve, and sample some functions.\n\n    \n    \n    # NOTE: please ensure previous imports and the gp_predict() function # are available to this code! from matplotlib import pyplot as plt num_train, num_test = 5, 100 X_train = np.linspace(0, 2*np.pi, num_train) y_train = np.sin(X_train) X_test = np.linspace(0, 2*np.pi, num_test) # we need to reshape the arrays since 2D inputs are expected multiple_y_star = gp_predict(X_train.reshape(-1, 1), y_train, X_test.reshape(-1, 1), num_sample_functions=10) # let's plot the sampled outputs for idx, y in enumerate(multiple_y_star): plt.plot(X_test, y, linewidth=0.75, label=f\"f_{idx+1}\") # let's also show the training data using a scatter plot plt.scatter(X_train, y_train, c='k', s=100, label='train') # let's now plot the true test function plt.plot(X_test, np.sin(X_test), linewidth=2, c='k', label=\"true fn\") plt.legend() plt.xlabel('x'); plt.ylabel('y') plt.show()\n\nThis is what we obtain:\n\nOutput from the above codes.\n\nThink of another model thats this concise but that affords us the same\nexpressivity as a GP - both in terms of the complexity of functions it can\nfit, and uncertainty quantification. I can think of k-Nearest Neighbors and\nLinear Regression as candidates for conciseness, but these are limited in\ntheir expressivity.\n\n### Summary and Challenges\n\nWe are finally at the end of our whirlwind tour of GPs. I think of them as a\nneat way to fit complex models and obtain uncertainty quantification. They can\nalso model noise in the observed data - something we haven\u2019t covered here. And\nthey\u2019re concise. So, why haven\u2019t they taken over the world? A big problem is\ntheir runtime.\n\nNote that GPs don\u2019t really have a training phase (at least that\u2019s we\u2019ll assume\nhere, but in practice, we do need to learn parameters such as the length\nscale) and keep around all the training data for test-time predictions.\nUnfortunately the volume of this data comes into play as expensive matrix\noperations at test-time. Recall these final parameters:\n\n  * Mean: KX\u2217XK\u22121XXy\n  * Covariance: KX\u2217X\u2217\u2212KX\u2217XK\u22121XXKXX\u2217\n\nThe inversion is probably the biggest culprit, since its runtime complexity is\nO(n3). You could make this a little faster (and numerically stable) using the\nCholesky decomposition (Algorithm 2.1 here); this reduces the constant factors\nin the runtime, is numerically stable, but is still O(n3). You might\nprecompute K\u22121XX - but you\u2019ll still need to keep the training data around (for\nKX\u2217X - so this is memory intensive) and, while the compute would then be\nlower, its now O(n2), which still is expensive for large training datasets.\n\nAs an aside, this is why GPs are non-parametric models. Contrast this with\nparametric models like Logistic Regression, where we distill (and then\ndiscard) the training data into a few parameters for use at test-time.\n\nThere has been lots of work on speeding up GPs, e.g., by using inducing\npoints, which are a small set of training points that produce approximately\nthe same results as the entire data (Wilson & Nickisch, 2015; Snelson &\nGhahramani, 2005). On a more pragmatic note though, the notion of what\u2019s a\n\u201clarge\u201d model wrt memory and compute has been challenged in the past few\nyears, by Deep Learning, and specifically by generative models like LLMs. It\nwould make sense to evaluate GPs for a task given the actual practical\nconstraints at hand.\n\nI\u2019ve listed some learning resources in the sequel post here.\n\nWe looked at one critical ingredient of BayesOpt here - surrogate models.\nWe\u2019ll discuss the other critical component - acquisition functions - in the\nnext post. That also contains a list of learning resources at the end. See you\nthere!\n\n## References\n\n  1. Turner, R., Eriksson, D., McCourt, M., Kiili, J., Laaksonen, E., Xu, Z., & Guyon, I. (2021). Bayesian Optimization is Superior to Random Search for Machine Learning Hyperparameter Tuning: Analysis of the Black-Box Optimization Challenge 2020. In H. J. Escalante & K. Hofmann (Eds.), Proceedings of the NeurIPS 2020 Competition and Demonstration Track (Vol. 133, pp. 3\u201326). PMLR. https://proceedings.mlr.press/v133/turner21a.html\n\n  2. White, C., Neiswanger, W., & Savani, Y. (2021). BANANAS: Bayesian Optimization with Neural Architectures for Neural Architecture Search. Proceedings of the AAAI Conference on Artificial Intelligence.\n\n  3. G\u00f3mez-Bombarelli, R., Wei, J. N., Duvenaud, D., Hern\u00e1ndez-Lobato, J. M., S\u00e1nchez-Lengeling, B., Sheberla, D., Aguilera-Iparraguirre, J., Hirzel, T. D., Adams, R. P., & Aspuru-Guzik, A. (2018). Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules. ACS Central Science, 4(2), 268\u2013276. https://doi.org/10.1021/acscentsci.7b00572\n\n  4. Boelrijk, J., Ensing, B., Forr\u00e9, P., & Pirok, B. W. J. (2023). Closed-loop automatic gradient design for liquid chromatography using Bayesian optimization. Analytica Chimica Acta, 1242, 340789. https://doi.org/https://doi.org/10.1016/j.aca.2023.340789\n\n  5. Ament, S., Witte, A., Garg, N., & Kusuma, J. (2023). Sustainable Concrete via Bayesian Optimization.\n\n  6. Mehta, V., Paria, B., Schneider, J., Neiswanger, W., & Ermon, S. (2022). An Experimental Design Perspective on Model-Based Reinforcement Learning. International Conference on Learning Representations. https://openreview.net/forum?id=0no8Motr-zO\n\n  7. Kirschner, J., Nonnenmacher, M., Mutn\u00fd, M., Krause, A., Hiller, N., Ischebeck, R., & Adelmann, A. (2019-11). Bayesian Optimisation for Fast and Safe Parameter Tuning of SwissFEL [Conference Paper]. In W. Decking, H. Sinn, G. Geloni, S. Schreiber, M. Marx, & V. R. W. Schaa (Eds.), FEL2019, Proceedings of the 39th International Free-Electron Laser Conference (pp. 707\u2013710). JACoW Publishing. https://doi.org/10.3929/ethz-b-000385955\n\n  8. Ghose, A., & Ravindran, B. (2020). Interpretability With Accurate Small Models. Frontiers in Artificial Intelligence, 3, 3. https://doi.org/10.3389/frai.2020.00003\n\n  9. Ghose, A., & Ravindran, B. (2019). Learning Interpretable Models Using an Oracle. CoRR, abs/1906.06852. http://arxiv.org/abs/1906.06852\n\n  10. Nguyen, E. T., & Ghose, A. (2023). Are Good Explainers Secretly Human-in-the-Loop Active Learners? AI&HCI Workshop at the 40th International Conference on Machine Learning, ICML. https://arxiv.org/abs/2306.13935\n\n  11. Solnik, B., Golovin, D., Kochanski, G., Karro, J. E., Moitra, S., & Sculley, D. (2017). Bayesian Optimization for a Better Dessert. Proceedings of the 2017 NIPS Workshop on Bayesian Optimization.\n\n  12. Bergstra, J., Bardenet, R., Bengio, Y., & K\u00e9gl, B. (2011). Algorithms for Hyper-parameter Optimization. Proceedings of the 24th International Conference on Neural Information Processing Systems, 2546\u20132554. http://dl.acm.org/citation.cfm?id=2986459.2986743\n\n  13. Neiswanger, W., Wang, K. A., & Ermon, S. (2021). Bayesian Algorithm Execution: Estimating Computable Properties of Black-box Functions Using Mutual Information. In M. Meila & T. Zhang (Eds.), Proceedings of the 38th International Conference on Machine Learning (Vol. 139, pp. 8005\u20138015). PMLR. https://proceedings.mlr.press/v139/neiswanger21a.html\n\n  14. Astudillo, R., & Frazier, P. I. (2022). Thinking inside the box: A tutorial on grey-box Bayesian optimization.\n\n  15. Makarova, A., Shen, H., Perrone, V., Klein, A., Faddoul, J. B., Krause, A., Seeger, M., & Archambeau, C. (2022). Automatic Termination for Hyperparameter Optimization. First Conference on Automated Machine Learning (Main Track).\n\n  16. Wang, Z., Shakibi, B., Jin, L., & Freitas, N. (2014). Bayesian Multi-Scale Optimistic Optimization. In S. Kaski & J. Corander (Eds.), Proceedings of the Seventeenth International Conference on Artificial Intelligence and Statistics (Vol. 33, pp. 1005\u20131014). PMLR. https://proceedings.mlr.press/v33/wang14d.html\n\n  17. Kim, J., & Choi, S. (2022). On Uncertainty Estimation by Tree-based Surrogate Models in Sequential Model-based Optimization . In G. Camps-Valls, F. J. R. Ruiz, & I. Valera (Eds.), Proceedings of The 25th International Conference on Artificial Intelligence and Statistics (Vol. 151, pp. 4359\u20134375). PMLR. https://proceedings.mlr.press/v151/kim22b.html\n\n  18. Snoek, J., Rippel, O., Swersky, K., Kiros, R., Satish, N., Sundaram, N., Patwary, M., Prabhat, M., & Adams, R. (2015). Scalable Bayesian Optimization Using Deep Neural Networks. In F. Bach & D. Blei (Eds.), Proceedings of the 32nd International Conference on Machine Learning (Vol. 37, pp. 2171\u20132180). PMLR. https://proceedings.mlr.press/v37/snoek15.html\n\n  19. Wilson, A., & Nickisch, H. (2015). Kernel Interpolation for Scalable Structured Gaussian Processes (KISS-GP). In F. Bach & D. Blei (Eds.), Proceedings of the 32nd International Conference on Machine Learning (Vol. 37, pp. 1775\u20131784). PMLR. https://proceedings.mlr.press/v37/wilson15.html\n\n  20. Snelson, E., & Ghahramani, Z. (2005). Sparse Gaussian Processes using Pseudo-inputs. In Y. Weiss, B. Sch\u00f6lkopf, & J. Platt (Eds.), Advances in Neural Information Processing Systems (Vol. 18). MIT Press. https://proceedings.neurips.cc/paper/2005/file/4491777b1aa8b5b32c2e8666dbe1a495-Paper.pdf\n\n## A Not-So Primordial Soup\n\n  * A Not-So Primordial Soup\n\n  * LinkedIn\n  * Quora\n\nMy thought-recorder.\n\n", "frontpage": false}
