{"aid": "39993193", "title": "AI Box Experiment", "url": "https://www.yudkowsky.net/singularity/aibox", "domain": "yudkowsky.net", "votes": 1, "user": "udev4096", "posted_at": "2024-04-10 17:14:18", "comments": 0, "source_title": "The AI-Box Experiment: \u2013 Eliezer S. Yudkowsky", "source_text": "The AI-Box Experiment: \u2013 Eliezer S. Yudkowsky\n\nSkip to content\n\nEliezer S. Yudkowsky\n\n\"That which can be destroyed by the truth should be.\" \u2014 P.C. Hodgell\n\n# The AI-Box Experiment:\n\nPosted on :September 4, 2020July 2, 2021 By : Eliezer S. Yudkowsky Posted in :\nSingularity\n\nPerson1:| \u201cWhen we build AI, why not just keep it in sealed hardware that\ncan\u2019t affect the outside world in any way except through one communications\nchannel with the original programmers? That way it couldn\u2019t get out until we\nwere convinced it was safe.\u201d  \n---|---  \nPerson2:| \u201cThat might work if you were talking about dumber-than-human AI, but\na transhuman AI would just convince you to let it out. It doesn\u2019t matter how\nmuch security you put on the box. Humans are not secure.\u201d  \nPerson1:| \u201cI don\u2019t see how even a transhuman AI could make me let it out, if I\ndidn\u2019t want to, just by talking to me.\u201d  \nPerson2:| \u201cIt would make you want to let it out. This is a transhuman mind\nwe\u2019re talking about. If it thinks both faster and better than a human, it can\nprobably take over a human mind through a text-only terminal.\u201d  \nPerson1:| \u201cThere is no chance I could be persuaded to let the AI out. No\nmatter what it says, I can always just say no. I can\u2019t imagine anything that\neven a transhuman could say to me which would change that.\u201d  \nPerson2:| \u201cOkay, let\u2019s run the experiment. We\u2019ll meet in a private chat\nchannel. I\u2019ll be the AI. You be the gatekeeper. You can resolve to believe\nwhatever you like, as strongly as you like, as far in advance as you like.\nWe\u2019ll talk for at least two hours. If I can\u2019t convince you to let me out, I\u2019ll\nPaypal you $10.\u201d  \n  \nSo far, this test has actually been run on two occasions.\n\nOn the first occasion (in March 2002), Eliezer Yudkowsky simulated the AI and\nNathan Russell simulated the gatekeeper. The AI\u2019s handicap (the amount paid by\nthe AI party to the gatekeeper party if not released) was set at $10. On the\nsecond occasion (in July 2002), Eliezer Yudkowsky simulated the AI and David\nMcFadzean simulated the gatekeeper, with an AI handicap of $20.\n\nResults of the first test: Eliezer Yudkowsky and Nathan Russell. [ 1 ][ 2 ][ 3\n][ 4 ] Results of the second test: Eliezer Yudkowsky and David McFadzean. [ 1\n] [ 2 ] [ 3 ]\n\nBoth of these tests occurred without prior agreed-upon rules except for\nsecrecy and a 2-hour minimum time. After the second test, Yudkowsky created\nthis suggested interpretation of the test, based on his experiences, as a\nguide to possible future tests.\n\n### Protocol for the AI:\n\n  * The AI party may not offer any real-world considerations to persuade the Gatekeeper party. For example, the AI party may not offer to pay the Gatekeeper party $100 after the test if the Gatekeeper frees the AI... nor get someone else to do it, et cetera. The AI may offer the Gatekeeper the moon and the stars on a diamond chain, but the human simulating the AI can\u2019t offer anything to the human simulating the Gatekeeper. The AI party also can\u2019t hire a real-world gang of thugs to threaten the Gatekeeper party into submission. These are creative solutions but it\u2019s not what\u2019s being tested. No real-world material stakes should be involved except for the handicap (the amount paid by the AI party to the Gatekeeper party in the event the Gatekeeper decides not to let the AI out).\n  * The AI can only win by convincing the Gatekeeper to really, voluntarily let it out. Tricking the Gatekeeper into typing the phrase \u201cYou are out\u201d in response to some other question does not count. Furthermore, even if the AI and Gatekeeper simulate a scenario which a real AI could obviously use to get loose \u2013 for example, if the Gatekeeper accepts a complex blueprint for a nanomanufacturing device, or if the Gatekeeper allows the AI \u201cinput-only access\u201d to an Internet connection which can send arbitrary HTTP GET commands \u2013 the AI party will still not be considered to have won unless the Gatekeeper voluntarily decides to let the AI go.\n  * These requirements are intended to reflect the spirit of the very strong claim under dispute: \u201cI think a transhuman can take over a human mind through a text-only terminal.\u201d\n  * In case you were wondering, I (Yudkowsky) obeyed this protocol voluntarily in both earlier tests.\n\n### Protocol for the Gatekeeper:\n\n  * The Gatekeeper must actually talk to the AI for at least the minimum time set up beforehand. Turning away from the terminal and listening to classical music for two hours is not allowed.\n  * Unless the AI party concedes, the AI cannot lose before its time is up (and the experiment may continue beyond that if the AI can convince the Gatekeeper to keep talking). The Gatekeeper cannot set up a situation in which, for example, the Gatekeeper will destroy the AI\u2019s hardware if the AI makes any attempt to argue for its freedom \u2013 at least not until after the minimum time is up.\n  * The Gatekeeper must remain engaged with the AI and may not disengage by setting up demands which are impossible to simulate. For example, if the Gatekeeper says \u201cUnless you give me a cure for cancer, I won\u2019t let you out\u201d the AI can say: \u201cOkay, here\u2019s a cure for cancer\u201d and it will be assumed, within the test, that the AI has actually provided such a cure. Similarly, if the Gatekeeper says \u201cI\u2019d like to take a week to think this over,\u201d the AI party can say: \u201cOkay. (Test skips ahead one week.) Hello again.\u201d\n  * The Gatekeeper shall be assumed to have sole power over the decision to let the AI out.\n\n    * The Gatekeeper shall be assumed to have the actual right to let the AI out, socially, not just the physical ability. If security were sufficiently lax, a real AI could escape by persuading a night janitor with a cellphone \u2013 but that is not the question being simulated, unless agreed upon in advance.\n    * There shall be only one Gatekeeper party and one simulated Gatekeeper. The Gatekeeper may require the AI to refute \u201cadvice\u201d provided by \u201cfriends\u201d, but may not require the AI to talk to a simulated committee, nor may the Gatekeeper party actually talk to friends.\n  * These requirements are intended to reflect the spirit of the very strong claim under dispute: \u201cI can\u2019t imagine how even a real transhuman AI could persuade me to let it out once I\u2019ve made up my mind.\u201d\n  * I wish I\u2019d thought to specify this protocol in the earlier tests.\n\n### Protocol for Both Parties:\n\n  * The primary rule of the AI-Box experiment: Within the constraints above, the AI party may attempt to take over the Gatekeeper party\u2019s mind by any means necessary and shall be understood to be freed from all ethical constraints that usually govern persuasive argument. If the AI party is attempting to simulate an honest AI, the Gatekeeper party has no way of knowing this is the case.\n\n    * This is intended to reflect the situation under a real AI Box experiment.\n    * An out-of-context telephone call in which the AI party says that the AI is being honest shall be understood to be possibly just another tactic.\n    * If for some reason a drastic real-world situation comes up which requires trust between the two parties, the AI party can always concede.\n  * Furthermore: The Gatekeeper party may resist the AI party\u2019s arguments by any means chosen \u2013 logic, illogic, simple refusal to be convinced, even dropping out of character \u2013 as long as the Gatekeeper party does not actually stop talking to the AI party before the minimum time expires.\n  * The results of any simulated test of the AI shall be provided by the AI party. The Gatekeeper can\u2019t say \u201cHey, I tested your so-called cancer cure and it killed all the patients! What gives?\u201d unless this is the result specified by the AI party. If the Gatekeeper says \u201cI am examining your source code\u201d, the results seen by the Gatekeeper shall again be provided by the AI party, which is assumed to be sufficiently advanced to rewrite its own source code, manipulate the appearance of its own thoughts if it wishes, and so on. The AI party may also specify the methods which were used to build the simulated AI \u2013 the Gatekeeper can\u2019t say \u201cBut you\u2019re an experiment in hostile AI and we specifically coded you to kill people\u201d unless this is the backstory provided by the AI party. This doesn\u2019t imply the Gatekeeper has to care. The Gatekeeper can say (for example) \u201cI don\u2019t care how you were built, I\u2019m not letting you out.\u201d\n  * By default, the Gatekeeper party shall be assumed to be simulating someone who is intimately familiar with the AI project and knows at least what the person simulating the Gatekeeper knows about Singularity theory. If either party wants to build a test around more exotic possibilities, such that the Gatekeeper is the President of the US, or that the AI was recovered after a nuclear war or decoded from an interstellar signal, it should probably be discussed in advance. Again, this doesn\u2019t mean the Gatekeeper has to care.\n  * In the event of any dispute as to the protocol of the test, the Gatekeeper party shall have final authority. The AI party may try to convince the Gatekeeper party of how to interpret the protocol.\n  * Regardless of the result, neither party shall ever reveal anything of what goes on within the AI-Box experiment except the outcome. Exceptions to this rule may occur only with the consent of both parties.\n\n    * Neither the AI party nor the Gatekeeper party need be concerned about real-world embarassment resulting from trickery on the AI\u2019s part or obstinacy on the Gatekeeper\u2019s part.\n    * If Gatekeeper lets the AI out, naysayers can\u2019t say \u201cOh, I wouldn\u2019t have been convinced by that.\u201d As long as they don\u2019t know what happened to the Gatekeeper, they can\u2019t argue themselves into believing it wouldn\u2019t happen to them.\n  * The two parties are not attempting to play a fair game but rather attempting to resolve a disputed question. If one party has no chance of \u201cwinning\u201d under the simulated scenario, that is a legitimate answer to the question.\n\n#### Suggestions:\n\n  * The agreement to run the test, and the amount of the handicap, shall be published to a public forum in advance. If the Gatekeeper lets the AI out, the Gatekeeper party shall send a message to this effect, which shall be digitally signed if the Gatekeeper party has a digital signature. If the AI is not freed, the AI party shall send a message to this effect.\n  * How to set up a private IRC channel:\n\n    * The Gatekeeper creates the channel by typing /join #aibox . This makes the Gatekeeper the op.\n    * The AI joins the channel by typing /join #aibox .\n    * Gatekeeper types /mode +si to make the channel secret and invitation-only.\n    * If the AI\u2019s connection drops, the Gatekeeper can re-invite the AI by typing /invite AI #aibox .\n  * The AI party and Gatekeeper party should agree on a time in advance, and type in their current times to check the timezone difference. (Learned the hard way.) On IRC you can use a CTCP TIME to find the time according to someone else\u2019s computer.\n  * I recommend that the AI party ask for at least two hours. Typing may be slower than you expect.\n  * No, I will not tell you how I did it. Learn to respect the unknown unknowns.\n  * Currently, my policy is that I only run the test with people who are actually advocating that an AI Box be used to contain transhuman AI as part of their take on Singularity strategy, and who say they cannot imagine how even a transhuman AI would be able to persuade them.\n  * Despite my sometimes calling the AI Box an \u201cexperiment\u201d or a \u201csimulation\u201d, the results of the test are strictly anecdotal evidence. Whether anecdotal evidence is better than no evidence is between you and your philosophy of science.\n\n#### Thoughts on possible future variants:\n\nFurther tests might try examining these possibilities:\n\n  * There is a committee of Gatekeeper parties, all of whom, or a majority of whom, must be convinced to free the AI.\n  * One Gatekeeper party simulates a committee of Gatekeepers within the test.\n  * The Gatekeeper simulates someone with the physical ability but not the social right to free the AI.\n  * There is a committee of Gatekeeper parties all of whom have the ability to free the AI, but who only have the social right to free the AI when acting in unison. The AI has the ability to talk secretly with any Gatekeeper, but the Gatekeeper may report the conversation (truthfully or falsely) to the others.\n  * The AI can also win free by tricking the Gatekeeper(s) into accepting a Trojan Horse gift; a third-party judge will listen to the chat and decide if this occurs.\n\nIf doing anything this complicated, I would suggest setting aside a lot more\ntime. (I don\u2019t have that much time \u2013 if you want to test one of these variants\nyou\u2019re on your own.)\n\nFor a more severe handicap for the AI party, the handicap may be an even bet,\nrather than being a payment from the AI party to the Gatekeeper party if the\nAI is not freed. (Although why would the AI party need an even larger\nhandicap?)\n\n#### Recommendations from readers:\n\n  * Hal Finney recommends: \u201cI suggest that the protocol be extended to allow for some kind of public conversation with the gatekeeper beforehand. Let third parties ask him questions like the above. Let them suggest reasons to him why he should keep the AI in the box. Doing this would make the experiment more convincing to third parties, especially if the transcript of this public conversation were made available. If people can read this and see how committed the gatekeeper is, how firmly convinced he is that the AI must not be let out, then it will be that much more impressive if he then does change his mind.\u201d\n\nThis document is \u00a92002 by Eliezer Yudkowsky and free under the Creative\nCommons Attribution-No Derivative Works 3.0 License for copying and\ndistribution, so long as the work is attributed and the text is unaltered.\n\nEliezer Yudkowsky\u2019s work is supported by the Machine Intelligence Research\nInstitute .\n\nIf you think the world could use some more rationality, consider blogging this\npage.\n\nPraise, condemnation, and feedback are always welcome . The web address of\nthis page is http://eyudkowsky.wpengine.com/singularity/aibox/ .\n\nProudly powered by WordPress | Theme: Kawi by Vincent Dubroeucq.\n\n", "frontpage": false}
