{"aid": "39988417", "title": "PullRequestBenchmark Challenge: Can AI Replace Your Dev Team?", "url": "https://github.com/mrconter1/PullRequestBenchmark", "domain": "github.com/mrconter1", "votes": 1, "user": "AIReach", "posted_at": "2024-04-10 08:46:47", "comments": 0, "source_title": "GitHub - mrconter1/PullRequestBenchmark: Evaluating LLMs performance in PR reviews as an indicator for their capability in creating PRs.", "source_text": "GitHub - mrconter1/PullRequestBenchmark: Evaluating LLMs performance in PR\nreviews as an indicator for their capability in creating PRs.\n\nSkip to content\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nmrconter1 / PullRequestBenchmark Public\n\n  * Notifications\n  * Fork 0\n  * Star 5\n\nEvaluating LLMs performance in PR reviews as an indicator for their capability\nin creating PRs.\n\n### License\n\nMIT license\n\n5 stars 0 forks Branches Tags Activity\n\nStar\n\nNotifications\n\n# mrconter1/PullRequestBenchmark\n\nThis commit does not belong to any branch on this repository, and may belong\nto a fork outside of the repository.\n\n1 Branch\n\n0 Tags\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\nRasmusLindahlRemove addressd718e25 \u00b7\n\n## History\n\n84 Commits  \n  \n### data\n\n|\n\n### data\n\n| Add eval samples from cli-spinners repo  \n  \n### pull_request_benchmark\n\n|\n\n### pull_request_benchmark\n\n| Add method to print estimated prompt sizes in tokens  \n  \n### .gitignore\n\n|\n\n### .gitignore\n\n| Enable pyllms dependency  \n  \n### CONTRIBUTING.md\n\n|\n\n### CONTRIBUTING.md\n\n| Add eval sample and update dataset structure documentation  \n  \n### LICENSE.md\n\n|\n\n### LICENSE.md\n\n| Add LICENSE.md  \n  \n### README.md\n\n|\n\n### README.md\n\n| Remove address  \n  \n### requirements.txt\n\n|\n\n### requirements.txt\n\n| Add requirements file  \n  \n## Repository files navigation\n\n# PullRequestBenchmark\n\nFellow software engineers, we are genuinely at the cusp of making ourselves\nobsolete through the power of automation! Can't wait to get there.\n\nThis effort might seem similar to SWE-bench at first glance, but there are key\ndifferences.\n\n## Expert PR reviewers are likely expert PR creators\n\nThe journey towards automating the programming profession as we know it today\nis both exciting and vitally important. This is the heart of our motivation\nfor leveraging Large Language Models (LLMs) to review PRs (Pull Requests) as a\nrepresentative step in this significant transition.\n\n  1. Assertion: Expertise in PR Review Capabilities Equates to Expertise in PR Creation Capability\n\na. Intuitive Argument (Competence Argument): If LLMs can expertly review and\nmake decisions on complex PRs\u2014covering major refactoring, architecture\nredesigns, large feature additions, intricate bug fixes, or advanced security\nmeasures\u2014it intuitively implies that they are also capable of creating complex\nhigh-quality PRs. The understanding, analysis, and judgment required to review\nsuch PRs would necessarily entail the ability to create them.\n\nb. Logical Argument (Bootstrap Argument): Leveraging LLMs' expert review\nskills, even initially basic PRs could be iteratively generated and refined.\nThis process of self-evaluation and improvement, akin to bootstrapping, could\npotentially lead to the creation of high-quality, human-level PRs over time.\n\n  2. Assertion: PR Review Skill Evaluation is Easier Than PR Creation Skill Assessment\n\nAssessing LLMs' PR review skills simplifies the evaluation process by allowing\ndirect comparisons to human expert responses, focusing on the nuanced\ndecision-making involved in software development.\n\n  3. Argument: Code Review Benchmarks Can Indicate Progress Toward Traditional Programming Role Obsolescence\n\nIf we accept the validity of both the first and second assertions, measuring\nthe code review capabilities of LLMs using benchmarks could help us track\nadvancements towards automating traditional programming roles. The ability of\nLLMs to generate complex pull requests autonomously would then imply a\npotential obsolescence of traditional programmer roles, signaling a\ntransformation in the software development industry.\n\n## Benchmark Format\n\n### Input to the Model\n\nModels are furnished with inputs akin to those a developer would consider,\nincluding:\n\n  * Entire Git History: Offering a lens into the project's evolution and coding standards.\n  * Pull Request Title and Description: Providing context and specifics of the proposed changes.\n  * Changeset: Detailing the exact additions, deletions, and modifications proposed.\n\n### Expected Output from the Model\n\nThe output is a binary decision, with the model delivering a verdict akin to a\nhuman reviewer's judgment:\n\n  * Approved: Signifies the PR aligns with project standards.\n  * Rejected: Indicates the PR falls short, with reasons for rejection articulated through specific feedback.\n\n## How to Contribute\n\nExtending the size of PullRequestBenchmark is greatly appreciated. Your\ncontributions play a vital role in this effort. Here's how you can help:\n\n  1. Identify Suitable Repositories: Start by locating repositories that align with the scope of our benchmark.\n  2. Find PRs: Within these repositories, search for either approved or rejected Pull Requests (PRs) that capture the essence of reviewing.\n  3. Format and Add Evaluation: Follow our guidelines to document the evaluation data point correctly and add them to our benchmark suite.\n\nFor detailed contribution guidelines, please refer to CONTRIBUTING.md.\n\n## Prompt Size and Distribution\n\nThe PullRequestBenchmark encompasses a wide range of prompt sizes, reflecting\nthe diversity of real-world software development scenarios. Prompt sizes,\nmeasured in tokens, vary significantly, with the smallest being around 125,506\ntokens and the largest exceeding 49,015,233 tokens. This variation underscores\nthe benchmark's comprehensive approach to evaluating LLMs across simple to\ncomplex pull request reviews. The distribution of prompt sizes illustrates the\nchallenges in software development tasks, ranging from minor code tweaks to\nsubstantial feature additions or optimizations.\n\n## License\n\nLicensed under the MIT License, PullRequestBenchmark encourages academic and\nresearch use, promoting advancements in AI for software development.\n\n## PullRequestBenchmark vs. SWE-bench\n\n  * Focus: PullRequestBenchmark evaluates LMs on reviewing pull requests to ensure they align with project standards. SWE-bench tests LMs' ability to solve real-world GitHub issues by generating fixes that pass unit tests.\n\n  * Objective: PullRequestBenchmark aims at decision-making in PR reviews, emphasizing quality assurance. SWE-bench focuses on problem-solving, specifically generating solutions for issues.\n\n  * Evaluation: PullRequestBenchmark assesses binary decision-making (approve or reject) on PRs. SWE-bench measures success by whether generated solutions resolve issues, validated through unit tests.\n\n  * Inputs: PullRequestBenchmark evaluates the reviewing of PRs, incorporating the full Git history and PR details to support these assessments. SWE-bench, focusing on code creation, considers only the current state of the repository through Issue-PR pairs.\n\n## About\n\nEvaluating LLMs performance in PR reviews as an indicator for their capability\nin creating PRs.\n\n### Topics\n\nnlp benchmark machine-learning llm\n\n### Resources\n\nReadme\n\n### License\n\nMIT license\n\nActivity\n\n### Stars\n\n5 stars\n\n### Watchers\n\n2 watching\n\n### Forks\n\n0 forks\n\nReport repository\n\n## Contributors 2\n\n  * RasmusLindahl\n  * mrconter1 Rasmus Lindahl\n\n## Languages\n\n  * Python 100.0%\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": false}
