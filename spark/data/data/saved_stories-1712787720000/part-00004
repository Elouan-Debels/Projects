{"aid": "39992482", "title": "An Introduction to the Node.js Performance API", "url": "https://betterstack.com/community/guides/scaling-nodejs/performance-apis/", "domain": "betterstack.com", "votes": 2, "user": "stanulilic", "posted_at": "2024-04-10 16:20:59", "comments": 0, "source_title": "An Introduction to the Node.js Performance API | Better Stack Community", "source_text": "An Introduction to the Node.js Performance API | Better Stack Community\n\nBack to Scaling Node.js Applications guides\n\n# An Introduction to the Node.js Performance API\n\nNode.js Performance\n\nStanley Ulili\n\nUpdated on April 9, 2024\n\n###### Contents\n\n  * Prerequisites\n  * Inside the Node.js Performance API\n  * Exploring the Node.js Performance APIs\n  * 1\\. PerformanceNodeTiming\n  * 2\\. Performance.now()\n  * 3\\. Performance.mark()\n  * 4\\. Performance.measure()\n  * 5\\. Performance.getEntries()\n  * 6\\. PerformanceObserver\n  * 7\\. Performance.clearMarks()\n  * 8\\. Performance.clearMeasures()\n  * 9\\. PerformanceResourceTiming\n  * Adjusting resource timing buffer sizes\n  * Final thoughts\n\nEver faced times when your application lags, impacting user experience with\nslow load times? Pinpointing the exact performance bottlenecks within your\napplication's code can often feel like searching for a needle in a haystack.\n\nThankfully, Node.js provides a powerful toolset for this challenge: the\nPerformance Measurement API, accessible through the perf_hooks module. This\nsuite of tools not only captures and stores performance metrics automatically\nbut also offers methods for actively measuring and enhancing your\napplication's performance.\n\nThrough this tutorial, you will learn to leverage these APIs for tracking\nvarious key metrics in your application.\n\n## Prerequisites\n\nTo follow through with this tutorial, you need basic knowledge of Node.js and\na recent version installed on your machine.\n\n## Inside the Node.js Performance API\n\nNode.js has adopted a subset of the Web Performance API in its perf_hooks\nmodule to streamline the process of tracking server performance. Its core\nelements are divided into four main categories:\n\n  1. High-resolution Time\n  2. Performance Timeline\n  3. User Timing\n  4. Resource Timing\n\nLet's begin by briefly exploring each component, starting with the High-\nresolution time:\n\n### 1\\. High-resolution Time\n\nPrior to the introduction of the Performance API in Node.js, developers often\nused Date.now() to track execution times of an operation. This method,\nhowever, is susceptible to system clock changes, which could affect the\naccuracy of the measurements:\n\nCopied!\n\n    \n    \n    const startTime = Date.now(); doTask(); // Example task console.log(`Task duration: ${Date.now() - startTime} milliseconds`);\n\nTo overcome this, Node.js now provides a High-resolution Time API that\nproduces a monotonic timestamp that is unaffected by clock adjustments. The\nmonotonic clock always progresses forward and cannot be adjusted backward:\n\nCopied!\n\n    \n    \n    import { performance } from \"node:perf_hooks\"; console.log(performance.now()); // produces a high resolution timestamp\n\nThe output from this snippet represents the milliseconds elapsed since the\nstart of the Node.js process till the console.log() statement:\n\nOutput\n\n    \n    \n    68.89512500003912\n\nTo differentiate further between Date.now() and performance.now(), here's a\nbrief comparison:\n\n-| Performance.now()| Date.now()  \n---|---|---  \nResolution| Sub-milliseconds| Milliseconds  \nOrigin| Performance.timeOrigin| Unix Epoch (January 1, 1970, UTC)  \nUses clock adjustments| No| Yes  \nMonotonically increasing| Yes| No  \n  \nNote that all the perf_hooks APIs use the high-resolution time for performance\nmeasurements so keep that in mind as we move forward with other examples.\n\n### 2\\. Performance Timeline\n\nThe Node.js performance timeline is designed to track performance metrics\nacross the entire lifespan of a program. These metrics are represented as\nsubclasses of the PerformanceEntry object, which is characterized by the\nfollowing key attributes:\n\n  * name: The identifier for the performance metric.\n  * duration: How long (in milliseconds) the metric took to complete.\n  * startTime: The high-resolution timestamp (in milliseconds) marking when the metric began.\n  * type: The category of the performance metric.\n\nPerformance measurements can be automatically entered in the timeline through\nthe Node.js process, APIs like fetch, or custom instrumentation. All entries\nin the timeline are derived from the PerformanceEntry class. Here's an\nexample:\n\nOutput\n\n    \n    \n    Performance { nodeTiming: PerformanceNodeTiming { name: 'node', entryType: 'node', startTime: 0, duration: 75.03541599959135, nodeStart: 20.790207999758422, v8Start: 35.35679099988192, bootstrapComplete: 60.899207999929786, environment: 47.57658299989998, loopStart: 67.81720799999312, loopExit: -1, idleTime: 0 }, timeOrigin: 1711344543959.874 }\n\nThis entry showcases the four core attributes of a PerformanceEntry instance\n\u2014name, duration, startTime, and type\u2014complemented by extra properties. You can\nview the Performance Timeline at any point in your program using\nperformance.getEntries().\n\n### 3\\. User Timing\n\nThe User Timing API allows custom metrics about your application to be added\nto the Node.js Performance Timeline. There are two types of user timings:\n\n#### 1\\. PerformanceMark\n\nPerformanceMark entries can be recorded at any point in your application using\nthe performance.mark() method:\n\nOutput\n\n    \n    \n    PerformanceMark { name: 'mark_fetch_start', entryType: 'mark', startTime: 21.876291000284255, duration: 0, detail: null }\n\n#### 2\\. PerformanceMeasure\n\nPerformanceMeasure allows you to measure the difference between two\nPerformanceMark entries in the timeline, creating a PerformanceMeasure object\nthat looks like this:\n\nOutput\n\n    \n    \n    PerformanceMeasure { name: 'measureTask', entryType: 'measure', startTime: 100, duration: 567.891 }\n\n### 4\\. Resource Timing\n\nResource Timing is the part of the Performance API that lets you retrieve\nnetwork timing data related to fetching resources. It provides detailed\ninformation about various stages involved in fetching a resource, such as the\ntime it takes for DNS lookup, establishing a connection, and the duration of\nthe request.\n\nEach resource timing is represented by a PerformanceResourceTiming entry that\nlooks like this:\n\nOutput\n\n    \n    \n    PerformanceResourceTiming { name: 'https://example.com/resource', entryType: 'resource', startTime: 50.125, duration: 2000.75, initiatorType: 'fetch', .... secureConnectionStart: 85, requestStart: 100, responseStart: 110, responseEnd: 2050.875, transferSize: 500, encodedBodySize: 200, decodedBodySize: 800 }\n\nWith some of the basic terms now understood, let's examine the specific\nperf_hooks APIs and methods more closely so that you're able to carry out\nperformance measurements in Node.js effectively.\n\n## Exploring the Node.js Performance APIs\n\nTo demonstrate the various APIs I'll be introducing in this section, we'll use\nthis simple function that returns a promise that resolves after a random\namount of time up to a specified maximum delay:\n\nrandom.js\n\nCopied!\n\n    \n    \n    function completeAfterRandomTime(maxDelay) { const delay = Math.random() * maxDelay; return new Promise((resolve) => { setTimeout(() => { resolve(`Completed after ${delay.toFixed(2)} milliseconds`); }, delay); }); } export { completeAfterRandomTime }\n\nThis helps simulate some work being done asynchronously, such as fetching data\nfrom a remote server so that you can apply various Performance API methods to\ntrack the performance of each invocation.\n\nBelow is a list of the API methods and interfaces we will cover:\n\n  1. PerformanceNodeTiming\n  2. Performance.now()\n  3. Performance.mark()\n  4. Performance.measure()\n  5. Performance.getEntries()\n  6. PerformanceObserver\n  7. Performance.clearMarks()\n  8. Performance.clearMeasures()\n  9. PerformanceResourceTiming\n  10. setResourceTimingBufferSize()\n\nLet's begin!\n\n## 1\\. PerformanceNodeTiming\n\nIn any Node.js application, the PerformanceNodeTiming object is the initial\nentry logged in the Performance Timeline. It offers insights into the app's\nstartup process and helps you identify early-stage performance issues by\ndetailing key phases such as the initialization of V8, bootstrapping, and\nperiods of idleness.\n\nTo inspect the PerformanceNodeTiming data, access the performance object from\nthe perf_hooks module as demonstrated below:\n\nCopied!\n\n    \n    \n    import { performance } from \"node:perf_hooks\"; console.log(performance);\n\nExecuting this will yield an output akin to:\n\nOutput\n\n    \n    \n    Performance { nodeTiming: PerformanceNodeTiming { name: 'node', entryType: 'node', startTime: 0, duration: 24.63241699989885, nodeStart: 1.9587079999037087, v8Start: 4.290624999906868, bootstrapComplete: 13.435583000071347, environment: 8.796583000104874, loopStart: 18.111583000048995, loopExit: -1, idleTime: 0 }, timeOrigin: 1711429082121.026 }\n\nThis nodeTiming object captures critical timings of the application's boot\nsequence, outlined as follows:\n\n  * name: Indicates the name, node in this case.\n  * entryType: Indicates the entry type set to node for Node.js processes.\n  * startTime: Marks the beginning of measurement with a high-resolution timestamp, with 0 denoting the beginning of the process execution.\n  * duration: The time taken from the startTime till the console.log() statement.\n  * nodeStart, v8Start, bootstrapComplete, environment, loopStart, and loopExit: These properties record various milestones in milliseconds, from process start through V8 initialization to the event loop's start and potential exit. loopExit is -1 because the event loop is still active at the time of recording.\n  * idleTime: Indicates how long the event loop was idle, starting at 0 if the event loop hasn't commenced.\n\nNow that you are familiar with the PerformanceNodeTiming entry, we will look\nat Performance.now() next.\n\n## 2\\. Performance.now()\n\nThe Performance.now() method enables precise measurement of task durations by\ngenerating high-resolution timestamps. You can use it directly in your code by\ncalling performance.now() but also note that all timestamps in\nPerformanceEntry objects are also generated internally through the now()\nmethod.\n\nTo measure the duration of a task, invoke the method at the start and end of\nthe operation you're measuring, then calculate the elapsed time by subtracting\nthese two values. Here's how it's applied:\n\nindex.js\n\nCopied!\n\n    \n    \n    import { performance } from 'node:perf_hooks'; import { completeAfterRandomTime } from './random.js'; const startTime = performance.now(); await completeAfterRandomTime(2000); const endTime = performance.now(); console.log(`Task completed in ${endTime - startTime} milliseconds.`);\n\nWhen executed, it outputs something like:\n\nOutput\n\n    \n    \n    Task completed in 1581.4101660000001 milliseconds\n\nWhile this method of time keeping suits many scenarios, the perf_hooks module\noffers even more versatile performance measurement tools, which we will\nexplore next.\n\n## 3\\. Performance.mark()\n\nThe performance.mark() method is used to place named markers in the\nPerformance Timeline. These markers facilitate the precise measurement of\ndurations between function calls and allow for their subsequent analysis\ndirectly from the timeline. Here's its signature:\n\nCopied!\n\n    \n    \n    performance.mark(name[, options])\n\nHere, name denotes the marker's identifier, and the optional options argument\nprovides additional context for the marker.\n\nConsider the example below:\n\nindex.js\n\nCopied!\n\n    \n    \n    import { performance } from 'node:perf_hooks'; import { completeAfterRandomTime } from './random.js'; const markStart = performance.mark('mark_function_start'); await completeAfterRandomTime(2000); const markEnd = performance.mark('mark_function_end'); console.log(markStart); console.log(markEnd);\n\nAfter you run the file, you will see an output that looks similar to this:\n\nOutput\n\n    \n    \n    PerformanceMark { name: 'mark_function_start', entryType: 'mark', startTime: 29.752355, duration: 0, detail: null } PerformanceMark { name: 'mark_function_end', entryType: 'mark', startTime: 1839.77119, duration: 0, detail: null }\n\nMarkers generated via performance.mark() include a startTime indicating when\nthey were placed in the Performance Timeline, which can later be used for\ncalculating the elapsed time for a task.\n\nAdditionally, you can enrich markers with context using the options parameter:\n\nindex.js\n\nCopied!\n\n    \n    \n    import { performance } from 'node:perf_hooks'; import { completeAfterRandomTime } from './random.js';\n    \n    const options = {\n    \n    detail: {\n    \n    functionName: 'completeAfterRandomTime',\n    \n    },\n    \n    };\n    \n    const markStart = performance.mark('mark_function_start', options);\n    \n    await completeAfterRandomTime(2000);\n    \n    const markEnd = performance.mark('mark_function_end', options);\n    \n    console.log(markStart); console.log(markEnd);\n\nThis yields markers enriched with further details:\n\nOutput\n\n    \n    \n    PerformanceMark { name: 'mark_function_start', entryType: 'mark', startTime: 30.379528, duration: 0, detail: { functionName: 'completeAfterRandomTime' } } PerformanceMark { name: 'mark_function_end', entryType: 'mark', startTime: 906.525729, duration: 0, detail: { functionName: 'completeAfterRandomTime' } }\n\nNext, we will delve into measuring the intervals between these markers.\n\n## 4\\. Performance.measure()\n\nPerformance.measure() is designed to generate a PerformanceMeasure object\nwithin the performance timeline that measures the period between two\npredefined PerformanceMark entries. Here's its signature:\n\nCopied!\n\n    \n    \n    performance.measure(name[, startMarkOrOptions[, endMark]])\n\nPassing just the name parameter measures the time from the program's start to\nthe method invocation:\n\nindex.js\n\nCopied!\n\n    \n    \n    import { performance } from \"node:perf_hooks\"; import { completeAfterRandomTime } from './random.js'; completeAfterRandomTime2000); console.log(performance.measure(\"programDuration\"));\n\nThis results in a PerformanceMeasure entry, capturing the duration since the\nprogram's start:\n\nOutput\n\n    \n    \n    PerformanceMeasure { name: 'programDuration', entryType: 'measure', startTime: 0, duration: 2014.819769 }\n\nFor measuring the interval between two markers, include their names as the\nsecond and third arguments, respectively:\n\nindex.js\n\nCopied!\n\n    \n    \n    import { performance } from 'node:perf_hooks'; import { completeAfterRandomTime } from './random.js'; const markStart = performance.mark('mark_function_start'); await completeAfterRandomTime(2000); const markEnd = performance.mark('mark_function_end'); console.log( performance.measure( 'measure_func_perf', 'mark_function_start', 'mark_function_end' ) );\n\nThe program then outputs a PerformanceMeasure reflecting the span between\nthese markers:\n\nOutput\n\n    \n    \n    PerformanceMeasure { name: 'measure_func_perf', entryType: 'measure', startTime: 29.751993, duration: 809.9894330000001 }\n\nHere, startTime indicates when the measurement commenced relative to the\nprogram's start (mark_function_start's startTime), and duration reveals the\nelapsed time between the markers. This feature offers a structured way to\npinpoint and assess performance metrics for specific code segments.\n\n## 5\\. Performance.getEntries()\n\nNow that you've learned about the mark() and measure() functions that add to\nthe Performance Timeline, it's time to see how we can retrieve these recorded\nentries. The performance.getEntries() method does just that, pulling\nPerformanceEntry objects from the timeline for review.\n\nConsider the following example:\n\nindex.js\n\nCopied!\n\n    \n    \n    import { performance } from 'node:perf_hooks'; import { completeAfterRandomTime } from './random.js'; const markStart = performance.mark('mark_function_start'); await completeAfterRandomTime(2000); const markEnd = performance.mark('mark_function_end'); performance.measure( 'measure_func_perf', 'mark_function_start', 'mark_function_end' ); console.log(performance.getEntries());\n\nYou will see an array of three entries representing the markers and their\nmeasurement:\n\nOutput\n\n    \n    \n    [ PerformanceMark { name: 'mark_function_start', entryType: 'mark', startTime: 29.210924, duration: 0, detail: null }, PerformanceMeasure { name: 'measure_func_perf', entryType: 'measure', startTime: 29.210924, duration: 1289.521015 }, PerformanceMark { name: 'mark_function_end', entryType: 'mark', startTime: 1318.731939, duration: 0, detail: null } ]\n\nIf your interest is in a specific entry, such as mark_function_start, the\nperformance.getEntriesByName() method allows for targeted retrieval:\n\nCopied!\n\n    \n    \n    console.log(performance.getEntriesByName('mark_function_start'));\n\nResulting in:\n\nOutput\n\n    \n    \n    [ PerformanceMark { name: 'mark_function_start', entryType: 'mark', startTime: 29.66662500053644, duration: 0, detail: null } ]\n\nAnother useful method is getEntriesByType(), which returns only the entries of\na certain type from the performance timeline such as marks or measures:\n\nCopied!\n\n    \n    \n    console.log(performance.getEntriesByType(\"mark\"));\n\n## 6\\. PerformanceObserver\n\nThe PerformanceObserver interface enhances how you work with the Node.js\nperformance timeline by offering a way to monitor performance metrics as\nthey're logged. This method is preferable to static retrieval functions like\ngetEntries() for two main reasons:\n\n  1. It provides real-time monitoring of the performance timeline, capturing entries as they happen, which is more efficient than querying the entire timeline repeatedly.\n\n  2. It allows for targeted observation, enabling you to specify exactly which types of entries to track, thereby optimizing resource use and avoiding the accumulation of unneeded data in memory.\n\nHere's a basic guide to utilizing a PerformanceObserver:\n\nCopied!\n\n    \n    \n    // Create a PerformanceObserver instance const observer = new PerformanceObserver((list, observer) => { for (const entry of list.getEntries()) { if (entry.name === '<your_entry_name>') { // Perform actions based on the measurement } } }); // Specify the entry types you want to observe observer.observe({ entryTypes: ['<entry_type>'] });\n\nIn this setup, you determine the types of performance entries you wish to\nobserve, such as mark or measure, and then process them within a callback\nfunction as they are recorded.\n\nHere is a more practical example that uses an observer:\n\nindex.js\n\nCopied!\n\n    \n    \n    import { performance, PerformanceObserver } from 'node:perf_hooks'; import { completeAfterRandomTime } from './random.js'; const observer = new PerformanceObserver((list, observer) => { for (const entry of list.getEntries()) { if (entry.name === 'measure_func_perf') { console.log(`Function execution time: ${entry.duration} milliseconds`); observer.disconnect(); } } }); observer.observe({ entryTypes: ['measure'] }); const markStart = performance.mark('mark_function_start'); await completeAfterRandomTime(2000); const markEnd = performance.mark('mark_function_end'); performance.measure( 'measure_func_perf', 'mark_function_start', 'mark_function_end' );\n\nThis practical implementation sets the observer to watch for measure entries,\nlogging the duration of a specified function once detected:\n\nCopied!\n\n    \n    \n    [output Function execution time: 7.202374999877065 milliseconds\n\nYou are not limited to observing performance entries of one type; you can\nspecify multiple entry types as needed:\n\nCopied!\n\n    \n    \n    observer.observe({ entryTypes: ['mark', 'measure'] });\n\nTo see the list of entry types supported by the observer, you can use:\n\nindex.js\n\nCopied!\n\n    \n    \n    import { PerformanceObserver } from \"node:perf_hooks\"; console.log(PerformanceObserver.supportedEntryTypes);\n\nOutput\n\n    \n    \n    [ 'dns', 'function', 'gc', 'http', 'http2', 'mark', 'measure', 'net', 'resource' ]\n\nNow that you are familiar with methods that help you inspect and add to the\nperformance timeline, the next step is to understand how to clear entries.\n\n## 7\\. Performance.clearMarks()\n\nIn long-running applications, you may need to reset the performance timeline\nso that it doesn't take too much memory. One of the methods you can use is\nperformance.clearMarks() function, which clears all PerformanceMark entries\nfrom the timeline:\n\nCopied!\n\n    \n    \n    performance.clearMarks();\n\nYou can also target and remove a specific mark by its name:\n\nCopied!\n\n    \n    \n    performance.clearMarks(\"mark_function_start\");\n\n## 8\\. Performance.clearMeasures()\n\nSimilarly, clearing PerformanceMeasure entries from the timeline is made easy\nwith:\n\nCopied!\n\n    \n    \n    performance.clearMeasures();\n\nSimilar to clearMarks(), you can also provide a name to delete a specific\nentry from the timeline.\n\n## 9\\. PerformanceResourceTiming\n\nEarlier in this article, we discussed the Resource Timing API which is used\nfor recording network-based metrics for requests made within the application.\nIf you're using the fetch API, you can inspect the PerformanceResourceTiming\nobjects created after each invocation like this:\n\nindex.js\n\nCopied!\n\n    \n    \n    import { performance } from 'node:perf_hooks'; const url = 'https://dummyjson.com/products/1'; const response = await fetch(url); await response.json(); console.log(performance.getEntriesByType('resource'));\n\nExecuting this program will yield:\n\nOutput\n\n    \n    \n    [ PerformanceResourceTiming { name: 'https://dummyjson.com/products/1', entryType: 'resource', startTime: 133.451, duration: 1240.7135, initiatorType: 'fetch', nextHopProtocol: undefined, workerStart: 0, redirectStart: 0, redirectEnd: 0, fetchStart: 133.451, domainLookupStart: undefined, domainLookupEnd: undefined, connectStart: undefined, connectEnd: undefined, secureConnectionStart: undefined, requestStart: 0, responseStart: 0, responseEnd: 1374.1645, transferSize: 300, encodedBodySize: 0, decodedBodySize: 0 } ]\n\nThe PerformanceResourceTiming object contains several key attributes:\n\n  * name: Identifies the resource's URL.\n  * entryType: Specifies the type of entry, which is resource for these objects.\n  * startTime: Records the precise moment the resource fetching commenced, represented in high-resolution milliseconds.\n  * duration: Reflects the total fetch time for the resource, calculated as the interval between responseEnd and startTime.\n  * initiatorType: Describes the method by which the resource fetch was initiated, such as \"fetch\".\n  * fetchStart: The exact timestamp marking the start of the resource fetch.\n  * responseEnd: The timestamp indicating the conclusion of the resource's response.\n  * transferSize: The total size of the resource received, in bytes, including the response headers and the payload.\n\nNote that this entry is only recorded after await response.json() is utilized.\n\n## Adjusting resource timing buffer sizes\n\nThe PerformanceResourceTiming object is automatically recorded for each fetch\nrequest and stored in a buffer limited to 250 entries by default. You may need\nto expand this buffer to accommodate the tracking of more network resources.\n\nTo modify the buffer size, use the setResourceTimingBufferSize() method as\nshown below:\n\nCopied!\n\n    \n    \n    performance.setResourceTimingBufferSize(600);\n\nThis adjustment allows for the inclusion of up to 600\nPerformanceResourceTiming objects within the performance timeline.\n\nAdditionally, you can clear all PerformanceResourceTiming entries from the\ntimeline with:\n\nCopied!\n\n    \n    \n    performance.clearResourceTimings();\n\nNode.js also emits a resourcetimingbufferfull event when the buffer hits its\nlimit. You can listen for this event to expand or clear the buffer as needed:\n\nCopied!\n\n    \n    \n    performance.on(\"resourcetimingbufferfull\", () => { if (performance.getEntriesByType(\"resource\").length < 900) { performance.setResourceTimingBufferSize(900); // Expanding the buffer if under a certain threshold } else { performance.clearResourceTimings(); // Clearing the buffer if the threshold is exceeded } });\n\n## Final thoughts\n\nIn this article, we explored the Performance Measurement APIs available in\nNode.js, and you learned how to use them to record various performance\nmetrics.\n\nStay tuned for our upcoming article, which will delve into practical\napplications of these API techniques in real-world scenarios.\n\nThanks for reading!\n\nArticle by\n\nStanley Ulili\n\nStanley is a freelance web developer and researcher from Malawi. He loves\nlearning new things and writing about them to understand and solidify\nconcepts. He hopes that by sharing his experience, others can learn something\nfrom them too!\n\nGot an article suggestion? Let us know\n\nNext article\n\nLoad Testing Node.js with Artillery: A Beginner's Guide\n\nObservability and monitoring reveals problems, but load testing with Artillery\ncan help you find them before they happen.\n\n\u2192\n\nThis work is licensed under a Creative Commons Attribution-NonCommercial-\nShareAlike 4.0 International License.\n\n  1. Basics\n\n    1. Express and Pug\n\n    2. Debugging Node.js Apps\n\n    3. Solutions to Common Node.js Errors\n\n  2. Job Scheduling\n\n    1. Job Scheduling in Node.js with Agenda\n\n    2. Job Scheduling in Node.js with BullMQ\n\n    3. Best Node Scheduling Libraries\n\n  3. High Performance & Scalability\n\n    1. Nginx Reverse Proxy for Node.js\n\n    2. Redis Caching in Node.js\n\n    3. Process Management with PM2\n\n    4. Using TypeScript with Node.js\n\n    5. Dockerizing Node.js Applications\n\n    6. Timeouts in Node.js\n\n    7. Debugging Node.js Memory Leaks\n\n    8. Introduction to Node.js Workers\n\n    9. Node.js Clustering\n\n    10. Node.js Performance API\n\n    11. Load Testing Node.js with Artillery\n\n  4. Logging\n\n    1. Logging in Nodejs\n\n    2. Logging with Pino\n\n    3. Logging with Winston\n\n    4. Node.js Logging Best Practices\n\n#### Make your mark\n\n##### Join the writer's program\n\nAre you a developer and love writing and sharing your knowledge with the\nworld? Join our guest writing program and get paid for writing amazing\ntechnical guides. We'll get them to the right readers that will appreciate\nthem.\n\nWrite for us\n\nWriter of the month\n\nMarin Bezhanov\n\nMarin is a software engineer and architect with a broad range of experience\nworking...\n\n##### Build on top of Better Stack\n\nWrite a script, app or project on top of Better Stack and share it with the\nworld. Make a public repository and share it with us at our email.\n\ncommunity@betterstack.com\n\nor submit a pull request and help us build better products for everyone.\n\nSee the full list of amazing projects on github\n\nBetter Stack lets you see inside any stack, debug any issue, and resolve any\nincident.\n\n+1 (201) 500-2007 hello@betterstack.com\n\nTerms of Use Privacy Policy GDPR System status\n\n\u00a9 2024 Better Stack, Inc.\n\n", "frontpage": false}
