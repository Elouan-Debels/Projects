{"aid": "40001392", "title": "Memory Ordering at Compile Time (2012)", "url": "https://preshing.com/20120625/memory-ordering-at-compile-time/", "domain": "preshing.com", "votes": 1, "user": "Tomte", "posted_at": "2024-04-11 12:27:47", "comments": 0, "source_title": "Memory Ordering at Compile Time", "source_text": "Memory Ordering at Compile Time\n\n# Preshing on Programming\n\nJun 25, 2012\n\n# Memory Ordering at Compile Time\n\nBetween the time you type in some C/C++ source code and the time it executes\non a CPU, the memory interactions of that code may be reordered according to\ncertain rules. Changes to memory ordering are made both by the compiler (at\ncompile time) and by the processor (at run time), all in the name of making\nyour code run faster.\n\nThe cardinal rule of memory reordering, which is universally followed by\ncompiler developers and CPU vendors, could be phrased as follows:\n\n> Thou shalt not modify the behavior of a single-threaded program.\n\nAs a result of this rule, memory reordering goes largely unnoticed by\nprogrammers writing single-threaded code. It often goes unnoticed in\nmultithreaded programming, too, since mutexes, semaphores and events are all\ndesigned to prevent memory reordering around their call sites. It\u2019s only when\nlock-free techniques are used \u2013 when memory is shared between threads without\nany kind of mutual exclusion \u2013 that the cat is finally out of the bag, and the\neffects of memory reordering can be plainly observed.\n\nMind you, it is possible to write lock-free code for multicore platforms\nwithout the hassles of memory reordering. As I mentioned in my introduction to\nlock-free programming, one can take advantage of sequentially consistent\ntypes, such as volatile variables in Java or C++11 atomics \u2013 possibly at the\nprice of a little performance. I won\u2019t go into detail about those here. In\nthis post, I\u2019ll focus on the impact of the compiler on memory ordering for\nregular, non-sequentially-consistent types.\n\n## Compiler Instruction Reordering\n\nAs you know, the job of a compiler is to convert human-readable source code\ninto machine-readable code for the CPU. During this conversion, the compiler\nis free to take many liberties.\n\nOnce such liberty is the reordering of instructions \u2013 again, only in cases\nwhere single-threaded program behavior does not change. Such instruction\nreordering typically happens only when compiler optimizations are enabled.\nConsider the following function:\n\n    \n    \n    int A, B; void foo() { A = B + 1; B = 0; }\n\nIf we compile this function using GCC 4.6.1 without compiler optimization, it\ngenerates the following machine code, which we can view as an assembly listing\nusing the -S option. The memory store to global variable B occurs right after\nthe store to A, just as it does in the original source code.\n\n    \n    \n    $ gcc -S -masm=intel foo.c $ cat foo.s ... mov eax, DWORD PTR _B (redo this at home...) add eax, 1 mov DWORD PTR _A, eax mov DWORD PTR _B, 0 ...\n\nCompare that to the resulting assembly listing when optimizations are enabled\nusing -O2:\n\n    \n    \n    $ gcc -O2 -S -masm=intel foo.c $ cat foo.s ... mov eax, DWORD PTR B mov DWORD PTR B, 0 add eax, 1 mov DWORD PTR A, eax ...\n\nThis time, the compiler has chosen to exercise its liberties, and reordered\nthe store to B before the store to A. And why shouldn\u2019t it? The cardinal rule\nof memory ordering is not broken. A single-threaded program would never know\nthe difference.\n\nOn the other hand, such compiler reorderings can cause problems when writing\nlock-free code. Here\u2019s a commonly-cited example, where a shared flag is used\nto indicate that some other shared data has been published:\n\n    \n    \n    int Value; int IsPublished = 0; void sendValue(int x) { Value = x; IsPublished = 1; }\n\nImagine what would happen if the compiler reordered the store to IsPublished\nbefore the store to Value. Even on a single-processor system, we\u2019d have a\nproblem: a thread could very well be pre-empted by the operating system\nbetween the two stores, leaving other threads to believe that Value has been\nupdated when in fact, it hasn\u2019t.\n\nOf course, the compiler might not reorder those operations, and the resulting\nmachine code would work fine as a lock-free operation on any multicore CPU\nhaving a strong memory model, such as an x86/64 \u2013 or in a single-processor\nenvironment, any type of CPU at all. If that\u2019s the case, we should consider\nourselves lucky. Needless to say, it\u2019s much better practice to recognize the\npossibility of memory reordering for shared variables, and to ensure that the\ncorrect ordering is enforced.\n\n## Explicit Compiler Barriers\n\nThe minimalist approach to preventing compiler reordering is by using a\nspecial directive known as a compiler barrier. I\u2019ve already demonstrated\ncompiler barriers in a previous post. The following is a full compiler barrier\nin GCC. In Microsoft Visual C++, _ReadWriteBarrier serves the same purpose.\n\n    \n    \n    int A, B; void foo() { A = B + 1; asm volatile(\"\" ::: \"memory\"); B = 0; }\n\nWith this change, we can leave optimizations enabled, and the memory store\ninstructions will remain in the desired order.\n\n    \n    \n    $ gcc -O2 -S -masm=intel foo.c $ cat foo.s ... mov eax, DWORD PTR _B add eax, 1 mov DWORD PTR _A, eax mov DWORD PTR _B, 0 ...\n\nSimilarly, if we want to guarantee our sendMessage example works correctly,\nand we only care about single-processor systems, then at an absolute minimum,\nwe must introduce compiler barriers here as well. Not only does the sending\noperation require a compiler barrier, to prevent the reordering of stores, but\nreceiving side needs one between the loads as well.\n\n    \n    \n    #define COMPILER_BARRIER() asm volatile(\"\" ::: \"memory\") int Value; int IsPublished = 0; void sendValue(int x) { Value = x; COMPILER_BARRIER(); // prevent reordering of stores IsPublished = 1; } int tryRecvValue() { if (IsPublished) { COMPILER_BARRIER(); // prevent reordering of loads return Value; } return -1; // or some other value to mean not yet received }\n\nAs I mentioned, compiler barriers are sufficient to prevent memory reordering\non a single-processor system. But it\u2019s 2012, and these days, multicore\ncomputing is the norm. If we want to ensure our interactions happen in the\ndesired order in a multiprocessor environment, and on any CPU architecture,\nthen a compiler barrier is not enough. We need either to issue a CPU fence\ninstruction, or perform any operation which acts as a memory barrier at\nruntime. I\u2019ll write more about those in the next post, Memory Barriers Are\nLike Source Control Operations.\n\nThe Linux kernel exposes several CPU fence instructions through preprocessor\nmacros such as smb_rmb, and those macros are reduced to simple compiler\nbarriers when compiling for a single-processor system.\n\n## Implied Compiler Barriers\n\nThere are other ways to prevent compiler reordering. Indeed, the CPU fence\ninstructions I just mentioned act as compiler barriers, too. Here\u2019s an example\nCPU fence instruction for PowerPC, defined as a macro in GCC:\n\n    \n    \n    #define RELEASE_FENCE() asm volatile(\"lwsync\" ::: \"memory\")\n\nAnywhere we place RELEASE_FENCE throughout our code, it will prevent certain\nkinds of processor reordering in addition to compiler reordering. For example,\nit can be used to make our sendValue function safe in a multiprocessor\nenvironment.\n\n    \n    \n    void sendValue(int x) { Value = x; RELEASE_FENCE(); IsPublished = 1; }\n\nIn the new C++11 (formerly known as C++0x) atomic library standard, every non-\nrelaxed atomic operation acts as a compiler barrier as well.\n\n    \n    \n    int Value; std::atomic<int> IsPublished(0); void sendValue(int x) { Value = x; // <-- reordering is prevented here! IsPublished.store(1, std::memory_order_release); }\n\nAnd as you might expect, every function containing a compiler barrier must act\nas a compiler barrier itself, even when the function is inlined. (However,\nMicrosoft\u2019s documentation suggests that may not have been the case in earlier\nversions of the Visual C++ compiler. Tsk, tsk!)\n\n    \n    \n    void doSomeStuff(Foo* foo) { foo->bar = 5; sendValue(123); // prevents reordering of neighboring assignments foo->bar2 = foo->bar; }\n\nIn fact, the majority of function calls act as compiler barriers, whether they\ncontain their own compiler barrier or not. This excludes inline functions,\nfunctions declared with the pure attribute, and cases where link-time code\ngeneration is used. Other than those cases, a call to an external function is\neven stronger than a compiler barrier, since the compiler has no idea what the\nfunction\u2019s side effects will be. It must forget any assumptions it made about\nmemory that is potentially visible to that function.\n\nWhen you think about it, this makes perfect sense. In the above code snippet,\nsuppose our implementation of sendValue exists in an external library. How\ndoes the compiler know that sendValue doesn\u2019t depend on the value of foo->bar?\nHow does it know sendValue will not modify foo->bar in memory? It doesn\u2019t.\nTherefore, to obey the cardinal rule of memory ordering, it must not reorder\nany memory operations around the external call to sendValue. Similarly, it\nmust load a fresh value for foo->bar from memory after the call completes,\nrather than assuming it still equals 5, even with optimization enabled.\n\n    \n    \n    $ gcc -O2 -S -masm=intel dosomestuff.c $ cat dosomestuff.s ... mov ebx, DWORD PTR [esp+32] mov DWORD PTR [ebx], 5 // Store 5 to foo->bar mov DWORD PTR [esp], 123 call sendValue // Call sendValue mov eax, DWORD PTR [ebx] // Load fresh value from foo->bar mov DWORD PTR [ebx+4], eax ...\n\nAs you can see, there are many instances where compiler instruction reordering\nis prohibited, and even when the compiler must reload certain values from\nmemory. I believe these hidden rules form a big part of the reason why people\nhave long been saying that volatile data types in C are not usually necessary\nin correctly-written multithreaded code.\n\n## Out-Of-Thin-Air Stores\n\nThink instruction reordering makes lock-free programming tricky? Before C++11\nwas standardized, there was technically no rule preventing the compiler from\ngetting up to even worse tricks. In particular, compilers were free to\nintroduce stores to shared memory in cases where there previously was none.\nHere\u2019s a very simplified example, inspired by the examples provided in\nmultiple articles by Hans Boehm.\n\n    \n    \n    int A, B; void foo() { if (A) B++; }\n\nThough it\u2019s rather unlikely in practice, nothing prevents a compiler from\npromoting B to a register before checking A, resulting in machine code\nequivalent to the following:\n\n    \n    \n    void foo() { register int r = B; // Promote B to a register before checking A. if (A) r++; B = r; // Surprise! A new memory store where there previously was none. }\n\nOnce again, the cardinal rule of memory ordering is still followed. A single-\nthreaded application would be none the wiser. But in a multithreaded\nenvironment, we now have a function which can wipe out any changes made\nconcurrently to B in other threads \u2013 even when A is 0. The original code\ndidn\u2019t do that. This type of obscure, technical non-impossibility is part of\nthe reason why people have been saying that C++ doesn\u2019t support threads,\ndespite the fact that we\u2019ve been happily writing multithreaded and lock-free\ncode in C/C++ for decades.\n\nI don\u2019t know anyone who ever fell victim to such \u201cout-of-thin-air\u201d stores in\npractice. Maybe it\u2019s just because for the type of lock-free code we tend to\nwrite, there aren\u2019t a whole lot of optimization opportunities fitting this\npattern. I suppose if I ever caught this type of compiler transformation\nhappening, I would search for a way to wrestle the compiler into submission.\nIf it\u2019s happened to you, let me know in the comments.\n\nIn any case, the new C++11 standard explictly prohibits such behavior from the\ncompiler in cases where it would introduce a data race. The wording can be\nfound in and around \u00a71.10.22 of the most recent C++11 working draft:\n\n> Compiler transformations that introduce assignments to a potentially shared\n> memory location that would not be modified by the abstract machine are\n> generally precluded by this standard.\n\n## Why Compiler Reordering?\n\nAs I mentioned at the start, the compiler modifies the order of memory\ninteractions for the same reason that the processor does it \u2013 performance\noptimization. Such optimizations are a direct consequence of modern CPU\ncomplexity.\n\nI may going out on a limb, but I somehow doubt that compilers did a whole lot\nof instruction reordering in the early 80\u2019s, when CPUs had only a few hundred\nthousand transistors at most. I don\u2019t think there would have been much point.\nBut since then, Moore\u2019s Law has provided CPU designers with about 10000 times\nthe number of transistors to play with, and those transistors have been spent\non tricks such as pipelining, memory prefetching, ILP and more recently,\nmulticore. As a result of some of those features, we\u2019ve seen architectures\nwhere the order of instructions in a program can make a significant difference\nin performance.\n\nThe first Intel Pentium released in 1993, with its so-called U and V-pipes,\nwas the first processor where I really remember people talking about\npipelining and the significance of instruction ordering. More recently,\nthough, when I step through x86 disassembly in Visual Studio, I\u2019m actually\nsurprised how little instruction reordering there is. On the other hand, out\nof the times I\u2019ve stepped through SPU disassembly on Playstation 3, I\u2019ve found\nthat the compiler really went to town. These are just anecdotal experiences;\nit may not reflect the experience of others, and certainly should not\ninfluence the way we enforce memory ordering in our lock-free code.\n\n\u00ab An Introduction to Lock-Free Programming Memory Barriers Are Like Source\nControl Operations \u00bb\n\nCheck out Plywood, a cross-platform, open source C++ framework:\n\n# Recent Posts\n\n  * How C++ Resolves a Function Call\n  * Flap Hero Code Review\n  * A Small Open Source Game In C++\n  * Automatically Detecting Text Encodings in C++\n  * I/O in Plywood\n  * A New Cross-Platform Open Source C++ Framework\n  * A Flexible Reflection System in C++: Part 2\n  * A Flexible Reflection System in C++: Part 1\n  * How to Write Your Own C++ Game Engine\n  * Can Reordering of Release/Acquire Operations Introduce Deadlock?\n  * Here's a Standalone Cairo DLL for Windows\n  * Learn CMake's Scripting Language in 15 Minutes\n  * How to Build a CMake-Based Project\n  * Using Quiescent States to Reclaim Memory\n  * Leapfrog Probing\n  * A Resizable Concurrent Map\n  * New Concurrent Hash Maps for C++\n  * You Can Do Any Kind of Atomic Read-Modify-Write Operation\n  * Safe Bitfields in C++\n  * Semaphores are Surprisingly Versatile\n\nCopyright \u00a9 2021 Jeff Preshing - Powered by Octopress\n\n", "frontpage": false}
