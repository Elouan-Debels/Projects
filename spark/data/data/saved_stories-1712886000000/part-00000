{"aid": "40005967", "title": "GCC -WTF: Optimizing my Ray Tracer", "url": "https://www.jakef.science/posts/gcc-wtf/", "domain": "jakef.science", "votes": 1, "user": "adpreese", "posted_at": "2024-04-11 19:37:57", "comments": 0, "source_title": "gcc -wtf: Optimizing my Ray Tracer, Part I", "source_text": "gcc -wtf: Optimizing my Ray Tracer, Part I - Jake's Blog\n\n# Jake's Blog\n\n22 Mar 2024\n\n# gcc -wtf: Optimizing my Ray Tracer, Part I\n\nInspired by others at the Recurse Center, I recently built a ray tracer by\nworking through the excellent book Ray Tracing in a Weekend. I\u2019ve been\nlearning C and was attracted to this project because it was complex enough to\nbe interesting but small enough to be time-constrained (my implementation is\n~1,000 lines of code); there\u2019s minimal dependencies so I can focus on learning\nC instead of learning APIs; and the end result is a pretty picture:\n\nCover image of Ray Tracing in a Weekend, rendered by my implementation.\n\nI was excited to create this magical, photorealistic image but was completely\nsurprised at how slow my program was. Rendering this image required 3+ hours\nof wall-clock time on my fancy new MacBook Pro.\n\nI didn\u2019t know what to expect, but that sure felt slow. I wanted to make it\nfaster. I pair programmed with a couple people and they quickly pointed out a\nvery simple solution: I was not passing any optimization level flags to the\ncompiler. We added -O2 to my Makefile, and my program was suddenly 15x faster.\n\nThis shocked me! The computer knew. It knew its default behavior was slow and\nshitty and it had a literal \u201cmake it go faster\u201d button. What was it doing? I\nhad a mental model of how my C code translated into low-level CPU operations,\nand I did not understand how something as seemingly benign as a compiler flag\ncould make such a huge difference. I wanted to understand. I did some\ngoogling, and chatGPT\u2019ing, and found vague answers referring to inlining and\nloop unrolling but I didn\u2019t have a strong intuition for how much that mattered\nor how they applied to my program. I found a list of the ~50 flags that go\ninto each optimization level, and I iteratively benchmarked my code with each\nflag turned on or off, but it didn\u2019t replicate the overall speedup (and I\nlater read that those flags only make up a subset of what -O2 does).\n\nSo I took things into my own hands. The first thing I did was look at my\nprogram\u2019s disassembly:\n\n    \n    \n    objdump -d my_ray_tracer | less\n\nThe optimized version was ~2200 lines of assembly while the unoptimized was\n~3500. 60% less code is significant! But also not 15x. So I opened it up and\ntried to read what it said.\n\nUnfortunately for me, I had never looked at arm64 assembly before. I felt I\nhad a reasonable grasp of assembly because I\u2019d recently spent 150 hours\nworking through a binary reverse engineering CTF (microcorruption, it\u2019s the\nbest), but this was foreign. I tried reading documentation but quickly got\nbored, so I instead took another cue from microcorruption and began to\ninteract with my program dynamically. I opened it up in lldb, a gdb-like\ndebugger for Mac. Microcorruption\u2019s UI was such an effective learning tool\nthat I wanted to replicate it, and I luckily found lldbinit, an lldb extension\nthat gives you a view of the disassembly, the original source code (if you\ncompile with debugging symbols), and the registers. As you step through\ninstruction-by-instruction, you can see which registers change and how.\n\nSnapshot of lldbinit, showing the registers, stack memory, and disassembly.\n\nI first focused on the smallest functions in my program, simple things that\nmultiply or subtract vectors, and stepped through the debugger for both\nversions. I expected these functions to purely consist of math operations, but\nthe unoptimized version was inundated with load and store instructions. I\nwasn\u2019t sure if this was widespread, or if these functions were called ten\ntimes or a billion times. I needed to collect more data.\n\nLuckily, lldb is completely scriptable. You can write Python code to plug into\na broad swath of its functionality. So I googled some more and copied some\nexample code and made a plugin that logged every executed instruction to a\nfile. I set breakpoints around the innermost loop of my ray tracer, propagated\na handful of rays in the optimized and unoptimized versions, and made a\nhistogram of the output.\n\nY-axis shows instruction, X-axis shows # of executions during 5 iterations of\nthe innermost loop. The unoptimized version is in blue, orange is the same\ncode compiled with -O2.\n\nIt turned out the unoptimized version was mostly doing loads and store\noperations (ldr, ldur, str, stur), 10x more frequently than the optimized\nversion. When I looked back at the disassembly, I could see this happening\neverywhere\u2014at the beginning and end of function calls, in between arithmetic\noperations, seemingly for no reason. Similarly, instructions associated with\nfunction calls (b, bl, ret) were 10x more frequent in the unoptimized version.\nWhile I knew \u201cfunction inlining\u201d was a thing and it made programs faster, I\nwas surprised by the sheer proportion of instructions that were apparently\nfiller.\n\nSome instructions were more common in the optimized version. I\u2019m not sure why\nthe optimized version is doing more floating-point math, but the higher\nprevalence of ldp instructions looked like a small win: instead of loading one\nvalue at a time, this instruction loads two values into a pair of registers at\nonce.\n\nWith this high-level understanding, I could now make more sense of the\nassembly. At a high-level, my ray tracer has four nested for-loops. The\ninnermost loop, a short math routine to calculate whether a ray hits a sphere,\nis executed a trillion times and dominates the run time. This is what it looks\nlike in C:\n\n    \n    \n    1 2 3 4 5 6 7 8 9 10 11 12\n\n|\n\n    \n    \n    point3_t center = sphere->center; double radius = sphere->radius; vec3_t a_c = subtract(r->origin, center); double a = length_squared(&r->direction); double half_b = dot(r->direction, a_c); double c = length_squared(&a_c) - radius*radius; double discriminant = half_b*half_b - a*c; if (discriminant < 0) { return false; } else { // do stuff }  \n  \n---|---  \n  \nHere is the disassembly (arm64, intel syntex) after compiling it with -O2:\n\n    \n    \n    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16\n\n|\n\n    \n    \n    ldp d25, d22, [x13, #-40]! ldp d23, d24, [x13, #16] fsub d19, d1, d25 fsub d27, d2, d22 fsub d28, d4, d23 fmul d26, d27, d6 fmadd d26, d5, d19, d26 fmadd d26, d0, d28, d26 fmul d27, d27, d27 fmadd d19, d19, d19, d27 fmadd d19, d28, d28, d19 fmsub d19, d24, d24, d19 fmul d19, d19, d16 fmadd d19, d26, d26, d19 fcmp d19, #0.0 b.mi 0x1000033c0 <_ray_color+0x88>  \n  \n---|---  \n  \nTo get oriented I\u2019ve highlighted the subtract call in both. This assembly is\nroughly how I imagined my C code was being executed: we load the sphere\u2019s\ncoordinates into registers (ldp), and we start subtracting and multiplying\nthose registers (fsub, fmul). Great.\n\nHowever, here is a snippet of assembly after compiling the same C code without\noptimization (the subtract() function call is again highlighted):\n\n    \n    \n    1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31\n\n|\n\n    \n    \n    sub sp, sp, #288 stp x20, x19, [sp, #256] stp x29, x30, [sp, #272] add x29, sp, #272 stur x0, [x29, #-32] stur x1, [x29, #-40] stur x2, [x29, #-48] stur x3, [x29, #-56] ldur x8, [x29, #-32] ldr q0, [x8] ldr x8, [x8, #16] stur x8, [x29, #-64] stur q0, [x29, #-80] ldur x8, [x29, #-32] ldr d0, [x8, #24] stur d0, [x29, #-88] ldur x8, [x29, #-40] ldr d2, [x8, #16] ldr d1, [x8, #8] ldr d0, [x8] ldur d5, [x29, #-64] ldur d4, [x29, #-72] ldur d3, [x29, #-80] bl 0x1000018dc <_subtract> stur d2, [x29, #-96] stur d1, [x29, #-104] stur d0, [x29, #-112] ldur x8, [x29, #-40] add x0, x8, #24 bl 0x1000017f8 <_length_squared> ; ...much more code below here...  \n  \n---|---  \n  \nThis is a mess! While the optimized version required 16 instructions for the\nentire routine, this version takes 24 just to get to the subtract call. Before\nthat we see a function prologue, a series of loads and stores of certain\nregisters. Inlining optimizes all of this away. Then there are more loads and\nstores until finally the subtract function is called (not inlined, so it will\nhave its own function prologue, etc.). And most interestingly, in between each\noperation (subtract(), length_squared()`) it stores the result to memory and\nloads the next value, which will be slow. When the compiler optimizes, it\nsomehow identifies the most-used variables and keeps them in registers,\navoiding unnecessary memory reads/writes.\n\nI later learned that clang is happy to tell you all about the optimizations it\ndoes if you pass it the flags -Rpass=.* and -fsave-optimization-record=yaml.\nBut this investigation gave me a much stronger intuition for what these\noptimizations do and why they work. And it also gave me ideas for more speed-\nups: I know SIMD is a thing, but I don\u2019t see it being used here (except\nperhaps the ldp instruction to load a pair of values?).\n\nCan we use SIMD to make this go faster?\n\n", "frontpage": false}
