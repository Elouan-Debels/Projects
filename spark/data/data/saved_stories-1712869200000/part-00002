{"aid": "40002896", "title": "Elicit Machine Learning Reading List", "url": "https://github.com/elicit/machine-learning-list", "domain": "github.com/elicit", "votes": 1, "user": "stuhlmueller", "posted_at": "2024-04-11 14:57:44", "comments": 0, "source_title": "GitHub - elicit/machine-learning-list", "source_text": "GitHub - elicit/machine-learning-list\n\nSkip to content\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nelicit / machine-learning-list Public\n\n  * Notifications\n  * Fork 0\n  * Star 6\n\nelicit.com/careers\n\n6 stars 0 forks Branches Tags Activity\n\nStar\n\nNotifications\n\n# elicit/machine-learning-list\n\nThis commit does not belong to any branch on this repository, and may belong\nto a fork outside of the repository.\n\n1 Branch\n\n0 Tags\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\nstuhlmuellerUpdate ToC4003af2 \u00b7\n\n## History\n\n7 Commits  \n  \n### README.md\n\n|\n\n### README.md\n\n| Update ToC  \n  \n## Repository files navigation\n\n# Elicit Machine Learning Reading List\n\n## Purpose\n\nThe purpose of this curriculum is to help new Elicit employees learn\nbackground in machine learning, with a focus on language models. I\u2019ve tried to\nstrike a balance between papers that are relevant for deploying ML in\nproduction and techniques that matter for longer-term scalability.\n\nIf you don\u2019t work at Elicit yet - we\u2019re hiring ML and software engineers.\n\n## How to read\n\nRecommended reading order:\n\n  1. Read \u201cTier 1\u201d for all topics\n  2. Read \u201cTier 2\u201d for all topics\n  3. Etc\n\n\u2728 Added after 2024/4/1\n\n## Table of contents\n\n  * Fundamentals\n\n    * Introduction to machine learning\n    * Transformers\n    * Key foundation model architectures\n    * Training and finetuning\n  * Reasoning and runtime strategies\n\n    * In-context reasoning\n    * Task decomposition\n    * Debate\n    * Tool use and scaffolding\n    * Honesty, factuality, and epistemics\n  * Applications\n\n    * Science\n    * Forecasting\n    * Search and ranking\n  * ML in practice\n\n    * Production deployment\n    * Benchmarks\n    * Datasets\n  * Advanced topics\n\n    * World models and causality\n    * Planning\n    * Uncertainty, calibration, and active learning\n    * Interpretability and model editing\n    * Reinforcement learning\n  * The big picture\n\n    * AI scaling\n    * AI safety\n    * Economic and social impacts\n    * Philosophy\n  * Maintainer\n\n## Fundamentals\n\n### Introduction to machine learning\n\nTier 1\n\n  * A short introduction to machine learning\n  * But what is a neural network?\n  * Gradient descent, how neural networks learn\n\nTier 2\n\n  * What is backpropagation really doing?\n  * An introduction to deep reinforcement learning\n\nTier 3\n\n  * The spelled-out intro to neural networks and backpropagation: building micrograd\n  * Backpropagation calculus\n\n### Transformers\n\nTier 1\n\n  * \u2728 But what is a GPT? Visual intro to transformers\n  * \u2728 Attention in transformers, visually explained\n  * the transformer ... \u201cexplained\u201d?\n  * The Illustrated Transformer\n  * The Illustrated GPT-2 (Visualizing Transformer Language Models)\n\nTier 2\n\n  * The Annotated Transformer\n  * Attention Is All You Need\n  * A Practical Survey on Faster and Lighter Transformers\n\nTier 3\n\n  * TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second\n  * Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets\n  * A Mathematical Framework for Transformer Circuits\n\n### Key foundation model architectures\n\nTier 1\n\n  * Language Models are Unsupervised Multitask Learners (GPT-2)\n  * Language Models are Few-Shot Learners (GPT-3)\n\nTier 2\n\n  * \u2728 LLaMA: Open and Efficient Foundation Language Models (LLaMA)\n  * \u2728 Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Mamba)\n  * Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (T5)\n  * Evaluating Large Language Models Trained on Code (OpenAI Codex)\n  * Training language models to follow instructions with human feedback (OpenAI Instruct)\n\nTier 3\n\n  * \u2728 Mistral 7B (Mistral)\n  * \u2728 Mixtral of Experts (Mixtral)\n  * \u2728 Gemini: A Family of Highly Capable Multimodal Models (Gemini)\n  * \u2728 Textbooks Are All You Need II: phi-1.5 technical report (phi 1.5)\n  * Scaling Instruction-Finetuned Language Models (Flan)\n\n### Training and finetuning\n\nTier 2\n\n  * \u2728 Tensor Programs V: Tuning Large Neural Networks via Zero-Shot Hyperparameter Transfer\n  * Learning to summarise with human feedback\n  * Training Verifiers to Solve Math Word Problems\n\nTier 3\n\n  * \u2728 Pretraining Language Models with Human Preferences\n  * \u2728 Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision\n  * Few-Shot Parameter-Efficient Fine-Tuning is Better and Cheaper than In-Context Learning\n  * LoRA: Low-Rank Adaptation of Large Language Models\n  * Unsupervised Neural Machine Translation with Generative Language Models Only\n\n## Reasoning and runtime strategies\n\n### In-context reasoning\n\nTier 2\n\n  * Chain of Thought Prompting Elicits Reasoning in Large Language Models\n  * Large Language Models are Zero-Shot Reasoners (Let's think step by step)\n  * Self-Consistency Improves Chain of Thought Reasoning in Language Models\n\nTier 3\n\n  * \u2728 Chain-of-Thought Reasoning Without Prompting\n  * \u2728 Why think step-by-step? Reasoning emerges from the locality of experience\n\n### Task decomposition\n\nTier 1\n\n  * Supervise Process, not Outcomes\n  * Supervising strong learners by amplifying weak experts\n\nTier 2\n\n  * \u2728 Tree of Thoughts: Deliberate Problem Solving with Large Language Models\n  * Factored cognition\n  * Iterated Distillation and Amplification\n  * Recursively Summarizing Books with Human Feedback\n  * Solving math word problems with process-based and outcome-based feedback\n\nTier 3\n\n  * Evaluating Arguments One Step at a Time\n  * Faithful Reasoning Using Large Language Models\n  * Humans consulting HCH\n  * Iterated Decomposition: Improving Science Q&A by Supervising Reasoning Processes\n  * Language Model Cascades\n\n### Debate\n\nTier 2\n\n  * AI safety via debate\n\nTier 3\n\n  * \u2728 Debate Helps Supervise Unreliable Experts\n  * Two-Turn Debate Doesn\u2019t Help Humans Answer Hard Reading Comprehension Questions\n\n### Tool use and scaffolding\n\nTier 2\n\n  * \u2728 Measuring the impact of post-training enhancements\n  * WebGPT: Browser-assisted question-answering with human feedback\n\nTier 3\n\n  * \u2728 AI capabilities can be significantly improved without expensive retraining\n  * \u2728 Automated Statistical Model Discovery with Language Models\n\n### Honesty, factuality, and epistemics\n\nTier 2\n\n  * \u2728 Self-critiquing models for assisting human evaluators\n\nTier 3\n\n  * \u2728 What Evidence Do Language Models Find Convincing?\n  * \u2728 How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions\n\n## Applications\n\n### Science\n\nTier 3\n\n  * \u2728 Can large language models provide useful feedback on research papers? A large-scale empirical analysis\n  * \u2728 Large Language Models Encode Clinical Knowledge\n  * \u2728 The Impact of Large Language Models on Scientific Discovery: a Preliminary Study using GPT-4\n  * A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers\n\n### Forecasting\n\nTier 3\n\n  * \u2728 AI-Augmented Predictions: LLM Assistants Improve Human Forecasting Accuracy\n  * \u2728 Approaching Human-Level Forecasting with Language Models\n  * \u2728 Are Transformers Effective for Time Series Forecasting?\n  * Forecasting Future World Events with Neural Networks\n\n### Search and ranking\n\nTier 2\n\n  * Learning Dense Representations of Phrases at Scale\n  * Text and Code Embeddings by Contrastive Pre-Training (OpenAI embeddings)\n\nTier 3\n\n  * \u2728 Large Language Models are Effective Text Rankers with Pairwise Ranking Prompting\n  * Not All Vector Databases Are Made Equal\n  * REALM: Retrieval-Augmented Language Model Pre-Training\n  * Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\n  * Task-aware Retrieval with Instructions\n\n## ML in practice\n\n### Production deployment\n\nTier 1\n\n  * Machine Learning in Python: Main developments and technology trends in data science, machine learning, and AI\n  * Machine Learning: The High Interest Credit Card of Technical Debt\n\nTier 2\n\n  * \u2728 Designing Data-Intensive Applications\n  * A Recipe for Training Neural Networks\n\n### Benchmarks\n\nTier 2\n\n  * \u2728 GPQA: A Graduate-Level Google-Proof Q&A Benchmark\n  * \u2728 SWE-bench: Can Language Models Resolve Real-World GitHub Issues?\n  * TruthfulQA: Measuring How Models Mimic Human Falsehoods\n\nTier 3\n\n  * FLEX: Unifying Evaluation for Few-Shot NLP\n  * Holistic Evaluation of Language Models (HELM)\n  * Measuring Massive Multitask Language Understanding\n  * RAFT: A Real-World Few-Shot Text Classification Benchmark\n  * True Few-Shot Learning with Language Models\n\n### Datasets\n\nTier 2\n\n  * Common Crawl\n  * The Pile: An 800GB Dataset of Diverse Text for Language Modeling\n\nTier 3\n\n  * Dialog Inpainting: Turning Documents into Dialogs\n  * MS MARCO: A Human Generated MAchine Reading COmprehension Dataset\n  * Microsoft Academic Graph\n  * TLDR9+: A Large Scale Resource for Extreme Summarization of Social Media Posts\n\n## Advanced topics\n\n### World models and causality\n\nTier 3\n\n  * \u2728 Emergent World Representations: Exploring a Sequence Model Trained on a Synthetic Task\n  * \u2728 From Word Models to World Models: Translating from Natural Language to the Probabilistic Language of Thought\n  * Language Models Represent Space and Time\n\n### Planning\n\n  * \u2728 Beyond A*: Better Planning with Transformers via Search Dynamics Bootstrapping\n  * \u2728 Cognitive Architectures for Language Agents\n\n### Uncertainty, calibration, and active learning\n\nTier 2\n\n  * \u2728 Experts Don't Cheat: Learning What You Don't Know By Predicting Pairs\n  * A Simple Baseline for Bayesian Uncertainty in Deep Learning\n  * Plex: Towards Reliability using Pretrained Large Model Extensions\n\nTier 3\n\n  * \u2728 Active Preference Inference using Language Models and Probabilistic Reasoning\n  * \u2728 Eliciting Human Preferences with Language Models\n  * Active Learning by Acquiring Contrastive Examples\n  * Describing Differences between Text Distributions with Natural Language\n  * Teaching Models to Express Their Uncertainty in Words\n\n### Interpretability and model editing\n\nTier 2\n\n  * Discovering Latent Knowledge in Language Models Without Supervision\n\nTier 3\n\n  * \u2728 Interpretability at Scale: Identifying Causal Mechanisms in Alpaca\n  * \u2728 Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks\n  * \u2728 Representation Engineering: A Top-Down Approach to AI Transparency\n  * \u2728 Studying Large Language Model Generalization with Influence Functions\n  * Interpretability in the Wild: a Circuit for Indirect Object Identification in GPT-2 small\n\n### Reinforcement learning\n\nTier 2\n\n  * \u2728 Direct Preference Optimization: Your Language Model is Secretly a Reward Model\n  * \u2728 Reflexion: Language Agents with Verbal Reinforcement Learning\n  * Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm (AlphaZero)\n  * MuZero: Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model\n\nTier 3\n\n  * \u2728 Open Problems and Fundamental Limitations of Reinforcement Learning from Human Feedback\n  * AlphaStar: mastering the real-time strategy game StarCraft II\n  * Decision Transformer\n  * Mastering Atari Games with Limited Data (EfficientZero)\n  * Mastering Stratego, the classic game of imperfect information (DeepNash)\n\n## The big picture\n\n### AI scaling\n\nTier 1\n\n  * Scaling Laws for Neural Language Models\n  * Takeoff speeds\n  * The Bitter Lesson\n\nTier 2\n\n  * AI and compute\n  * Scaling Laws for Transfer\n  * Training Compute-Optimal Large Language Models (Chinchilla)\n\nTier 3\n\n  * Emergent Abilities of Large Language Models\n  * Transcending Scaling Laws with 0.1% Extra Compute (U-PaLM)\n\n### AI safety\n\nTier 1\n\n  * Three impacts of machine intelligence\n  * What failure looks like\n  * Without specific countermeasures, the easiest path to transformative AI likely leads to AI takeover\n\nTier 2\n\n  * \u2728 An Overview of Catastrophic AI Risks\n  * Clarifying \u201cWhat failure looks like\u201d (part 1)\n  * Deep RL from human preferences\n  * The alignment problem from a deep learning perspective\n\nTier 3\n\n  * \u2728 Scheming AIs: Will AIs fake alignment during training in order to get power?\n  * Measuring Progress on Scalable Oversight for Large Language Models\n  * Risks from Learned Optimization in Advanced Machine Learning Systems\n  * Scalable agent alignment via reward modelling\n\n### Economic and social impacts\n\nTier 3\n\n  * \u2728 Explosive growth from AI automation: A review of the arguments\n  * \u2728 Language Models Can Reduce Asymmetry in Information Markets\n\n### Philosophy\n\nTier 2\n\n  * Meaning without reference in large language models\n\n## Maintainer\n\nandreas@elicit.com\n\n## About\n\nelicit.com/careers\n\n### Topics\n\nmachine-learning transformers artificial-intelligence language-model\n\n### Resources\n\nReadme\n\nActivity\n\nCustom properties\n\n### Stars\n\n6 stars\n\n### Watchers\n\n3 watching\n\n### Forks\n\n0 forks\n\nReport repository\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": false}
