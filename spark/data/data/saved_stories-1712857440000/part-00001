{"aid": "40001100", "title": "Microsoft pitched OpenAI's DALL-E as battlefield tool for U.S. Military", "url": "https://theintercept.com/2024/04/10/microsoft-openai-dalle-ai-military-use/", "domain": "theintercept.com", "votes": 3, "user": "geox", "posted_at": "2024-04-11 11:42:47", "comments": 0, "source_title": "Microsoft Pitched OpenAI\u2019s DALL-E as Battlefield Tool for U.S. Military", "source_text": "Microsoft Pitched OpenAI\u2019s DALL-E as Battlefield Tool for U.S. Military\n\n  * Politics\n  * Justice\n  * National Security\n  * World\n  * Technology\n  * Environment\n\nSupport Us\n\n  * Special Investigations\n  * Voices\n  * Podcasts\n  * Videos\n  * Documents\n\n  * About\n  * More Ways to Donate\n  * Impact & Reports\n  * Join Newsletter\n  * Become a Source\n\n\u00a9 THE INTERCEPT\n\nALL RIGHTS RESERVED\n\nTerms of Use Privacy\n\nSupport Us\n\nIn this photo illustration, a DALL-E 2 software logo is seen on a smartphone\nscreen in Ukraine on Feb. 21, 2023. Photo Illustration by Pavlo GoncharSOPA\nImages/LightRocket via Getty Images\n\n# Microsoft Pitched OpenAI\u2019s DALL-E as Battlefield Tool for U.S. Military\n\nAny battlefield use of the software would be a dramatic turnaround for OpenAI,\nwhich describes its mission as developing AI that can benefit all of humanity.\n\nSam Biddle\n\nApril 10 2024, 8:00 a.m.\n\nSupport Us\n\nIn this photo illustration, a DALL-E 2 software logo is seen on a smartphone\nscreen in Ukraine on Feb. 21, 2023. Photo Illustration by Pavlo GoncharSOPA\nImages/LightRocket via Getty Images\n\n  * Share on Facebook\n  * Share on X\n  * Share on LinkedIn\n  * Share on WhatsApp\n\nMicrosoft last year proposed using OpenAI\u2019s mega-popular image generation\ntool, DALL-E, to help the Department of Defense build software to execute\nmilitary operations, according to internal presentation materials reviewed by\nThe Intercept. The revelation comes just months after OpenAI silently ended\nits prohibition against military work.\n\nThe Microsoft presentation deck, titled \u201cGenerative AI with DoD Data,\u201d\nprovides a general breakdown of how the Pentagon can make use of OpenAI\u2019s\nmachine learning tools, including the immensely popular ChatGPT text generator\nand DALL-E image creator, for tasks ranging from document analysis to machine\nmaintenance. (Microsoft invested $10 billion in the ascendant machine learning\nstartup last year, and the two businesses have become tightly intertwined. In\nFebruary, The Intercept and other digital news outlets sued Microsoft and\nOpenAI for using their journalism without permission or credit.)\n\nThe Microsoft document is drawn from a large cache of materials presented at\nan October 2023 Department of Defense \u201cAI literacy\u201d training seminar hosted by\nthe U.S. Space Force in Los Angeles. The event included a variety of\npresentation from machine learning firms, including Microsoft and OpenAI,\nabout what they have to offer the Pentagon.\n\nThe publicly accessible files were found on the website of Alethia Labs, a\nnonprofit consultancy that helps the federal government with technology\nacquisition, and discovered by journalist Jack Poulson. On Wednesday, Poulson\npublished a broader investigation into the presentation materials. Alethia\nLabs has worked closely with the Pentagon to help it quickly integrate\nartificial intelligence tools into its arsenal, and since last year has\ncontracted with the Pentagon\u2019s main AI office. The firm did not respond to a\nrequest for comment.\n\n## Most Read\n\nGoogle Won\u2019t Say Anything About Israel Using Its Photo Software to Create Gaza\n\u201cHit List\u201d\n\nSam Biddle\n\nA MAGA Troll\u2019s Memes Were Blatant Disinfomation. But Were They a Crime?\n\nShawn Musgrave\n\nThe Other Players Who Helped (Almost) Make the World\u2019s Biggest Backdoor Hack\n\nNikita Mazurov\n\nOne page of the Microsoft presentation highlights a variety of \u201ccommon\u201d\nfederal uses for OpenAI, including for defense. One bullet point under\n\u201cAdvanced Computer Vision Training\u201d reads: \u201cBattle Management Systems: Using\nthe DALL-E models to create images to train battle management systems.\u201d Just\nas it sounds, a battle management system is a command-and-control software\nsuite that provides military leaders with a situational overview of a combat\nscenario, allowing them to coordinate things like artillery fire, airstrike\ntarget identification, and troop movements. The reference to computer vision\ntraining suggests artificial images conjured by DALL-E could help Pentagon\ncomputers better \u201csee\u201d conditions on the battlefield, a particular boon for\nfinding \u2014 and annihilating \u2014 targets.\n\nIn an emailed statement, Microsoft told The Intercept that while it had\npitched the Pentagon on using DALL-E to train its battlefield software, it had\nnot begun doing so. \u201cThis is an example of potential use cases that was\ninformed by conversations with customers on the art of the possible with\ngenerative AI.\u201d Microsoft, which declined to attribute the remark to anyone at\nthe company, did not explain why a \u201cpotential\u201d use case was labeled as a\n\u201ccommon\u201d use in its presentation.\n\nOpenAI spokesperson Liz Bourgeous said OpenAI was not involved in the\nMicrosoft pitch and that it had not sold any tools to the Department of\nDefense. \u201cOpenAI\u2019s policies prohibit the use of our tools to develop or use\nweapons, injure others or destroy property,\u201d she wrote. \u201cWe were not involved\nin this presentation and have not had conversations with U.S. defense agencies\nregarding the hypothetical use cases it describes.\u201d\n\nBourgeous added, \u201cWe have no evidence that OpenAI models have been used in\nthis capacity. OpenAI has no partnerships with defense agencies to make use of\nour API or ChatGPT for such purposes.\u201d\n\nAt the time of the presentation, OpenAI\u2019s policies seemingly would have\nprohibited a military use of DALL-E. Microsoft told The Intercept that if the\nPentagon used DALL-E or any other OpenAI tool through a contract with\nMicrosoft, it would be subject to the usage policies of the latter company.\nStill, any use of OpenAI technology to help the Pentagon more effectively kill\nand destroy would be a dramatic turnaround for the company, which describes\nits mission as developing safety-focused artificial intelligence that can\nbenefit all of humanity.\n\n> \u201cIt\u2019s not possible to build a battle management system in a way that\n> doesn\u2019t, at least indirectly, contribute to civilian harm.\u201d\n\n\u201cIt\u2019s not possible to build a battle management system in a way that doesn\u2019t,\nat least indirectly, contribute to civilian harm,\u201d Brianna Rosen, a visiting\nfellow at Oxford University\u2019s Blavatnik School of Government who focuses on\ntechnology ethics.\n\nRosen, who worked on the National Security Council during the Obama\nadministration, explained that OpenAI\u2019s technologies could just as easily be\nused to help people as to harm them, and their use for the latter by any\ngovernment is a political choice. \u201cUnless firms such as OpenAI have written\nguarantees from governments they will not use the technology to harm civilians\n\u2014 which still probably would not be legally-binding \u2014 I fail to see any way in\nwhich companies can state with confidence that the technology will not be used\n(or misused) in ways that have kinetic effects.\u201d\n\nThe presentation document provides no further detail about how exactly\nbattlefield management systems could use DALL-E. The reference to training\nthese systems, however, suggests that DALL-E could be to used to furnish the\nPentagon with so-called synthetic training data: artificially created scenes\nthat closely resemble germane, real-world imagery. Military software designed\nto detect enemy targets on the ground, for instance, could be shown a massive\nquantity of fake aerial images of landing strips or tank columns generated by\nDALL-E in order to better recognize such targets in the real world.\n\nEven putting aside ethical objections, the efficacy of such an approach is\ndebatable. \u201cIt\u2019s known that a model\u2019s accuracy and ability to process data\naccurately deteriorates every time it is further trained on AI-generated\ncontent,\u201d said Heidy Khlaaf, a machine learning safety engineer who previously\ncontracted with OpenAI. \u201cDall-E images are far from accurate and do not\ngenerate images reflective even close to our physical reality, even if they\nwere to be fine-tuned on inputs of Battlefield management system. These\ngenerative image models cannot even accurately generate a correct number of\nlimbs or fingers, how can we rely on them to be accurate with respect to a\nrealistic field presence?\u201d\n\n## Join Our Newsletter\n\n## Original reporting. Fearless journalism. Delivered to you.\n\nI'm in\n\nIn an interview last month with the Center for Strategic and International\nStudies, Capt. M. Xavier Lugo of the U.S. Navy envisioned a military\napplication of synthetic data exactly like the kind DALL-E can crank out,\nsuggesting that faked images could be used to train drones to better see and\nrecognize the world beneath them.\n\nLugo, mission commander of the Pentagon\u2019s generative AI task force and member\nof the Department of Defense Chief Digital and Artificial Intelligence Office,\nis listed as a contact at the end of the Microsoft presentation document. The\npresentation was made by Microsoft employee Nehemiah Kuhns, a \u201ctechnology\nspecialist\u201d working on the Space Force and Air Force.\n\nThe Air Force is currently building the Advanced Battle Management System, its\nportion of a broader multibillion-dollar Pentagon project called the Joint\nAll-Domain Command and Control, which aims to network together the entire U.S.\nmilitary for expanded communication across branches, AI-powered data analysis,\nand, ultimately, an improved capacity to kill. Through JADC2, as the project\nis known, the Pentagon envisions a near-future in which Air Force drone\ncameras, Navy warship radar, Army tanks, and Marines on the ground all\nseamlessly exchange data about the enemy in order to better destroy them.\n\nOn April 3, U.S. Central Command revealed it had already begun using elements\nof JADC2 in the Middle East.\n\nThe Department of Defense didn\u2019t answer specific questions about the Microsoft\npresentation, but spokesperson Tim Gorman told The Intercept that \u201cthe [Chief\nDigital and Artificial Intelligence Office\u2019s] mission is to accelerate the\nadoption of data, analytics, and AI across DoD. As part of that mission, we\nlead activities to educate the workforce on data and AI literacy, and how to\napply existing and emerging commercial technologies to DoD mission areas.\u201d\n\nWhile Microsoft has long reaped billions from defense contracts, OpenAI only\nrecently acknowledged it would begin working with the Department of Defense.\nIn response to The Intercept\u2019s January report on OpenAI\u2019s military-industrial\nabout face, the company\u2019s spokesperson Niko Felix said that even under the\nloosened language, \u201cOur policy does not allow our tools to be used to harm\npeople, develop weapons, for communications surveillance, or to injure others\nor destroy property.\u201d\n\n> \u201cThe point is you\u2019re contributing to preparation for warfighting.\u201d\n\nWhether the Pentagon\u2019s use of OpenAI software would entail harm or not might\ndepend on a literal view of how these technologies work, akin to arguments\nthat the company that helps build the gun or trains the shooter is not\nresponsible for where it\u2019s aimed or pulling the trigger. \u201cThey may be\nthreading a needle between the use of [generative AI] to create synthetic\ntraining data and its use in actual warfighting,\u201d said Lucy Suchman, professor\nemerita of anthropology of science and technology at Lancaster University.\n\u201cBut that would be a spurious distinction in my view, because the point is\nyou\u2019re contributing to preparation for warfighting.\u201d\n\nUnlike OpenAI, Microsoft has little pretense about forgoing harm in its\n\u201cresponsible AI\u201d document and openly promotes the use of its machine learning\ntools in the military\u2019s \u201ckill chain.\u201d\n\n## Related\n\n### OpenAI Quietly Deletes Ban on Using ChatGPT for \u201cMilitary and Warfare\u201d\n\nFollowing its policy reversal, OpenAI was also quick to emphasize to the\npublic and business press that its collaboration with the military was of a\ndefensive, peaceful nature. In a January interview at Davos responding to The\nIntercept\u2019s reporting, OpenAI vice president of global affairs Anna Makanju\nassured panel attendees that the company\u2019s military work was focused on\napplications like cybersecurity initiatives and veteran suicide prevention,\nand that the company\u2019s groundbreaking machine learning tools were still\nforbidden from causing harm or destruction.\n\nContributing to the development of a battle management system, however, would\nplace OpenAI\u2019s military work far closer to warfare itself. While OpenAI\u2019s\nclaim of avoiding direct harm could be technically true if its software does\nnot directly operate weapons systems, Khlaaf, the machine learning safety\nengineer, said, its \u201cuse in other systems, such as military operation planning\nor battlefield assessments\u201d would ultimately impact \u201cwhere weapons are\ndeployed or missions are carried out.\u201d\n\nIndeed, it\u2019s difficult to imagine a battle whose primary purpose isn\u2019t causing\nbodily harm and property damage. An Air Force press release from March, for\nexample, describes a recent battle management system exercise as delivering\n\u201clethality at the speed of data.\u201d\n\nOther materials from the AI literacy seminar series make clear that \u201charm\u201d is,\nultimately, the point. A slide from a welcome presentation given the day\nbefore Microsoft\u2019s asks the question, \u201cWhy should we care?\u201d The answer: \u201cWe\nhave to kill bad guys.\u201d In a nod to the \u201cliteracy\u201d aspect of the seminar, the\nslide adds, \u201cWe need to know what we\u2019re talking about... and we don\u2019t yet.\u201d\n\n  * Share on Facebook\n  * Share on X\n  * Share on LinkedIn\n  * Share on WhatsApp\n\n## Contact the author:\n\nSam Biddle sam.biddle@theintercept.com @sambiddle.29 on Signal\n@sambiddle.bsky.social on Bluesky @samfbiddle on X\n\n## Related\n\n### OpenAI Quietly Deletes Ban on Using ChatGPT for \u201cMilitary and Warfare\u201d\n\n### Profits Skyrocket for AI Gun Detection Used in Schools \u2014 Despite Dubious\nResults\n\n### The Internet\u2019s New Favorite AI Proposes Torturing Iranians and Surveilling\nMosques\n\n### The Microsoft Police State: Mass Surveillance, Facial Recognition, and the\nAzure Cloud\n\n## Latest Stories\n\n### Biden Administration Fears Iran Might Target U.S. Forces Over Israel\nStrike\n\nKen Klippenstein\n\n\\- Apr. 10\n\nIran threatens to attack the U.S. if it assists Israel in defending against\nany Damascus retaliation.\n\nIntercepted Podcast\n\n### Amid Gaza War, College Campuses Become Free Speech \u201cTesting Ground\u201d\n\nIntercepted\n\n\\- Apr. 10\n\nProtests at universities are now being met with a wave of censorship and\nsuppression, targeting students most directly.\n\n### Terror Hunters Trade Hamas for ISIS-K, Perhaps With Some Relief\n\nDaniel Boguslaw\n\n\\- Apr. 9\n\nFor the feds, ISIS-K as the new domestic terrorism threat avoids dealing with\nthe politics of the Gaza war.\n\nJoin The Conversation\n\n## Join Our Newsletter Original reporting. Fearless journalism. Delivered to\nyou.\n\nBy signing up, I agree to receive emails from The Intercept and to the Privacy\nPolicy and Terms of Use.\n\n\u00a9 The Intercept. All rights reserved\n\n(opens in a new tab)\n\n", "frontpage": false}
