{"aid": "39986859", "title": "Web Scraping Tutorial Using Crawlee", "url": "https://blog.apify.com/crawlee-web-scraping-tutorial/", "domain": "apify.com", "votes": 1, "user": "sauain", "posted_at": "2024-04-10 04:07:29", "comments": 0, "source_title": "Crawlee web scraping tutorial", "source_text": "Crawlee web scraping tutorial\n\nBack to blog\n\nCrawlee\n\nTutorial\n\nWeb scraping\n\nApr 2, 2024 7 min read\n\n# Crawlee web scraping tutorial\n\nGive your scraping skills a boost with the most popular full-stack web\nscraping library for Node.js\n\nPosted by\n\nPercival Villalva\n\n### Content\n\n  1. Getting started with Crawlee\n  2. Building a HackerNews scraper\n\n    1. main.js\n    2. routes.js\n    3. Default handler - enqueueing links\n    4. \u201cDETAIL\u201d handler\n    5. Testing and output\n  3. What\u2019s next?\n\nIf you've found this article, you likely have some experience with web\nscraping and are familiar with tools like Axios, Cheerio, Puppeteer, and\nPlaywright. These tools each serve a role in the web scraping process, where\nyou need an HTTP client like Axios to retrieve web content and an HTML parser\nlike Cheerio to process it. For JavaScript-heavy sites, browser automation\ntools like Puppeteer or Playwright become necessary.\n\nJuggling these tools can quickly become complicated, especially considering\nthe constantly changing nature of the web and when dealing with websites that\nemploy advanced anti-blocking measures.\n\nInstead of having to juggle a myriad of libraries, wouldn't it be easier to\nhave a single library that integrates all these functionalities, along with\nscraping-specific features like proxy rotation, browser fingerprinting, and\nstreamlined pagination?\n\nIf you just wet your pants with excitement at the prospect of a unified,\npowerful web scraping library, then let me introduce you to Crawlee.\n\nCrawlee is an open-source Node.js web scraping and browser automation library\ndesigned to handle a wide range of web scraping scenarios, from simple static\npages to dynamic, JavaScript-heavy pages. For a more in-depth look into\nCrawlee, I highly suggest you watch the video below.\n\nLet this video introduce you to Crawlee and its features\n\nIn practice, if a website implements JavaScript rendering, Crawlee allows you\nto simply switch to a browser crawler instead of rewriting your entire code.\nShould you discover an efficient API later, you can easily revert to the\nprevious settings.\n\nOn top of that, Crawlee enhances your proxies\u2019 longevity by intelligently\nrotating them and using sophisticated fingerprints, ensuring your crawlers\nmimic human behavior and, consequently, drastically reducing your scraper\u2019s\nblocking rates.\n\nTo showcase some of Crawlee\u2019s functionalities, let\u2019s build a HackerNews\nScraper.\n\n## Getting started with Crawlee\n\nThe easiest way to set up a Crawlee project is through the CLI command below.\nIt utilizes npx which is a package runner that comes with npm. In this\ntutorial, we'll start by creating a crawler for HackerNews.\n\n    \n    \n    npx crawlee create hackernews-crawler\n\nBash\n\nRight after running the command above, you'll be prompted to select a template\nfor your Crawlee project. All templates come with a JavaScript and TypeScript\nversion, so choose the one that fits you best. HackerNews doesn't rely on\nJavaScript to load content on its webpage, so we can go ahead with the\nCheerioCrawler template.\n\nCrawlee will take care of installing all the necessary dependencies to get our\nproject ready. After the installation is complete, go ahead and move into the\nnewly created directory, hackernews-crawler\n\n    \n    \n    cd hackernews-crawler\n\nBash\n\nCrawlee actually already generated a fully functional template for us, so\nlet\u2019s see what happens if we simply run it.\n\n    \n    \n    npm start\n\nBash\n\nThis template code is set up to crawl Crawlee\u2019s own website. But the reason\nwhy I wanted to run it is to show you how Crawlee\u2019s system works. If you go to\nstorage \u2192 datasets you will see that it's populated with the scraped results.\nData storage is a central part of what makes Crawlee special, and working with\nthe different storages can give us a lot of flexibility in how we handle our\ndata.\n\n## Building a HackerNews scraper\n\nNow that the CheerioCrawler template is set up, the next step is to adjust it\nfor our particular use case: scraping HackerNews.\n\nFirst things first, you might have noticed that Crawlee separates its crawling\nlogic into two files: main.js and routes.js with the bulk of our crawling\nlogic being in the routes.js file. This helps us maintain our code organized\nwhile handling different contexts. Don\u2019t worry, its purpose will become clear\nsoon enough.\n\nBefore we get to the actual code, let\u2019s briefly describe what action we want\nour crawler to perform.\n\n### main.js\n\nWe'll start by replacing the template code on main.js with the code below:\n\n    \n    \n    import { CheerioCrawler, ProxyConfiguration } from 'crawlee'; import { router } from './routes.js'; const proxyConfiguration = new ProxyConfiguration({ proxyUrls: [ 'http://<username>:<password>@proxy.apify.com:8000', ], }); const crawler = new CheerioCrawler({ requestHandler: router, proxyConfiguration, }); await crawler.run([`https://news.ycombinator.com/`]);\n\nJavaScript\n\nIn the code you just saw, besides setting our starting point for the crawler,\nwe also set up the proxyConfiguration. In web scraping, employing proxies is\nnot always essential but becomes crucial for scraping complex sites with\nsophisticated anti-scraping measures, as it significantly reduces the chances\nof being blocked. In our case, attempting to scrape HackerNews without a proxy\noften leads to receiving a 503 error from the server, so we will need to use a\nproxy to scrape HN effectively.\n\nIn the code example above, I\u2019m using the Apify Proxy URL. Crawlee allows you\nto use proxies from any provider, but if you don't already have one, you can\nsign up for a free Apify account to obtain a proxy URL. Once you have an Apify\naccount, navigate to Proxy \u2192 HTTP Settings to access the Apify Proxy URL and\nyour unique proxy credentials, which you'll need to input into the crawler.\n\nWith proxies out of the way, we will now work on the actual scraping logic in\nthe routes.js file.\n\n### routes.js\n\nThe logic in this file is divided into two handlers. The default handler logic\nis applied to every unlabelled request. On the other hand, the second\nhandler\u2019s logic will only be applied to requests containing the \u201cDETAIL\u201d\nlabel.\n\n    \n    \n    import { Dataset, createCheerioRouter } from 'crawlee'; export const router = createCheerioRouter(); // Default Handler applied to every request without a label router.addDefaultHandler(async ({ enqueueLinks, log, $, request }) => { log.info(`\ud83d\udd0e Scraping ${request.url}`); // Step 1: Enqueue posts' links and label these links as \"DETAIL\" await enqueueLinks({ selector: '.subline .age a', label: 'DETAIL', }); // Step 2: Enqueue next page await enqueueLinks({ selector: '.morelink', }); }); // Handler applied only to requests with the \"DETAIL\" label router.addHandler('DETAIL', async ({ enqueueLinks, log, $, request }) => { const $post = $('.athing'); // Handle HN Urls const hnPostUrl = 'https://news.ycombinator.com/item?id=' + $post.attr('id'); const postUrl = $post.find('.titleline a').attr('href'); const url = postUrl.includes('item?id=') ? hnPostUrl : postUrl; // Handle Job Listings const authorName = $post.find('+tr .subline .hnuser').text(); log.info(authorName); const author = authorName !== '' ? authorName : 'YC Jobs'; const pointsCount = $post.find('+tr .score').text().replace(' points', ''); const points = pointsCount !== '' ? pointsCount : 'Job listing'; // Handle comments const ct = $('.athing.comtr'); const comments = ct .map((_, comment) => { const $comment = $(comment); return { commentAuthor: $comment.find('.comhead a.hnuser').text(), commentContent: $comment .find('.comment span.commtext') .text() .trim(), }; }) .toArray(); // Define the structure of the scraped data const data = { hnPostUrl, url, title: $post.find('td:nth-child(3) > span > a').text(), author, points, date: $post.find('+tr .age').attr('title'), comments, }; // Push data to Crawlee's dataset await Dataset.pushData(data); });\n\nJavaScript\n\nTo make things clearer, let\u2019s break down each handler\u2019s functionality.\n\n### Default handler - enqueueing links\n\nAs previously mentioned, the default handler is applied to every request\nwithout a label, making it the ideal handler for us to use to enqueue the\nlinks we need to scrape HackerNews.\n\n    \n    \n    // Default Handler applied to every request without a label router.addDefaultHandler(async ({ enqueueLinks, log, $, request }) => { log.info(`\ud83d\udd0e Scraping ${request.url}`); // Step 1: Enqueue posts' links and label these links as \"DETAIL\" await enqueueLinks({ selector: '.subline .age a', label: 'DETAIL', }); // Step 2: Enqueue next page await enqueueLinks({ selector: '.morelink', }); });\n\nJavaScript\n\nThe initial enqueueLinks function gathers all post URLs from HackerNews that\nconform to the specified CSS selector within the function. Each of these\nrequests is tagged with the label 'DETAIL', allowing us to specifically\naddress these requests later with a dedicated handler to manage the scraping\nprocess for these pages.\n\nThe subsequent enqueueLinks function queues the next page to ensure the\nscraping of all posts on HackerNews continues until there are no more pages\navailable.\n\n### \u201cDETAIL\u201d handler\n\nIn the default handler, we tagged all enqueued HackerNews post URL requests\nwith the label \"DETAIL\". With this setup, we can now establish a specialized\nhandler that outlines the processing logic for these tagged requests.\n\n    \n    \n    // Handler applied only to requests with the \"DETAIL\" label router.addHandler('DETAIL', async ({ enqueueLinks, log, $, request }) => { const $post = $('.athing'); // Handle HN Urls const hnPostUrl = 'https://news.ycombinator.com/item?id=' + $post.attr('id'); const postUrl = $post.find('.titleline a').attr('href'); const url = postUrl.includes('item?id=') ? hnPostUrl : postUrl; // Handle Job Listings const authorName = $post.find('+tr .subline .hnuser').text(); const author = authorName !== '' ? authorName : 'YC Jobs'; const pointsCount = $post.find('+tr .score').text().replace(' points', ''); const points = pointsCount !== '' ? pointsCount : 'Job listing'; // Handle comments const ct = $('.athing.comtr'); const comments = ct .map((_, comment) => { const $comment = $(comment); return { commentAuthor: $comment.find('.comhead a.hnuser').text(), commentContent: $comment .find('.comment span.commtext') .text() .trim(), }; }) .toArray(); // Define the structure of the scraped data const data = { hnPostUrl, url, title: $post.find('td:nth-child(3) > span > a').text(), author, points, date: $post.find('+tr .age').attr('title'), comments, }; // Push data to Crawlee's dataset await Dataset.pushData(data); });\n\nJavaScript\n\nIn this code, we focus on pulling out key details from posts, like the post's\nown link on HackerNews, the main link it shares, its title, the writer,\npoints, date, and comments. We also make sure to handle special posts, such as\nShow/Ask HN and job listings from YC, which might have a different page\nstructure.\n\n### Testing and output\n\nNow that our scraper is ready, let's test it. Type npm start in your terminal\nto begin the crawl. Wait a few minutes for it to finish. When it's done,\nCrawlee's dataset folder will be filled with results similar to the one shown\nin the picture below.\n\n## What\u2019s next?\n\nThis was just a small peek into what you can do with the full-stack web\nscraping library that's Crawlee. Like what you see? Give Crawlee a star on\nGitHub! And if you're keen to learn more about the library and boost your\nscraping skills, check out this detailed tutorial on using Crawlee to scrape\nAmazon products or watch the video guide below.\n\nLearn to build an Amazon scraper with Crawlee\n\nPercival Villalva\n\nDeveloper Advocate on a mission to help developers build scalable, human-like\nbots for data extraction and web automation.\n\n#### TAGS\n\nCrawlee Tutorial Web scraping Web crawling\n\nTweet Share on Facebook Share on Linkedin Copy linkCopied!\n\n#### Related articles\n\nCrawlee\n\nWeb scraping\n\n## Crawlee data storage types: saving files, screenshots, and JSON results\n\nPercival Villalva\n\nNov 28, 2023\n\nComparison\n\nTypeScript\n\nJavaScript\n\n## TypeScript vs. JavaScript: which to use for web scraping?\n\nNatasha Lekh\n\nFeb 20, 2023\n\nWeb scraping\n\nAI\n\nCOVID-19\n\n## What's the future of web scraping in 2023?\n\nNatasha Lekh\n\nJan 20, 2023\n\n## Get started now\n\nStep up your web scraping and automation\n\nSign up for free\n\n#### Solutions\n\n  * Apify Store\n  * Apify Enterprise\n  * Plans and pricing\n  * Web scraping\n  * Use cases\n  * Success stories\n  * Industries\n  * New ideas\n\n#### Developers\n\n  * Web scraping academy\n  * Apify SDK\n  * Apify CLI\n  * Open source\n  * Universal Web Scraper\n  * Web scraping with Python\n  * Web scraping templates\n  * Build paid Actors\n\n#### Platform\n\n  * Documentation\n  * Actors\n  * Proxy\n  * Storage\n  * Integrations\n  * Changelog\n  * Status\n  * Log in\n\n#### Resources\n\n  * Help & support\n  * Partner programs\n  * Become an affiliate\n  * For universities\n  * What is web scraping?\n  * Data for generative AI\n  * Best web scraping tools\n  * Google Maps Scraper\n\n#### Company\n\n  * About\n  * Blog\n  * Write for Apify\n  * Jobs\n  * Contact us\n\nJoin our developer community on Discord\n\nCookie settings Terms of use Privacy policy Cookie policy\n\n\u00a9 2024 Apify\n\n", "frontpage": false}
