{"aid": "39979067", "title": "CodeGemma \u2013 an official Google release for code LLMs", "url": "https://huggingface.co/blog/codegemma", "domain": "huggingface.co", "votes": 1, "user": "homarp", "posted_at": "2024-04-09 13:11:25", "comments": 0, "source_title": "CodeGemma - an official Google release for code LLMs", "source_text": "CodeGemma - an official Google release for code LLMs\n\nHugging Face\n\nBack to Articles\n\n# CodeGemma - an official Google release for code LLMs\n\nPublished April 9, 2024\n\nUpdate on GitHub\n\nUpvote\n\n8\n\npcuenq Pedro Cuenca\n\nosanseviero Omar Sanseviero\n\nreach-vb Vaibhav Srivastav\n\nphilschmid Philipp Schmid\n\nmishig Mishig Davaadorj\n\nloubnabnl Loubna Ben Allal\n\nCodeGemma is a family of open-access versions of Gemma specialized in code,\nand we\u2019re excited to collaborate with Google on its release to make it as\naccessible as possible.\ud83e\udd17\n\nCodeGemma comes in three flavors:\n\n  * A 2B base model specialized in infilling and open-ended generation.\n  * A 7B base model trained with both code infilling and natural language.\n  * A 7B instruct model a user can chat with about code.\n\nWe\u2019ve collaborated with Google to ensure the best integration into the Hugging\nFace ecosystem. You can find the three open-access models ready to use on the\nHub. Among the features and integrations being released, we have:\n\n  * Models on the Hub, with their model cards and licenses. There are versions for the transformers library, checkpoints for use with Google\u2019s original codebases, and full-precision GGUF files that the community can quantize.\n  * Transformers integration\n  * Integration with Google Cloud\n  * Integration with Inference Endpoints\n  * Code benchmarks\n\n## Table of contents\n\n  * What is CodeGemma\n\n    * Evaluation Results\n    * Prompt format\n  * Using CodeGemma\n\n    * Using Transformers\n    * Integration with Google Cloud\n    * Integration with Inference Endpoints\n  * Additional Resources\n\n## What is CodeGemma?\n\nCodeGemma is a family of code-specialist LLM models by Google, based on the\npre-trained 2B and 7B Gemma checkpoints. CodeGemma are further trained on an\nadditional 500 billion tokens of primarily English language data, mathematics,\nand code to improve on logical and mathematical reasoning, and are suitable\nfor code completion and generation.\n\nCodeGemma 2B was trained exclusively on Code Infilling and is meant for fast\ncode completion and generation, especially in settings where latency and/or\nprivacy are crucial. CodeGemma 7B training mix includes code infilling data\n(80%) and natural language. It can be used for code completion, as well as\ncode and language understanding and generation. CodeGemma 7B Instruct was\nfine-tuned for instruction following on top of CodeGemma 7B. It\u2019s meant for\nconversational use, especially around code, programming, or mathematical\nreasoning topics. All the models have the same 8K token context size as their\npredecessors.\n\nThis image is from the original report\n\n### Evaluation Results\n\nCodeGemma-7B outperforms similarly-sized 7B models except DeepSeek-Coder-7B on\nHumanEval, a popular benchmark for evaluating code models on Python. The same\ngoes for the evaluation of other programming languages like Java, JavaScript,\nand C++ from MultiPL-E, a translation of HumanEval. According to the technical\nreport, the model performs best on GSM8K among 7B models. The instruct version\nCodeGemma-7B-it improves on the most popular languages on both HumanEval and\nMBPP (cf paper table 5). For more details, you can check the BigCode\nleaderboard or some metrics below.\n\nModel| Pretraining size [tokens]| Python| JavaScript  \n---|---|---|---  \n10B+ models  \nStarCoder 2 15B| 4,000B+| 44.15| 44.24  \nCode Llama 13B| 2,500B| 35.07| 38.26  \n7B models  \nDeepSeek Coder 7B| 2,000B| 45.83| 45.9  \nCodeGemma 7B| 500B of extra training| 40.13| 43.06  \nCode Llama 7B| 2,500B| 29.98| 31.8  \nStarCoder 2 7B| 3,500B+| 34.09| 35.35  \nStarCoderBase 7B| 3,000B+| 28.37| 27.35  \n<3B models  \nCodeGemma 2B| 500B of extra training| 27.28| 29.94  \nStable Code 3B| 1,300B| 30.72| 28.75  \nStarCoder 2 3B| 3,000B+| 31.44| 35.37  \n  \nModel| Pretraining size [tokens]| Python| JavaScript  \n---|---|---|---  \n10B+ models  \nCode Llama 13B| 2,620B| 50.6| 40.92  \nCode Llama 13B| 2,620B| 42.89| 40.66  \n7B models  \nCodeGemma 7B| 500B| 52.74| 47.71  \nCode Llama 7B| 2,620B| 40.48| 36.34  \nCode Llama 7B| 2,620B| 25.65| 33.11  \n  \nHere is a table from the original report with a breakdown per language.\n\n### Prompt format\n\nCodeGemma 2B and CodeGemma 7B use infilling (code, comments, docstrings,\nimport statements) for code completion. CodeGemma was trained for this task\nusing the fill-in-the-middle (FIM) objective, where you provide a prefix and a\nsuffix as context for the completion. The following tokens are used to\nseparate the different parts of the input:\n\n  * <|fim_prefix|> precedes the context before the completion we want to run.\n  * <|fim_suffix|> precedes the suffix. You must put this token exactly where the cursor would be positioned in an editor, as this is the location where the model will code complete.\n  * <|fim_middle|> is the prompt that invites the model to run the generation.\n\nIn addition to these, there's also <|file_separator|>, which provides multi-\nfile contexts. We\u2019ll show examples of use in the Using with transformers\nsection.\n\nCodeGemma 7B Instruct uses the same prompt format as the base Gemma\nInstruction-tuned versions, following this conversation structure:\n\n    \n    \n    <bos><start_of_turn>user knock knock<end_of_turn> <start_of_turn>model who is there<end_of_turn> <start_of_turn>user LaMDA<end_of_turn> <start_of_turn>model LaMDA who?<end_of_turn>\n\nAs is the case with Gemma, the easiest way to reproduce this format is with\nthe chat template available in transformers.\n\n## Using CodeGemma\n\n### Using Transformers\n\nWith Transformers release 4.39, you can use CodeGemma and leverage all the\ntools within the Hugging Face ecosystem, such as:\n\n  * training and inference scripts and examples\n  * safe file format (safetensors)\n  * integrations with tools such as bitsandbytes (4-bit quantization), PEFT (parameter efficient fine-tuning), and Flash Attention 2\n  * utilities and helpers to run generation with the model\n  * mechanisms to export the models to deploy\n\nLike the Gemma models, CodeGemma is compatible with torch.compile() for an\nimportant inference speedup.\n\nBonus: We made a Colab notebook for you to try out the model at the touch of a\nbutton here.\n\nTo use CodeGemma with transformers, make sure to use the latest release:\n\n    \n    \n    pip install --upgrade transformers\n\nThe following snippet shows how to use codegemma-2b for code completion with\ntransformers. It requires about 6 GB of RAM using float16 precision, making it\nperfectly suitable for consumer GPUs and on-device applications.\n\n    \n    \n    from transformers import GemmaTokenizer, AutoModelForCausalLM import torch model_id = \"google/codegemma-2b\" tokenizer = GemmaTokenizer.from_pretrained(model_id) model = AutoModelForCausalLM.from_pretrained( model_id, torch_dtype=torch.float16 ).to(\"cuda:0\") prompt = '''\\ <|fim_prefix|>import datetime def calculate_age(birth_year): \"\"\"Calculates a person's age based on their birth year.\"\"\" current_year = datetime.date.today().year <|fim_suffix|> return age<|fim_middle|>\\ ''' inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device) prompt_len = inputs[\"input_ids\"].shape[-1] outputs = model.generate(**inputs, max_new_tokens=100) print(tokenizer.decode(outputs[0][prompt_len:]))\n\nObserve that the <|fim_suffix|> token appears in the position where the cursor\nwould be placed in an editor, marking the position for the generation.\n<|fim_prefix|> provides the context that precedes the cursor, and the\nremaining until <|fim_middle|> is additional context after the cursor. Either\nof them can be empty if the cursor is located at the beginning or end of the\nfile.\n\nThe previous code may return something like the following:\n\n    \n    \n    age = current_year - birth_year<|file_separator|>test_calculate_age.py <|fim_suffix|> assert calculate_age(1990) == 33 assert calculate_age(1980) == 43 assert calculate_age(1970) == 53 assert calculate_age(1960) == 63 assert calculate_age(1950) == 73\n\nNote the extra content after the correct completion. This is particularly the\ncase for CodeGemma 7B, which is more verbose and tends to provide additional\ncode or comments after completion. We must ignore everything that appears\nafter the FIM tokens or the EOS token for code infilling. We can stop\ngeneration early with transformers by providing a list of terminators to the\ngenerate function, like this:\n\n    \n    \n    FIM_PREFIX = '<|fim_prefix|>' FIM_SUFFIX = '<|fim_suffix|>' FIM_MIDDLE = '<|fim_middle|>' FIM_FILE_SEPARATOR = '<|file_separator|>' terminators = tokenizer.convert_tokens_to_ids( [FIM_PREFIX, FIM_MIDDLE, FIM_SUFFIX, FIM_FILE_SEPARATOR] ) terminators += [tokenizer.eos_token_id] outputs = model.generate( **inputs, max_new_tokens=100, eos_token_id=terminators, )\n\nIn this case, generation will stop as soon as the first delimiter is found:\n\n    \n    \n    age = current_year - birth_year<|file_separator|>\n\n### A note on precision\n\nThe original CodeGemma checkpoints are released in bfloat16 precision. If you\nload the model without indicating a torch_dtype, PyTorch will upcast them to\nfloat32. Casting to float16 is perfectly fine for use, and it can be much\nfaster than bfloat16 on certain hardware. For maximum precision, we recommend\nyou use bfloat16 rather than float32.\n\nYou can also automatically quantize the model, loading it in 8-bit or 4-bit\nmode. 4-bit loading of CodeGemma 7B takes about 9 GB of memory to run, making\nit compatible with many consumer cards and all the GPUs in Google Colab. This\nis how you\u2019d load the generation pipeline in 4-bit:\n\n    \n    \n    pipeline = pipeline( \"text-generation\", model=model, model_kwargs={ \"torch_dtype\": torch.float16, \"quantization_config\": {\"load_in_4bit\": True} }, )\n\n### Integration with Google Cloud\n\nYou can deploy and train Gemma on Google Cloud through Vertex AI or Google\nKubernetes Engine (GKE), using Text Generation Inference and Transformers.\n\nTo deploy the CodeGemma model from Hugging Face, go to the model page and\nclick on Deploy -> Google Cloud. This will bring you to the Google Cloud\nConsole, where you can 1-click deploy CodeGemma on Vertex AI or GKE, powered\nby Text Generation Inference.\n\nYou can also access CodeGemma directly through the Vertex AI Model Garden.\n\n## Integration with Inference Endpoints\n\nYou can deploy CodeGemma on Hugging Face's Inference Endpoints, which uses\nText Generation Inference as the backend. Text Generation Inference is a\nproduction-ready inference container developed by Hugging Face to enable easy\ndeployment of large language models. It has features such as continuous\nbatching, token streaming, tensor parallelism for fast inference on multiple\nGPUs, production-ready logging and tracing, and is distributed under the\nApache 2 license.\n\nTo deploy a CodeGemma model, go to the model page and click on the Deploy ->\nInference Endpoints widget. You can learn more about Deploying LLMs with\nHugging Face Inference Endpoints in a previous blog post. Note that T4s do not\nsupport the bfloat16 format, so you will need to use a different GPU option.\n\n    \n    \n    from huggingface_hub import InferenceClient client = InferenceClient(model=IE_ENDPOINT) prompt = \"\"\"\\ <|fim_prefix|>import <|fim_suffix|> if __name__ == '__main__': sys.exit(0)<|fim_middle|>\\ \"\"\" client.text_generation(prompt=prompt)\n\n## Additional Resources\n\n  * Models on the Hub\n  * Code Leaderboard\n  * Technical Report\n\nMore Articles from our Blog\n\n## Blazing Fast SetFit Inference with \ud83e\udd17 Optimum Intel on Xeon\n\nBy April 3, 2024 guest\n\n## Binary and Scalar Embedding Quantization for Significantly Faster & Cheaper\nRetrieval\n\nBy March 22, 2024 guest \u2022 8\n\nUpvote\n\n8\n\n", "frontpage": false}
