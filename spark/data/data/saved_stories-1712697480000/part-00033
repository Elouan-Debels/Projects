{"aid": "39979172", "title": "Common statistical tests are linear models (or: how to teach stats)", "url": "https://lindeloev.github.io/tests-as-linear/", "domain": "lindeloev.github.io", "votes": 2, "user": "sebg", "posted_at": "2024-04-09 13:22:51", "comments": 0, "source_title": "Common statistical tests are linear models (or: how to teach stats)", "source_text": "Common statistical tests are linear models (or: how to teach stats)\n\n  * 1 The simplicity underlying common tests\n\n  * 2 Settings and toy data\n\n  * 3 Pearson and Spearman correlation\n\n  * 4 One mean\n    * 4.1 One sample t-test and Wilcoxon signed-rank\n    * 4.2 Paired samples t-test and Wilcoxon matched pairs\n\n  * 5 Two means\n    * 5.1 Independent t-test and Mann-Whitney U\n    * 5.2 Welch\u2019s t-test\n\n  * 6 Three or more means\n    * 6.1 One-way ANOVA and Kruskal-Wallis\n    * 6.2 Two-way ANOVA (plot in progress!)\n    * 6.3 ANCOVA\n\n  * 7 Proportions: Chi-square is a log-linear model\n    * 7.1 Goodness of fit\n    * 7.2 Contingency tables\n\n  * 8 Sources and further equivalences\n\n  * 9 Teaching materials and a course outline\n\n  * 10 Limitations\n\n# Common statistical tests are linear models (or: how to teach stats)\n\nBy Jonas Kristoffer Lindel\u00f8v (blog, profile). Last updated: 28 June, 2019 (See\nchangelog). Check out the Python version and the Twitter summary.\n\nThis document is summarised in the table below. It shows the linear models\nunderlying common parametric and \u201cnon-parametric\u201d tests. Formulating all the\ntests in the same language highlights the many similarities between them. Get\nit as an image or as a PDF.\n\n# 1 The simplicity underlying common tests\n\nMost of the common statistical models (t-test, correlation, ANOVA; chi-square,\netc.) are special cases of linear models or a very close approximation. This\nbeautiful simplicity means that there is less to learn. In particular, it all\ncomes down to y=a\u22c5x+b which most students know from highschool. Unfortunately,\nstats intro courses are usually taught as if each test is an independent tool,\nneedlessly making life more complicated for students and teachers alike.\n\nThis needless complexity multiplies when students try to rote learn the\nparametric assumptions underlying each test separately rather than deducing\nthem from the linear model.\n\nFor this reason, I think that teaching linear models first and foremost and\nthen name-dropping the special cases along the way makes for an excellent\nteaching strategy, emphasizing understanding over rote learning. Since linear\nmodels are the same across frequentist, Bayesian, and permutation-based\ninferences, I\u2019d argue that it\u2019s better to start with modeling than p-values,\ntype-1 errors, Bayes factors, or other inferences.\n\nConcerning the teaching of \u201cnon-parametric\u201d tests in intro-courses, I think\nthat we can justify lying-to-children and teach \u201cnon-parametric\u201d\" tests as if\nthey are merely ranked versions of the corresponding parametric tests. It is\nmuch better for students to think \u201cranks!\u201d than to believe that you can\nmagically throw away assumptions. Indeed, the Bayesian equivalents of \u201cnon-\nparametric\u201d\" tests implemented in JASP literally just do (latent) ranking and\nthat\u2019s it. For the frequentist \u201cnon-parametric\u201d\" tests considered here, this\napproach is highly accurate for N > 15.\n\nUse the menu to jump to your favourite section. There are links to lots of\nsimilar (though more scattered) stuff under sources and teaching materials. I\nhope that you will join in suggesting improvements or submitting improvements\nyourself in the Github repo to this page. Let\u2019s make it awesome!\n\n# 2 Settings and toy data\n\nUnfold this if you want to see functions and other settings for this notebook:\n\n    \n    \n    Show Source\n    \n    # Load packages for data handling and plotting library(tidyverse) library(patchwork) library(broom) # Reproducible \"random\" results set.seed(40) # Generate normal data with known parameters rnorm_fixed = function(N, mu = 0, sd = 1) scale(rnorm(N)) * sd + mu # Plot style. theme_axis = function(P, jitter = FALSE, xlim = c(-0.5, 2), ylim = c(-0.5, 2), legend.position = NULL) { P = P + theme_bw(15) + geom_segment( x = -1000, xend = 1000, y = 0, yend = 0, lty = 2, color = 'dark gray', lwd = 0.5 ) + geom_segment( x = 0, xend = 0, y = -1000, yend = 1000, lty = 2, color = 'dark gray', lwd = 0.5 ) + coord_cartesian(xlim = xlim, ylim = ylim) + theme( axis.title = element_blank(), axis.text = element_blank(), axis.ticks = element_blank(), panel.border = element_blank(), panel.grid = element_blank(), legend.position = legend.position ) # Return jittered or non-jittered plot? if (jitter) { P + geom_jitter(width = 0.1, size = 2) } else { P + geom_point(size = 2) } }\n\nFor a start, we\u2019ll keep it simple and play with three standard normals in wide\n(a, b, c) and long format (value, group):\n\n    \n    \n    # Wide format (sort of) #y = rnorm_fixed(50, mu=0.3, sd=2) # Almost zero mean. y = c(rnorm(15), exp(rnorm(15)), runif(20, min = -3, max = 0)) # Almost zero mean, not normal x = rnorm_fixed(50, mu = 0, sd = 1) # Used in correlation where this is on x-axis y2 = rnorm_fixed(50, mu = 0.5, sd = 1.5) # Used in two means # Long format data with indicator value = c(y, y2) group = rep(c('y1', 'y2'), each = 50)\n\n# 3 Pearson and Spearman correlation\n\n### 3.0.1 Theory: As linear models\n\nModel: the recipe for y is a slope (\u03b21) times x plus an intercept (\u03b20, aka a\nstraight line).\n\ny=\u03b20+\u03b21xH0:\u03b21=0\n\n... which is a math-y way of writing the good old y=ax+b (here ordered as\ny=b+ax). In R we are lazy and write y ~ 1 + x which R reads like y = 1*number\n+ x*othernumber and the task of t-tests, lm, etc., is simply to find the\nnumbers that best predict y.\n\nEither way you write it, it\u2019s an intercept (\u03b20) and a slope (\u03b21) yielding a\nstraight line:\n\n    \n    \n    Show Source\n    \n    # Fixed correlation D_correlation = data.frame(MASS::mvrnorm(30, mu = c(0.9, 0.9), Sigma = matrix(c(1, 0.8, 1, 0.8), ncol = 2), empirical = TRUE)) # Correlated data # Add labels (for next plot) D_correlation$label_num = sprintf('(%.1f,%.1f)', D_correlation$X1, D_correlation$X2) D_correlation$label_rank = sprintf('(%i,%i)', rank(D_correlation$X1), rank(D_correlation$X2)) # Plot it fit = lm(I(X2 * 0.5 + 0.4) ~ I(X1 * 0.5 + 0.2), D_correlation) intercept_pearson = coefficients(fit)[1] P_pearson = ggplot(D_correlation, aes(x=X1*0.5+0.2, y=X2*0.5+0.4)) + geom_smooth(method=lm, se=FALSE, lwd=2, aes(colour='beta_1')) + geom_segment(x=-100, xend=100, y=intercept_pearson, yend=intercept_pearson, lwd=2, aes(color=\"beta_0\")) + scale_color_manual(name=NULL, values=c(\"blue\", \"red\"), labels=c(bquote(beta[0]*\" (intercept)\"), bquote(beta[1]*\" (slope)\"))) theme_axis(P_pearson, legend.position = c(0.4, 0.9))\n\nThis is often simply called a regression model which can be extended to\nmultiple regression where there are several \u03b2s and on the right-hand side\nmultiplied with the predictors. Everything below, from one-sample t-test to\ntwo-way ANOVA are just special cases of this system. Nothing more, nothing\nless.\n\nAs the name implies, the Spearman rank correlation is a Pearson correlation on\nrank-transformed x and y:\n\nrank(y)=\u03b20+\u03b21\u22c5rank(x)H0:\u03b21=0\n\nI\u2019ll introduce ranks in a minute. For now, notice that the correlation\ncoefficient of the linear model is identical to a \u201creal\u201d Pearson correlation,\nbut p-values are an approximation which is is appropriate for samples greater\nthan N=10 and almost perfect when N > 20.\n\nSuch a nice and non-mysterious equivalence that many students are left unaware\nof! Visualizing them side by side including data labels, we see this rank-\ntransformation in action:\n\n    \n    \n    Show Source\n    \n    # Spearman intercept intercept_spearman = coefficients(lm(rank(X2) ~ rank(X1), D_correlation))[1] # Spearman plot P_spearman = ggplot(D_correlation, aes(x=rank(X1), y=rank(X2))) + geom_smooth(method=lm, se=FALSE, lwd=2, aes(color='beta_1')) + geom_text(aes(label=label_rank), nudge_y=1, size=3, color='dark gray') + geom_segment(x=-100, xend=100, y=intercept_spearman, yend=intercept_spearman, lwd=2, aes(color='beta_0')) + scale_color_manual(name=NULL, values=c(\"blue\", \"red\"), labels=c(bquote(beta[0]*\" (intercept)\"), bquote(beta[1]*\" (slope)\"))) # Stich together using patchwork (theme_axis(P_pearson, legend.position=c(0.5, 0.1)) + geom_text(aes(label=label_num), nudge_y=0.1, size=3, color='dark gray') + labs(title=' Pearson')) + (theme_axis(P_spearman, xlim=c(-7.5, 30), ylim=c(-7.5, 30), legend.position=c(0.5, 0.1)) + labs(title=' Spearman'))\n\n### 3.0.2 Theory: rank-transformation\n\nrank simply takes a list of numbers and \u201creplace\u201d them with the integers of\ntheir rank (1st smallest, 2nd smallest, 3rd smallest, etc.). So the result of\nthe rank-transformation rank(c(3.6, 3.4, -5.0, 8.2)) is 3, 2, 1, 4. See that\nin the figure above?\n\nA signed rank is the same, just where we rank according to absolute size first\nand then add in the sign second. So the signed rank here would be 2, 1, -3, 4.\nOr in code:\n\n    \n    \n    signed_rank = function(x) sign(x) * rank(abs(x))\n\nI hope I don\u2019t offend anyone when I say that ranks are easy; yet it\u2019s all you\nneed to do to convert most parametric tests into their \u201cnon-parametric\u201d\ncounterparts! One interesting implication is that many \u201cnon-parametric tests\u201d\nare about as parametric as their parametric counterparts with means, standard\ndeviations, homogeneity of variance, etc. - just on rank-transformed data.\nThat\u2019s why I put \u201cnon-parametric\u201d in quotation marks.\n\n### 3.0.3 R code: Pearson correlation\n\nIt couldn\u2019t be much simpler to run these models in R. They yield identical p\nand t, but there\u2019s a catch: lm gives you the slope and even though that is\nusually much more interpretable and informative than the correlation\ncoefficient r, you may still want r. Luckily, the slope becomes r if x and y\nhave identical standard deviations. For now, we will use scale(x) to make\nSD(x)=1.0 and SD(y)=1.0:\n\n    \n    \n    a = cor.test(y, x, method = \"pearson\") # Built-in b = lm(y ~ 1 + x) # Equivalent linear model: y = Beta0*1 + Beta1*x c = lm(scale(y) ~ 1 + scale(x)) # On scaled vars to recover r\n\nResults:\n\nmodel| p.value| t| r| conf.low| conf.high  \n---|---|---|---|---|---  \ncor.test| 0.738| -0.3365| -0.0485| -0.3225| 0.233  \nlm scaled| 0.738| -0.3365| -0.0485| -0.3384| 0.2414  \nlm| 0.738| -0.3365| -0.0872| -0.6081| 0.4337  \n      \n    \n    Show R output\n    \n    ## ## Pearson's product-moment correlation ## ## data: y and x ## t = -0.33651, df = 48, p-value = 0.738 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.3225066 0.2329799 ## sample estimates: ## cor ## -0.04851394 ## ## ## Call: ## lm(formula = y ~ 1 + x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.6265 -1.1753 -0.3718 0.6607 5.7109 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) -0.09522 0.25647 -0.371 0.712 ## x -0.08718 0.25907 -0.337 0.738 ## ## Residual standard error: 1.814 on 48 degrees of freedom ## Multiple R-squared: 0.002354, Adjusted R-squared: -0.01843 ## F-statistic: 0.1132 on 1 and 48 DF, p-value: 0.738 ## ## ## Call: ## lm(formula = scale(y) ~ 1 + scale(x)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.4616 -0.6541 -0.2069 0.3677 3.1780 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) 1.722e-17 1.427e-01 0.000 1.000 ## scale(x) -4.851e-02 1.442e-01 -0.337 0.738 ## ## Residual standard error: 1.009 on 48 degrees of freedom ## Multiple R-squared: 0.002354, Adjusted R-squared: -0.01843 ## F-statistic: 0.1132 on 1 and 48 DF, p-value: 0.738\n\nThe CIs are not exactly identical, but very close.\n\n### 3.0.4 R code: Spearman correlation\n\nNote that we can interpret the slope which is the number of ranks y change for\neach rank on x. I think that this is a pretty interesting number. However, the\nintercept is less interpretable since it lies at rank(x)=0 which is impossible\nsince x starts at 1.\n\nSee the identical r (now \u201crho\u201d) and p:\n\n    \n    \n    # Spearman correlation a = cor.test(y, x, method = \"spearman\") # Built-in b = lm(rank(y) ~ 1 + rank(x)) # Equivalent linear model\n\nLet\u2019s look at the results:\n\nmodel| p.value| rho  \n---|---|---  \ncor.test| 0.7072| -0.0543  \nlm| 0.708| -0.0543  \n      \n    \n    Show R output\n    \n    ## ## Spearman's rank correlation rho ## ## data: y and x ## S = 21956, p-value = 0.7072 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## -0.05430972 ## ## ## Call: ## lm(formula = rank(y) ~ 1 + rank(x)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -23.8211 -12.0056 -0.0272 12.5215 25.6677 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) 26.88490 4.22287 6.366 6.89e-08 *** ## rank(x) -0.05431 0.14412 -0.377 0.708 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 14.71 on 48 degrees of freedom ## Multiple R-squared: 0.00295, Adjusted R-squared: -0.01782 ## F-statistic: 0.142 on 1 and 48 DF, p-value: 0.708\n\n# 4 One mean\n\n## 4.1 One sample t-test and Wilcoxon signed-rank\n\n### 4.1.1 Theory: As linear models\n\nt-test model: A single number predicts y.\n\ny=\u03b20H0:\u03b20=0\n\nIn other words, it\u2019s our good old y=\u03b20+\u03b21\u2217x where the last term is gone since\nthere is no x (essentially x=0, see left figure below).\n\nThe same is to a very close approximately true for Wilcoxon signed-rank test,\njust with the signed ranks of y instead of y itself (see right panel below).\n\nsigned_rank(y)=\u03b20\n\nThis approximation is good enough when the sample size is larger than 14 and\nalmost perfect if the sample size is larger than 50.\n\n    \n    \n    Show Source\n    \n    # T-test D_t1 = data.frame(y = rnorm_fixed(20, 0.5, 0.6), x = runif(20, 0.93, 1.07)) # Fix mean and SD P_t1 = ggplot(D_t1, aes(y = y, x = 0)) + stat_summary(fun.y=mean, geom = \"errorbar\", aes(ymax = ..y.., ymin = ..y.., color='beta_0'), lwd=2) + scale_color_manual(name = NULL, values = c(\"blue\"), labels = c(bquote(beta[0] * \" (intercept)\"))) + geom_text(aes(label = round(y, 1)), nudge_x = 0.2, size = 3, color = 'dark gray') + labs(title=' T-test') # Wilcoxon D_t1_rank = data.frame(y = signed_rank(D_t1$y)) P_t1_rank = ggplot(D_t1_rank, aes(y = y, x = 0)) + stat_summary(fun.y = mean, geom = \"errorbar\", aes(ymax = ..y.., ymin = ..y.., color = 'beta_0'), lwd = 2) + scale_color_manual(name = NULL, values = c(\"blue\"), labels = c(bquote(beta[0] * \" (intercept)\"))) + geom_text(aes(label = y), nudge_x = 0.2, size = 3, color = 'dark gray') + labs(title=' Wilcoxon') # Stich together using patchwork theme_axis(P_t1, ylim = c(-1, 2), legend.position = c(0.6, 0.1)) + theme_axis(P_t1_rank, ylim = NULL, legend.position = c(0.6, 0.1))\n\n### 4.1.2 R code: One-sample t-test\n\nTry running the R code below and see that the linear model (lm) produces the\nsame t, p, and r as the built-in t.test. The confidence interval is not\npresented in the output of lm but is also identical if you use\nconfint(lm(...)):\n\n    \n    \n    # Built-in t-test a = t.test(y) # Equivalent linear model: intercept-only b = lm(y ~ 1)\n\nResults:\n\nmodel| mean| p.value| t| df| conf.low| conf.high  \n---|---|---|---|---|---|---  \nt.test| -0.0952| 0.7095| -0.3747| 49| -0.6059| 0.4155  \nlm| -0.0952| 0.7095| -0.3747| 49| -0.6059| 0.4155  \n      \n    \n    Show R output\n    \n    ## ## One Sample t-test ## ## data: y ## t = -0.37466, df = 49, p-value = 0.7095 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## -0.6059252 0.4154934 ## sample estimates: ## mean of x ## -0.09521589 ## ## ## Call: ## lm(formula = y ~ 1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.6877 -1.1888 -0.3123 0.5868 5.5883 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) -0.09522 0.25414 -0.375 0.71 ## ## Residual standard error: 1.797 on 49 degrees of freedom\n\n### 4.1.3 R code: Wilcoxon signed-rank test\n\nIn addition to matching p-values, lm also gives us the mean signed rank, which\nI find to be an informative number.\n\n    \n    \n    # Built-in a = wilcox.test(y) # Equivalent linear model b = lm(signed_rank(y) ~ 1) # See? Same model as above, just on signed ranks # Bonus: of course also works for one-sample t-test c = t.test(signed_rank(y))\n\nResults:\n\nmodel| p.value| mean_rank  \n---|---|---  \nwilcox.test| 0.213  \nlm| 0.2146| -5.18  \nt.test| 0.2146| -5.18  \n      \n    \n    Show R output\n    \n    ## ## Wilcoxon signed rank test with continuity correction ## ## data: y ## V = 508, p-value = 0.213 ## alternative hypothesis: true location is not equal to 0 ## ## ## Call: ## lm(formula = signed_rank(y) ~ 1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -41.82 -22.57 -5.32 18.68 55.18 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) -5.18 4.12 -1.257 0.215 ## ## Residual standard error: 29.13 on 49 degrees of freedom ## ## ## One Sample t-test ## ## data: signed_rank(y) ## t = -1.2573, df = 49, p-value = 0.2146 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## -13.459062 3.099062 ## sample estimates: ## mean of x ## -5.18\n\n## 4.2 Paired samples t-test and Wilcoxon matched pairs\n\n### 4.2.1 Theory: As linear models\n\nt-test model: a single number (intercept) predicts the pairwise differences.\n\ny2\u2212y1=\u03b20H0:\u03b20=0\n\nThis means that there is just one y=y2\u2212y1 to predict and it becomes a one-\nsample t-test on the pairwise differences. The visualization is therefore also\nthe same as for the one-sample t-test. At the risk of overcomplicating a\nsimple substraction, you can think of these pairwise differences as slopes\n(see left panel of the figure), which we can represent as y-offsets (see right\npanel of the figure):\n\n    \n    \n    Show Source\n    \n    # Data for plot N = nrow(D_t1) start = rnorm_fixed(N, 0.2, 0.3) D_tpaired = data.frame( x = rep(c(0, 1), each = N), y = c(start, start + D_t1$y), id = 1:N ) # Plot P_tpaired = ggplot(D_tpaired, aes(x = x, y = y)) + geom_line(aes(group = id)) + labs(title = ' Pairs') # Use patchwork to put them side-by-side theme_axis(P_tpaired) + theme_axis(P_t1, legend.position = c(0.6, 0.1))\n\nSimilarly, the Wilcoxon matched pairs only differ from Wilcoxon signed-rank in\nthat it\u2019s testing the signed ranks of the pairwise y\u2212x differences.\n\nsigned_rank(y2\u2212y1)=\u03b20H0:\u03b20=0\n\n### 4.2.2 R code: Paired sample t-test\n\n    \n    \n    a = t.test(y, y2, paired = TRUE) # Built-in paired t-test b = lm(y - y2 ~ 1) # Equivalent linear model\n\nResults:\n\nmodel| mean| p.value| df| t| conf.low| conf.high  \n---|---|---|---|---|---|---  \nt.test| -0.5952| 0.0934| 49| -1.7108| -1.2944| 0.104  \nlm| -0.5952| 0.0934| 49| -1.7108| -1.2944| 0.104  \n      \n    \n    Show R output\n    \n    ## ## Paired t-test ## ## data: y and y2 ## t = -1.7108, df = 49, p-value = 0.09345 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.2943926 0.1039608 ## sample estimates: ## mean of the differences ## -0.5952159 ## ## ## Call: ## lm(formula = y - y2 ~ 1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.9699 -1.4071 -0.0062 1.0771 7.2116 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) -0.5952 0.3479 -1.711 0.0934 . ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 2.46 on 49 degrees of freedom\n\n### 4.2.3 R code: Wilcoxon matched pairs\n\nAgain, we do the signed-ranks trick. This is still an approximation, but a\nclose one:\n\n    \n    \n    # Built-in Wilcoxon matched pairs a = wilcox.test(y, y2, paired = TRUE) # Equivalent linear model: b = lm(signed_rank(y - y2) ~ 1) # Bonus: identical to one-sample t-test ong signed ranks c = t.test(signed_rank(y - y2))\n\nResults:\n\nmodel| p.value| mean_rank_diff  \n---|---|---  \nwilcox.test| 0.0447  \nlm| 0.0429| -8.34  \nt.test| 0.0429| -8.34  \n      \n    \n    Show R output\n    \n    ## ## Wilcoxon signed rank test with continuity correction ## ## data: y and y2 ## V = 429, p-value = 0.04466 ## alternative hypothesis: true location shift is not equal to 0 ## ## ## Call: ## lm(formula = signed_rank(y - y2) ~ 1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -40.66 -23.16 -4.66 20.84 58.34 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) -8.340 4.013 -2.078 0.0429 * ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 28.37 on 49 degrees of freedom ## ## ## One Sample t-test ## ## data: signed_rank(y - y2) ## t = -2.0785, df = 49, p-value = 0.04293 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## -16.4036084 -0.2763916 ## sample estimates: ## mean of x ## -8.34\n\nFor large sample sizes (N >> 100), this approaches the sign test to a\nreasonable degree, but this approximation is too inaccurate to flesh out here.\n\n# 5 Two means\n\n## 5.1 Independent t-test and Mann-Whitney U\n\n### 5.1.1 Theory: As linear models\n\nIndependent t-test model: two means predict y.\n\nyi=\u03b20+\u03b21xiH0:\u03b21=0\n\nwhere xi is an indicator (0 or 1) saying whether data point i was sampled from\none or the other group. Indicator variables (also called \u201cdummy coding\u201d)\nunderly a lot of linear models and we\u2019ll take an aside to see how it works in\na minute.\n\nMann-Whitney U (also known as Wilcoxon rank-sum test for two independent\ngroups; no signed rank this time) is the same model to a very close\napproximation, just on the ranks of x and y instead of the actual values:\n\nrank(yi)=\u03b20+\u03b21xiH0:\u03b21=0\n\nTo me, equivalences like this make \u201cnon-parametric\u201d statistics much easier to\nunderstand. The approximation is appropriate when the sample size is larger\nthan 11 in each group and virtually perfect when N > 30 in each group.\n\n### 5.1.2 Theory: Dummy coding\n\nDummy coding can be understood visually. The indicator is on the x-axis so\ndata points from the first group are located at x=0 and data points from the\nsecond group is located at x=1. Then \u03b20 is the intercept (blue line) and \u03b21 is\nthe slope between the two means (red line). Why? Because when \u0394x=1 the slope\nequals the difference because:\n\nslope=\u0394y/\u0394x=\u0394y/1=\u0394y=difference\n\nMagic! Even categorical differences can be modelled using linear models! It\u2019s\na true Swizz army knife.\n\n    \n    \n    Show Source\n    \n    # Data N = 20 # Number of data points per group D_t2 = data.frame( x = rep(c(0, 1), each=N), y = c(rnorm_fixed(N, 0.3, 0.3), rnorm_fixed(N, 1.3, 0.3)) ) # Plot P_t2 = ggplot(D_t2, aes(x=x, y=y)) + stat_summary(fun.y = mean, geom = \"errorbar\", aes(ymax = ..y.., ymin = ..y.., color = 'something'), lwd = 2) + geom_segment(x = -10, xend = 10, y = 0.3, yend = 0.3, lwd = 2, aes(color = 'beta_0')) + geom_segment(x = 0, xend = 1, y = 0.3, yend = 1.3, lwd = 2, aes(color = 'beta_1')) + scale_color_manual(name = NULL, values = c(\"blue\", \"red\", \"darkblue\"), labels=c(bquote(beta[0]*\" (group 1 mean)\"), bquote(beta[1]*\" (slope = difference)\"), bquote(beta[0]+beta[1]%.%1*\" (group 2 mean)\"))) #scale_x_discrete(breaks=c(0.5, 1.5), labels=c('1', '2')) theme_axis(P_t2, jitter = TRUE, xlim = c(-0.3, 2), legend.position = c(0.53, 0.08))\n\n### 5.1.3 Theory: Dummy coding (continued)\n\nIf you feel like you get dummy coding now, just skip ahead to the next\nsection. Here is a more elaborate explanation of dummy coding:\n\nIf a data point was sampled from the first group, i.e., when xi=0, the model\nsimply becomes y=\u03b20+\u03b21\u22c50=\u03b20. In other words, the model predicts that that data\npoint is beta0. It turns out that the \u03b2 which best predicts a set of data\npoints is the mean of those data points, so \u03b20 is the mean of group 1.\n\nOn the other hand, data points sampled from the second group would have xi=1\nso the model becomes yi=\u03b20+\u03b21\u22c51=\u03b20+\u03b21. In other words, we add \u03b21 to \u201cshift\u201d\nfrom the mean of the first group to the mean of the second group. Thus \u03b21\nbecomes the mean difference between the groups.\n\nAs an example, say group 1 is 25 years old (\u03b20=25) and group 2 is 28 years old\n(\u03b21=3), then the model for a person in group 1 is y=25+3\u22c50=25 and the model\nfor a person in group 2 is y=25+3\u22c51=28.\n\nHooray, it works! For first-timers it takes a few moments to understand dummy\ncoding, but you only need to know addition and multiplication to get there!\n\n### 5.1.4 R code: independent t-test\n\nAs a reminder, when we write y ~ 1 + x in R, it is shorthand for y=\u03b20\u22c51+\u03b21\u22c5x\nand R goes on computing the \u03b2s for you. Thus y ~ 1 + x is the R-way of writing\ny=a\u22c5x+b.\n\nNotice the identical t, df, p, and estimates. We can get the confidence\ninterval by running confint(lm(...)).\n\n    \n    \n    # Built-in independent t-test on wide data a = t.test(y, y2, var.equal = TRUE) # Be explicit about the underlying linear model by hand-dummy-coding: group_y2 = ifelse(group == 'y2', 1, 0) # 1 if group == y2, 0 otherwise b = lm(value ~ 1 + group_y2) # Using our hand-made dummy regressor # Note: We could also do the dummy-coding in the model # specification itself. Same result. c = lm(value ~ 1 + I(group == 'y2'))\n\nResults:\n\nmodel| mean_y| difference| p.value| df| conf.low| conf.high  \n---|---|---|---|---|---|---  \nt.test| -0.0872| 0.738| 48| -0.233| 0.3225  \nlm| -0.0872| 98| -0.0617| 1.2521  \n      \n    \n    Show R output\n    \n    ## ## Two Sample t-test ## ## data: y and y2 ## t = -1.798, df = 98, p-value = 0.07525 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.25214980 0.06171803 ## sample estimates: ## mean of x mean of y ## -0.09521589 0.50000000 ## ## ## Call: ## lm(formula = value ~ 1 + group_y2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.6877 -1.0311 -0.2435 0.6106 5.5883 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) -0.09522 0.23408 -0.407 0.6851 ## group_y2 0.59522 0.33104 1.798 0.0753 . ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 1.655 on 98 degrees of freedom ## Multiple R-squared: 0.03194, Adjusted R-squared: 0.02206 ## F-statistic: 3.233 on 1 and 98 DF, p-value: 0.07525 ## ## ## Call: ## lm(formula = value ~ 1 + I(group == \"y2\")) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.6877 -1.0311 -0.2435 0.6106 5.5883 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) -0.09522 0.23408 -0.407 0.6851 ## I(group == \"y2\")TRUE 0.59522 0.33104 1.798 0.0753 . ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 1.655 on 98 degrees of freedom ## Multiple R-squared: 0.03194, Adjusted R-squared: 0.02206 ## F-statistic: 3.233 on 1 and 98 DF, p-value: 0.07525\n\n### 5.1.5 R code: Mann-Whitney U\n\n    \n    \n    # Wilcoxon / Mann-Whitney U a = wilcox.test(y, y2) # As linear model with our dummy-coded group_y2: b = lm(rank(value) ~ 1 + group_y2) # compare to linear model above\n\nmodel| p.value| rank_diff  \n---|---|---  \nwilcox.test| 0.0248  \nlm| 0.0238| 13.04  \n      \n    \n    Show R output\n    \n    ## ## Wilcoxon rank sum test with continuity correction ## ## data: y and y2 ## W = 924, p-value = 0.02484 ## alternative hypothesis: true location shift is not equal to 0 ## ## ## Call: ## lm(formula = rank(value) ~ 1 + group_y2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -49.02 -22.26 -1.50 23.27 56.02 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) 43.980 4.017 10.948 <2e-16 *** ## group_y2 13.040 5.681 2.295 0.0238 * ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 28.41 on 98 degrees of freedom ## Multiple R-squared: 0.05102, Adjusted R-squared: 0.04133 ## F-statistic: 5.269 on 1 and 98 DF, p-value: 0.02385\n\n## 5.2 Welch\u2019s t-test\n\nThis is identical to the (Student\u2019s) independent t-test above except that\nStudent\u2019s assumes identical variances and Welch\u2019s t-test does not. So the\nlinear model is the same but we model one variance per group. We can do this\nusing the nlme package (see more details here):\n\n    \n    \n    # Built-in a = t.test(y, y2, var.equal=FALSE) # As linear model with per-group variances b = nlme::gls(value ~ 1 + group_y2, weights = nlme::varIdent(form=~1|group), method=\"ML\")\n\nResults:\n\nmodel| mean_y| mean_diff| p.value| t| conf.low| conf.high  \n---|---|---|---|---|---|---  \nt.test| -0.0952| 0.5952| 0.0753| -1.798| -0.062| 1.2524  \ngls| -0.0952| 0.5952| 0.0753| -1.798| -0.0536| 1.244  \n      \n    \n    Show R output\n    \n    ## ## Welch Two Sample t-test ## ## data: y and y2 ## t = -1.798, df = 94.966, p-value = 0.07535 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.25241218 0.06198041 ## sample estimates: ## mean of x mean of y ## -0.09521589 0.50000000 ## ## Generalized least squares fit by maximum likelihood ## Model: value ~ 1 + group_y2 ## Data: NULL ## AIC BIC logLik ## 388.9273 399.348 -190.4636 ## ## Variance function: ## Structure: Different standard deviations per stratum ## Formula: ~1 | group ## Parameter estimates: ## y1 y2 ## 1.0000000 0.8347122 ## ## Coefficients: ## Value Std.Error t-value p-value ## (Intercept) -0.0952159 0.2541379 -0.3746623 0.7087 ## group_y2 0.5952159 0.3310379 1.7980295 0.0753 ## ## Correlation: ## (Intr) ## group_y2 -0.768 ## ## Standardized residuals: ## Min Q1 Med Q3 Max ## -1.5789902 -0.6351823 -0.1639999 0.3735898 3.1413235 ## ## Residual standard error: 1.778965 ## Degrees of freedom: 100 total; 98 residual\n\n# 6 Three or more means\n\nANOVAs are linear models with (only) categorical predictors so they simply\nextend everything we did above, relying heavily on dummy coding. Do make sure\nto read the section on dummy coding if you haven\u2019t already.\n\n## 6.1 One-way ANOVA and Kruskal-Wallis\n\n### 6.1.1 Theory: As linear models\n\nModel: One mean for each group predicts y.\n\ny=\u03b20+\u03b21x1+\u03b22x2+\u03b23x3+...H0:y=\u03b20\n\nwhere xi are indicators (x=0 or x=1) where at most one xi=1 while all others\nare xi=0.\n\nNotice how this is just \u201cmore of the same\u201d of what we already did in other\nmodels above. When there are only two groups, this model is y=\u03b20+\u03b21\u2217x, i.e.\nthe independent t-test. If there is only one group, it is y=\u03b20, i.e. the one-\nsample t-test. This is easy to see in the visualization below - just cover up\na few groups and see that it matches the other visualizations above.\n\n    \n    \n    Show Source\n    \n    # Figure N = 15 D_anova1 = data.frame( y = c( rnorm_fixed(N, 0.5, 0.3), rnorm_fixed(N, 0, 0.3), rnorm_fixed(N, 1, 0.3), rnorm_fixed(N, 0.8, 0.3) ), x = rep(0:3, each = 15) ) ymeans = aggregate(y~x, D_anova1, mean)$y P_anova1 = ggplot(D_anova1, aes(x=x, y=y)) + stat_summary(fun.y=mean, geom = \"errorbar\", aes(ymax = ..y.., ymin = ..y.., color='intercepts'), lwd=2) + geom_segment(x = -10, xend = 100, y = 0.5, yend = 0.5, lwd = 2, aes(color = 'beta_0')) + geom_segment(x = 0, xend = 1, y = ymeans[1], yend = ymeans[2], lwd = 2, aes(color = 'betas')) + geom_segment(x = 1, xend = 2, y = ymeans[1], yend = ymeans[3], lwd = 2, aes(color = 'betas')) + geom_segment(x = 2, xend = 3, y = ymeans[1], yend = ymeans[4], lwd = 2, aes(color = 'betas')) + scale_color_manual( name = NULL, values = c(\"blue\", \"red\", \"darkblue\"), labels=c( bquote(beta[0]*\" (group 1 mean)\"), bquote(beta[1]*\", \"*beta[2]*\", etc. (slopes/differences to \"*beta[0]*\")\"), bquote(beta[0]*\"+\"*beta[1]*\", \"*beta[0]*\"+\"*beta[2]*\", etc. (group 2, 3, ... means)\") ) ) theme_axis(P_anova1, xlim = c(-0.5, 4), legend.position = c(0.7, 0.1))\n\nA one-way ANOVA has a log-linear counterpart called goodness-of-fit test which\nwe\u2019ll return to. By the way, since we now regress on more than one x, the one-\nway ANOVA is a multiple regression model.\n\nThe Kruskal-Wallis test is simply a one-way ANOVA on the rank-transformed y\n(value):\n\nrank(y)=\u03b20+\u03b21x1+\u03b22x2+\u03b23x3+...\n\nThis approximation is good enough for 12 or more data points. Again, if you do\nthis for just one or two groups, we\u2019re already acquainted with those\nequations, i.e. the Wilcoxon signed-rank test or the Mann-Whitney U test\nrespectively.\n\n### 6.1.2 Example data\n\nWe make a three-level factor with the levels a, b, and c so that the one-way\nANOVA basically becomes a \u201cthree-sample t-test\u201d. Then we manually do the dummy\ncoding of the groups.\n\n    \n    \n    # Three variables in \"long\" format N = 20 # Number of samples per group D = data.frame( value = c(rnorm_fixed(N, 0), rnorm_fixed(N, 1), rnorm_fixed(N, 0.5)), group = rep(c('a', 'b', 'c'), each = N), # Explicitly add indicator/dummy variables # Could also be done using model.matrix(~D$group) #group_a = rep(c(1, 0, 0), each=N), # This is the intercept. No need to code group_b = rep(c(0, 1, 0), each = N), group_c = rep(c(0, 0, 1), each = N) ) # N of each level\n\nvalue| group| group_b| group_c  \n---|---|---|---  \n-1.2582| a| 0| 0  \n0.085| a| 0| 0  \n-1.9297| a| 0| 0  \n0.697| a| 0| 0  \n0.5866| a| 0| 0  \n0.3162| a| 0| 0  \n0.5462| a| 0| 0  \n0.0517| a| 0| 0  \n0.687| a| 0| 0  \n0.6063| a| 0| 0  \n  \nShowing 1 to 10 of 60 entries\n\nPrevious123456Next\n\nWith group a\u2019s intercept omni-present, see how exactly one other parameter is\nadded to predict value for group b and c in a given row (scroll to he end).\nThus data points in group b never affect the estimates in group c.\n\n### 6.1.3 R code: one-way ANOVA\n\nOK, let\u2019s see the identity between a dedicated ANOVA function (car::Anova) and\nthe dummy-coded in-your-face linear model in lm.\n\n    \n    \n    # Compare built-in and linear model a = car::Anova(aov(value ~ group, D)) # Dedicated ANOVA function b = lm(value ~ 1 + group_b + group_c, data = D) # As in-your-face linear model\n\nResults:\n\nmodel| df| df.residual| F| p.value  \n---|---|---|---|---  \nAnova| 2| 57| 5| 0.00998  \nlm| 2| 57| -0.33651| 0.73795  \n      \n    \n    Show R output\n    \n    ## Anova Table (Type II tests) ## ## Response: value ## Sum Sq Df F value Pr(>F) ## group 10 2 5 0.009984 ** ## Residuals 57 57 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Call: ## lm(formula = value ~ 1 + group_b + group_c, data = D) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.3183 -0.6719 -0.1182 0.6895 1.8880 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) 1.720e-16 2.236e-01 0.000 1.00000 ## group_b 1.000e+00 3.162e-01 3.162 0.00251 ** ## group_c 5.000e-01 3.162e-01 1.581 0.11938 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 1 on 57 degrees of freedom ## Multiple R-squared: 0.1493, Adjusted R-squared: 0.1194 ## F-statistic: 5 on 2 and 57 DF, p-value: 0.009984\n\nActually, car::Anova and aov are wrappers around lm so the identity comes as\nno surprise. It only shows that the dummy-coded formula, which had a direct\ninterpretation as a linear model, is the one that underlies the shorthand\nnotation syntax y ~ factor. Indeed, the only real reason to use aov and\ncar::Anova rather than lm is to get a nicely formatted ANOVA table.\n\nThe default output of lm returns parameter estimates as well (bonus!), which\nyou can see if you unfold the R output above. However, because this IS the\nANOVA model, you can also get parameter estimates out into the open by calling\ncoefficients(aov(...)).\n\nNote that I do not use the aov function because it computes type-I sum of\nsquares, which is widely discouraged. There is a BIG polarized debate about\nwhether to use type-II (as car::Anova does by default) or type-III sum of\nsquares (set car::Anova(..., type=3)), but let\u2019s skip that for now.\n\n### 6.1.4 R code: Kruskal-Wallis\n\n    \n    \n    a = kruskal.test(value ~ group, D) # Built-in b = lm(rank(value) ~ 1 + group_b + group_c, D) # As linear model c = car::Anova(aov(rank(value) ~ group, D)) # The same model, using a dedicated ANOVA function. It just wraps lm.\n\nResults:\n\nmodel| df| p.value  \n---|---|---  \nkruskal.test| 2| 0.0312  \nlm| 2| 0.0283  \n      \n    \n    Show R output\n    \n    ## ## Kruskal-Wallis rank sum test ## ## data: value by group ## Kruskal-Wallis chi-squared = 6.9348, df = 2, p-value = 0.0312 ## ## ## Call: ## lm(formula = rank(value) ~ 1 + group_b + group_c, data = D) ## ## Residuals: ## Min 1Q Median 3Q Max ## -27.200 -13.950 -0.875 14.912 27.450 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) 23.750 3.732 6.363 3.63e-08 *** ## group_b 14.450 5.278 2.738 0.00824 ** ## group_c 5.800 5.278 1.099 0.27645 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 16.69 on 57 degrees of freedom ## Multiple R-squared: 0.1175, Adjusted R-squared: 0.08657 ## F-statistic: 3.796 on 2 and 57 DF, p-value: 0.02834 ## ## Anova Table (Type II tests) ## ## Response: rank(value) ## Sum Sq Df F value Pr(>F) ## group 2115.1 2 3.796 0.02834 * ## Residuals 15879.9 57 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n## 6.2 Two-way ANOVA (plot in progress!)\n\n### 6.2.1 Theory: As linear models\n\nModel: one mean per group (main effects) plus these means multiplied across\nfactors (interaction effects). The main effects are the one-way ANOVAs above,\nthough in the context of a larger model. The interaction effect is harder to\nexplain in the abstract even though it\u2019s just a few numbers multiplied with\neach other. I will leave that to the teachers to keep focus on equivalences\nhere :-)\n\nSwitching to matrix notation:\n\ny=\u03b20+\u03b21X1+\u03b22X2+\u03b23X1X2H0:\u03b23=0\n\nHere \u03b2i are vectors of betas of which only one is selected by the indicator\nvector Xi. The H0 shown here is the interaction effect. Note that the\nintercept \u03b20, to which all other \u03b2s are relative, is now the mean for the\nfirst level of all factors.\n\nContinuing with the dataset from the one-way ANOVA above, let\u2019s add a crossing\nfactor mood so that we can test the group:mood interaction (a 3x2 ANOVA). We\nalso do the dummy coding of this factor needed for the linear model.\n\n    \n    \n    # Crossing factor D$mood = c('happy', 'sad') # Dummy coding D$mood_happy = ifelse(D$mood == 'happy', 1, 0) # 1 if mood==happy. 0 otherwise. #D$mood_sad = ifelse(D$mood == 'sad', 1, 0) # Same, but we won't be needing this\n\nvalue| group| group_b| group_c| mood| mood_happy  \n---|---|---|---|---|---  \n-1.2582| a| 0| 0| happy| 1  \n0.085| a| 0| 0| sad| 0  \n-1.9297| a| 0| 0| happy| 1  \n0.697| a| 0| 0| sad| 0  \n0.5866| a| 0| 0| happy| 1  \n0.3162| a| 0| 0| sad| 0  \n0.5462| a| 0| 0| happy| 1  \n0.0517| a| 0| 0| sad| 0  \n0.687| a| 0| 0| happy| 1  \n0.6063| a| 0| 0| sad| 0  \n  \nShowing 1 to 10 of 60 entries\n\nPrevious123456Next\n\n\u03b20 is now the happy guys from group a!\n\n    \n    \n    Show Source\n    \n    # Add intercept line # Add cross... # Use other data? means = lm(value ~ mood * group, D)$coefficients P_anova2 = ggplot(D, aes(x=group, y=value, color=mood)) + geom_segment(x = -10, xend = 100, y = means[1], yend = 0.5, col = 'blue', lwd = 2) + stat_summary(fun.y = mean, geom = \"errorbar\", aes(ymax = ..y.., ymin = ..y..), lwd = 2) theme_axis(P_anova2, xlim = c(-0.5, 3.5)) + theme(axis.text.x = element_text())\n\n### 6.2.2 R code: Two-way ANOVA\n\nNow let\u2019s turn to the actual modeling in R. We compare a dedicated ANOVA\nfunction (car::Anova; see One-Way ANOVA why) to the linear model (lm). Notice\nthat in ANOVA, we are testing a full factor interaction all at once which\ninvolves many parameters (two in this case), so we can\u2019t look at the overall\nmodel fit nor any particular parameter for the result. Therefore, I use a\nlikelihood-ratio test to compare a full two-way ANOVA model (\u201csaturated\u201d) to\none without the interaction effect(s). The anova function does this test. Even\nthough that looks like cheating, it\u2019s just computing likelihoods, p-values,\netc. on the models that were already fitted, so it\u2019s legit!\n\n    \n    \n    # Dedicated two-way ANOVA functions a = car::Anova(aov(value ~ mood * group, D), type='II') # Normal notation. \"*\" both multiplies and adds main effects b = car::Anova(aov(value ~ mood + group + mood:group, D)) # Identical but more verbose about main effects and interaction # Testing the interaction terms as linear model. full = lm(value ~ 1 + group_b + group_c + mood_happy + group_b:mood_happy + group_c:mood_happy, D) # Full model null = lm(value ~ 1 + group_b + group_c + mood_happy, D) # Without interaction c = anova(null, full) # whoop whoop, same F, p, and Dfs\n\nResults:\n\nmodel| F| df| p.value| sumsq| res.sumsq  \n---|---|---|---|---|---  \nAnova mood:group| 0.0001| 2| 0.9999| 0.0002| 56.4813  \nlm LRT| 0.0001| 2| 0.9999| 0.0002| 56.4813  \n      \n    \n    Show R output\n    \n    ## Anova Table (Type II tests) ## ## Response: value ## Sum Sq Df F value Pr(>F) ## mood 0.519 1 0.4958 0.48438 ## group 10.000 2 4.7803 0.01226 * ## mood:group 0.000 2 0.0001 0.99993 ## Residuals 56.481 54 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## Analysis of Variance Table ## ## Model 1: value ~ 1 + group_b + group_c + mood_happy ## Model 2: value ~ 1 + group_b + group_c + mood_happy + group_b:mood_happy + ## group_c:mood_happy ## Res.Df RSS Df Sum of Sq F Pr(>F) ## 1 56 56.481 ## 2 54 56.481 2 0.00015625 1e-04 0.9999\n\nBelow, I present approximate main effect models, though exact calculation of\nANOVA main effects is more involved if it is to be accurate and furthermore\ndepend on whether type-II or type-III sum of squares are used for inference.\n\nLook at the model summary statistics to find values comparable to the Anova-\nestimated main effects above.\n\n    \n    \n    # Main effect of group. e = lm(value ~ 1 + group_b + group_c, D) # Main effect of mood. f = lm(value ~ 1 + mood_happy, D)\n\nterm| model| df| F| p.value  \n---|---|---|---|---  \ngroup| Anova| 2| 4.78035| 0.01226  \ngroup| lm| 2| 5| 0.00998  \nmood| Anova| 1| 0.49579| 0.48438  \nmood| lm| 1| 0.45241| 0.50386  \n      \n    \n    Show R output\n    \n    ## ## Call: ## lm(formula = value ~ 1 + group_b + group_c, data = D) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.3183 -0.6719 -0.1182 0.6895 1.8880 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) 1.720e-16 2.236e-01 0.000 1.00000 ## group_b 1.000e+00 3.162e-01 3.162 0.00251 ** ## group_c 5.000e-01 3.162e-01 1.581 0.11938 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 1 on 57 degrees of freedom ## Multiple R-squared: 0.1493, Adjusted R-squared: 0.1194 ## F-statistic: 5 on 2 and 57 DF, p-value: 0.009984 ## ## ## Call: ## lm(formula = value ~ 1 + mood_happy, data = D) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.72530 -0.67261 0.05145 0.79348 2.29498 ## ## Coefficients: ## Estimate Std. Error t value Pr(>|t|) ## (Intercept) 0.5930 0.1955 3.034 0.00361 ** ## mood_happy -0.1859 0.2764 -0.673 0.50386 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 1.071 on 58 degrees of freedom ## Multiple R-squared: 0.00774, Adjusted R-squared: -0.009368 ## F-statistic: 0.4524 on 1 and 58 DF, p-value: 0.5039\n\n## 6.3 ANCOVA\n\nThis is simply ANOVA with a continuous regressor added so that it now contains\ncontinuous and (dummy-coded) categorical predictors. For example, if we\ncontinue with the one-way ANOVA example, we can add age and it is now called a\none-way ANCOVA:\n\ny=\u03b20+\u03b21x1+\u03b22x2+...+\u03b23age\n\n... where xi are our usual dummy-coded indicator variables. \u03b20 is now the mean\nfor the first group at age=0. You can turn all ANOVAs into ANCOVAs this way,\ne.g. by adding \u03b2N\u22c5age to our two-way ANOVA in the previous section. But let us\ngo ahead with our one-way ANCOVA, starting by adding age to our dataset:\n\n    \n    \n    # Update data with a continuous covariate D$age = D$value + rnorm_fixed(nrow(D), sd = 3) # Correlated to value\n\nThis is best visualized using colors for groups instead of x-position. The \u03b2s\nare still the average y-offset of the data points, only now we model each\ngroup using a slope instead of an intercept. In other words, the one-way ANOVA\nis sort of one-sample t-tests model for each group (y=\u03b20) while the one-way\nANCOVA is sort of Pearson correlation model for each group (yi=\u03b20+\u03b2i+\u03b21\u22c5age):\n\n    \n    \n    Show Source\n    \n    # For linear model plot D$pred = predict(lm(value ~ age + group, D)) # Plot P_ancova = ggplot(D, aes(x=age, y=value, color=group, shape=group)) + geom_line(aes(y=pred), lwd=2) # Theme it theme_axis(P_ancova, xlim=NULL, ylim=NULL, legend.position=c(0.8, 0.2)) + theme(axis.title=element_text())\n\nAnd now some R code to run the one-way ANCOVA as a linear model:\n\n    \n    \n    # Dedicated ANCOVA functions. The order of factors matter in pure-aov (type-I variance). # Use type-II (default for car::Anova) or type-III (set type=3), a = car::Anova(aov(value ~ group + age, D)) #a = aov(value ~ group + age, D) # Predictor order matters. Not nice! # As dummy-coded linear model. full = lm(value ~ 1 + group_b + group_c + age, D) # Testing main effect of age using Likelihood-ratio test null_age = lm(value ~ 1 + group_b + group_c, D) # Full without age. One-way ANOVA! result_age = anova(null_age, full) # Testing main effect of groupusing Likelihood-ratio test null_group = lm(value ~ 1 + age, D) # Full without group. Pearson correlation! result_group = anova(null_group, full)\n\nResults:\n\nterm| model| F| df| p.value| sumsq| res.sumsq| res.df  \n---|---|---|---|---|---|---|---  \nage| Anova| 15.6752| 1| 0.0002| 12.4658| 44.5342| 56  \nage| lm| 15.6752| 1| 0.0002| 12.4658| 44.5342| 56  \ngroup| Anova| 1.791| 2| 0.1762| 2.8486| 44.5342| 56  \ngroup| lm| 1.791| 2| 0.1762| 2.8486| 44.5342| 56  \n      \n    \n    Show R output\n    \n    ## Anova Table (Type II tests) ## ## Response: value ## Sum Sq Df F value Pr(>F) ## group 2.849 2 1.791 0.1762177 ## age 12.466 1 15.675 0.0002144 *** ## Residuals 44.534 56 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## Analysis of Variance Table ## ## Model 1: value ~ 1 + group_b + group_c ## Model 2: value ~ 1 + group_b + group_c + age ## Res.Df RSS Df Sum of Sq F Pr(>F) ## 1 57 57.000 ## 2 56 44.534 1 12.466 15.675 0.0002144 *** ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## Analysis of Variance Table ## ## Model 1: value ~ 1 + age ## Model 2: value ~ 1 + group_b + group_c + age ## Res.Df RSS Df Sum of Sq F Pr(>F) ## 1 58 47.383 ## 2 56 44.534 2 2.8486 1.791 0.1762\n\n# 7 Proportions: Chi-square is a log-linear model\n\nRecall that when you take the logarithm, you can easily make statements about\nproportions, i.e., that for every increase in x, y increases a certain\npercentage. This turns out to be one of the simplest (and therefore best!)\nways to make count data and contingency tables intelligible. See this nice\nintroduction to Chi-Square tests as linear models.\n\n## 7.1 Goodness of fit\n\n### 7.1.1 Theory: As log-linear model\n\nModel: a single intercept predicts log(y).\n\nI\u2019ll refer you to take a look at the section on contingency tables which is\nbasically a \u201ctwo-way goodness of fit\u201d.\n\n### 7.1.2 Example data\n\nFor this, we need some wide count data:\n\n    \n    \n    # Data in long format D = data.frame(mood = c('happy', 'sad', 'meh'), counts = c(60, 90, 70)) # Dummy coding for the linear model D$mood_happy = ifelse(D$mood == 'happy', 1, 0) D$mood_sad = ifelse(D$mood == 'sad', 1, 0)\n\nmood| counts| mood_happy| mood_sad  \n---|---|---|---  \nhappy| 60| 1| 0  \nsad| 90| 0| 1  \nmeh| 70| 0| 0  \n  \n### 7.1.3 R code: Goodness of fit\n\nNow let\u2019s see that the Goodness of fit is just a log-linear equivalent to a\none-way ANOVA. We set family = poisson() which defaults to setting a\nlogarithmic link function (family = poisson(link='log')).\n\n    \n    \n    # Built-in test a = chisq.test(D$counts) # As log-linear model, comparing to an intercept-only model full = glm(counts ~ 1 + mood_happy + mood_sad, data = D, family = poisson()) null = glm(counts ~ 1, data = D, family = poisson()) b = anova(null, full, test = 'Rao') # Note: glm can also do the dummy coding for you: c = glm(counts ~ mood, data = D, family = poisson())\n\nLet\u2019s look at the results:\n\nmodel| Chisq| df| p.value  \n---|---|---|---  \nchisq.test| 6.3636| 2| 0.0415  \nglm LRT| 6.3636| 2| 0.0415  \n      \n    \n    Show R output\n    \n    ## ## Chi-squared test for given probabilities ## ## data: D$counts ## X-squared = 6.3636, df = 2, p-value = 0.04151 ## ## Analysis of Deviance Table ## ## Model 1: counts ~ 1 ## Model 2: counts ~ 1 + mood_happy + mood_sad ## Resid. Df Resid. Dev Df Deviance Rao Pr(>Chi) ## 1 2 6.2697 ## 2 0 0.0000 2 6.2697 6.3636 0.04151 * ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nNote the strange anova(..., test='Rao') which merely states that p-values\nshould be computed using the (Rao) score test. We could also have jotted in\ntest='Chisq' or test='LRT' which would have yielded approximate p-values. You\nmay think that we\u2019re cheating here, sneaking in some sort of Chi-Square model\npost-hoc. However, anova only specifies how p-values are calculated whereas\nall the log-linear modeling happened in glm.\n\nBy the way, if there are only two counts and a large sample size (N > 100),\nthis model begins to approximate the binomial test, binom.test, to a\nreasonable degree. But this sample size is larger than most use cases, so I\nwon\u2019t raise to a rule-of-thumb and won\u2019t dig deeper into it here.\n\n## 7.2 Contingency tables\n\n### 7.2.1 Theory: As log-linear model\n\nThe theory here will be a bit more convoluted, and I mainly write it up so\nthat you can get the feeling that it really is just a log-linear two-way ANOVA\nmodel. Let\u2019s get started...\n\nFor a two-way contingency table, the model of the count variable y is a\nmodeled using the marginal proportions of a contingency table. Why this makes\nsense, is too involved to go into here, but see the relevant slides by\nChristoph Scheepers here for an excellent exposition. The model is composed of\na lot of counts and the regression coefficients Ai and Bi:\n\nyi=N\u22c5xi(Ai/N)\u22c5zj(Bj/N)\u22c5xij/((Aixi)/(Bjzj)/N)\n\nWhat a mess!!! Here, i is the row index, j is the column index, xsomething is\nthe sum of that row and/or column, N=sum(y). Remember that y is a count\nvariable, so N is just the total count.\n\nWe can simplify the notation by defining the proportions: \u03b1i=xi(Ai/N),\n\u03b2i=xj(Bi/N) and \u03b1i\u03b2j=xij/(Aixi)/(Bjzj)/N. Let\u2019s write the model again:\n\nyi=N\u22c5\u03b1i\u22c5\u03b2j\u22c5\u03b1i\u03b2j\n\nAh, much prettier. However, there is still lot\u2019s of multiplication which makes\nit hard to get an intuition about how the actual numbers interact. We can make\nit much more intelligible when we remember that log(A\u22c5B)=log(A)+log(B). Doing\nlogarithms on both sides, we get:\n\nlog(yi)=log(N)+log(\u03b1i)+log(\u03b2j)+log(\u03b1i\u03b2j)\n\nSnuggly! Now we can get a better grasp on how the regression coefficients\n(which are proportions) independently contribute to y. This is why logarithms\nare so nice for proportions. Note that this is just the two-way ANOVA model\nwith some logarithms added, so we are back to our good old linear models -\nonly the interpretation of the regression coefficients have changed! And we\ncannot use lm anymore in R.\n\n### 7.2.2 Example data\n\nHere we need some long data and we need it in table format for chisq.test:\n\n    \n    \n    # Contingency data in long format for linear model D = data.frame( mood = c('happy', 'happy', 'meh', 'meh', 'sad', 'sad'), sex = c('male', 'female', 'male', 'female', 'male', 'female'), Freq = c(100, 70, 30, 32, 110, 120) ) # ... and as table for chisq.test D_table = D %>% spread(key = mood, value = Freq) %>% # Mood to columns select(-sex) %>% # Remove sex column as.matrix() # Dummy coding of D for linear model (skipping mood==\"sad\" and gender==\"female\") # We could also use model.matrix(D$Freq~D$mood*D$sex) D$mood_happy = ifelse(D$mood == 'happy', 1, 0) D$mood_meh = ifelse(D$mood == 'meh', 1, 0) D$sex_male = ifelse(D$sex == 'male', 1, 0)\n\nmood| sex| Freq| mood_happy| mood_meh| sex_male  \n---|---|---|---|---|---  \nhappy| male| 100| 1| 0| 1  \nhappy| female| 70| 1| 0| 0  \nmeh| male| 30| 0| 1| 1  \nmeh| female| 32| 0| 1| 0  \nsad| male| 110| 0| 0| 1  \nsad| female| 120| 0| 0| 0  \n  \n### 7.2.3 R code: Chi-square test\n\nNow let\u2019s show the equivalence between a chi-square model and a log-linear\nmodel. This is very similar to our two-way ANOVA above:\n\n    \n    \n    # Built-in chi-square. It requires matrix format. a = chisq.test(D_table) # Using glm to do a log-linear model, we get identical results when testing the interaction term: full = glm(Freq ~ 1 + mood_happy + mood_meh + sex_male + mood_happy*sex_male + mood_meh*sex_male, data = D, family = poisson()) null = glm(Freq ~ 1 + mood_happy + mood_meh + sex_male, data = D, family = poisson()) b = anova(null, full, test = 'Rao') # Could also use test='LRT' or test='Chisq' # Note: let glm do the dummy coding for you full = glm(Freq ~ mood * sex, family = poisson(), data = D) c = anova(full, test = 'Rao') # Note: even simpler syntax using MASS:loglm (\"log-linear model\") d = MASS::loglm(Freq ~ mood + sex, D)\n\nmodel| Chisq| df| p.value  \n---|---|---|---  \nchisq.test| 5.0999| 2| 0.0781  \nglm| 5.0999| 2| 0.0781  \nloglm| 5.0999| 2| 0.0781  \n      \n    \n    Show R output\n    \n    ## ## Pearson's Chi-squared test ## ## data: D_table ## X-squared = 5.0999, df = 2, p-value = 0.07809 ## ## Analysis of Deviance Table ## ## Model 1: Freq ~ 1 + mood_happy + mood_meh + sex_male ## Model 2: Freq ~ 1 + mood_happy + mood_meh + sex_male + mood_happy * sex_male + ## mood_meh * sex_male ## Resid. Df Resid. Dev Df Deviance Rao Pr(>Chi) ## 1 2 5.1199 ## 2 0 0.0000 2 5.1199 5.0999 0.07809 . ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## Analysis of Deviance Table ## ## Model: poisson, link: log ## ## Response: Freq ## ## Terms added sequentially (first to last) ## ## ## Df Deviance Resid. Df Resid. Dev Rao Pr(>Chi) ## NULL 5 111.130 ## mood 2 105.308 3 5.821 94.132 < 2e-16 *** ## sex 1 0.701 2 5.120 0.701 0.40235 ## mood:sex 2 5.120 0 0.000 5.100 0.07809 . ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## Call: ## MASS::loglm(formula = Freq ~ mood + sex, data = D) ## ## Statistics: ## X^2 df P(> X^2) ## Likelihood Ratio 5.119915 2 0.07730804 ## Pearson 5.099859 2 0.07808717 ## ## Call: ## glm(formula = Freq ~ mood * sex, family = poisson(), data = D) ## ## Deviance Residuals: ## [1] 0 0 0 0 0 0 ## ## Coefficients: ## Estimate Std. Error z value Pr(>|z|) ## (Intercept) 4.2485 0.1195 35.545 < 2e-16 *** ## moodmeh -0.7828 0.2134 -3.668 0.000244 *** ## moodsad 0.5390 0.1504 3.584 0.000339 *** ## sexmale 0.3567 0.1558 2.289 0.022094 * ## moodmeh:sexmale -0.4212 0.2981 -1.413 0.157670 ## moodsad:sexmale -0.4437 0.2042 -2.172 0.029819 * ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 1.1113e+02 on 5 degrees of freedom ## Residual deviance: 3.9968e-15 on 0 degrees of freedom ## AIC: 48.254 ## ## Number of Fisher Scoring iterations: 3\n\nIf you unfold the raw R output, I\u2019ve included summary(full) so that you can\nsee the raw regression coefficients. Being a log-linear model, these are the\npercentage increasein y over and above the intercept if that category obtains.\n\n# 8 Sources and further equivalences\n\nHere are links to other sources who have exposed bits and pieces of this\npuzzle, including many further equivalences not covered here:\n\n  * My original exposition of the idea at Cross Validated\n  * An earlier question by me about non-parametric tests and a helpful answer.\n  * This question and replies on t-tests and ANOVA at StackOverflow\n  * These slides by Christoph Scheepers on Chi-Square as log-linear models.\n  * This notebook by Philip M. Alday on Chi-square, binomial, multinomial, and poisson tests as log-linear and logistic models. These \u201cequivalences\u201d are less exact than what I presented above, and were therefore not included here. They are still great for a conceptual understanding of these tests, though!\n  * This article by Kristoffer Magnusson on RM-ANOVA and growth models using lme4::lmer mixed models.\n  * This post by Thom Baguley on the Friedman test. That post was actually the one that inititated my exploration of linear equivalences to \u201cnon-parametric\u201d\" tests which ultimately pushed me over the edge to write up the present article.\n\n# 9 Teaching materials and a course outline\n\nMost advanced stats books (and some intro-books) take the \u201ceverything is GLMM\u201d\napproach as well. However, the \u201clinear model\u201d part often stays at the\nconceptual level, rather than being made explicit. I wanted to make linear\nmodels the tool in a concise way. Luckily, more beginner-friendly materials\nhave emerged lately:\n\n  * Russ Poldrack\u2019s open-source book \u201cStatistical Thinking for the 21st century\u201d (start at chapter 5 on modeling)\n  * Jeff Rouder\u2019s course notes, introducing model comparison using just R2 and BIC. It avoids all the jargon on p-values, F-values, etc. The full materials and slides are available here.\n\nHere are my own thoughts on what I\u2019d do. I\u2019ve taught parts of this with great\nsuccess already, but not the whole program since I\u2019m not assigned to teach a\nfull course yet.\n\nI would spend 50% of the time on linear modeling of data since this contains\n70% of what students need to know (bullet 1 below). The rest of the course is\nfleshing out what happens when you have one group, two groups, etc.\n\nNote that whereas the understanding of sampling and hypothesis testing is\nusually the first focus of mainstream stats courses, it is saved for later\nhere to build upon students\u2019 prior knowledge, rather than throwing a lot of\nconceptually novel material at them.\n\n  1. Fundamentals of regression:\n\n    1. Recall from high-school: y=a\u22c5x+b, and getting a really good intuition about slopes and intercepts. Understanding that this can be written using all variable names, e.g., money = profit * time + starting_money or y=\u03b21x+\u03b22\u22171 or, suppressing the coefficients, as y ~ x + 1. If the audience is receptive, convey the idea of these models as a solution to differential equations, specifying how y changes with x.\n\n    2. Extend to a few multiple regression as models. Make sure to include plenty of real-life examples and exercises at this point to make all of this really intuitive. Marvel at how briefly these models allow us to represent large datasets.\n\n    3. Introduce the idea of rank-transforming non-metric data and try it out.\n\n    4. Teach the three assumptions: independence of data points, normality of residuals, and homoscedasticity.\n\n    5. Confidence/credible intervals on the parameters. Stress that the Maximum-Likelihood estimate is extremely unlikely, so intervals are more important.\n\n    6. Briefly introduce R2 for the simple regression models above. Mention in passing that this is called the Pearson and Spearman correlation coefficients.\n\n  2. Special case #1: One or two means (t-tests, Wilcoxon, Mann-Whitney):\n\n    1. One mean: When there is only one x-value, the regression model simplifies to y=b. If y is non-metric, you can rank-transform it. Apply the assumptions (homoscedasticity doesn\u2019t apply since there is only one x). Mention in passing that these intercept-only models are called one-sample t-test and Wilcoxon Signed Rank test respectively.\n\n    2. Two means: If we put two variables 1 apart on the x-axis, the difference between the means is the slope. Great! It is accessible to our swizz army knife called linear modeling. Apply the assumption checks to see that homoscedasticity reduces to equal variance between groups. This is called an independent t-test. Do a few worked examples and exercises, maybe adding Welch\u2019s test, and do the rank-transformed version, called Mann-Whitney U.\n\n    3. Paired samples: Violates the independence assumption. After computing pairwise differences, this is equivalent to 2.1 (one intercept), though it is called the paired t-test and Wilcoxon\u2019s matched pairs.\n\n  3. Special case #2: Three or more means (ANOVAs)\n\n    1. Dummy coding of categories: How one regression coefficient for each level of a factor models an intercept for each level when multiplied by a binary indicator. This is just extending what we did in 2.1. to make this data accessible to linear modeling.\n\n    2. Means of one variable: One-way ANOVA.\n\n    3. Means of two variables: Two-way ANOVA.\n\n  4. Special case #3: Three or more proportions (Chi-Square)\n\n    1. Logarithmic transformation: Making multiplicative models linear using logarithms, thus modeling proportions. See this excellent introduction to the equivalence of log-linear models and Chi-Square tests as models of proportions. Also needs to introduce (log-)odds ratios. When the multiplicative model is made summative using logarithms, we just add the dummy-coding trick from 3.1, and see that the models are identical to the ANOVA models in 3.2 and 3.3, only the interpretation of the coefficients have changed.\n\n    2. Proportions of one variable: Goodness of fit.\n\n    3. Proportions of two variables: Contingency tables.\n\n  5. Hypothesis testing:\n\n    1. Hypothesis testing as model comparisons: Hypothesis testing is the act of choosing between a full model and one where a parameter is fixed to a particular value (often zero, i.e., effectively excluded from the model) instead of being estimated. For example, when fixing one of the two means to zero in the t-test, we study how well a single mean (a one-sample t-test) explains all the data from both groups. If it does a good job, we prefer this model over the two-mean model because it is simpler. So hypothesis testing is just comparing linear models to make more qualitative statements than the truly quantitative statements which were covered in bullets 1-4 above. As tests of single parameters, hypothesis testing is therefore less informative However, when testing multiple parameters at the same time (e.g., a factor in ANOVA), model comparison becomes invaluable.\n\n    2. Likelihood ratios: Likelihood ratios are the swizz army knife which will do model comparison all the way from the one-sample t-test to GLMMs. BIC penalizes model complexity. Moreover, add priors and you\u2019ve got Bayes Factors. One tool, and you\u2019re done. I\u2019ve used LRTs in the ANOVAs above.\n\n# 10 Limitations\n\nI have made a few simplifications for clarity:\n\n  1. I have not covered assumptions in the examples. This will be another post! But all assumptions of all tests come down to the usual three: a) independence of data points, b) normally distributed residuals, and c) homoscedasticity.\n\n  2. I assume that all null hypotheses are the absence of an effect, but everything works the same for non-zero null hypotheses.\n\n  3. I have not discussed inference. I am only including p-values in the comparisons as a crude way to show the equivalences between the underlying models since people care about p-values. Parameter estimates will show the same equivalence. How to do inference is another matter. Personally, I\u2019m a Bayesian, but going Bayesian here would render it less accessible to the wider audience. Also, doing robust models would be preferable, but fail to show the equivalences.\n\n  4. Several named tests are still missing from the list and may be added at a later time. This includes the Sign test (require large N to be reasonably approximated by a linear model), Friedman as RM-ANOVA on rank(y), McNemar, and Binomial/Multinomial. See stuff on these in the section on links to further equivalences. If you think that they should be included here, feel free to submit \u201csolutions\u201d to the github repo of this doc!\n\n", "frontpage": false}
