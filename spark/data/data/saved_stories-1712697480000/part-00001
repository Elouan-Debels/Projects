{"aid": "39978883", "title": "High Performance Computing on Kubernetes", "url": "https://treebeardtech.beehiiv.com/p/hpc-for-kubernetes", "domain": "treebeardtech.beehiiv.com", "votes": 1, "user": "alex-treebeard", "posted_at": "2024-04-09 12:53:29", "comments": 0, "source_title": "High Performance Computing (HPC) on Kubernetes", "source_text": "High Performance Computing (HPC) on Kubernetes\n\n  * treebeardtech\n  * Posts\n  * High Performance Computing (HPC) on Kubernetes\n\n# High Performance Computing (HPC) on Kubernetes\n\n## ML Platform Engineering Tips\n\nAlex Remedios April 04, 2024\n\nMachine learning (ML) engineering has evolved into a discipline and career\npath over the last few years. Whilst software engineers build web, mobile, and\nembedded experiences, ML engineers deliver model versions, inferences, and\nentire RAG applications.\n\nWhat does this mean for an engineering organisation as a whole? My main\ntakeaway is that the platform engineering team that is responsible for\nincreasing the ML team\u2019s leverage must provide a different set of primitives\nthat will support an ML engineer as they go through their MLOps processes.\n\nMaximising an ML team\u2019s leverage requires (a) minimising toil\u2014manual\nworkflows, (b) increasing their delivery velocity, and (c) mitigating inherent\nrisks such as security or cost management failures. If you can do these three\nthings, you\u2019ll be able to scale up your operations.\n\nThis advice can apply to many different types of ML components:\n\n  * \u201cServerless\u201d Inference applications used for real-time customer interactions for fraud detection, product recommendations, or chatbots\n\n  * Asynchronous inference applications used for image and video generation or understanding, potentially using long-running workers reading from some stream of requests\n\n  * Batch systems which can be used for data-prep, training, offline inference, or evaluation\n\nIn this post, we\u2019ll explore some of the options available to ML Platform\nengineers for providing batch processing capabilities to internal customers on\ntheir Kubernetes platform.\n\n## From HPC to Kubernetes\n\nCloud-native computing on top of Kubernetes has become the de-facto standard\nfor new software projects. This is simple for many use cases, but high-\nperformance computing (HPC) is not a simple area.\n\nAs big data applications have evolved from low-level distributed computing\nlibraries like MPI to frameworks such as Spark and Ray, the underlying\nplatforms such as Slurm and LSF are also being challenged by Kubernetes which\ncan be adapted to provide an HPC job-queue interface.\n\nBuilding out an HPC environment on Kubernetes requires understanding the\nlandscape of tools for constructing a more productive, efficient, and secure\nenvironment for ML engineering.\n\n### Kuberay\n\nThe Ray project is the most successful and universal approach to making the\nPython programming language scale to large distributed environments.\n\nIts success with ML engineers means that the Kuberay Operator is a promising\napproach for increasing the agency of team members. This project effectively\nturns your K8s cluster into a Ray platform which can be used to provide self-\nservice Ray clusters and jobs to any team.\n\n(Kuberay GitHub)\n\n### Kubeflow Spark Operator\n\nWhilst Ray is attractive for how Python-native it is. Spark has been around a\nlot longer, meaning there is a huge number of Spark applications and\npractitioners around.\n\nThis spark operator is similar to kuberay, except it manages spark clusters.\nIt was originally developed by Google Cloud and recently donated to the\nKubeflow project (read more here).\n\n(Spark Operator GitHub)\n\n### Volcano\n\nWhilst the first two projects provide a Pythonic entry point to distributed\nsystems, it\u2019s important to ensure that jobs are reliably executed with\nefficient usage of cloud resources.\n\nAs previously mentioned, HPC/job-queue workloads have different requirements\nto many other applications you may want to host on Kubernetes. This is true\nspecifically for pod scheduling logic, which by default is handled by the\nkube-scheduler.\n\nML teams may need features for scheduling jobs according to priority or\nwaiting for a whole set of jobs to be ready before running them.\n\nThis is what the volcano project lets you achieve, and it does so by replacing\nthe default kube-scheduler.\n\n(Volcano GitHub)\n\n### Kueue\n\nWhilst Volcano provides advanced scheduling features by replacing the kube-\nscheduler, Kueue can do so by complementing the scheduler.\n\nKueue provides job queueing and prioritisation via the admission webhook \u2014\ni.e. it catches jobs as you create them and suspends them until it\u2019s their\nturn.\n\n(Kueue GitHub)\n\n### Armada\n\nKueue and Volcano both provide relatively lightweight modifications to\nKubernetes\u2019 scheduling features but there is a cost to this. Pending jobs are\nstored in the cluster config store (etcd) which can cause an availability risk\ndepending on the size of the job queue.\n\nArmada fixes this issue by providing this functionality with its own control\nplane (as opposed to using the Kubernetes control plane).\n\nHPC users can submit jobs directly to the Armada API, which will gradually\nsubmit them to the Kubernetes control plane when ready.\n\nThanks to this design choice Armada can scale to a large number of jobs and is\npositioned well for multi-cluster environments.\n\n(Armada GitHub)\n\n## Conclusion\n\nJust like how the progress of AI has added the concept of a machine learning\nengineer to product teams, it has added machine learning platform engineering\nto infrastructure teams.\n\nServing ML engineers requires dedicated solutions for the type of system they\nare building, whether it be a serverless inference application, an async\ninference application, or a batch system.\n\nAs Kubernetes plays a central role in cloud infrastructure, we\u2019ve highlighted\n5 open source projects that can be used in batch/HPC systems as you progress\non your ML platform engineering journey.\n\n## Footer\n\n#### treebeardtech\n\nHomePosts\n\nAbout\n\nAbout\n\n\u00a9 2024 treebeardtech.\n\nPrivacy Policy\n\nTerms of Use\n\nPowered by beehiiv\n\n", "frontpage": false}
