{"aid": "39979021", "title": "A Guide to Structured Generation Using Constrained Decoding", "url": "https://www.aidancooper.co.uk/constrained-decoding/", "domain": "aidancooper.co.uk", "votes": 1, "user": "sebg", "posted_at": "2024-04-09 13:07:07", "comments": 0, "source_title": "A Guide to Structured Generation Using Constrained Decoding", "source_text": "A Guide to Structured Generation Using Constrained Decoding\n\nImpromptu Engineer\n\nSign in Subscribe\n\nApr 8, 2024 13 min read Archive\n\n# A Guide to Structured Generation Using Constrained Decoding\n\nThe how, why, power, and pitfalls of constraining generative language model\noutputs\n\n### Introduction\n\nWe often want specific outputs when interacting with generative language\nmodels. This is especially true in programming domains, where a generated\noutput may become a direct input to a function. But sometimes, no matter how\nexplicit you are with your instructions, generative language models will get\ntoo creative, deviate off task, or simply succumb to their urge to yap.\n\nFortunately, there are techniques that ensure language models only return\noutputs that conform to your requirements. This article serves as a\npractitioner's guide for perhaps the most powerful of these techniques:\nconstrained decoding. We'll cover what structured generation and constrained\ndecoding are, how they work, best practices, useful patterns, and pitfalls to\navoid.\n\n#### ^Note to reader\n\nThis article is a living document being continuously updated as the field\nevolves. Terminology such as \"structured generation\" and \"constrained\ndecoding\" do not have consensus definitions, and their scopes are evolving\nover time \u2014 the discussion here reflects the author's perspective and may\ndiffer from other texts.\n\nIf you feel strongly that something in this article is misdefined or\nmisconceptualised, let me know!\n\n  1. Introduction\n  2. Motivating Example\n  3. What Is Structured Generation?\n  4. What Is Constrained Decoding and How Does It Work?\n  5. How to Use Constrained Decoding\n  6. What Are the Pitfalls?\n  7. What Are Some Useful Tips and Patterns?\n  8. Conclusion\n\n### Motivating Example\n\nTo illustrate constrained decoding, we'll explore an example use case\n(inspired by SGLang's docs). We want to generate JSON outputs that represent\nHarry Potter characters, according to the following schema:\n\n    \n    \n    { \"name\": \"Harry Potter\", \"house\": \"Gryffindor\", \"blood status\": \"Half-blood\", \"wand\": { \"wood\": \"Holly wood\", \"core\": \"Phoenix feather\", \"length\": 11.0 } }\n\nMore context on this will follow, but for now, here's what this might look\nlike without constrained decoding and other structured generation techniques\nusing the SGLang framework:\n\n    \n    \n    import sglang as sgl @sgl.function def harry_potter_gen(s, name): s += sgl.user(f\"Using JSON, describe the character {name} from Harry Potter.\") s += sgl.assistant(sgl.gen(\"json\", max_tokens=256)) sgl.set_default_backend(sgl.RuntimeEndpoint(\"http://localhost:30000\")) state = harry_potter_gen.run(\"Hermione Granger\") print(state[\"json\"]) # >>> Sure. Here's the character description based on JSON: # >>> # >>> ```json # >>> { # >>> \"name\": \"Hermione Granger\", # >>> \"age\": 15, # >>> \"species\": \"Human\", # >>> \"nationality\": \"British\", # >>> \"gender\": \"Female\", # >>> \"house\": \"Hufflepuff\", # >>> \"personality\": \"Intelligent, bookish, and compassionate\", # >>> \"status\": \"Alive\", # >>> \"birth_date\": \"1991-01-03\", # >>> \"death_date\": null # >>> } # >>> ```\n\nWe can see that the generative language model does a fine job of describing\nHermione in JSON format. However, the JSON does not conform to our desired\nschema. It is also accompanied by unwanted commentary that makes this output\ninconvenient to parse for further processing.\n\nThis article will demonstrate how constrained decoding avoids these issues.\n\n### What Is Structured Generation?\n\nIn the context of generative language models, structured generation\nencompasses a range of techniques that aim to generate outputs with a desired\nstructure.\n\nStructured output could take any form depending on the user's requirements,\nbut the archetypal example would be a JSON object with a specific schema.\nAnother example of structured output could be strings that match a regex\npattern, such as an email address, a telephone number, or even just a simple\n\"Y\" or \"N\". Combining these ideas, a common structured generation goal could\nbe to output a JSON object whose keys conform to a desired schema and whose\nvalues are consistent with expected data types, enumerations, and regex\npatterns.\n\nAnother variation on structured generation are context-free grammars, which\nare used to generate outputs that follow sets of rules that ensure validity\n(e.g., to form a working SQL query).\n\nStructured generation can be achieved in various ways, including:\n\n  1. Prompt design: the simplest approach is to include a description of the desired output structure inside the prompt. This could also involve few-shot prompting, whereby examples of inputs and outputs are included in the prompt.\n  2. Fine-tuning: a model can be subjected to further training for a specialised task using input-output pairs that demonstrate the desired output structure. This will incline the model to generate similarly-shaped responses during inference.\n  3. Multi-stage prompting: rather than have the model directly generate structured output, you can have the model respond to a series of prompts, and then assemble the structured output yourself outside of the generative process.\n  4. Specialised services: OpenAI offers an optional JSON mode that ensures API responses are returned as valid JSON, although it doesn't provide strong guarantees about the schema and contents of the JSON.\n\nThese techniques work with varying degrees of success, depending on the\ndifficulty of the task and the capability of the model. However, there's a\nmore forceful method that can guarantee precise outputs, even when working\nwith relatively weak models applied to complex tasks: constrained decoding.\n\n### What Is Constrained Decoding and How Does It Work?\n\nIn the context of structured generation, constrained decoding is a technique\nthat manipulates a generative model's token generation process to constrain\nits next-token predictions to only tokens that do not violate the required\noutput structure.\n\nState of the art constrained decoding skips the parts of the structured output\nthat are boilerplate scaffolding or tokens that can be uniquely determined\nbased on preceding tokens and the constraints of the desired output. Only the\nparts of the output that strictly require generation are sampled from a\nrestricted set of compatible tokens in the model's next-token probability\ndistribution.\n\nFor a full exploration of the mechanics behind constrained decoding, I refer\nthe reader to excellent articles and papers from the teams behind Outlines [1]\n[2] and SGLang [3]. Constrained decoding is an area of active innovation that\ncontinues to benefit from increasingly effective optimisations.\n\n#### Additional benefits of constrained decoding\n\nAs well as guaranteeing compliant outputs, the mechanics of constrained\ndecoding outlined above can also reduce inference costs and improve throughput\nby:\n\n  1. Increasing token-generation speed. Constrained decoding simplifies the next-token prediction space, accelerating generation \u2014 especially when implemented with clever optimisations that allow some token generation steps to be outright skipped.\n  2. Reducing the number of generated tokens. The throughput of text generation systems is almost always bottlenecked by the speed of token generation, and for many structured generation tasks, much of the output is scaffolding that can bypass generation. For instance, for a rigid JSON schema with fixed field definitions, we can save a lot of time by only generating the values and not the surrounding boilerplate.\n\nThere's even precedent suggesting that constrained decoding can improve task\nperformance.\n\n### How to Use Constrained Decoding\n\n\ud83d\udca1\n\nConstrained decoding is only compatible with generative language models that\nmake their complete next-token probability distribution available: i.e.,\nconstrained decoding is only possible for models run locally; not external\nAPIs.\n\nExternal APIs may offer some structured generation functionality, such as Open\nAI's JSON mode, but at the time of writing, I'm not aware of any that support\nfull-fledged constrained decoding.\n\nThere are various frameworks that enable constrained decoding to be leveraged\nwith local models, including: SGLang, Outlines, guidance, and DSPy Assertions.\nIn this article, I elect to use SGLang (which builds on Outlines under the\nhood) to illustrate examples, although the same concepts apply across all\nframeworks.\n\nThese frameworks are generally pretty agnostic towards the local model used,\nand will usually be compatible with most popular models. In this article, any\noutputs that accompany the examples have been generated using\ngoogle/gemma-2b-it.\n\nThere are three main ways that a structured output can be defined for\nconstrained decoding: regex, code, and generative constructs.\n\n#### Structured output using regex\n\nRegular expressions (regex) are a powerful way to define structured output, as\nthey offer maximum specificity over the format and contents. Here's how regex\ncan be used in SGLang for constrained decoding:\n\n    \n    \n    import sglang as sgl character_regex = ( r\"\"\"\\{ \"name\": \"[\\w\\d\\s]{1,16}\", \"house\": \"(Gryffindor|Slytherin|Ravenclaw|Hufflepuff)\", \"blood status\": \"(Pure-blood|Half-blood|Muggle-born)\", \"wand\": \\{ \"wood\": \"[\\w\\d\\s]{1,16}\", \"core\": \"[\\w\\d\\s]{1,16}\", \"length\": [0-9]{1,2}\\.[0-9]{0,2} \\} \\}\"\"\" ) @sgl.function def harry_potter_gen(s, name): s += sgl.user(f\"Please describe the character {name} from Harry Potter.\") s += sgl.assistant(sgl.gen(\"json\", max_tokens=256, regex=character_regex)) sgl.set_default_backend(sgl.RuntimeEndpoint(\"http://localhost:30000\")) state = harry_potter_gen.run(\"Hermione Granger\") character_json = state[\"json\"]\n\nThe main downside of regex is that it's tedious to write and maintain \u2014\nespecially in the context of an active codebase with an evolving data model.\nAnother downside to regex in SGLang is that it introduces a considerable\ncompilation time when initialising run that is not encountered when using the\nspecialised generative constructs.\n\nDepending on the framework, regex may produce different outputs to other\ngenerative methods due to algorithmic differences for token selection (more on\nthis in the pitfalls section).\n\n#### Structured output as code\n\nStructured output can also be defined as code using Pydantic models. These can\nthen be dynamically converted into regex for constrained decoding. Using\nPydantic models like this is convenient, as it ensures alignment between your\napplication code and the constrained decoding process. To fully streamline\nyour application, you can also use string interpolation to describe the\ndesired output structure in your instruction prompt. This is a major benefit\nover alternative approaches, where you may need to reimplement your data model\nin multiple places in multiple ways, risking code drift. Pydantic models can\nbe used within SGLang like so:\n\n    \n    \n    import sglang as sgl from sglang.srt.constrained import build_regex_from_object from pydantic import BaseModel class Wand(BaseModel): wood: str core: str length: float class Character(BaseModel): name: str house: str blood_status: str wand: Wand @sgl.function def harry_potter_gen(s, name): s += sgl.user(f\"Please describe the character {name} from Harry Potter.\") s += sgl.assistant( sgl.gen(\"json\", max_tokens=256, regex=build_regex_from_object(Character)) ) sgl.set_default_backend(sgl.RuntimeEndpoint(\"http://localhost:30000\")) state = harry_potter_gen.run(\"Hermione Granger\") character_json = state[\"json\"]\n\nDefining structured output using Pydantic models suffers from the same\ndownsides as regex, as this is what they get converted into under the hood.\n\nWhatsmore, the conversion process for complex Pydantic models involving\nmultiple levels of models can be unreliable or yield poorly constructed regex,\noften rendering this approach unviable in practice. Case in point: the above\nexample does not actually work, as the conversion is invalid, resulting in\nmalformed outputs. This is a bug in the underlying Outlines package which will\nlikely get resolved in SGLang soon.\n\n#### Structured output using interleaved generative constructs\n\nThe third option is to define your structured output as alternating static\nstrings and generative constructs. This has the benefit of splitting out your\ndata model into more manageable components where the generative parts of the\ntask can be defined individually. In SGLang, the task performance of the gen\nconstructs using functionality such as choices can be superior to their regex\nequivalents due to algorithmic differences in their implementations (more on\nthis in the pitfalls section).\n\nThe guidance package provides an illustrative example of this pattern. For our\nHarry Potter task, free text values for fields such as \"name\" can either be\nconstrained by gen using regex or by using the stop='\"' mechanic to escape the\ntoken generation process (without stop='\"', the model would continue to\npredict the rest of the JSON without constraints). Here's how generative\nconstructs can be used in SGLang:\n\n    \n    \n    import sglang as sgl @sgl.function def harry_potter_gen(s, name): s += sgl.user(f\"Please describe the character {name} from Harry Potter.\") s += sgl.assistant('''{ \"name\": \"''' + sgl.gen(\"name\", max_tokens=32, stop='\"') + '''\", \"house\": \"''' + sgl.gen( \"house\", choices=[\"Gryffindor\", \"Slytherin\", \"Ravenclaw\", \"Hufflepuff\"] ) + '''\", \"blood status\": \"''' + sgl.gen( \"blood status\", choices=[\"Pure-blood\", \"Half-blood\", \"Muggle-born\"] ) + '''\", \"wand\": { \"wood\": \"''' + sgl.gen(\"wood\", regex=r\"[\\w\\d\\s]{1,16}\") + '''\", \"core\": \"''' + sgl.gen(\"core\", regex=r\"[\\w\\d\\s]{1,16}\") + '''\", \"length\": ''' + sgl.gen(\"length\", regex=r\"[0-9]{1,2}\\.[0-9]{0,2}\") + ''' } }''') sgl.set_default_backend(sgl.RuntimeEndpoint(\"http://localhost:30000\")) state = harry_potter_gen.run(\"Hermione Granger\") character_json = state.messages()[1][\"content\"]\n\nDepending on the framework and implementation, managing the scaffolding around\nthe generative constructs can be syntactically awkward. In this SGLang\nexample, we rely on a finickety mix of single and triple quotes. We also have\nto access the final JSON output in a more opaque way via\nstate.messages()[1][\"content\"] rather than state[\"json\"] as per the previous\napproaches, which is fragile when actively experimenting with the prompting\nstrategy.\n\n### What Are the Pitfalls?\n\n#### Remember: the model does not know the constraints in advance\n\nMost constrained decoding pitfalls are due to misalignment between what the\ngenerative model wants to output and what the model is forced to output. The\nmechanics of next-token prediction and constrained decoding are such that the\nmodel does not know what is coming in advance: it generates predictions only\nbased on the tokens that precede it.\n\nThis can result in poor performance if the model is forced to output something\nunnatural. For instance, in this example, we observe unexpected behaviour if\nthe model is constrained to output country options that start with a lowercase\nletter:\n\n    \n    \n    import sglang as sgl @sgl.function def which_country_upper(s, city): s += sgl.user(f\"In which country is {city} located?\") s += sgl.assistant(sgl.gen(\"country\", choices=[\"France\", \"Spain\", \"Italy\"])) @sgl.function def which_country_lower(s, city): s += sgl.user(f\"In which country is {city} located?\") s += sgl.assistant(sgl.gen(\"country\", choices=[\"france\", \"spain\", \"italy\"])) sgl.set_default_backend(sgl.RuntimeEndpoint(\"http://localhost:30000\")) state_upper = which_country_upper.run(\"Paris\") state_lower = which_country_lower.run(\"Paris\") print(state_upper[\"country\"]) # >>> France print(state_lower[\"country\"]) # >>> spain\n\nThe best way to mitigate this is to ensure the model understands the task and\nexpectations by providing sufficient information in the prompt. If working\nwith JSON, it's also worth considering the ordering of your schema fields, and\nstructuring them logically such that they're amenable to next-token\nprediction. This may also include using descriptive field names, and not\nconstraining the model to esoteric field values.\n\nIdeally, we would inspect the actual token probabilities to identify cases\nwhere the model has been forced to output something with low confidence. It\nwould be great if we could triage outputs where the predictions do not meet\ncertain confidence thresholds. However, I'm not aware of this functionality\nexisting in the frameworks I'm familiar with.\n\n#### Beware greedy token selection\n\nAnother pitfall to be mindful of is how your constrained decoding is being\nimplemented algorithmically. This will vary depending on the framework and\nconstructs you're using. For instance, in SGLang, constrained decoding for a\nmultiple-choice string is different when configured as regex versus gen's\nchoices argument. The former uses a greedy next-token prediction, whereas the\nlatter uses beam search. We can see how this can go wrong in the following\nexample:\n\n    \n    \n    import sglang as sgl @sgl.function def us_president_choices(s): s += sgl.user(\"Name a US president.\") s += sgl.assistant( \"An example of a US president is \" + sgl.gen(\"president\", choices=[\"Donald Duck\", \"Millard Fillmore\"]) ) @sgl.function def us_president_regex(s): s += sgl.user(\"Name a US president.\") s += sgl.assistant( \"An example of a US president is \" + sgl.gen(\"president\", regex=r\"(Donald Duck|Millard Fillmore)\") ) sgl.set_default_backend(sgl.RuntimeEndpoint(\"http://localhost:30000\")) state_choices = us_president_choices.run() state_regex = us_president_regex.run() print(state_choices[\"president\"]) # >>> Millard Fillmore print(state_regex[\"president\"]) # >>> Donald Duck\n\nThe greedy algorithm used by the regex implementation is short-sighted, and\ncannot resist choosing the \"Donald\" option, despite ultimately landing on an\nincorrect answer. The choices approach avoids this by evaluating the quality\nof all options in their entirety, and not just based on the most attractive\ninitial token.\n\n### What Are Some Useful Tips and Patterns?\n\n#### How to generate a varying number of structured outputs\n\nA tricky but not uncommon structured generation requirement is to generate\nzero, one, or many outputs that adhere to constraints, from a single input.\nSpecifying such constraints in regex is sometimes theoretically possible, but\nusually ill-advised in practice for all but the most simple string patterns. A\nbetter approach is to use control flow to enable the model to generate\nmultiple structured outputs in a series of responses. Here's a pattern for\nthis in SGLang:\n\n    \n    \n    import sglang as sgl @sgl.function def harry_potter_gen(s, names, max_characters=5): s += sgl.user(f\"Describe these Harry Potter characters: {', '.join(names)}\") n = 1 while n <= max_characters: s += sgl.assistant('''{ \"name\": \"''' + sgl.gen(\"name\", max_tokens=32, stop='\"') + '''\", \"house\": \"''' + sgl.gen( \"house\", choices=[\"Gryffindor\", \"Slytherin\", \"Ravenclaw\", \"Hufflepuff\"] ) + '''\", \"blood status\": \"''' + sgl.gen( \"blood status\", choices=[\"Pure-blood\", \"Half-blood\", \"Muggle-born\"] ) + '''\", \"wand\": { \"wood\": \"''' + sgl.gen(\"wood\", max_tokens=32, stop='\"') + '''\", \"core\": \"''' + sgl.gen(\"core\", max_tokens=32, stop='\"') + '''\", \"length\": ''' + sgl.gen(\"length\", regex=r\"[0-9]{1,2}\\.[0-9]{0,2}\") + ''' } }''') s += sgl.user(\"Are there any more characters to describe? (Y/N)\") s += sgl.assistant(sgl.gen(f\"continue_{n}\", choices=[\"Y\", \"N\"])) if s[f\"continue_{n}\"] == \"N\": break n += 1 s += sgl.user(f\"OK, describe the next character.\") s[\"n\"] = min(n, max_characters) sgl.set_default_backend(sgl.RuntimeEndpoint(\"http://localhost:30000\")) state = harry_potter_gen.run([\"Ron Weasley\", \"Snape\"]) characters_json = [state.messages()[1+i*4][\"content\"] for i in range(state[\"n\"])]\n\n#### Post-processing of structured outputs\n\nThere are scenarios where you can achieve better task performance by defining\nintermediary structured outputs for constrained decoding, and then converting\nthis to your final structured output during post-processing.\n\nThis can be the case when using enumerations that the model handles poorly.\nThis can be mitigated by temporarily adding more intuitive aliases. Here's a\ncontrived example (that would be better avoided using gen's choices instead of\nregex):\n\n    \n    \n    import sglang as sgl @sgl.function def grass(s): s += sgl.user(f\"What colour is grass?\") s += sgl.assistant(sgl.gen(\"colour\", regex=f\"(Grey|Forest green)\")) colours = {\"Grey\": \"Grey\", \"Forest green\": \"Forest green\", \"Green\": \"Forest green\"} @sgl.function def grass_aliases(s): s += sgl.user(f\"What colour is grass?\") s += sgl.assistant(sgl.gen(\"colour\", regex=f\"(Grey|Forest green|Green)\")) sgl.set_default_backend(sgl.RuntimeEndpoint(\"http://localhost:30000\")) state = grass.run() state_aliases = grass_aliases.run() print(state[\"colour\"]) # >>> Grey print(colours[state_aliases[\"colour\"]]) # >>> Forest green\n\nAnother case where post-processing can be useful is for JSON schemas that\ncontain optional fields. Rather than use complex regex to specify the optional\nfields, it's usually easiest to include all fields in the structured output\nand have the generative model populate unneeded field values with\nplaceholders. These fields can then be deleted during post-processing. Even\nfor expansive JSON schemas with many optional fields, this is inexpensive with\nconstrained decoding as this JSON scaffolding does not actually get generated.\n\n### Conclusion\n\nConstrained decoding is a powerful but underutilised technique for structured\ngeneration. It's a significant advantage that local models have over external\nAPIs, and can enable relatively small and inexpensive models to perform\ncomparably to much larger alternatives.\n\nThe throughput and latency improvements that constrained decoding provide can\nalso be extremely compelling. In my experience, SGLang has particularly\nimpressive backend optimisations that increases the speed of JSON generation\nby an order of magnitude versus unconstrained equivalents (whilst also\nguaranteeing well-formed outputs...!).\n\nConstrained decoding is a vital part of the structured generation toolkit that\nI encourage the reader to try for any tasks where e a reliable output\nstructure would be beneficial.\n\nSubscribe\n\nGitHub - AidanCooper/constrained-decoding: A guide to structured generation\nusing constrained decoding\n\nA guide to structured generation using constrained decoding -\nAidanCooper/constrained-decoding\n\nGitHubAidanCooper\n\nCode for this analysis can be found on GitHub\n\n### Published by:\n\n### You might also like...\n\nJul\n\n22\n\n## Modern Data Engineering and the Lost Art of Data Modelling\n\nNecessity was the mother of invention. Now, an abundance of cheap storage and\ncompute makes for data anarchy.\n\nJul 22, 2023\n\n5 min read\n\nJun\n\n07\n\n## Approximating Shapley Values for Machine Learning\n\nThe how and why of Shapley value approximation, explained in code\n\nJun 7, 2023\n\n6 min read\n\nApr\n\n07\n\n## Gnillehcs' Model of Integration\n\nWhat happens to segregated communities as people increasingly seek diversity?\n\nApr 7, 2023\n\n3 min read\n\nDec\n\n31\n\n## How Shapley Values Work\n\nIn this article, we will explore how Shapley values work - not using cryptic\nformulae, but by way of code and simplified explanations\n\nDec 31, 2022\n\n10 min read\n\nAug\n\n13\n\n## Industry Perspective: Tree-Based Models vs Deep Learning for Tabular Data\n\nTree-based models aren't just highly performant - they offer a host of other\nadvantages\n\nAug 13, 2022\n\n3 min read\n\n### Member discussion\n\nImpromptu Engineer \u00a9 2024\n\nPowered by Ghost\n\n", "frontpage": false}
