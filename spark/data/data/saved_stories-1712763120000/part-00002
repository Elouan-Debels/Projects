{"aid": "39988716", "title": "Deep Bug", "url": "https://www.marginalia.nu/log/a_104_dep_bug/", "domain": "marginalia.nu", "votes": 3, "user": "marginalia_nu", "posted_at": "2024-04-10 09:30:59", "comments": 0, "source_title": "Deep Bug", "source_text": "Deep Bug @ marginalia.nu\n\n# Deep Bug\n\nPosted: 2024-04-10\n\nThe project has been haunted by a mysterious bug since sometime February. It\nrelates to the code that constructs the index, particularly the code that\nmerges partial indices.\n\nIn short the search engine constucts the reverse index through successive\nmerging of smaller indices, which reduces the overall memory requirement.\n\nYou can conceptualize the revese index itself as two files, one with offset\npointers into another file, which has sorted numbers. This code runs after\neach partition finishes crawling and processing its data, and has a run time\nof about 4 hours.\n\nSuddenly the code that merges the indexes started to randomly fail.\n\nWhat failed was the code that copies sorted numbers from one of the older\nindices to one of the newer, in the case when a keyword is present in only one\nof the indexes and no actual merging of sorted lists is needed.\n\nIf code suddenly starts acting up when the amount of data increases, all my\nintuition screams that this must be a 32 bit integer overflow. The index\nconstruction does all of its work in the bermuda triangle of 32 bit errors\nthat is 1-32 GB file size range, so it really does seem like a very probable\nsuspect.\n\nI went over the code looking for such a thing, and found nothing. I added a\nbunch of guard clauses and assertions to get some hint about what was going\non, but these didn\u2019t turn up much other than the copy operation attempting to\ncopy outside the file.\n\nTo my surprise, while doing this the construction process suddenly went\nthrough! The index it constructed was sane. Had I fixed it by perturbing the\ncode somehow? I ran it again and it failed.\n\nSome non-determinism is expected since the indexes are merged in parallel, so\nthe order they\u2019re merged in is a bit random even if the merging itself is\ncompletely deterministic. I felt this explained the random success: I got\nlucky with the ordering.\n\nSince I had a work-around I left this be for a while. I had other work that\nwas more pressing. The seven remaining partitions were coaxed through by just\nhitting the restart button until it worked, taking one or more days of\nhammering restarts. Annoying and time consuming, but it worked.\n\nThis week the final partition came through, and I had a moment to look deeper\ninto this enigma.\n\nMaybe it wasn\u2019t an integer overflow? I checked the git log for any suspect\nchanges between December when it worked and February when it didn\u2019t work, and\nI found nothing. Again I combed over the code looking for integer overflows,\nand still found nothing.\n\nI did find a function that shrinks the merged index, which is initially\nallocated to be the total size of the inputs. Since the merge function removes\nduplicates, the merged index is often smaller than the sum of its parts, but\nit\u2019s impossible to know how small before actually merging.\n\nThis seemed like a plausible explanation, since if the files were truncated\ntoo hard, it would explain the files seemingly being \u201ctoo small\u201d. I disabled\nthe truncation, and still got errors.\n\nIn investigating this, I stumbled on a very curious aberration. I\u2019ll sketch\nout the gist of it:\n\n    \n    \n    val = read-only mmapped file not subject to change counts = zeroed mmap:ed file long offset = 0; for (int i = 0; i < length; i++) { counts[i] = val[i]; offset += val[i]; } long size = 0; for (int i = 0; i < length; i++) { size += counts[i]; } // ... assert (size == offset); // ... truncate(size);\n\nAll this runs in sequence on the same thread, and nothing writes to counts\nother than this code.\n\nThe assertion would fail.\n\nAlright, so it is an integer overflow! ... you might think. Nope. These are\nall 64 bit longs, and the values being added are small and few enough not to\noverflow a 32 bit integer. At this point I could reliably isolate the\nparticular inputs that triggered the behavior with the assert, so I did just\nthat.\n\nThere are four lights, damnit!\n\nAt this point I patched production so that if this assertion were to fail, the\ninput files were copied over to a separate directory. This gave me a 2 GB set\nof test-data that had been known to trigger the error that I could exfiltrate\nand examine locally.\n\nThe bug of course did not re-appear when I ran this code in isolation, but\nthat actually says something important. I\u2019d reduced the problem down to\ndeterministic code being non-deterministic and outputting logical\ncontradictions. This feels like a good reason to question other things than\nthe program logic.\n\nIt\u2019s probably safe to assume the problem is either the JVM, the linux kernel,\nor the hardware, in decreasing order of likelihood.\n\nThe JVM is at the top of the suspect list since one of the things that had in\nfact changed since the bug appeared was that the project migrated over to\ngraalvm from openjdk.\n\nHardware errors typically don\u2019t target only a specific function over and over,\nexcept I guess if you have some Homer-style divine grudge going on. Having not\nskipped over any libations, enraged gods could probably be ruled out, and to\nfurther sow doubts about this I was able to reproduce the error on another\nmachine, not with the exfiltrated data but through reindexing Wikipedia\u2019s data\nrepeatedly. This also probably rules out some sort of linux kernel bug, since\nthe kernels were a few versions apart; one machine was SMP and the other was\nnot, one with ECC one not, etc.\n\nSo at this point the only remaining suspect on my list was the graalvm JDK. I\nmigrated the project to the latest graalvm version, from jdk-21 to jdk-22\nthrough oracle\u2019s official docker images. This did absolutely nothing to\naddress the problem. I switched the docker build process to use a temurin\n(openjdk) image instead of graalvm, and... the problem went away! Since then,\nit\u2019s worked over and over.\n\nAt this point I\u2019d love to file a bug report, although I\u2019m having trouble\nisolating the problem enough to do so. \u201cDoesn\u2019t work\u201d is famously not an error\ndescription. The relevant process is something like 5 KLOC, and the smallest\ndataset that triggers the bug somewhat reliably is about 54 GB.\n\nI\u2019ve isolated the code that manifests the bug and run it 50,000 times on the\ndata exfiltrated from production on a machine that\u2019s previously been able to\ntrigger the problem, and it\u2019s run without a hitch, so there appears to be some\nsort of spooky action at a distance going on. I\u2019ve tried perturbing it in\nvarious ways, introducing page faults and so on, since that\u2019s happening a lot\nin production, again to no avail.\n\nIn general I\u2019d argue you haven\u2019t fixed a bug unless you understand why it\nhappened and why your fix worked, which makes this frustrating, since every\nindication is that the bug exists within proprietary code that is out of my\nreach. This feels like an impasse, not a solution, even though the error has\nnominally been corrected.\n\nI can at least say make statements about what hasn\u2019t caused the error.\n\nmarginalia.nu \u00a9 2024 Viktor Lofgren <kontakt@marginalia.nu>\n\nConsider donating to support the development effort of Marginalia Search and\nthe other services!\n\n", "frontpage": true}
