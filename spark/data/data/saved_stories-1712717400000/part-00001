{"aid": "39984074", "title": "PyDTS: A Python Toolkit for Deep Learning Time Series Modelling", "url": "https://www.mdpi.com/1099-4300/26/4/311", "domain": "mdpi.com", "votes": 1, "user": "PaulHoule", "posted_at": "2024-04-09 20:48:13", "comments": 0, "source_title": "PyDTS: A Python Toolkit for Deep Learning Time Series Modelling", "source_text": "Entropy | Free Full-Text | PyDTS: A Python Toolkit for Deep Learning Time Series Modelling\n\nLoading [MathJax]/jax/output/HTML-CSS/fonts/Gyre-Pagella/Main/Regular/Main.js\n\n  * Consent\n  * Details\n  * [#IABV2SETTINGS#]\n  * About\n\n## This website uses cookies\n\nWe use cookies to personalise content and ads, to provide social media\nfeatures and to analyse our traffic. We also share information about your use\nof our site with our social media, advertising and analytics partners who may\ncombine it with other information that you\u2019ve provided to them or that they\u2019ve\ncollected from your use of their services.\n\nShow details\n\n  * Necessary cookies help make a website usable by enabling basic functions like page navigation and access to secure areas of the website. The website cannot function properly without these cookies.\n\n    * Cookiebot\n\n1\n\nLearn more about this provider\n\n1.gifUsed to count the number of sessions to the website, necessary for\noptimizing CMP product delivery.\n\nExpiry: SessionType: Pixel\n\n    * Crazyegg\n\n2\n\nLearn more about this provider\n\n_ce.cchStores the user's cookie consent state for the current domain\n\nExpiry: SessionType: HTTP\n\nce_successful_csp_checkDetects whether user behaviour tracking should be\nactive on the website.\n\nExpiry: PersistentType: HTML\n\n    * Google\n\n1\n\nLearn more about this provider\n\ntest_cookieUsed to check if the user's browser supports cookies.\n\nExpiry: 1 dayType: HTTP\n\n    * LinkedIn\n\n2\n\nLearn more about this provider\n\nli_gcStores the user's cookie consent state for the current domain\n\nExpiry: 180 daysType: HTTP\n\nbscookieThis cookie is used to identify the visitor through an application.\nThis allows the visitor to login to a website through their LinkedIn\napplication for example.\n\nExpiry: 1 yearType: HTTP\n\n    * commenting.mdpi.com\n\n2\n\nSESS#Preserves users states across page requests.\n\nExpiry: SessionType: HTTP\n\nXSRF-TOKENEnsures visitor browsing-security by preventing cross-site request\nforgery. This cookie is essential for the security of the website and visitor.\n\nExpiry: SessionType: HTTP\n\n    * commenting.mdpi.com consent.cookiebot.com\n\n2\n\nCookieConsent [x2]Stores the user's cookie consent state for the current\ndomain\n\nExpiry: 1 yearType: HTTP\n\n    * matomo.mdpi.com\n\n1\n\n_pk_testcookie_domainThis cookie determines whether the browser accepts\ncookies.\n\nExpiry: 1 dayType: HTTP\n\n    * mdpi.com\n\n3\n\n__cfruidThis cookie is a part of the services provided by Cloudflare -\nIncluding load-balancing, deliverance of website content and serving DNS\nconnection for website operators.\n\nExpiry: SessionType: HTTP\n\ncf_clearanceThis cookie is used to distinguish between humans and bots.\n\nExpiry: 1 yearType: HTTP\n\nMDPIPHPSESSIDPending\n\nExpiry: SessionType: HTTP\n\n    * mdpi.com mdpi.org mdpi-res.com sciprofiles.com\n\n4\n\n__cf_bm [x4]This cookie is used to distinguish between humans and bots. This\nis beneficial for the website, in order to make valid reports on the use of\ntheir website.\n\nExpiry: 1 dayType: HTTP\n\n    * www.jisc.ac.uk\n\n2\n\nAWSALBRegisters which server-cluster is serving the visitor. This is used in\ncontext with load balancing, in order to optimize user experience.\n\nExpiry: 7 daysType: HTTP\n\nAWSALBCORSRegisters which server-cluster is serving the visitor. This is used\nin context with load balancing, in order to optimize user experience.\n\nExpiry: 7 daysType: HTTP\n\n    * www.mdpi.com\n\n7\n\ncf_chl_1This cookie is a part of the services provided by Cloudflare -\nIncluding load-balancing, deliverance of website content and serving DNS\nconnection for website operators.\n\nExpiry: 1 dayType: HTTP\n\niconify0Used by the website's content management system (CMS) to determine how\nthe website's menu-tabs should be displayed.\n\nExpiry: PersistentType: HTML\n\niconify1This cookie is set to ensure proper product displays on the website.\n\nExpiry: PersistentType: HTML\n\niconify2Used by the website's content management system (CMS) to determine how\nthe website's menu-tabs should be displayed.\n\nExpiry: PersistentType: HTML\n\niconify3Determines the device used to access the website. This allows the\nwebsite to be formatted accordingly.\n\nExpiry: PersistentType: HTML\n\niconify-countUsed by the website's content management system (CMS) to\ndetermine how the website's menu-tabs should be displayed.\n\nExpiry: PersistentType: HTML\n\niconify-versionUsed by the website's content management system (CMS) to\ndetermine how the website's menu-tabs should be displayed.\n\nExpiry: PersistentType: HTML\n\n  * Preference cookies enable a website to remember information that changes the way the website behaves or looks, like your preferred language or the region that you are in.\n\n    * LinkedIn\n\n1\n\nLearn more about this provider\n\nlidcRegisters which server-cluster is serving the visitor. This is used in\ncontext with load balancing, in order to optimize user experience.\n\nExpiry: 1 dayType: HTTP\n\n    * www.mdpi.com\n\n2\n\nmdpi_layout_typeThis cookie is used to store user setting of using fixed\ndesktop layout instead of the default responsive layout\n\nExpiry: 1 yearType: HTTP\n\nsettingsThis cookie is used to determine the preferred language of the visitor\nand sets the language accordingly on the website, if possible.\n\nExpiry: PersistentType: HTML\n\n  * Statistic cookies help website owners to understand how visitors interact with websites by collecting and reporting information anonymously.\n\n    * Crazyegg\n\n8\n\nLearn more about this provider\n\n_ce.clock_dataCollects data on the user\u2019s navigation and behavior on the\nwebsite. This is used to compile statistical reports and heatmaps for the\nwebsite owner.\n\nExpiry: 1 dayType: HTTP\n\n_ce.clock_eventCollects data on the user\u2019s navigation and behavior on the\nwebsite. This is used to compile statistical reports and heatmaps for the\nwebsite owner.\n\nExpiry: 1 dayType: HTTP\n\n_ce.gtldHolds which URL should be presented to the visitor when visiting the\nsite.\n\nExpiry: SessionType: HTTP\n\n_ce.sCollects data on the user\u2019s navigation and behavior on the website. This\nis used to compile statistical reports and heatmaps for the website owner.\n\nExpiry: 1 yearType: HTTP\n\ncebsTracks the individual sessions on the website, allowing the website to\ncompile statistical data from multiple visits. This data can also be used to\ncreate leads for marketing purposes.\n\nExpiry: SessionType: HTTP\n\ncebsp_Collects data on the user\u2019s navigation and behavior on the website. This\nis used to compile statistical reports and heatmaps for the website owner.\n\nExpiry: SessionType: HTTP\n\nce_fvdCollects data on the user\u2019s navigation and behavior on the website. This\nis used to compile statistical reports and heatmaps for the website owner.\n\nExpiry: PersistentType: HTML\n\ncetabidSets a unique ID for the session. This allows the website to obtain\ndata on visitor behaviour for statistical purposes.\n\nExpiry: SessionType: HTML\n\n    * Google\n\n5\n\nLearn more about this provider\n\ncollectUsed to send data to Google Analytics about the visitor's device and\nbehavior. Tracks the visitor across devices and marketing channels.\n\nExpiry: SessionType: Pixel\n\n_gaRegisters a unique ID that is used to generate statistical data on how the\nvisitor uses the website.\n\nExpiry: 2 yearsType: HTTP\n\n_ga_#Used by Google Analytics to collect data on the number of times a user\nhas visited the website as well as dates for the first and most recent visit.\n\nExpiry: 2 yearsType: HTTP\n\n_gatUsed by Google Analytics to throttle request rate\n\nExpiry: 1 dayType: HTTP\n\n_gidRegisters a unique ID that is used to generate statistical data on how the\nvisitor uses the website.\n\nExpiry: 1 dayType: HTTP\n\n    * Hotjar\n\n5\n\nLearn more about this provider\n\nhjActiveViewportIdsThis cookie contains an ID string on the current session.\nThis contains non-personal information on what subpages the visitor enters \u2013\nthis information is used to optimize the visitor's experience.\n\nExpiry: PersistentType: HTML\n\nhjViewportIdSaves the user's screen size in order to adjust the size of images\non the website.\n\nExpiry: SessionType: HTML\n\n_hjSession_#Collects statistics on the visitor's visits to the website, such\nas the number of visits, average time spent on the website and what pages have\nbeen read.\n\nExpiry: 1 dayType: HTTP\n\n_hjSessionUser_#Collects statistics on the visitor's visits to the website,\nsuch as the number of visits, average time spent on the website and what pages\nhave been read.\n\nExpiry: 1 yearType: HTTP\n\n_hjTLDTestRegisters statistical data on users' behaviour on the website. Used\nfor internal analytics by the website operator.\n\nExpiry: SessionType: HTTP\n\n    * LinkedIn\n\n1\n\nLearn more about this provider\n\nAnalyticsSyncHistoryUsed in connection with data-synchronization with third-\nparty analysis service.\n\nExpiry: 30 daysType: HTTP\n\n    * Twitter Inc.\n\n1\n\nLearn more about this provider\n\npersonalization_idThis cookie is set by Twitter - The cookie allows the\nvisitor to share content from the website onto their Twitter profile.\n\nExpiry: 400 daysType: HTTP\n\n    * matomo.mdpi.com\n\n2\n\n_pk_id#Collects statistics on the user's visits to the website, such as the\nnumber of visits, average time spent on the website and what pages have been\nread.\n\nExpiry: 1 yearType: HTTP\n\n_pk_ses#Used by Piwik Analytics Platform to track page requests from the\nvisitor during the session.\n\nExpiry: 1 dayType: HTTP\n\n    * www.mdpi.com\n\n1\n\nsentryReplaySessionRegisters data on visitors' website-behaviour. This is used\nfor internal analysis and website optimization.\n\nExpiry: SessionType: HTML\n\n  * Marketing cookies are used to track visitors across websites. The intention is to display ads that are relevant and engaging for the individual user and thereby more valuable for publishers and third party advertisers.\n\n    * Meta Platforms, Inc.\n\n3\n\nLearn more about this provider\n\nlastExternalReferrerDetects how the user reached the website by registering\ntheir last URL-address.\n\nExpiry: PersistentType: HTML\n\nlastExternalReferrerTimeDetects how the user reached the website by\nregistering their last URL-address.\n\nExpiry: PersistentType: HTML\n\n_fbpUsed by Facebook to deliver a series of advertisement products such as\nreal time bidding from third party advertisers.\n\nExpiry: 3 monthsType: HTTP\n\n    * Google\n\n2\n\nLearn more about this provider\n\npagead/1p-user-list/#Tracks if the user has shown interest in specific\nproducts or events across multiple websites and detects how the user navigates\nbetween sites. This is used for measurement of advertisement efforts and\nfacilitates payment of referral-fees between websites.\n\nExpiry: SessionType: Pixel\n\ntdRegisters statistical data on users' behaviour on the website. Used for\ninternal analytics by the website operator.\n\nExpiry: SessionType: Pixel\n\n    * LinkedIn\n\n4\n\nLearn more about this provider\n\nbcookieUsed by the social networking service, LinkedIn, for tracking the use\nof embedded services.\n\nExpiry: 1 yearType: HTTP\n\nli_sugrCollects data on user behaviour and interaction in order to optimize\nthe website and make advertisement on the website more relevant.\n\nExpiry: 3 monthsType: HTTP\n\nUserMatchHistoryEnsures visitor browsing-security by preventing cross-site\nrequest forgery. This cookie is essential for the security of the website and\nvisitor.\n\nExpiry: 30 daysType: HTTP\n\nli_adsIdCollects data on user behaviour and interaction in order to optimize\nthe website and make advertisement on the website more relevant.\n\nExpiry: PersistentType: HTML\n\n    * Twitter Inc.\n\n3\n\nLearn more about this provider\n\ni/adsct [x2]The cookie is used by Twitter.com in order to determine the number\nof visitors accessing the website through Twitter advertisement content.\n\nExpiry: SessionType: Pixel\n\nmuc_adsCollects data on user behaviour and interaction in order to optimize\nthe website and make advertisement on the website more relevant.\n\nExpiry: 400 daysType: HTTP\n\n    * YouTube\n\n22\n\nLearn more about this provider\n\n#-#Pending\n\nExpiry: SessionType: HTML\n\niU5q-!O9@$Registers a unique ID to keep statistics of what videos from YouTube\nthe user has seen.\n\nExpiry: SessionType: HTML\n\nLAST_RESULT_ENTRY_KEYUsed to track user\u2019s interaction with embedded content.\n\nExpiry: SessionType: HTTP\n\nLogsDatabaseV2:V#||LogsRequestsStorePending\n\nExpiry: PersistentType: IDB\n\nnextIdUsed to track user\u2019s interaction with embedded content.\n\nExpiry: SessionType: HTTP\n\nremote_sidNecessary for the implementation and functionality of YouTube video-\ncontent on the website.\n\nExpiry: SessionType: HTTP\n\nrequestsUsed to track user\u2019s interaction with embedded content.\n\nExpiry: SessionType: HTTP\n\nServiceWorkerLogsDatabase#SWHealthLogNecessary for the implementation and\nfunctionality of YouTube video-content on the website.\n\nExpiry: PersistentType: IDB\n\nTESTCOOKIESENABLEDUsed to track user\u2019s interaction with embedded content.\n\nExpiry: 1 dayType: HTTP\n\nVISITOR_INFO1_LIVETries to estimate the users' bandwidth on pages with\nintegrated YouTube videos.\n\nExpiry: 180 daysType: HTTP\n\nVISITOR_PRIVACY_METADATAStores the user's cookie consent state for the current\ndomain\n\nExpiry: 180 daysType: HTTP\n\nYSCRegisters a unique ID to keep statistics of what videos from YouTube the\nuser has seen.\n\nExpiry: SessionType: HTTP\n\nyt.innertube::nextIdRegisters a unique ID to keep statistics of what videos\nfrom YouTube the user has seen.\n\nExpiry: PersistentType: HTML\n\nytidb::LAST_RESULT_ENTRY_KEYStores the user's video player preferences using\nembedded YouTube video\n\nExpiry: PersistentType: HTML\n\nYtIdbMeta#databasesUsed to track user\u2019s interaction with embedded content.\n\nExpiry: PersistentType: IDB\n\nyt-remote-cast-availableStores the user's video player preferences using\nembedded YouTube video\n\nExpiry: SessionType: HTML\n\nyt-remote-cast-installedStores the user's video player preferences using\nembedded YouTube video\n\nExpiry: SessionType: HTML\n\nyt-remote-connected-devicesStores the user's video player preferences using\nembedded YouTube video\n\nExpiry: PersistentType: HTML\n\nyt-remote-device-idStores the user's video player preferences using embedded\nYouTube video\n\nExpiry: PersistentType: HTML\n\nyt-remote-fast-check-periodStores the user's video player preferences using\nembedded YouTube video\n\nExpiry: SessionType: HTML\n\nyt-remote-session-appStores the user's video player preferences using embedded\nYouTube video\n\nExpiry: SessionType: HTML\n\nyt-remote-session-nameStores the user's video player preferences using\nembedded YouTube video\n\nExpiry: SessionType: HTML\n\n    * cdn.pbgrd.com\n\n2\n\npagead/gen_204Collects data on visitor behaviour from multiple websites, in\norder to present more relevant advertisement - This also allows the website to\nlimit the number of times that they are shown the same advertisement.\n\nExpiry: SessionType: Pixel\n\ncsiCollects data on visitors' preferences and behaviour on the website - This\ninformation is used make content and advertisement more relevant to the\nspecific visitor.\n\nExpiry: SessionType: Pixel\n\n    * pub.mdpi-res.com\n\n1\n\nOAIDRegisters a unique ID that identifies a returning user's device. The ID is\nused for targeted ads.\n\nExpiry: 1 yearType: HTTP\n\n  * Unclassified cookies are cookies that we are in the process of classifying, together with the providers of individual cookies.\n\n    * Crazyegg\n\n1\n\nLearn more about this provider\n\n_ce.irvPending\n\nExpiry: SessionType: HTTP\n\n    * matomo.mdpi.com\n\n1\n\n_pk_hsr.0.01efPending\n\nExpiry: 1 dayType: HTTP\n\n    * www.mdpi.com\n\n3\n\nhypothesis.testKeyPending\n\nExpiry: PersistentType: HTML\n\nmdpi_layout_type_v2Pending\n\nExpiry: 1 yearType: HTTP\n\nsettings_cachedPending\n\nExpiry: SessionType: HTTP\n\nCross-domain consent[#BULK_CONSENT_DOMAINS_COUNT#] [#BULK_CONSENT_TITLE#]\n\nList of domains your consent applies to: [#BULK_CONSENT_DOMAINS#]\n\nCookie declaration last updated on 3/25/24 by Cookiebot\n\n## [#IABV2_TITLE#]\n\n[#IABV2_BODY_INTRO#]\n\n[#IABV2_BODY_LEGITIMATE_INTEREST_INTRO#]\n\n[#IABV2_BODY_PREFERENCE_INTRO#]\n\n[#IABV2_BODY_PURPOSES_INTRO#]\n\n[#IABV2_BODY_PURPOSES#]\n\n[#IABV2_BODY_FEATURES_INTRO#]\n\n[#IABV2_BODY_FEATURES#]\n\n[#IABV2_BODY_PARTNERS_INTRO#]\n\n[#IABV2_BODY_PARTNERS#]\n\nCookies are small text files that can be used by websites to make a user's\nexperience more efficient.\n\nThe law states that we can store cookies on your device if they are strictly\nnecessary for the operation of this site. For all other types of cookies we\nneed your permission.\n\nThis site uses different types of cookies. Some cookies are placed by third\nparty services that appear on our pages.\n\nYou can at any time change or withdraw your consent from the Cookie\nDeclaration on our website.\n\nLearn more about who we are, how you can contact us and how we process\npersonal data in our Privacy Policy.\n\nPlease state your consent ID and date when you contact us regarding your\nconsent.\n\nPowered by Cookiebot by Usercentrics\n\nNext Article in Journal\n\nSome Theoretical Foundations of Bare-Simulation Optimization of Some Directed\nDistances between Fuzzy Sets Respectively Basic Belief Assignments\n\nPrevious Article in Journal\n\nPrediction Consistency Regularization for Learning with Noise Labels Based on\nContrastive Clustering\n\nPrevious Article in Special Issue\n\nA Unifying Generator Loss Function for Generative Adversarial Networks\n\n## Journals\n\nActive Journals Find a Journal Proceedings Series\n\n## Topics\n\n## Information\n\nFor Authors For Reviewers For Editors For Librarians For Publishers For\nSocieties For Conference Organizers\n\nOpen Access Policy Institutional Open Access Program Special Issues Guidelines\nEditorial Process Research and Publication Ethics Article Processing Charges\nAwards Testimonials\n\n## Author Services\n\n## Initiatives\n\nSciforum MDPI Books Preprints.org Scilit SciProfiles Encyclopedia JAMS\nProceedings Series\n\n## About\n\nOverview Contact Careers News Press Blog\n\nSign In / Sign Up\n\n## Notice\n\nclear\n\n## Notice\n\nYou are accessing a machine-readable page. In order to be human-readable,\nplease install an RSS reader.\n\nContinue Cancel\n\nclear\n\nAll articles published by MDPI are made immediately available worldwide under\nan open access license. No special permission is required to reuse all or part\nof the article published by MDPI, including figures and tables. For articles\npublished under an open access Creative Common CC BY license, any part of the\narticle may be reused without permission provided that the original article is\nclearly cited. For more information, please refer to\nhttps://www.mdpi.com/openaccess.\n\nFeature papers represent the most advanced research with significant potential\nfor high impact in the field. A Feature Paper should be a substantial original\nArticle that involves several techniques or approaches, provides an outlook\nfor future research directions and describes possible research applications.\n\nFeature papers are submitted upon individual invitation or recommendation by\nthe scientific editors and must receive positive feedback from the reviewers.\n\nEditor\u2019s Choice articles are based on recommendations by the scientific\neditors of MDPI journals from around the world. Editors select a small number\nof articles recently published in the journal that they believe will be\nparticularly interesting to readers, or important in the respective research\narea. The aim is to provide a snapshot of some of the most exciting work\npublished in the various research areas of the journal.\n\nOriginal Submission Date Received: .\n\n  * Journals\n\n    *       * Active Journals\n      * Find a Journal\n      * Proceedings Series\n\n  * Topics\n  * Information\n\n    *       * For Authors\n      * For Reviewers\n      * For Editors\n      * For Librarians\n      * For Publishers\n      * For Societies\n      * For Conference Organizers\n\n      * Open Access Policy\n      * Institutional Open Access Program\n      * Special Issues Guidelines\n      * Editorial Process\n      * Research and Publication Ethics\n      * Article Processing Charges\n      * Awards\n      * Testimonials\n\n  * Author Services\n  * Initiatives\n\n    *       * Sciforum\n      * MDPI Books\n      * Preprints.org\n      * Scilit\n      * SciProfiles\n      * Encyclopedia\n      * JAMS\n      * Proceedings Series\n\n  * About\n\n    *       * Overview\n      * Contact\n      * Careers\n      * News\n      * Press\n      * Blog\n\nSign In / Sign Up Submit\n\nJournals\n\nEntropy\n\nVolume 26\n\nIssue 4\n\n10.3390/e26040311\n\nSubmit to this Journal Review for this Journal Propose a Special Issue\n\n\u25ba \u25bc Article Menu\n\n## Article Menu\n\n  * Academic Editors\n\nShuangming Yang\n\nShujian Yu\n\nLuis Gonzalo S\u00e1nchez Giraldo\n\nBadong Chen\n\nBoris Ryabko\n\nShow more...\n\n  * Subscribe SciFeed\n  * Recommended Articles\n  * Related Info Link\n\n    * Google Scholar\n\n  * More by Authors Links\n\n    * on DOAJ\n\n      * Schirmer, P. A.\n      * Mporas, I.\n\n    * on Google Scholar\n\n      * Schirmer, P. A.\n      * Mporas, I.\n\n    * on PubMed\n\n      * Schirmer, P. A.\n      * Mporas, I.\n\n/ajax/scifeed/subscribe\n\nArticle Views 561\n\n  * Table of Contents\n\n    * Abstract\n    * Introduction\n    * Time Series Modelling Architecture\n    * Modelling Approaches\n    * Experimental Setup\n    * Experimental Results\n    * Discussion\n    * Conclusions\n    * Author Contributions\n    * Funding\n    * Institutional Review Board Statement\n    * Data Availability Statement\n    * Conflicts of Interest\n    * References\n\nAltmetric share Share announcement Help format_quote Cite question_answer\nDiscuss in SciProfiles thumb_up\n\n...\n\nEndorse textsms\n\n...\n\nComment\n\n## Need Help?\n\n### Support\n\nFind support for a specific problem in the support section of our website.\n\nGet Support\n\n### Feedback\n\nPlease let us know what you think of our products and services.\n\nGive Feedback\n\n### Information\n\nVisit our dedicated information section to learn more about MDPI.\n\nGet Information\n\nclear\n\n## JSmol Viewer\n\nclear\n\nfirst_page\n\nDownload PDF\n\nsettings\n\nOrder Article Reprints\n\nFont Type:\n\nArial Georgia Verdana\n\nFont Size:\n\nAa Aa Aa\n\nLine Spacing:\n\nColumn Width:\n\nBackground:\n\nOpen AccessArticle\n\n# PyDTS: A Python Toolkit for Deep Learning Time Series Modelling\n\nby Pascal A. Schirmer\n\nPascal A. Schirmer\n\nSciProfiles Scilit Preprints.org Google Scholar\n\n^ *,\u2020^ and Iosif Mporas\n\nIosif Mporas\n\nSciProfiles Scilit Preprints.org Google Scholar\n\n^ \u2020^\n\nSchool of Physics, Engineering, and Computer Science, University of\nHertfordshire, Hatfield AL10 9AB, UK\n\n^*\n\nAuthor to whom correspondence should be addressed.\n\n^\u2020\n\nThese authors contributed equally to this work.\n\nEntropy 2024, 26(4), 311; https://doi.org/10.3390/e26040311\n\nSubmission received: 27 February 2024 / Revised: 20 March 2024 / Accepted: 29\nMarch 2024 / Published: 31 March 2024\n\n(This article belongs to the Special Issue Information-Theoretic Methods in\nDeep Learning: Theory and Applications)\n\nDownload keyboard_arrow_down\n\nDownload PDF Download PDF with Cover Download XML Download Epub\n\nBrowse Figures\n\nVersions Notes\n\nArticle Views\n\nCitations -\n\n## Abstract\n\nIn this article, the topic of time series modelling is discussed. It\nhighlights the criticality of analysing and forecasting time series data\nacross various sectors, identifying five primary application areas: denoising,\nforecasting, nonlinear transient modelling, anomaly detection, and degradation\nmodelling. It further outlines the mathematical frameworks employed in a time\nseries modelling task, categorizing them into statistical, linear algebra, and\nmachine- or deep-learning-based approaches, with each category serving\ndistinct dimensions and complexities of time series problems. Additionally,\nthe article reviews the extensive literature on time series modelling,\ncovering statistical processes, state space representations, and machine and\ndeep learning applications in various fields. The unique contribution of this\nwork lies in its presentation of a Python-based toolkit for time series\nmodelling (PyDTS) that integrates popular methodologies and offers practical\nexamples and benchmarking across diverse datasets.\n\nKeywords:\n\ntime series modelling; forecasting; nonlinear modelling; denoising; anomaly\ndetection; degradation modelling; deep learning; machine learning\n\n## 1\\. Introduction\n\nTime series modelling has gained significant interest in the last decades due\nto the rise of machine learning and big data. It stands out as a crucial\ndomain with diverse applications, ranging from financial forecasting to\nclimate modelling [1,2]. The ability to analyse and forecast time series data\nhas become increasingly important for timely informed decision making in\nvarious fields. Five different areas of applications can mainly be identified:\nfirst, denoising (or source separation), where the signal ground truth is\nisolated from a noisy observation, e.g., speech denoising [3] or separation of\nenergy signals [4]; second, forecasting, where future signal values are\npredicted based on the signal\u2019s history, e.g., grid load or weather\nforecasting [5]; third, nonlinear transient modelling, where nonlinear and\npossibly underdetermined problems are solved for time series inputs, e.g.,\ntransient thermal, structural, or fluid modelling [6]; fourth, anomaly\ndetection, where outliers are identified in a large population of time series\ndata, e.g., faulty samples in production sequences or failures under\nthermal/mechanical stress [7]; and fifth, degradation modelling, where a\nvariable changes slowly over time, e.g., ageing of electric components and\nstructures or expiration of food [8,9].\n\nTo model the above phenomena in time series signals, several mathematical\napproaches have been proposed in the literature. These approaches can be\nfundamentally split into three categories, namely, statistical, linear\nalgebra, and machine- or deep-learning (ML, DL)-based ones. The dimensionality\nof the problem, i.e., the input and output dimension, as well as the problem\nevaluation over time, i.e., if the data have a constant mean value, highly\ndetermines which of the above techniques can be used to model the time series\nproblem. For example, statistical models like autoregression or moving average\nprocesses are restricted to one-dimensional time series and have been applied\nto linear statistical problems and short-term ahead prediction [10].\nConversely, in the case of two or more variables, linear algebra models like\nstate-space (SS) systems can be used to capture the input and output relation\nof multidimensional time series [11]. Most recently, machine and deep learning\nmodels have been used to capture complex multidimensional and possibly\nnonlinear relations between input and output samples of time series data [12],\nlike long short-term memory (LSTM) [13], one-dimensional convolutional neural\nnetworks (CNNs) [14], or transformer models [15].\n\nThe topic of time series modelling has also been studied extensively in the\nliterature. Modelling of statistical processes has been discussed in [16],\nwith specific applications like wind speed modelling [17] or electricity or\nemission forecasting [18,19]. Similarly, state-space representations have been\nreviewed in [20]. In detail, state-space models have been proposed for thermal\nmodelling in buildings [21] or battery electric vehicles [22], as well as in\nmethodologies for solar irradiance forecasting in combination with exponential\nsmoothing [23]. Moreover, numerous articles on machine and deep learning have\nbeen published covering the topics of feature extraction [24] and modelling\napproaches [25,26]. In specific, machine and deep learning approaches have\nbeen used for forecasting in applications like renewable energies [27], grid\nloads [28], and weather events [29]. Furthermore, deep learning models have\nbeen used for denoising in medical applications [30] and in renewable energy\ngeneration [31]. Similarly, nonlinear applications have been studied including\nstructural dynamic problems [32], time delay approximations in optical systems\n[33], or transient thermal modelling [34]. Deep learning approaches have also\nbeen used in anomaly detection [35] and degradation modelling [36]. Most\nrecently, also combinations of these approaches, e.g., deep state space models\n[37], or informed neural networks have been proposed [38]. Moreover, federated\nlearning applications sharing one common model and approaches implemented on\nmicroprocessor hardware have been investigated [39].\n\nSeveral different toolkits for time series modelling have been proposed\npreviously, including Nixtla [40], AutoTS, Darts [41], and Sktime [42]. Each\nof these toolkits has a different purpose and different functionalities. While\nNixtla and AutoTS only implement time series forecasting, Darts additionally\nimplements anomaly detection, while Sktime implements forecasting,\nclassification, regression, and data transformations. Likewise, PyDTS offers\nforecasting, classification, and regression functionalities, but additionally\nfocuses on specific applications like denoising, nonlinear modelling, or\ndegradation. The aim is to reduce the threshold of using deep-learning-based\nmodelling as far as possible by offering a one-click functionality without\nneeding to copy code, download, and preprocess data or plot results. The\ncontributions of this article are as follows: First, the topic of time series\nmodelling is reviewed. Second, a Python-based toolkit for time series\nmodelling (PyDTS) with deep learning is presented, which incorporates the most\nused approaches and provides time series modelling examples for a wide range\nof datasets and benchmarking results. The results of these examples can be\nreproduced by calling one single function. Third, the article explains the\neffect of the free parameters, and the user can try these changes by simply\nchanging one parameter without the need for changing the code while observing\nthe changes based on a standard set of accuracy metrics and plots. Fourth, all\nresults are evaluated on real-world datasets without the use of any synthetic\nor exemplary datasets. The toolkit is available on GitHub\n(https://github.com/pascme05/PyDTS, accessed on 27 February 2024).\n\nThe remainder of the article is structured as follows: In Section 2, a\ngeneralized architecture for time series modelling is described, also\nintroducing the different applications of time series modelling. In Section 3,\ndifferent modelling approaches are presented. An experimental setup and\nresults for different datasets and applications are presented in Section 4.\nFinally, discussion and conclusions are provided in Section 5 and Section 6,\nrespectively.\n\n## 2\\. Time Series Modelling Architecture\n\nAs outlined in Section 2, time series modelling has several applications. In\nthis section, a generalized modelling architecture is introduced, while\nspecific approaches including their mathematical formulation are presented in\nSection 2.1\u2013Section 2.5. Let us consider an input time series signal with T\ntime samples of M input values each and a multivariate output signal with the\nsame number of time samples and N output values; we can formulate the\ninput\u2013output relation as follows (1):\n\nwhere is an arbitrary nonlinear function parametrized by a set of free\nparameters . The goal of a time series modelling architecture is to model the\ninput and output relation as in (2):\n\nwhere is an arbitrary regression or classification function aiming to\napproximate and its free parameters, and is the predicted output. The\ngeneralized architecture is illustrated in Figure 1:\n\nFigure 1. Generalized time series architecture.\n\nAs illustrated in Figure 1, the general architecture consists of five steps:\nfirst, preprocessing, e.g., resampling or filtering, of the raw feature input\nvector, x resulting into ; second, window framing into time frames with a\nwindow length W; third, feature extraction based on the time frame signals\nconverting to a feature input vector with F input features; and finally,\npredicting and optionally postprocessing the model output . In specific, when\npredicting time series signals, the input and output relation can be modelled\nusing three different approaches, which can be distinguished by their input\nand output dimensionality in the temporal domain. The three approaches are\nsequence-to-point modelling, sequence-to-subsequence modelling, and sequence-\nto-sequence modelling [43] and are conceptually illustrated in Figure 2.\n\nFigure 2. Relation between input and output dimensionality for frame-based\ntime series modelling: (a) sequence-to-point, (b) sequence-to-subsequence, and\n(c) sequence-to-sequence.\n\nThe PyDTS toolkit replicates the above structure, providing modules for\npreprocessing, framing, feature extraction, modelling approach, and\npostprocessing. The different modules offered by PyDTS and the flow diagram\nfor the different operations are illustrated in Figure 3 and Figure 4.\n\nFigure 3. Overview of implemented modules and functionalities in the PyDTS\ntoolkit. Inputs and preprocessing are indicated in red, features and data\nsequencing in yellow, modelling in green, postprocessing in blue, and visual\nelements and outputs in purple.\n\nFigure 4. Internal data pipeline of PyDTS including training and testing\nmodules and external data, model, and setup databases.\n\nIn the following, the mathematical formulation of time series modelling with\napplication in denoising, forecasting, nonlinear modelling, anomaly detection,\nand degradation modelling are provided.\n\n#### 2.1. Denoising\n\nOne of the most common time series prediction tasks is denoising, where the\nground-truth data are retrieved based on a distorted observation. Without loss\nof generality, the problem can be formulated as in (3):\n\nwhere is the output signal, is the input signal, and is the noise. Here, we\nuse as an example of denoising the energy disaggregation task, where appliance\nenergy signatures (clean signal) are extracted from the aggregated data (noisy\nsignal) [44]. Since multiple signals are extracted from a single observation,\nit is a single-channel blind source separation problem, i.e., a problem with\nvery high signal-to-noise ratio. The problem can be mathematically formulated\nas in (4):\n\nwhere is the aggregated signal, is the m-th appliance signal, and is additive\nnoise from unknown devices, from electromagnetic interference on the\ntransmission lines and from line coupling. The goal is to denoise the signal\nby isolating the signature of each appliance.\n\n#### 2.2. Forecasting\n\nLoad forecasting is a task where future values, e.g., weather, energy\nconsumption, or power draw, are predicted based on previous values of the same\ntime series signal [45]. The aim is to model temporal information based on\nprevious samples and accurately predict future values. Assuming linearity, the\nproblem can be mathematically formulated as in (5):\n\nwhere is the signal of interest, are signals with additional information and\nare constant in the linear case, and is stochastic noise. In this article,\nenergy consumption prediction has been used as an example; i.e., future energy\nconsumption values are predicted based on the consumption of previous days and\nadditional information, e.g., weather or socioeconomic information [46].\n\n#### 2.3. Nonlinear Modelling\n\nNonlinear modelling is a task where the relation between input and output\nvalues is nonlinear. As an example application of nonlinear modelling, thermal\nmodelling of power electronics and electric machinery is considered [47]. In\nthis application, the fundamental heat conduction equation itself is linear,\nbut nonlinearities are introduced through thermal coupling or losses, which\nare themselves a nonlinear function of temperature. Fundamentally, the\ntemperature on a component can be modelled as in (6) and (7):\n\nwhere is a time-dependent heat source that is generated by a current flowing\nthrough a nonlinear temperature-dependent resistance . The temperature is then\ncalculated using (7):\n\nwhere is the mass density, the specific heat capacity, and k the thermal\nconductivity. Furthermore, is a spatial function projecting the heat source on\nthe respective volume.\n\n#### 2.4. Anomaly Detection\n\nAnomaly detection describes the task of finding outliers within the data.\nOften, these data are highly unbalanced; i.e., there are much more positive\nthan negative values or vice versa. The aim is to efficiently detect a small\nnumber of outliers within large amounts of time series data. The problem can\nbe mathematically formulated as follows (8):\n\nwhere is the anomaly detection status of the signal; i.e., if a sample at time\nt is normal or anomalous, are the input signals that provide indication for\nthe status signal, is a function calculating the probability for a sample to\nbe anomalous, and is a threshold to convert the prediction into a binary\nvariable. In this article, we used as an example model motor faults based on\nvibration data.\n\n#### 2.5. Degradation Modelling\n\nDegradation modelling is a task where a relation between input parameters,\ntime, and slow-varying output parameters exists. The aim is to describe the\nslow-varying degradation based on the initial state and the loads applied over\ntime. The problem can be mathematically formulated as in (9):\n\nwhere is the degradation signal; are load signals stressing the component,\ne.g., temperature or mechanical stress; and is stochastic noise. It must be\nnoted that this problem depends on the initial state of . In this article, the\nexample case is to predict degradation data of lithium-ion batteries, i.e.,\nthe change of cell capacitance over time, using temperature, current, and\nvoltage as input features.\n\n## 3\\. Modelling Approaches\n\nTo implement the classification or regression function from (1), three\napproaches exist, namely, statistical, linear algebra, and machine or deep\nlearning (ML, DL). In the following subsections, each of these three\napproaches is briefly explained.\n\n#### 3.1. Statistical Modelling\n\nAssuming that the output function is a one-dimensional time series and only\ndepends on previous values and stochastic white noise , then the relation\nbetween input and output can be expressed using statistical models based on\nautoregression and averaging (ARMA) [48], as described in (10):\n\nwhere c is a constant, is a weighting factor for the autoregression term, and\nis a weighting factor for the moving average.\n\n#### 3.2. Linear Algebra Modelling\n\nIf there are two processes, with one process being latent, thus describing a\nhidden time-varying structure, state-space representations have been used for\nthe system identification of first-order systems with M inputs and N outputs\n[49]. The mathematical formulation for continuous parameter time-invariant\ncoefficients is shown in (11):\n\nwhere and are the internal system states and the derivatives with L being the\nnumber of states, is the system matrix, is the input matrix, is the output\nmatrix, and is the feed-forward matrix. This model belongs to the category of\nwhite box modelling [50], where the states and the evolution of the states can\nbe physically interpreted and, most importantly, also observed (12) and\ncontrolled (13) if the following restrictions are satisfied [49]:\n\n#### 3.3. Machine and Deep Learning\n\nWhile the above techniques have limitations regarding the dimensionality of\nthe input and output channels or the nonlinearity of the relation between\ninput and output features, machine and deep learning models offer the highest\nflexibility in modelling an arbitrary function. In detail, the output of an\nartificial neural network with one hidden layer is shown in (14):\n\nwhere and are the activation functions and the weights of the respective\nlayer, and J is the number of nodes in the hidden layer. The weights can then\nbe determined iteratively using backpropagation and a loss function, as shown\nin (15):\n\n#### 3.4. Comparison\n\nEach of the above modelling approaches has its advantages and disadvantages. A\ncomparison list of relevant properties is shown in Table 1. Whenever, the\nrespective property can be deducted directly from the model equation in\nSection 3.1\u2013Section 3.3, e.g., the dimensionality of the input/output or the\ninterpretability of the internal state. Table 1 lists the respective equation;\notherwise, relevant literature is provided.\n\nTable 1. Comparison of relevant properties between different modelling\napproaches: (+): comparatively better, (o): neutral, and (-): comparatively\nworse.\n\nAs can be seen in Table 1, machine and deep learning approaches suffer\nespecially from larger computational complexity, memory requirements, and a\nlack of physical interpretation of the model parameters [50,51]. Statistical\nmodels present advantages, but at the same time, they are limited in 1D-only\ninput and output dimensionality [48], as can be also seen from (10). This\nrestriction makes statistical modelling approaches not feasible for most of\nthe presented tasks in Section 2. In terms of transferability, deep learning\napproaches have very good transferability properties working as automated\nfeature extraction engines [52]; however, they require extensive amounts of\ntraining data and have many hyperparameters to optimize [50,53]. Finally, as\nexplained in Section 3.3, machine and deep learning models enable nonlinear\nmodelling due to the nonlinear activation functions in (14). Because of the\nlimitation of statistical and linear algebra models with respect to the input\nand output dimension in the following sections, the focus will be on machine\nand deep learning approaches.\n\n## 4\\. Experimental Setup\n\nThe time series modelling architecture described in Section 2 was evaluated\nusing the datasets, models, and experimental protocols presented below.\n\n#### 4.1. Datasets\n\nThe proposed time series prediction methods have been evaluated using publicly\navailable datasets consisting of real-world data; i.e., no synthetic data have\nbeen used. In the following, each of the datasets is briefly explained. For\ndisaggregation energy data (denoising), the AMPds2 dataset has been used,\nwhich includes 20 electrical appliances and the aggregated energy consumption\nof a Canadian household measured between 2012 and 2014 [54]. For energy\nconsumption forecasting, the energy consumption of Tetouan, a city in the\nnorth of Morocco, has been used [55]. For nonlinear modelling, the motor\ntemperature dataset in [47] has been used, which includes 185 h of measured\ntemperatures of a state-of-the-art permanent magnet synchronous machine from a\nTesla Model 3. To predict anomalies, motor vibration data have been used,\nwhich were previously classified into faulty and faultless motors [56]. To\nmodel degradation, the dataset from [57] was used, which includes lithium-ion\nbattery cells measured over several cycles of charging and discharging under\ndifferent conditions. The datasets, including their most important properties,\nare summarized in Table 2.\n\nTable 2. Short description of the datasets. The feature column includes the\nfollowing abbreviations: active power (P), reactive power (Q), apparent power\n(S), current (I), voltage (V), temperature (T), relative humidity (), solar\nirradiance (), wind speed (), rotational speed (n), torque (M), and\nacceleration (A). Similarly, the outputs include the appliance current (), the\nper-phase power (), the stator winding and rotor magnet temperatures (), the\nmotor state, and the remaining battery charge ().\n\nAs can be seen in Table 2, the datasets cover a wide range of sampling\nfrequencies, total number of samples, and input features, allowing for testing\nthe PyDTS toolkit on different data inputs. Additionally, for the input\nfeatures, the output that will be predicted is shown, as well as the max,\nmean, and standard deviation of the output. These values are included to\nprovide a standard to the performance of the regression or classification\nmodels. For example, if the standard deviation of a dataset is close to zero,\nthere are very few changes in the output signal; thus, a naive predictor would\nbe sufficient to predict the outputs. Similarly, if the maximum predicted\nerror of a model is equal to the maximum value of the output signal, while the\naverage is close to zero, that indicates that the model is predicting well on\naverage, but there are instances in which it fails to make an accurate\nprediction.\n\n#### 4.2. Preprocessing\n\nDuring preprocessing, the input data have been normalized using mean\u2013std\nnormalization for input features (16):\n\nwhere is the input feature scaled by the mean () and standard deviation () of\nthe training data. Similarly, min\u2013max normalization has been used for the\noutput features (17):\n\nwhere is the output feature scaled by the minimum and maximum values of the\ntraining data. Furthermore, the optimal number of samples for the input window\nhas been determined by grid search for each of the datasets tabulated in Table\n1 with the exception of the anomaly detection as it is predefined in that\ndataset. The results are shown in Figure 5.\n\nFigure 5. Grid search for the optimal number of input samples depending on the\ntime series problem.\n\nAs can be seen in Figure 5, the optimal number of input samples strongly\nvaries with the problem under investigation. In detail, when denoising\nelectrical appliances signatures, the optimal input length is around 30 min,\nwhich is a typical operational duration for electrical appliances [58]. For\nthe forecasting of electrical power consumption, the optimal input length was\nfound to be around 24 h, which is typical due to working and living habits. It\ncan also be observed that at around 12 h, 36 h, and 48 h, there are\nsignificant improvements. For modelling degradation data, no upper limit could\nbe found since the degradation is a slow-varying property and it would be best\nto feed the complete degradation cycle at once, which is not possible due to\nthe number of samples. The optimal input length for modelling the thermal\nbehaviour of the electrical machine was found to be 20 min, which is in the\norder of the thermal time constant of the machine, and it is in line with\n[59]. Unless otherwise stated, the modelling approaches are based on sequence-\nto-point modelling using the optimized length of input samples from Figure 5,\nwith one sample overlap between consecutive frames.\n\n#### 4.3. Model Structure and Parametrization\n\nTo implement the regression function for the approaches discussed in Section\n2, different ML and DL approaches have been used. For ML approaches\nespecially, random forest (RF) and K-nearest neighbours (KNN) have been\nevaluated, while for anomaly detection, also support vector machine (SVM) has\nbeen tested. The free parameters have been found using exhaustive automated\nparameter optimization on a bootstrap training dataset. The results are\npresented in Table 3.\n\nTable 3. Optimized model parameters for ML approaches including KNN, RF, and\nSVM.\n\nSimilarly, for DL models, DNN, LSTM, and CNN architectures have been\nevaluated. The architectures are illustrated in Figure 6.\n\nFigure 6. DL layer architectures for DNNs, LSTM, and CNN models. For CNNs, the\nnotation of the convolutional layer is Conv1D(x,y) with x being the number of\nfilters and y being the kernel size. For pooling layers MaxPool(x,y), x is the\nsize and y the stride, while for LSTM and DNN layers, x denotes the number of\nneurons.\n\nUnless otherwise stated, the above architectures have been used when being\nreferred to CNN, LSTM, and DNN. For specific applications, the free\nparameters, i.e., the number of hidden layers, neurons, the kernel sizes, and\nthe filters, have been optimized using the hyperband tuner from Keras.\nAdditionally, the hyperparameters and solver parameters tabulated in Table 4\nhave been used.\n\nTable 4. Hyper- and solver parameters for deep learning models including DNN,\nCNN, and LSTM.\n\n## 5\\. Experimental Results\n\nIn this section, the experimental results are presented when using the data,\nthe parametrizations, and models from Section 4. The results are evaluated in\nterms mean absolute error (MAE), root mean square error (RMSE), mean square\nerror (MSE), and the normalized mean square error (NMSE):\n\nwhere is the true signal, is the predicted value, and T is the total number of\nsamples. Since not all modelling approaches are applicable for each of the\nscenarios, due to their limitations with respect to the input and output\ndimensionality, the following results are presented for machine and deep\nlearning approaches. Each of these approaches can be reproduced with the PyDTS\ntoolkit using the predefined configuration stored under the setup directory\n(https://github.com/pascme05/PyDTS/tree/main/setup/journal, accessed on 26\nFebruary 2024). Unless otherwise stated, the results were calculated using\nfivefold cross-validation using 10% of the training data for validation.\n\n#### 5.1. Denoising\n\nFor the denoising task, the energy of a Canadian household [54] has been\ndisaggregated; i.e., the appliance-specific energy consumption has been\nextracted based on the observation of the total energy consumption of the\nhousehold. Specifically, we focused on five different appliances: the\ndishwasher (DWE), the fridge (FRE), the heat pump (HPE), the wall oven (WOE),\nand the cloth dryer (CDE). For input features, active power (P), reactive\npower (Q), apparent power (S), and current (I) were used, while the output\nfeature was the current for each device. The average results for all the five\nappliances and different machine and deep learning models are tabulated in\nTable 5.\n\nTable 5. Average results (A) for the energy disaggregation task for fivefold\ncross-validation using different models and accuracy metrics. The best\nperformances are indicated with bold notation.\n\nAs can be seen in Table 5, LSTM outperforms all other regression models for\nall accuracy metrics except for the maximum error. In this scenario, only 1D\ntime series inputs were used to disaggregate the signals, and LSTM has shown\noutperforming results in application with 1D time series, including temporal\ninformation, i.e., where future samples depend on previous samples.\nFurthermore, the results for the best-performing model (LSTM) have been\nevaluated at the device level and are presented in Table 6.\n\nTable 6. Per-device results (A) for the energy disaggregation task for\nfivefold cross-validation using LSTM as regression model and different\naccuracy metrics.\n\nAs can be seen in Table 6, all appliances show low disaggregation errors,\nexcept the dishwasher, which shows poor performance that could be attributed\nto its lower activity, which is in line with other approaches reported on the\nsame dataset [58]. Moreover, the results have been compared with the state-of-\nthe-art approaches in the literature. The results are presented in Table 7.\n\nTable 7. Comparison with the literature for the energy disaggregation task.\n\nAs can be seen in Table 7, the PyDTS toolkit reports results similar to the\nones from previously reported approaches on the same dataset and is only\noutperformed by specifically optimized approaches for the energy\ndisaggregation task. Moreover, a set of numerical predictions and ground-truth\ndata is illustrated in Figure 7 for the best-performing LSTM model from PyDTS.\nIn detail, a 12 h period with high appliance activity on 9 January 2013 at\n12:00 p.m. was selected, where FRE, HPE, and CDE are active at the same time.\n\nFigure 7. Predicted appliance current draw for 12 h for three different (FRE,\nHPE, and CDE) appliances from the AMPds2 dataset on 9 January 2013 at 12:00\np.m.\n\nAs can be seen in Figure 7, the LSTM model is able to extract all three\nappliance signatures from the aggregated data with high accuracy. There are\nonly minor errors during the active periods where the current ripple is not\nprecisely predicted.\n\n#### 5.2. Forecasting\n\nFor the forecasting task, the energy consumption of a city in Morocco [55] has\nbeen used. As input features, the previous power consumption values of the\nthree-phase grid have been chosen. Additionally, these values have been\nextended by environmental features, namely, the ambient temperature, the wind\nspeed, the relative humidity, and the solar irradiance. The output feature,\nwhich is predicted, is the power consumption on phase-leg L1. The results for\nan ahead forecast of 24 h are presented for different regression models in\nTable 8 using Seq2Point and in Table 9 using Seq2Seq approaches.\n\nTable 8. Forecasting errors (kW) using Seq2Point for a 24 h ahead prediction\nwindow with different models and accuracy metrics using fivefold cross-\nvalidation. The best performances are indicated with bold notation.\n\nTable 9. Forecasting errors (kW) using Seq2Seq for a 24 h ahead prediction\nwindow with different models and accuracy metrics using fivefold cross-\nvalidation. The best performances are indicated with bold notation.\n\nAs can be seen in Table 8 and Table 9, Seq2Seq approaches outperform Seq2Point\napproaches for all deep learning approaches with LSTM being able to capture\nthe temporal relation reporting an average error equal to 2.36 kW. However,\nwhen considering Seq2Point approaches, RF shows improved performance reporting\nan average error of 1.60 kW but showing a significantly higher maximum error\nof 17.88 kW compared with the best-performing LSTM approach, which has a\nmaximum error of 12.12 kW. The best performance is illustrated for 1 week in\nFigure 8.\n\nFigure 8. Forecasted power consumption and error for phase L1 for 1 week using\nRF as regression model.\n\nAs can be seen in Figure 8, the predicted power consumption is close to the\nactual value with errors between 1 and 5 kW. Interestingly, the errors at the\nbeginning and ending of the week are higher than at the middle of the week,\nwhich is probably due to a higher fluctuation of power demand at these times.\n\n#### 5.3. Nonlinear Modelling\n\nFor the nonlinear modelling task, the temperature prediction of a permanent\nmagnet synchronous machine [47] has been considered. In detail, four different\ntemperature hot spots have been evaluated, namely, the stator winding, the\nstator tooth, the stator yoke, and the magnet temperature inside the rotor. As\ninput features, the ambient and the coolant temperature, the stator current\nand voltages, and the mechanical torque as well as the rotational speed have\nbeen used. The output is the maximum stator winding () and the rotor magnet ()\ntemperature. The results in terms of MAE, RMSE, and MAX error are tabulated in\nTable 10 for stator and rotor temperatures, respectively.\n\nTable 10. Temperature prediction results for 5-fold cross validation using\ndifferent regression models and performance metrics. Due to memory\nrestrictions the LSTM input was reduced to 500 samples. The best performances\nare indicated with bold notation.\n\nAs can be seen in Table 10, the rotor temperature shows worse performances\nacross all models in terms of accuracy as its losses and thus temperatures are\nmuch more difficult to model based on the available inputs. Furthermore, deep\nlearning models outperform machine learning models due to their ability to\nbetter capture the nonlinear relationship between the input feature vector and\nthe temperature rise of the electric machine. To further compare the results,\nthe experiments from [59] have been repeated using the same split for\ntraining, testing, and validation data. The results for the best-performing\nCNN model are tabulated in Table 11.\n\nTable 11. Results for MSE (K2) and MAX (K) errors for different testing IDs,\ntheir respective time (hr), and temperature hot spots using a CNN regression\nmodel per hot spot.\n\nAs can be seen in Table 11, the difficulty in estimating the temperatures in\nthe different test IDs varies significantly, with the lowest errors being\nfound in test ID 62 and the highest in test ID 72. On average, the results are\nbetter for the stator temperatures, which is in line with the input features\nbeing mostly stator quantities. In Figure 9, the temperature predictions for\nstator winding and magnet temperature are illustrated for all three testing\nIDs.\n\nFigure 9. Predicted temperature for stator winding and rotor magnet for IDs\n60, 62, and 72.\n\nAs can be seen in Figure 9, stator temperatures are much better predicted than\nrotor temperatures. Especially during heat-up and cool-down phases, the rotor\ntemperature is not correctly predicted. This is probably due to the change in\nthe heat transfer coefficient and the fact that the rotor is thermally\nisolated through the air gap; thus, the heat path is not based on heat\nconduction as in the stator, but a combination of heat convection and\nconduction. To compare the results with the previously published literature, a\ncomparison of average errors was made in Table 12.\n\nTable 12. Comparison for temperature prediction using different models and\nnumber of input features.\n\nAs can be seen in Table 12, the results obtained from the baseline CNN model\nimplemented in PyDTS are comparable to the results obtained from other machine\nor deep learning architectures. Only physical informed approaches like thermal\nneural networks [59] perform significantly better.\n\n#### 5.4. Anomaly Detection\n\nFor the anomaly detection task, the vibration data of combustion engines, in\nnormal and faulty states, have been used. As an input feature, the\nacceleration signal has been used, while the output is a binary variable\nindicating the healthy or faulty state of the motor [56]. Since, in this\ndataset, the training and test scenarios are presplit, the results will not be\npresented for fivefold cross-validation as in the previous experiments but\nusing the predefined splitting of the data. In detail, the results were\ncalculated three times, using raw input samples of the acceleration data,\nusing statistical features of the acceleration data (mean, min, max, std,\nrange, etc.) [44], and using frequency domain features (e.g., magnitudes of\nthe Fourier transform signal or wavelets) [64,65]. The results in terms of\naccuracy (ACC) and F1-score (F1) are tabulated in Table 13 for different\nclassification models.\n\nTable 13. Classification results in terms of ACC and F1 for anomaly detection\nusing different classification models. The best performances are indicated\nwith bold notation.\n\nAs can be seen in Table 13, DL approaches clearly outperform ML-based\napproaches when using raw data operating as automated feature extraction\nengines. ML techniques show good results on frequency domain features as the\nrelevant information is extracted when computing the Fourier coefficients.\nWhen using statistical features, none of the classification models can perform\nwell, as the averaging effect in the time domain eliminates the vibration\nsignatures discriminating healthy and faulty samples. To give more insights\ninto the prediction accuracy, the confusion matrix of the best-performing CNN\nmodel is illustrated in Figure 10 for all three different feature setups.\n\nFigure 10. Confusion matrices for (a) raw, (b) statistical, and (c) frequency\ndomain features for the CNN model.\n\n#### 5.5. Degradation Modelling\n\nFor the degradation modelling task, the ageing data of lithium-ion battery\ncells [57] have been used during charging and discharging. As input features,\nthe cell current and voltage as well as the cell temperature have been used.\nThe output is the degradation curve of the maximum remaining cell capacity for\neach charging and discharging cycle. The results for different regression\nmodels and accuracy metrics are tabulated in Table 14 for Seq2Point learning\nand in Table 15 for Seq2Seq learning. It must be noted that machine learning\napproaches are not able to perform Seq2Seq learning due to their restriction\nof the input dimensionality.\n\nTable 14. Degradation errors for different regression models and performance\nmetrics using Seq2Point learning. The best performances are indicated with\nbold notation.\n\nTable 15. Degradation errors for different regression models and performance\nmetrics using Seq2Seq learning. The best performances are indicated with bold\nnotation.\n\nAs can be seen in Table 14 and Table 15, deep learning approaches are\nsignificantly outperforming machine learning approaches due to their ability\nto model longer temporal characteristics. In detail, DNNs outperform all other\nmodels for all performance metrics except for the maximum error. The predicted\ndegradation curve is illustrated in Figure 11.\n\nFigure 11. Ground-truth and predicted remaining cell charge and prediction\nerror using the best-performing DNN model (for visibility, the predicted\noutput has been filtered with a median filter of a length of 100 samples).\n\nAs shown in Figure 11, the predicted output closely follows the measured\ndegradation curve and is also capturing the frequent relaxation of the cell\nmaterial, e.g., after 50 h. The maximum error is approximately 0.075 Ah being\n12.3% of the remaining cell capacitance. On average, the model is\nunderestimating the remaining capacity with around 0.01 Ah being 1.7% of the\naverage cell capacitance.\n\n## 6\\. Discussion\n\nIn this section, discussion on transferability is provided in Section 6.1,\nexecution time and model size in Section 6.2, and model optimization and model\norder reduction in Section 6.3.\n\n#### 6.1. Transfer Learning\n\nIn transfer learning, the aim is to predict the output of new data based on a\nmodel that was pretrained on other data for a usually similar application. Two\ndifferent approaches are investigated, namely, the intratransferability and\nthe intertransferability. During intratransferability, the new data come from\nthe same data domain, e.g., a different phase of the same electrical grid,\nwhile in intertransferability, the data only come from the same application\ndomain, e.g., the same type of electrical appliance in a different consumer\nhousehold. Both types of transferability will be considered in this\nsubsection. The intratransferability setup is based on the electrical load\nforecasting of Section 5.2, predicting the load of phase 2 using a model\ntrained on phase 1. The intertransferability setup is based on the\ndisaggregation setup of Section 5.1 and [52], extracting the load signatures\nof a fridge, microwave, and dishwasher in a different household using the REDD\ndataset [66] (houses 1 and 2). The results for the intratransferability setup\nare tabulated in Table 16.\n\nTable 16. Intratransferability scenario based on load forecasting between\nphases 1 (L1) and 2 (L2). The best performances are indicated with bold\nnotation.\n\nAs can be seen in Table 16, the performance when predicting phase 2 based on a\nmodel of phase 1 leads to a decrease in all evaluated accuracy metrics and all\nregression models with a loss between 0.35% and 73.27%. However, due to the\ndata coming from the same domain, the average accuracy is still relatively\nhigh between 87.44% and 93.28%. In detail, LSTM shows better performance\ncapturing the temporal information of phase 1 and transferring it to phase 2,\nshowing significantly lowest loss in accuracy by only 0.35\u20134.63%. The results\nfor the intertransferability setup are tabulated in Table 17.\n\nTable 17. Intertransferability scenario based on energy disaggregation between\ndifferent consumer households (REDD-1,2). The best performances are indicated\nwith bold notation.\n\nAs can be seen in Table 17, the loss in performance is substantially increased\ncompared with the intratransferability setup by 13.31\u2013204.00%. This is due to\nthe much more complex task of modelling similar devices in a completely\ndifferent environment. Overall, CNN is achieving the best absolute performance\nfor both the baseline and the transferability scenario.\n\n#### 6.2. Execution Time and Model Size\n\nModel size and execution time determine the real-time capability and the\nutilization on hardware applications. Different models and application\nscenarios have been benchmarked on a personal computer using an AMD Ryzen\n3700, an Nvidia RTX3070, and 32 GB of 3600 MHz DDR4 RAM. The model sizes after\ntraining are tabulated in Table 18.\n\nTable 18. Model size of the trained model including all parameters for\ndifferent scenarios.\n\nFrom Table 18, it is observed that while the model size of CNN, LSTM, and DNN\nonly depends on the size of the feature input vector, KNN stores all training\nsamples to compute neighbouring distances and RF creates more trees, thus\nhaving significantly higher memory requirements for large datasets.\nAdditionally, while the DNN and CNN models are sensitive to the window length\nof the input feature vector, the LSTM model has barely increased in model size\ndue to its long short-term memory cells. The training and inference times are\nreported in Table 19.\n\nTable 19. Training (T) and inference time (I) per sample (s) for different\nmodels and scenarios.\n\nAs can be seen in Table 19, the training time per sample of deep learning\napproaches depends mainly on the convergence of the model. Conversely, the\ntraining time per sample for RF depends on the complexity and the number of\ndifferent states that are extracted, while it is close to zero for KNN, which\ndoes not have any trainable parameters. Considering inference time, deep\nlearning approaches are mostly dependent on the model size and the size of the\ninput feature vector. Conversely, RF has very low inference time as it only\nperforms comparison at the branches of the different decision trees, while KNN\nhas large inference times because it compares every sample in the testing data\nwith the training data.\n\n#### 6.3. Optimal Models and Model Order Reduction\n\nTo further improve the performance of a deep learning model in terms of model\nsize and/or performance, the input feature vector and the model parameters can\nbe optimized. To optimize the input feature vector, the importance of the\ninput with respect to the output can be evaluated. Possible ranking algorithms\ninclude principal component analysis (PCA), correlation coefficients, or the\nReliefF algorithm [67]. The feature ranking for the nonlinear modelling task\nis illustrated in Figure 12.\n\nFigure 12. Feature ranking for the nonlinear modelling task for 13 features:\ncoolant/ambient temperature (, ), stator voltages (, , ), stator currents (, ,\n), torque (), rotational speed (), apparent power (), and products or\ncurrent/power and rotational speed (, ).\n\nAs can be seen in Figure 12, the stator and rotor temperature are dominated by\nthe cooling temperature (heat conduction to the coolant), the ambient\ntemperature (heat convection to the ambient), the stator voltage and stator\ncurrent (ohmic and iron losses), and the rotational speed (coupling or stator\nand rotor temperature through airflow inside the machine). Furthermore, a\nKeras hyperparameter tuner can be used to optimize the parameters of the CNN\nmodel to account for the changed input feature dimensionality. The results of\nthe reduced-order model using 6 input features instead of 13 are tabulated in\nTable 20.\n\nTable 20. Temperature prediction results for stator winding and magnet\ntemperature in terms of MSE (K2) for different testing IDs and models.\nBaseline scenarios are denoted with \u2018Base\u2019, while reduced-order configurations\nare denoted with \u2019MOR\u2019.\n\nAs can be seen in Table 20, a reduced-order model reports even better\nperformances for stator quantities, showing improvement by 34.1%. Conversely,\nthe rotor performance decreased by 26.9%, which is probably due to the missing\ntorque values and the complex power as these quantities are directly related\nto the rotor shaft.\n\n## 7\\. Conclusions\n\nA machine and deep learning Python toolkit for modelling time series data has\nbeen introduced. Five different scenarios, namely, denoising, forecasting,\nnonlinear modelling, anomaly detection, and degradation modelling, have been\nevaluated using real-word datasets and different machine and deep learning\nmodels. It was shown that the PyDTS toolkit and the models implemented in the\ntoolkit can achieve performance close to the state of the art of the\nrespective approach. Additionally, to benchmark the different approaches, the\ntopics of transfer learning, hardware requirements, and model optimization\nhave been discussed. The authors hope that the paper, accompanied by the PyDTS\ntoolkit, will help new researchers entering the area of time series modelling\nand hopefully will create new ideas.\n\n## Author Contributions\n\nConceptualization, P.A.S.; methodology, P.A.S.; software, P.A.S.;\nwriting\u2014original draft preparation, P.A.S. and I.M.; writing\u2014review and\nediting, I.M. All authors have read and agreed to the published version of the\nmanuscript.\n\n## Funding\n\nThis research received no external funding.\n\n## Institutional Review Board Statement\n\nNot applicable.\n\n## Data Availability Statement\n\nThe data on code are publicly available on GitHub at\nhttps://github.com/pascme05/PyDTS (accessed on 26 February 2024).\n\n## Conflicts of Interest\n\nThe authors declare no conflicts of interest.\n\n## References\n\n  1. Barra, S.; Carta, S.M.; Corriga, A.; Podda, A.S.; Recupero, D.R. Deep learning and time series-to-image encoding for financial forecasting. IEEE/CAA J. Autom. Sin. 2020, 7, 683\u2013692. [Google Scholar] [CrossRef]\n  2. Mudelsee, M. Climate Time Series Analysis; Atmospheric and Oceanographic Sciences Library; Springer: Cham, Switzerland, 2010; Volume 397. [Google Scholar]\n  3. Mporas, I.; Ganchev, T.; Kocsis, O.; Fakotakis, N. Context-adaptive pre-processing scheme for robust speech recognition in fast-varying noise environment. Signal Process. 2011, 91, 2101\u20132111. [Google Scholar] [CrossRef]\n  4. Rasul, K.; Seward, C.; Schuster, I.; Vollgraf, R. Autoregressive denoising diffusion models for multivariate probabilistic time series forecasting. In Proceedings of the International Conference on Machine Learning, PMLR, Virtual, 18\u201324 July 2021; pp. 8857\u20138868. [Google Scholar]\n  5. Almalaq, A.; Edwards, G. A review of deep learning methods applied on load forecasting. In Proceedings of the 2017 16th IEEE International Conference on Machine Learning and Applications (ICMLA), Cancun, Mexico, 18\u201321 December 2017; pp. 511\u2013516. [Google Scholar]\n  6. Osorio, J.D.; Wang, Z.; Karniadakis, G.; Cai, S.; Chryssostomidis, C.; Panwar, M.; Hovsapian, R. Forecasting solar-thermal systems performance under transient operation using a data-driven machine learning approach based on the deep operator network architecture. Energy Convers. Manag. 2022, 252, 115063. [Google Scholar] [CrossRef]\n  7. Hsieh, R.J.; Chou, J.; Ho, C.H. Unsupervised online anomaly detection on multivariate sensing time series data for smart manufacturing. In Proceedings of the 2019 IEEE 12th Conference on Service-Oriented Computing and Applications (SOCA), Kaohsiung, Taiwan, 18\u201321 November 2019; pp. 90\u201397. [Google Scholar]\n  8. Vichard, L.; Harel, F.; Ravey, A.; Venet, P.; Hissel, D. Degradation prediction of PEM fuel cell based on artificial intelligence. Int. J. Hydrogen Energy 2020, 45, 14953\u201314963. [Google Scholar] [CrossRef]\n  9. Fengou, L.C.; Mporas, I.; Spyrelli, E.; Lianou, A.; Nychas, G.J. Estimation of the microbiological quality of meat using rapid and non-invasive spectroscopic sensors. IEEE Access 2020, 8, 106614\u2013106628. [Google Scholar] [CrossRef]\n  10. Contreras, J. ARIMA models to predict next-day electricity process. IEEE Trans. Power Syst. 2004, 19, 366\u2013374. [Google Scholar]\n  11. Chen, K.; Yu, J. Short-term wind speed prediction using an unscented Kalman filter based state-space support vector regression approach. Appl. Energy 2014, 113, 690\u2013705. [Google Scholar] [CrossRef]\n  12. Lim, B.; Zohren, S. Time-series forecasting with deep learning: A survey. Philos. Trans. R. Soc. A 2021, 379, 20200209. [Google Scholar] [CrossRef]\n  13. Siami-Namini, S.; Tavakoli, N.; Namin, A.S. The performance of LSTM and BiLSTM in forecasting time series. In Proceedings of the 2019 IEEE International Conference on Big Data (Big Data), Los Angeles, CA, USA, 9\u201312 December 2019; pp. 3285\u20133292. [Google Scholar]\n  14. Koprinska, I.; Wu, D.; Wang, Z. Convolutional neural networks for energy time series forecasting. In Proceedings of the 2018 International Joint Conference on Neural Networks (IJCNN), Rio de Janeiro, Brazil, 8\u201313 July 2018; pp. 1\u20138. [Google Scholar]\n  15. Chen, Y.; Ren, K.; Wang, Y.; Fang, Y.; Sun, W.; Li, D. ContiFormer: Continuous-time transformer for irregular time series modeling. Adv. Neural Inf. Process. Syst. 2024, 36. [Google Scholar]\n  16. Alwan, L.C.; Roberts, H.V. Time-series modeling for statistical process control. J. Bus. Econ. Stat. 1988, 6, 87\u201395. [Google Scholar] [CrossRef]\n  17. Lojowska, A.; Kurowicka, D.; Papaefthymiou, G.; van der Sluis, L. Advantages of ARMA-GARCH wind speed time series modeling. In Proceedings of the 2010 IEEE 11th International Conference on Probabilistic Methods Applied to Power Systems, Singapore, 14\u201317 June 2010; pp. 83\u201388. [Google Scholar]\n  18. Chujai, P.; Kerdprasop, N.; Kerdprasop, K. Time series analysis of household electric consumption with ARIMA and ARMA models. In Proceedings of the International Multiconference of Engineers and Computer Scientists, IAENG, Hong Kong, China, 13\u201315 March 2013; Volume 1, pp. 295\u2013300. [Google Scholar]\n  19. Mahla, S.K.; Parmar, K.S.; Singh, J.; Dhir, A.; Sandhu, S.S.; Chauhan, B.S. Trend and time series analysis by ARIMA model to predict the emissions and performance characteristics of biogas fueled compression ignition engine. Energy Sources Part A Recover. Util. Environ. Eff. 2023, 45, 4293\u20134304. [Google Scholar] [CrossRef]\n  20. Durbin, J.; Koopman, S.J. Time Series Analysis by State Space Methods; OUP: Oxford, UK, 2012; Volume 38. [Google Scholar]\n  21. Yang, S.; Wan, M.P.; Ng, B.F.; Zhang, T.; Babu, S.; Zhang, Z.; Chen, W.; Dubey, S. A state-space thermal model incorporating humidity and thermal comfort for model predictive control in buildings. Energy Build. 2018, 170, 25\u201339. [Google Scholar] [CrossRef]\n  22. Hu, X.; Lin, S.; Stanton, S.; Lian, W. A State Space Thermal Model for HEV/EV Battery Modeling; Technical Report, SAE Technical Paper; SAE: Warrendale, PA, USA, 2011. [Google Scholar]\n  23. Dong, Z.; Yang, D.; Reindl, T.; Walsh, W.M. Short-term solar irradiance forecasting using exponential smoothing state space model. Energy 2013, 55, 1104\u20131113. [Google Scholar] [CrossRef]\n  24. L\u00e4ngkvist, M.; Karlsson, L.; Loutfi, A. A review of unsupervised feature learning and deep learning for time-series modeling. Pattern Recognit. Lett. 2014, 42, 11\u201324. [Google Scholar] [CrossRef]\n  25. Gamboa, J.C.B. Deep learning for time-series analysis. arXiv 2017, arXiv:1701.01887. [Google Scholar]\n  26. Han, Z.; Zhao, J.; Leung, H.; Ma, K.F.; Wang, W. A review of deep learning models for time series prediction. IEEE Sens. J. 2019, 21, 7833\u20137848. [Google Scholar] [CrossRef]\n  27. Wang, H.; Lei, Z.; Zhang, X.; Zhou, B.; Peng, J. A review of deep learning for renewable energy forecasting. Energy Convers. Manag. 2019, 198, 111799. [Google Scholar] [CrossRef]\n  28. Hafeez, G.; Alimgeer, K.S.; Khan, I. Electric load forecasting based on deep learning and optimized by heuristic algorithm in smart grid. Appl. Energy 2020, 269, 114915. [Google Scholar] [CrossRef]\n  29. Hewage, P.; Trovati, M.; Pereira, E.; Behera, A. Deep learning-based effective fine-grained weather forecasting model. Pattern Anal. Appl. 2021, 24, 343\u2013366. [Google Scholar] [CrossRef]\n  30. Antczak, K. Deep recurrent neural networks for ECG signal denoising. arXiv 2018, arXiv:1807.11551. [Google Scholar]\n  31. Peng, Z.; Peng, S.; Fu, L.; Lu, B.; Tang, J.; Wang, K.; Li, W. A novel deep learning ensemble model with data denoising for short-term wind speed forecasting. Energy Convers. Manag. 2020, 207, 112524. [Google Scholar] [CrossRef]\n  32. Peng, H.; Yan, J.; Yu, Y.; Luo, Y. Time series estimation based on deep learning for structural dynamic nonlinear prediction. Structures 2021, 29, 1016\u20131031. [Google Scholar] [CrossRef]\n  33. Gao, X.; Zhu, W.; Yang, Q.; Zeng, D.; Deng, L.; Chen, Q.; Cheng, M. Time delay estimation from the time series for optical chaos systems using deep learning. Opt. Express 2021, 29, 7904\u20137915. [Google Scholar] [CrossRef]\n  34. Padr\u00f3s, M.S.; Schirmer, P.A.; Mporas, I. Estimation of Cooling Circuits\u2019 Temperature in Battery Electric Vehicles Using Karhunen Loeve Expansion and LSTM. In Proceedings of the 2022 30th European Signal Processing Conference (EUSIPCO), Belgrade, Serbia, 29 August\u20132 September 2022; pp. 1546\u20131550. [Google Scholar]\n  35. Munir, M.; Siddiqui, S.A.; Dengel, A.; Ahmed, S. DeepAnT: A deep learning approach for unsupervised anomaly detection in time series. IEEE Access 2018, 7, 1991\u20132005. [Google Scholar] [CrossRef]\n  36. Zhang, W.; Li, X.; Li, X. Deep learning-based prognostic approach for lithium-ion batteries with adaptive time-series prediction and on-line validation. Measurement 2020, 164, 108052. [Google Scholar] [CrossRef]\n  37. Gedon, D.; Wahlstr\u00f6m, N.; Sch\u00f6n, T.B.; Ljung, L. Deep state space models for nonlinear system identification. IFAC-PapersOnLine 2021, 54, 481\u2013486. [Google Scholar] [CrossRef]\n  38. Bicer, E.A.; Schirmer, P.A.; Schreivogel, P.; Schrag, G. Electric Vehicle Thermal Management System Modeling with Informed Neural Networks. In Proceedings of the 2023 25th European Conference on Power Electronics and Applications (EPE\u201923 ECCE Europe), Aalborg, Denmark, 4\u20138 September 2023; pp. 1\u20138. [Google Scholar]\n  39. Schwermer, R.; Bicer, E.A.; Schirmer, P.; Mayer, R.; Jacobsen, H.A. Federated Computing in Electric Vehicles to Predict Coolant Temperature. In Proceedings of the 24th International Middleware Conference: Industrial Track, Bologna, Italy, 11\u201315 December 2023; pp. 8\u201314. [Google Scholar]\n  40. Garza, F.; Canseco, M.M.; Chall\u00fa, C.; Olivares, K.G. StatsForecast: Lightning Fast Forecasting with Statistical and Econometric Models; PyCon: Salt Lake City, UT, USA, 2022. [Google Scholar]\n  41. Herzen, J.; L\u00c3\u00a4ssig, F.; Piazzetta, S.G.; Neuer, T.; Tafti, L.; Raille, G.; Pottelbergh, T.V.; Pasieka, M.; Skrodzki, A.; Huguenin, N.; et al. Darts: User-Friendly Modern Machine Learning for Time Series. J. Mach. Learn. Res. 2022, 23, 1\u20136. [Google Scholar]\n  42. L\u00f6ning, M.; Bagnall, A.; Ganesh, S.; Kazakov, V.; Lines, J.; Kir\u00e1ly, F.J. sktime: A unified interface for machine learning with time series. arXiv 2019, arXiv:1909.07872. [Google Scholar]\n  43. Schirmer, P.A.; Mporas, I. Non-Intrusive Load Monitoring: A Review. IEEE Trans. Smart Grid 2023, 14, 769\u2013784. [Google Scholar] [CrossRef]\n  44. Schirmer, P.A.; Mporas, I. Statistical and Electrical Features Evaluation for Electrical Appliances Energy Disaggregation. Sustainability 2019, 11, 3222. [Google Scholar] [CrossRef]\n  45. Schirmer, P.A.; Mporas, I.; Paraskevas, M. Evaluation of Regression Algorithms and Features on the Energy Disaggregation Task. In Proceedings of the 2019 10th International Conference on Information, Intelligence, Systems and Applications (IISA), Patras, Greece, 15\u201317 July 2019; pp. 1\u20134. [Google Scholar] [CrossRef]\n  46. Schirmer, P.A.; Geiger, C.; Mporas, I. Residential Energy Consumption Prediction Using Inter-Household Energy Data and Socioeconomic Information. In Proceedings of the 2020 28th European Signal Processing Conference (EUSIPCO), Patras, Greece, 18\u201321 January 2020. [Google Scholar]\n  47. Kirchg\u00e4ssner, W.; Wallscheid, O.; B\u00f6cker, J. Estimating electric motor temperatures with deep residual machine learning. IEEE Trans. Power Electron. 2020, 36, 7480\u20137488. [Google Scholar] [CrossRef]\n  48. Shumway, R.H.; Stoffer, D.S. Time Series Analysis and Its Applications; Springer: Cham, Switzerland, 2000. [Google Scholar]\n  49. Chen, C.T. Linear System Theory and Design, 3rd ed.; Oxford University Press, Inc.: Cary, NC, USA, 1998. [Google Scholar]\n  50. Loyola-Gonz\u00e1lez, O. Black-Box vs. White-Box: Understanding Their Advantages and Weaknesses From a Practical Point of View. IEEE Access 2019, 7, 154096\u2013154113. [Google Scholar] [CrossRef]\n  51. Zheng, H.S.; Liu, Y.Y.; Hsu, C.F.; Yeh, T.T. StreamNet: Memory-Efficient Streaming Tiny Deep Learning Inference on the Microcontroller. In Proceedings of the Thirty-Seventh Conference on Neural Information Processing Systems; 2023. Available online: https://nips.cc/media/neurips-2023/Slides/72782_KsNdwFo.pdf (accessed on 26 February 2024).\n  52. Schirmer, P.A.; Mporas, I. Device and Time Invariant Features for Transferable Non-Intrusive Load Monitoring. IEEE Open Access J. Power Energy 2022, 9, 121\u2013130. [Google Scholar] [CrossRef]\n  53. Chen, X.W.; Lin, X. Big Data Deep Learning: Challenges and Perspectives. IEEE Access 2014, 2, 514\u2013525. [Google Scholar] [CrossRef]\n  54. Makonin, S.; Ellert, B.; Baji\u0107, I.V.; Popowich, F. Electricity, water, and natural gas consumption of a residential house in Canada from 2012 to 2014. Sci. Data 2016, 3, 1\u201312. [Google Scholar] [CrossRef] [PubMed]\n  55. Soriano, F. Electric Power Consumption Dataset. 2023. Available online: https://www.kaggle.com/datasets/fedesoriano/electric-power-consumption (accessed on 26 February 2024).\n  56. Wichard, J.D. Classification of Ford Motor Data. Comput. Sci. 2008. Available online: http://www.j-wichard.de/publications/FordPaper.pdf (accessed on 26 February 2024).\n  57. Bills, A.; Sripad, S.; Fredericks, L.; Guttenberg, M.; Charles, D.; Frank, E.; Viswanathan, V. A battery dataset for electric vertical takeoff and landing aircraft. Sci. Data 2023, 10, 344. [Google Scholar] [CrossRef] [PubMed]\n  58. Schirmer, P.A.; Mporas, I. Low-Frequency Energy Disaggregation based on Active and Reactive Power Signatures. In Proceedings of the 2021 29th European Signal Processing Conference (EUSIPCO), Dublin, Ireland, 23\u201327 August 2021; pp. 1426\u20131430. [Google Scholar] [CrossRef]\n  59. Kirchg\u00e4ssner, W.; Wallscheid, O.; B\u00f6cker, J. Thermal neural networks: Lumped-parameter thermal modeling with state-space machine learning. Eng. Appl. Artif. Intell. 2023, 117, 105537. [Google Scholar] [CrossRef]\n  60. Makonin, S.; Popowich, F.; Bajic, I.V.; Gill, B.; Bartram, L. Exploiting HMM Sparsity to Perform Online Real-Time Nonintrusive Load Monitoring. IEEE Trans. Smart Grid 2016, 7, 2575\u20132585. [Google Scholar] [CrossRef]\n  61. Harell, A.; Makonin, S.; Bajic, I.V. Wavenilm: A Causal Neural Network for Power Disaggregation from the Complex Power Signal. In Proceedings of the ICASSP 2019\u20142019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Brighton, UK, 12\u201317 May 2019; pp. 8335\u20138339. [Google Scholar] [CrossRef]\n  62. Schirmer, P.A.; Mporas, I. Energy Disaggregation Using Fractional Calculus. In Proceedings of the ICASSP 2020\u20142020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Barcelona, Spain, 4\u20138 May 2020; pp. 3257\u20133261. [Google Scholar] [CrossRef]\n  63. Kirchg\u00e4ssner, W.; Wallscheid, O.; B\u00f6cker, J. Data-driven permanent magnet temperature estimation in synchronous motors with supervised machine learning: A benchmark. IEEE Trans. Energy Convers. 2021, 36, 2059\u20132067. [Google Scholar] [CrossRef]\n  64. Schirmer, P.A.; Mporas, I. Energy Disaggregation from Low Sampling Frequency Measurements Using Multi-Layer Zero Crossing Rate. In Proceedings of the ICASSP 2020\u20142020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Barcelona, Spain, 4\u20138 May 2020; pp. 3777\u20133781. [Google Scholar] [CrossRef]\n  65. Schirmer, P.A.; Mporas, I. A Wavelet Scattering Approach for Load Identification with Limited Amount of Training Data. In Proceedings of the ICASSP 2023\u20142023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Rhodes Island, Greece, 4\u201310 June 2023; pp. 1\u20135. [Google Scholar] [CrossRef]\n  66. Kolter, J.Z.; Johnson, M.J. REDD: A public data set for energy disaggregation research. In Workshop on Data Mining Applications in Sustainability (SIGKDD); Citeseer: San Diego, CA, USA, 2011; Volume 25, pp. 59\u201362. [Google Scholar]\n  67. Robnik-\u0160ikonja, M.; Kononenko, I. Theoretical and empirical analysis of ReliefF and RReliefF. Mach. Learn. 2003, 53, 23\u201369. [Google Scholar] [CrossRef]\n\nFigure 1. Generalized time series architecture.\n\nFigure 2. Relation between input and output dimensionality for frame-based\ntime series modelling: (a) sequence-to-point, (b) sequence-to-subsequence, and\n(c) sequence-to-sequence.\n\nFigure 3. Overview of implemented modules and functionalities in the PyDTS\ntoolkit. Inputs and preprocessing are indicated in red, features and data\nsequencing in yellow, modelling in green, postprocessing in blue, and visual\nelements and outputs in purple.\n\nFigure 4. Internal data pipeline of PyDTS including training and testing\nmodules and external data, model, and setup databases.\n\nFigure 5. Grid search for the optimal number of input samples depending on the\ntime series problem.\n\nFigure 6. DL layer architectures for DNNs, LSTM, and CNN models. For CNNs, the\nnotation of the convolutional layer is Conv1D(x,y) with x being the number of\nfilters and y being the kernel size. For pooling layers MaxPool(x,y), x is the\nsize and y the stride, while for LSTM and DNN layers, x denotes the number of\nneurons.\n\nFigure 7. Predicted appliance current draw for 12 h for three different (FRE,\nHPE, and CDE) appliances from the AMPds2 dataset on 9 January 2013 at 12:00\np.m.\n\nFigure 8. Forecasted power consumption and error for phase L1 for 1 week using\nRF as regression model.\n\nFigure 9. Predicted temperature for stator winding and rotor magnet for IDs\n60, 62, and 72.\n\nFigure 10. Confusion matrices for (a) raw, (b) statistical, and (c) frequency\ndomain features for the CNN model.\n\nFigure 11. Ground-truth and predicted remaining cell charge and prediction\nerror using the best-performing DNN model (for visibility, the predicted\noutput has been filtered with a median filter of a length of 100 samples).\n\nFigure 12. Feature ranking for the nonlinear modelling task for 13 features:\ncoolant/ambient temperature (, ), stator voltages (, , ), stator currents (, ,\n), torque (), rotational speed (), apparent power (), and products or\ncurrent/power and rotational speed (, ).\n\nTable 1. Comparison of relevant properties between different modelling\napproaches: (+): comparatively better, (o): neutral, and (-): comparatively\nworse.\n\nProperties| Ref. and Eq.| Linear Algebra| Statistical Modelling| Machine\nLearning  \n---|---|---|---|---  \nRuntime| [51]| o| +| -  \nMemory| [51]| o| +| -  \nInterpretability| (12)\u2013(14)| +| o| -  \nDimensionality| (10), (11), (14)| o| -| +  \nTransferability| [52]| o| -| +  \nNonlinear| (10), (11), (14)| o| -| +  \nHyperparameters| (10), (11), (14)| o| +| -  \nTraining data| [53]| +| o| -  \n  \nTable 2. Short description of the datasets. The feature column includes the\nfollowing abbreviations: active power (P), reactive power (Q), apparent power\n(S), current (I), voltage (V), temperature (T), relative humidity (), solar\nirradiance (), wind speed (), rotational speed (n), torque (M), and\nacceleration (A). Similarly, the outputs include the appliance current (), the\nper-phase power (), the stator winding and rotor magnet temperatures (), the\nmotor state, and the remaining battery charge ().\n\nName| Ref.| Scenario| Length| Sampling| Features| Output| Max| Mean| Std  \n---|---|---|---|---|---|---|---|---|---  \nAMPds2| [54]| Denoise| 2 y| 60 s| P, Q, S, I| 105| 0.8| 10.9  \nEnergy| [55]| Forecast| 1 y| 10 min| P, T, ,| 52.2| 23.7| 12.2  \nMotor Temp.| [47]| Nonlinear| 185 h| 0.5 s| V, I, T, M, n| 141.4| 57.5| 22.7  \nFord Motor| [56]| Anomaly| 1.4 h| 2 ms| s| 1.0| 0.49| 0.50  \nBattery Health| [57]| Degradation| 57 days| 2.5 s| V, I, T| 1.92| 1.54| 0.17  \n  \nTable 3. Optimized model parameters for ML approaches including KNN, RF, and\nSVM.\n\nModel| Parameter| Optimal| Range| Step  \n---|---|---|---|---  \nKNN| Neighbors| 140| 10\u2013200| 5  \nRF| Max. Depth| 10| 5-25| 5  \nSplit| 4| 2\u201310| 2  \n#-Trees| 128| 2\u2013256  \nSVM| Kernel| rbf| linear, rbf, poly| -  \nC| 100| 1\u2013200| 20  \nGamma| 0.1| 0.001\u20131  \n  \nTable 4. Hyper- and solver parameters for deep learning models including DNN,\nCNN, and LSTM.\n\nHyperparameters| Solver Parameters  \n---|---  \nBatch| 1000| optimizer| adam  \nEpochs| 50\u2013200| loss| mae  \nPatience| 15| Learning rate  \nValidation steps| 50| Beta1| 0.9  \nShuffle| False| Beta2| 0.999  \n  \nTable 5. Average results (A) for the energy disaggregation task for fivefold\ncross-validation using different models and accuracy metrics. The best\nperformances are indicated with bold notation.\n\nModel| NMSE| RMSE| MSE| MAE| MAX  \n---|---|---|---|---|---  \nCNN| 92.48| 0.64| 0.41| 0.08| 29.01  \nLSTM| 94.51| 0.60| 0.36| 0.08| 30.54  \nDNN| 94.39| 0.66| 0.44| 0.08| 31.85  \nRF| 81.39| 0.63| 0.40| 0.10| 28.60  \nKNN| 74.11| 1.15| 1.32| 0.21| 31.09  \n  \nTable 6. Per-device results (A) for the energy disaggregation task for\nfivefold cross-validation using LSTM as regression model and different\naccuracy metrics.\n\nDevice| NMSE| RMSE| MSE| MAE| MAX  \n---|---|---|---|---|---  \nDWE| 49.79| 0.87| 0.76| 0.12| 6.76  \nFRE| 95.15| 0.24| 0.06| 0.13| 3.41  \nHPE| 97.55| 0.63| 0.40| 0.07| 7.21  \nWOE| 91.50| 0.63| 0.40| 0.03| 30.61  \nCDE| 97.66| 0.62| 0.38| 0.02| 40.73  \nAvg| 94.51| 0.60| 0.36| 0.08| 30.54  \n  \nTable 7. Comparison with the literature for the energy disaggregation task.\n\nRef.| Year| Model| NMSE| RMSE| MAE  \n---|---|---|---|---|---  \n[60]| 2016| HMM| 94.1%| -| -  \n[61]| 2019| CNN| 93.9%| -| -  \n[62]| 2020| CNN| 94.7%| -| -  \n[58]| 2021| CNN| 95.8%| -| -  \n[43]| 2022| CNN| 94.7%| 0.48| 0.06  \nThis Work| 2023| LSTM| 94.5%| 0.60| 0.08  \n  \nTable 8. Forecasting errors (kW) using Seq2Point for a 24 h ahead prediction\nwindow with different models and accuracy metrics using fivefold cross-\nvalidation. The best performances are indicated with bold notation.\n\nModel| NMSE| RMSE| MSE| MAE| MAX  \n---|---|---|---|---|---  \nCNN| 95.72| 3.62| 13.10| 2.77| 18.49  \nLSTM| 95.55| 3.85| 14.82| 2.88| 18.19  \nDNN| 95.61| 3.74| 13.99| 2.85| 17.90  \nRF| 97.50| 2.42| 5.87| 1.60| 17.88  \nKNN| 93.98| 4.96| 24.60| 3.88| 18.63  \n  \nTable 9. Forecasting errors (kW) using Seq2Seq for a 24 h ahead prediction\nwindow with different models and accuracy metrics using fivefold cross-\nvalidation. The best performances are indicated with bold notation.\n\nModel| NMSE| RMSE| MSE| MAE| MAX  \n---|---|---|---|---|---  \nCNN| 95.88| 3.54| 12.53| 2.67| 18.61  \nLSTM| 95.99| 3.01| 9.06| 2.36| 12.12  \nDNN| 95.66| 3.71| 13.76| 2.81| 17.26  \n  \nTable 10. Temperature prediction results for 5-fold cross validation using\ndifferent regression models and performance metrics. Due to memory\nrestrictions the LSTM input was reduced to 500 samples. The best performances\nare indicated with bold notation.\n\nModel| NMSE| RMSE| MSE| MAE| MAX  \n---|---|---|---|---|---  \nCNN| 97.67| 95.19| 4.54| 7.59| 20.61| 57.61| 3.06| 5.53| 76.43| 54.18  \nLSTM| 96.71| 93.23| 6.39| 10.6| 40.83| 112.4| 4.28| 7.85| 77.15| 60.05  \nDNN| 97.37| 95.21| 5.32| 7.81| 28.30| 61.00| 3.43| 5.59| 76.52| 59.20  \nRF| 96.04| 94.66| 7.63| 8.30| 58.22| 68.89| 5.26| 4.43| 73.73| 47.87  \nKNN| 86.40| 89.85| 22.79| 14.98| 519.4| 224.4| 17.39| 11.45| 82.24| 57.96  \n  \nTable 11. Results for MSE (K2) and MAX (K) errors for different testing IDs,\ntheir respective time (hr), and temperature hot spots using a CNN regression\nmodel per hot spot.\n\nID| Time| Stator Winding| Stator Tooth| Stator Yoke| Magnet  \n---|---|---|---|---|---  \nMSE| MAX| MSE| MAX| MSE| MAX| MSE| MAX  \n60| 1.7| 2.41| 5.03| 1.68| 4.28| 1.16| 3.14| 22.62| 9.90  \n62| 3.3| 2.75| 6.23| 1.25| 3.78| 1.22| 3.96| 17.49| 9.74  \n74| 3.0| 3.33| 6.18| 2.42| 5.43| 1.80| 5.00| 14.47| 10.81  \nAvg| 8.0| 2.90| 6.23| 1.78| 5.43| 1.42| 5.00| 17.45| 10.81  \n  \nTable 12. Comparison for temperature prediction using different models and\nnumber of input features.\n\nRef.| Year| Model| MSE| MAX| Features  \n---|---|---|---|---|---  \n[63]| 2021| MLP| 5.58| 14.29| 81  \n[63]| 2021| OLS| 4.47| 9.85| 81  \n[47]| 2020| CNN| 4.43| 15.54| 81  \n[59]| 2023| TNN| 2.87| 6.02| 5  \nThis Work| 2023| CNN| 5.89| 10.81| 13  \n  \nTable 13. Classification results in terms of ACC and F1 for anomaly detection\nusing different classification models. The best performances are indicated\nwith bold notation.\n\nModel| Raw| Statistical| Frequency  \n---|---|---|---  \nACC| F1| ACC| F1| ACC| F1  \nCNN| 92.35| 92.34| 56.52| 55.87| 94.85| 94.85  \nLSTM| 51.06| 50.52| 55.30| 54.90| 51.59| 35.12  \nDNN| 80.15| 80.15| 56.52| 56.13| 94.77| 94.77  \nRF| 72.80| 72.77| 59.09| 59.10| 92.42| 92.42  \nKNN| 72.80| 72.76| 58.11| 58.12| 88.94| 88.90  \nSVM| 51.59| 35.12| 58.41| 58.01| 94.47| 94.47  \n  \nTable 14. Degradation errors for different regression models and performance\nmetrics using Seq2Point learning. The best performances are indicated with\nbold notation.\n\nModel| NMSE| RMSE| MSE| MAE| MAX  \n---|---|---|---|---|---  \nCNN| 98.00| 0.08| 0.01| 0.06| 0.36  \nLSTM| 97.85| 0.08| 0.01| 0.07| 0.39  \nDNN| 98.64| 0.06| 0.01| 0.04| 0.49  \nRF| 95.15| 0.16| 0.03| 0.15| 0.38  \nKNN| 97.43| 0.10| 0.01| 0.08| 0.35  \n  \nTable 15. Degradation errors for different regression models and performance\nmetrics using Seq2Seq learning. The best performances are indicated with bold\nnotation.\n\nModel| NMSE| RMSE| MSE| MAE| MAX  \n---|---|---|---|---|---  \nCNN| 98.26| 0.07| 0.01| 0.05| 0.34  \nLSTM| 97.74| 0.09| 0.01| 0.07| 0.41  \nDNN| 97.85| 0.09| 0.01| 0.07| 0.41  \n  \nTable 16. Intratransferability scenario based on load forecasting between\nphases 1 (L1) and 2 (L2). The best performances are indicated with bold\nnotation.\n\nModel| L2 (Train L2)| L2 (Train L1)| Loss (%)  \n---|---|---|---  \nNMSE| RMSE| MAE| NMSE| RMSE| MAE| NMSE| RMSE| MAE  \nCNN| 92.02| 4.22| 3.36| 87.61| 6.34| 5.19| 4.79| 50.24| 54.46  \nLSTM| 93.21| 3.58| 2.81| 92.88| 3.70| 2.94| 0.35| 3.35| 4.63  \nDNN| 92.81| 3.86| 3.03| 87.44| 6.40| 5.25| 5.79| 65.80| 73.27  \nRF| 96.02| 2.35| 1.71| 93.28| 3.44| 2.78| 2.85| 46.38| 62.57  \nKNN| 91.66| 4.37| 3.49| 89.07| 5.58| 4.56| 2.83| 27.69| 30.66  \n  \nTable 17. Intertransferability scenario based on energy disaggregation between\ndifferent consumer households (REDD-1,2). The best performances are indicated\nwith bold notation.\n\nModel| REDD2 (Train REDD2)| REDD2 (Train REDD1)| Loss (%)  \n---|---|---|---  \nNMSE| RMSE| MAE| NMSE| RMSE| MAE| NMSE| RMSE| MAE  \nCNN| 92.60| 39.44| 5.45| 76.12| 70.83| 16.57| 16.48| 79.59| 204.0  \nLSTM| 86.65| 84.36| 9.83| 71.26| 94.88| 19.95| 15.39| 12.47| 102.9  \nDNN| 85.02| 76.83| 11.03| 55.19| 106.4| 31.10| 29.83| 38.49| 181.9  \nRF| 89.19| 41.38| 7.96| 75.88| 67.77| 16.74| 13.31| 63.77| 110.3  \nKNN| 92.48| 31.32| 5.54| 70.09| 79.57| 20.76| 22.39| 154.1| 274.7  \n  \nTable 18. Model size of the trained model including all parameters for\ndifferent scenarios.\n\nModel| Denoise| Forecast| Nonlinear| Anomaly| Degradation  \n---|---|---|---|---|---  \n30 \u00d7 4| 144 \u00d7 8| 1000 \u00d7 13| 500 \u00d7 1| 140 \u00d7 3  \nCNN| 2.91 MB| 6.37 MB| 32.0 MB| 9.49 MB| 6.20 MB  \nLSTM| 4.29 MB| 4.30 MB| 4.34 MB| 4.26 MB| 4.27 MB  \nDNN| 1.92 MB| 5.00 MB| 40.6 MB| 2.30 MB| 2.81 MB  \nRF| 37.7 MB| 12.1 MB| 58.4 MB| 2.80 MB| 9.16 MB  \nKNN| 3.94 GB| 0.33 GB| 26.9 GB| 7.05 MB| 162.4 MB  \n  \nTable 19. Training (T) and inference time (I) per sample (s) for different\nmodels and scenarios.\n\nModel| Denoise| Forecast| Non-Linear| Anomaly| Degradation  \n---|---|---|---|---|---  \nT| I| T| I| T| I| T| I| T| I  \nCNN| 530| 59| 2570| 120| 2610| 190| 8650| 478| 1540| 109  \nLSTM| 540| 87| 6790| 255| 10,300| 556| 6540| 893| 2410| 232  \nDNN| 310| 22| 1500| 33| 3070| 95| 3760| 76| 1510| 31  \nRF| 9 \u00d7| 15| 5710| 5.5| 20 \u00d7| 24| 90| 20| 2170| 3.1  \nKNN| 0| 6 \u00d7| 0| 967| 0| 42 \u00d7| 0| 97| 0| 854  \n  \nTable 20. Temperature prediction results for stator winding and magnet\ntemperature in terms of MSE (K2) for different testing IDs and models.\nBaseline scenarios are denoted with \u2018Base\u2019, while reduced-order configurations\nare denoted with \u2019MOR\u2019.\n\nID| Time (h)| Stator Winding| Rotor Magnet  \n---|---|---|---  \nBase| MOR| Base| MOR  \n60| 1.7| 2.41| 1.34| 22.62| 16.68  \n62| 3.3| 2.75| 1.79| 17.49| 31.11  \n74| 3.0| 3.33| 2.37| 14.47| 15.39  \nAvg| 8.0| 2.90| 1.91| 17.45| 22.15  \n  \nDisclaimer/Publisher\u2019s Note: The statements, opinions and data contained in\nall publications are solely those of the individual author(s) and\ncontributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s)\ndisclaim responsibility for any injury to people or property resulting from\nany ideas, methods, instructions or products referred to in the content.  \n---  \n\u00a9 2024 by the authors. Licensee MDPI, Basel, Switzerland. This article is an\nopen access article distributed under the terms and conditions of the Creative\nCommons Attribution (CC BY) license\n(https://creativecommons.org/licenses/by/4.0/).\n\n## Share and Cite\n\nMDPI and ACS Style\n\nSchirmer, P.A.; Mporas, I. PyDTS: A Python Toolkit for Deep Learning Time\nSeries Modelling. Entropy 2024, 26, 311. https://doi.org/10.3390/e26040311\n\nAMA Style\n\nSchirmer PA, Mporas I. PyDTS: A Python Toolkit for Deep Learning Time Series\nModelling. Entropy. 2024; 26(4):311. https://doi.org/10.3390/e26040311\n\nChicago/Turabian Style\n\nSchirmer, Pascal A., and Iosif Mporas. 2024. \"PyDTS: A Python Toolkit for Deep\nLearning Time Series Modelling\" Entropy 26, no. 4: 311.\nhttps://doi.org/10.3390/e26040311\n\nNote that from the first issue of 2016, this journal uses article numbers\ninstead of page numbers. See further details here.\n\n## Article Metrics\n\nYes\n\n### Citations\n\nNo citations were found for this article, but you may check on Google Scholar\n\nNo\n\n### Article Access Statistics\n\nFor more information on the journal statistics, click here.\n\nMultiple requests from the same IP address are counted as one view.\n\nZoom | Orient | As Lines | As Sticks | As Cartoon | As Surface | Previous Scene | Next Scene\n\n## Cite\n\nExport citation file: BibTeX | EndNote | RIS\n\nMDPI and ACS Style\n\nSchirmer, P.A.; Mporas, I. PyDTS: A Python Toolkit for Deep Learning Time\nSeries Modelling. Entropy 2024, 26, 311. https://doi.org/10.3390/e26040311\n\nAMA Style\n\nSchirmer PA, Mporas I. PyDTS: A Python Toolkit for Deep Learning Time Series\nModelling. Entropy. 2024; 26(4):311. https://doi.org/10.3390/e26040311\n\nChicago/Turabian Style\n\nSchirmer, Pascal A., and Iosif Mporas. 2024. \"PyDTS: A Python Toolkit for Deep\nLearning Time Series Modelling\" Entropy 26, no. 4: 311.\nhttps://doi.org/10.3390/e26040311\n\nNote that from the first issue of 2016, this journal uses article numbers\ninstead of page numbers. See further details here.\n\nclear\n\nEntropy, EISSN 1099-4300, Published by MDPI\n\nRSS Content Alert\n\n### Further Information\n\nArticle Processing Charges Pay an Invoice Open Access Policy Contact MDPI Jobs\nat MDPI\n\n### Guidelines\n\nFor Authors For Reviewers For Editors For Librarians For Publishers For\nSocieties For Conference Organizers\n\n### MDPI Initiatives\n\nSciforum MDPI Books Preprints.org Scilit SciProfiles Encyclopedia JAMS\nProceedings Series\n\n### Follow MDPI\n\nLinkedIn Facebook Twitter\n\n\u00a9 1996-2024 MDPI (Basel, Switzerland) unless otherwise stated\n\nDisclaimer\n\nDisclaimer/Publisher\u2019s Note: The statements, opinions and data contained in\nall publications are solely those of the individual author(s) and\ncontributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s)\ndisclaim responsibility for any injury to people or property resulting from\nany ideas, methods, instructions or products referred to in the content.\n\nTerms and Conditions Privacy Policy\n\nWe use cookies on our website to ensure you get the best experience. Read more\nabout our cookies here.\n\nAccept\n\n## Share Link\n\nCopy\n\nclear\n\n## Share\n\nclear\n\nBack to TopTop\n\n", "frontpage": false}
