{"aid": "40066464", "title": "How we optimized cluster-wide scheduling for sticky workloads", "url": "https://www.channable.com/tech/how-we-optimized-cluster-wide-scheduling-for-sticky-workloads", "domain": "channable.com", "votes": 2, "user": "rkrzr", "posted_at": "2024-04-17 15:56:04", "comments": 0, "source_title": "How we optimized cluster-wide scheduling for sticky workloads", "source_text": "How we optimized cluster-wide scheduling for sticky workloads\n\n  * Consent\n  * Details\n  * [#IABV2SETTINGS#]\n  * About\n\n## This website uses cookies\n\nWe use cookies to personalise content and ads, to provide social media\nfeatures and to analyse our traffic. We also share information about your use\nof our site with our social media, advertising and analytics partners who may\ncombine it with other information that you\u2019ve provided to them or that they\u2019ve\ncollected from your use of their services.\n\nShow details\n\n  * Necessary cookies help make a website usable by enabling basic functions like page navigation and access to secure areas of the website. The website cannot function properly without these cookies.\n\n    * Cookiebot\n\n2\n\nLearn more about this provider\n\n1.gifUsed to count the number of sessions to the website, necessary for\noptimizing CMP product delivery.\n\nExpiry: SessionType: Pixel\n\nCookieConsentStores the user's cookie consent state for the current domain\n\nExpiry: 1 yearType: HTTP\n\n    * Google\n\n5\n\nLearn more about this provider\n\nrc::aThis cookie is used to distinguish between humans and bots. This is\nbeneficial for the website, in order to make valid reports on the use of their\nwebsite.\n\nExpiry: PersistentType: HTML\n\nrc::bThis cookie is used to distinguish between humans and bots.\n\nExpiry: SessionType: HTML\n\nrc::cThis cookie is used to distinguish between humans and bots.\n\nExpiry: SessionType: HTML\n\nrc::d-15#This cookie is used to distinguish between humans and bots.\n\nExpiry: PersistentType: HTML\n\nrc::fThis cookie is used to distinguish between humans and bots.\n\nExpiry: PersistentType: HTML\n\n    * LinkedIn\n\n2\n\nLearn more about this provider\n\nli_gcStores the user's cookie consent state for the current domain\n\nExpiry: 180 daysType: HTTP\n\nbscookiePending\n\nExpiry: 1 yearType: HTTP\n\n    * Microsoft\n\n4\n\nLearn more about this provider\n\n__VASTUtil__Necessary for the implementation of video-content on the website.\n\nExpiry: PersistentType: HTML\n\nai_sessionPreserves users states across page requests.\n\nExpiry: 1 dayType: HTTP\n\nakacd_#Used in connection with phased release which allows the website owner\nto assign a certain number of users to a specific version of the website.\n\nExpiry: 3 monthsType: HTTP\n\nobject(#-#-##:#:#.#)Holds the users timezone.\n\nExpiry: PersistentType: HTML\n\n    * New Relic\n\n1\n\nLearn more about this provider\n\nJSESSIONIDPending\n\nExpiry: SessionType: HTTP\n\n    * VWO\n\n1\n\nLearn more about this provider\n\n_vwo_uuid_v2This cookie is set to make split-tests on the website, which\noptimizes the website's relevance towards the visitor \u2013 the cookie can also be\nset to improve the visitor's experience on a website.\n\nExpiry: 1 yearType: HTTP\n\n    * google.com www.recaptcha.net\n\n2\n\n_GRECAPTCHA [x2]This cookie is used to distinguish between humans and bots.\nThis is beneficial for the website, in order to make valid reports on the use\nof their website.\n\nExpiry: 180 daysType: HTTP\n\n    * gtm.channable.com\n\n1\n\n_set_cookieThis cookie determines whether the browser accepts cookies.\n\nExpiry: SessionType: Pixel\n\n    * hubspot.com vimeo.com\n\n4\n\n__cf_bm [x2]This cookie is used to distinguish between humans and bots. This\nis beneficial for the website, in order to make valid reports on the use of\ntheir website.\n\nExpiry: 1 dayType: HTTP\n\n_cfuvid [x2]This cookie is a part of the services provided by Cloudflare -\nIncluding load-balancing, deliverance of website content and serving DNS\nconnection for website operators.\n\nExpiry: SessionType: HTTP\n\n  * Preference cookies enable a website to remember information that changes the way the website behaves or looks, like your preferred language or the region that you are in.\n\n    * www.flaticon.com\n\n3\n\ncityStores the selected store location.\n\nExpiry: SessionType: HTTP\n\ncountryThe cookie determines the preferred language and country-setting of the\nvisitor - This allows the website to show content most relevant to that region\nand language.\n\nExpiry: SessionType: HTTP\n\ncurrencySaves the visitor's currency preferences.\n\nExpiry: SessionType: HTTP\n\n  * Statistic cookies help website owners to understand how visitors interact with websites by collecting and reporting information anonymously.\n\n    * Google\n\n1\n\nLearn more about this provider\n\ntdRegisters statistical data on users' behaviour on the website. Used for\ninternal analytics by the website operator.\n\nExpiry: SessionType: Pixel\n\n    * Hotjar\n\n5\n\nLearn more about this provider\n\n_hjSession_#Collects statistics on the visitor's visits to the website, such\nas the number of visits, average time spent on the website and what pages have\nbeen read.\n\nExpiry: 1 dayType: HTTP\n\n_hjSessionUser_#Collects statistics on the visitor's visits to the website,\nsuch as the number of visits, average time spent on the website and what pages\nhave been read.\n\nExpiry: 1 yearType: HTTP\n\n_hjTLDTestRegisters statistical data on users' behaviour on the website. Used\nfor internal analytics by the website operator.\n\nExpiry: SessionType: HTTP\n\nhjActiveViewportIdsThis cookie contains an ID string on the current session.\nThis contains non-personal information on what subpages the visitor enters \u2013\nthis information is used to optimize the visitor's experience.\n\nExpiry: PersistentType: HTML\n\nhjViewportIdSaves the user's screen size in order to adjust the size of images\non the website.\n\nExpiry: SessionType: HTML\n\n    * Hubspot\n\n4\n\nLearn more about this provider\n\n__hsscIdentifies if the cookie data needs to be updated in the visitor's\nbrowser.\n\nExpiry: 1 dayType: HTTP\n\n__hssrcUsed to recognise the visitor's browser upon reentry on the website.\n\nExpiry: SessionType: HTTP\n\n__hstcSets a unique ID for the session. This allows the website to obtain data\non visitor behaviour for statistical purposes.\n\nExpiry: 180 daysType: HTTP\n\nhubspotutkSets a unique ID for the session. This allows the website to obtain\ndata on visitor behaviour for statistical purposes.\n\nExpiry: 180 daysType: HTTP\n\n    * Leadinfo\n\n3\n\nLearn more about this provider\n\n_li_ses.#Tracks the individual sessions on the website, allowing the website\nto compile statistical data from multiple visits. This data can also be used\nto create leads for marketing purposes.\n\nExpiry: PersistentType: HTML\n\nsnowplowOutQueue_#_post2Registers statistical data on users' behaviour on the\nwebsite. Used for internal analytics by the website operator.\n\nExpiry: PersistentType: HTML\n\nsnowplowOutQueue_#_post2.expiresRegisters statistical data on users' behaviour\non the website. Used for internal analytics by the website operator.\n\nExpiry: PersistentType: HTML\n\n    * LinkedIn\n\n1\n\nLearn more about this provider\n\nAnalyticsSyncHistoryUsed in connection with data-synchronization with third-\nparty analysis service.\n\nExpiry: 30 daysType: HTTP\n\n    * Spotify\n\n1\n\nLearn more about this provider\n\nsentryReplaySessionRegisters data on visitors' website-behaviour. This is used\nfor internal analysis and website optimization.\n\nExpiry: SessionType: HTML\n\n    * Twitter Inc.\n\n1\n\nLearn more about this provider\n\npersonalization_idThis cookie is set by Twitter - The cookie allows the\nvisitor to share content from the website onto their Twitter profile.\n\nExpiry: 400 daysType: HTTP\n\n    * VWO\n\n17\n\nLearn more about this provider\n\n_vis_opt_exp_#_combiUsed by Visual Website Optimizer to ensure that the same\nuser interface variant is displayed for each visit, if the user is\nparticipating in a design experiment.\n\nExpiry: 100 daysType: HTTP\n\n_vis_opt_exp_#_goal_#Used by Visual Website Optimizer to designate if a goal\nof a design experiment has been reached.\n\nExpiry: 100 daysType: HTTP\n\n_vis_opt_sUsed by Visual Website Optimizer to determine if the visitor is\nparticipating in a design experiment.\n\nExpiry: 100 daysType: HTTP\n\n_vis_opt_test_cookieUsed to check if the user's browser supports cookies.\n\nExpiry: SessionType: HTTP\n\n_vwo_dsCollects data on the user's visits to the website, such as the number\nof visits, average time spent on the website and what pages have been loaded\nwith the purpose of generating reports for optimising the website content.\n\nExpiry: 3 monthsType: HTTP\n\n_vwo_referrerRegisters data on visitors' website-behaviour. This is used for\ninternal analysis and website optimization.\n\nExpiry: SessionType: HTTP\n\n_vwo_snCollects statistics on the visitor's visits to the website, such as the\nnumber of visits, average time spent on the website and what pages have been\nread.\n\nExpiry: 1 dayType: HTTP\n\n_vwo_uuidUsed by Visual Website Optimizer to ensure that the same user\ninterface variant is displayed for each visit, if the user is participating in\na design experiment.\n\nExpiry: 10 yearsType: HTTP\n\n_vwo_709877_configPending\n\nExpiry: PersistentType: HTML\n\nanalyzeThis cookie is used by the website\u2019s operator in context with multi-\nvariate testing. This is a tool used to combine or change content on the\nwebsite. This allows the website to find the best variation/edition of the\nsite.\n\nExpiry: SessionType: Pixel\n\neu01/c.gifPending\n\nExpiry: SessionType: Pixel\n\neu01/ee.gifPending\n\nExpiry: SessionType: Pixel\n\neu01/l.gifPending\n\nExpiry: SessionType: Pixel\n\neu01/s.gifPending\n\nExpiry: SessionType: Pixel\n\neu01/v.gifThis cookie is used by the website\u2019s operator in context with multi-\nvariate testing. This is a tool used to combine or change content on the\nwebsite. This allows the website to find the best variation/edition of the\nsite.\n\nExpiry: SessionType: Pixel\n\nvwo_apm_sentPending\n\nExpiry: PersistentType: HTML\n\nvwoSnThis cookie is set to make split-tests on the website, which optimizes\nthe website's relevance towards the visitor \u2013 the cookie can also be set to\nimprove the visitor's experience on a website.\n\nExpiry: PersistentType: HTML\n\n    * Vimeo\n\n1\n\nLearn more about this provider\n\nvuidCollects data on the user's visits to the website, such as which pages\nhave been read.\n\nExpiry: 2 yearsType: HTTP\n\n    * channable.com\n\n3\n\nFPGSIDRegisters statistical data on users' behaviour on the website. Used for\ninternal analytics by the website operator.\n\nExpiry: 1 dayType: HTTP\n\nFPIDRegisters statistical data on users' behaviour on the website. Used for\ninternal analytics by the website operator.\n\nExpiry: 400 daysType: HTTP\n\nFPLCRegisters a unique ID that is used to generate statistical data on how the\nvisitor uses the website.\n\nExpiry: 1 dayType: HTTP\n\n    * load.gtm.channable.com\n\n2\n\n_gaRegisters a unique ID that is used to generate statistical data on how the\nvisitor uses the website.\n\nExpiry: 2 yearsType: HTTP\n\n_ga_#Used by Google Analytics to collect data on the number of times a user\nhas visited the website as well as dates for the first and most recent visit.\n\nExpiry: 2 yearsType: HTTP\n\n    * www.channable.com\n\n1\n\npvRegisters statistical data on users' behaviour on the website. Used for\ninternal analytics by the website operator.\n\nExpiry: SessionType: HTML\n\n  * Marketing cookies are used to track visitors across websites. The intention is to display ads that are relevant and engaging for the individual user and thereby more valuable for publishers and third party advertisers.\n\n    * Meta Platforms, Inc.\n\n3\n\nLearn more about this provider\n\n_fbpUsed by Facebook to deliver a series of advertisement products such as\nreal time bidding from third party advertisers.\n\nExpiry: 3 monthsType: HTTP\n\nlastExternalReferrerDetects how the user reached the website by registering\ntheir last URL-address.\n\nExpiry: PersistentType: HTML\n\nlastExternalReferrerTimeDetects how the user reached the website by\nregistering their last URL-address.\n\nExpiry: PersistentType: HTML\n\n    * Google\n\n6\n\nLearn more about this provider\n\nIDEUsed by Google DoubleClick to register and report the website user's\nactions after viewing or clicking one of the advertiser's ads with the purpose\nof measuring the efficacy of an ad and to present targeted ads to the user.\n\nExpiry: 1 yearType: HTTP\n\npagead/landingCollects data on visitor behaviour from multiple websites, in\norder to present more relevant advertisement - This also allows the website to\nlimit the number of times that they are shown the same advertisement.\n\nExpiry: SessionType: Pixel\n\ntest_cookieUsed to check if the user's browser supports cookies.\n\nExpiry: 1 dayType: HTTP\n\nads/ga-audiencesUsed by Google AdWords to re-engage visitors that are likely\nto convert to customers based on the visitor's online behaviour across\nwebsites.\n\nExpiry: SessionType: Pixel\n\npagead/1p-conversion/#/Pending\n\nExpiry: SessionType: Pixel\n\npagead/1p-user-list/#Tracks if the user has shown interest in specific\nproducts or events across multiple websites and detects how the user navigates\nbetween sites. This is used for measurement of advertisement efforts and\nfacilitates payment of referral-fees between websites.\n\nExpiry: SessionType: Pixel\n\n    * Hubspot\n\n2\n\nLearn more about this provider\n\n__ptbe.gifPending\n\nExpiry: SessionType: Pixel\n\n__ptq.gifSends data to the marketing platform Hubspot about the visitor's\ndevice and behaviour. Tracks the visitor across devices and marketing\nchannels.\n\nExpiry: SessionType: Pixel\n\n    * Leadinfo\n\n3\n\nLearn more about this provider\n\n_li_id.#Tracks the individual sessions on the website, allowing the website to\ncompile statistical data from multiple visits. This data can also be used to\ncreate leads for marketing purposes.\n\nExpiry: PersistentType: HTML\n\n_li_id.#.expiresTracks the individual sessions on the website, allowing the\nwebsite to compile statistical data from multiple visits. This data can also\nbe used to create leads for marketing purposes.\n\nExpiry: PersistentType: HTML\n\n_li_ses.#.expiresTracks the individual sessions on the website, allowing the\nwebsite to compile statistical data from multiple visits. This data can also\nbe used to create leads for marketing purposes.\n\nExpiry: PersistentType: HTML\n\n    * LinkedIn\n\n5\n\nLearn more about this provider\n\nbcookieUsed by the social networking service, LinkedIn, for tracking the use\nof embedded services.\n\nExpiry: 1 yearType: HTTP\n\nli_sugrCollects data on user behaviour and interaction in order to optimize\nthe website and make advertisement on the website more relevant.\n\nExpiry: 3 monthsType: HTTP\n\nlidcUsed by the social networking service, LinkedIn, for tracking the use of\nembedded services.\n\nExpiry: 1 dayType: HTTP\n\nUserMatchHistoryUsed to track visitors on multiple websites, in order to\npresent relevant advertisement based on the visitor's preferences.\n\nExpiry: 30 daysType: HTTP\n\nli_adsIdCollects data on user behaviour and interaction in order to optimize\nthe website and make advertisement on the website more relevant.\n\nExpiry: PersistentType: HTML\n\n    * Microsoft\n\n15\n\nLearn more about this provider\n\ntotalCallsUsed in context with video-advertisement. The cookie limits the\nnumber of times a user is shown the same advertisement. The cookie is also\nused to ensure relevance of the video-advertisement to the specific user.\n\nExpiry: PersistentType: HTML\n\ntotalCallsTimeoutUsed in context with video-advertisement. The cookie limits\nthe number of times a user is shown the same advertisement. The cookie is also\nused to ensure relevance of the video-advertisement to the specific user.\n\nExpiry: PersistentType: HTML\n\n_uetsidUsed to track visitors on multiple websites, in order to present\nrelevant advertisement based on the visitor's preferences.\n\nExpiry: PersistentType: HTML\n\n_uetsid_expContains the expiry-date for the cookie with corresponding name.\n\nExpiry: PersistentType: HTML\n\n_uetvidUsed to track visitors on multiple websites, in order to present\nrelevant advertisement based on the visitor's preferences.\n\nExpiry: PersistentType: HTML\n\n_uetvid_expContains the expiry-date for the cookie with corresponding name.\n\nExpiry: PersistentType: HTML\n\nMRUsed to track visitors on multiple websites, in order to present relevant\nadvertisement based on the visitor's preferences.\n\nExpiry: 7 daysType: HTTP\n\nMSPTCThis cookie registers data on the visitor. The information is used to\noptimize advertisement relevance.\n\nExpiry: 1 yearType: HTTP\n\nMUIDUsed widely by Microsoft as a unique user ID. The cookie enables user\ntracking by synchronising the ID across many Microsoft domains.\n\nExpiry: 1 yearType: HTTP\n\n_uetsidCollects data on visitor behaviour from multiple websites, in order to\npresent more relevant advertisement - This also allows the website to limit\nthe number of times that they are shown the same advertisement.\n\nExpiry: 1 dayType: HTTP\n\n_uetvidUsed to track visitors on multiple websites, in order to present\nrelevant advertisement based on the visitor's preferences.\n\nExpiry: 1 yearType: HTTP\n\nMC1Used by Microsoft to keep statistics on what pages the user has visited and\nhow often an ad click leads to a purchase or other actions on the advertiser's\nwebsite.\n\nExpiry: 1 yearType: HTTP\n\nMS0Used by Microsoft to keep statistics on what pages the user has visited and\nhow often an ad click leads to a purchase or other actions on the advertiser's\nwebsite.\n\nExpiry: 1 dayType: HTTP\n\nMicrosoftApplicationsTelemetryDeviceIdSets a specific device ID. This ID is\nused by Microsoft to track users' website behaviour and the interaction with\nMicrosoft application on the specific device.\n\nExpiry: 1 yearType: HTTP\n\nMSFPCUsed widely by Microsoft as a unique user ID. The cookie enables user\ntracking by synchronising the ID across many Microsoft domains.\n\nExpiry: 1 yearType: HTTP\n\n    * Reddit\n\n3\n\nLearn more about this provider\n\nrp.gifNecessary for the implementation of the Reddit.com's share-button\nfunction.\n\nExpiry: SessionType: Pixel\n\n_rdt_uuid [x2]Used to track visitors on multiple websites, in order to present\nrelevant advertisement based on the visitor's preferences.\n\nExpiry: 3 monthsType: HTTP\n\n    * Spotify\n\n2\n\nLearn more about this provider\n\nsp_landingUsed to implement audio-content from Spotify on the website. Can\nalso be used to register user interaction and preferences in context with\naudio-content - This can serve statistics and marketing purposes.\n\nExpiry: 1 dayType: HTTP\n\nsp_tUsed to implement audio-content from Spotify on the website. Can also be\nused to register user interaction and preferences in context with audio-\ncontent - This can serve statistics and marketing purposes.\n\nExpiry: 1 yearType: HTTP\n\n    * Twitter Inc.\n\n6\n\nLearn more about this provider\n\n1/i/adsct [x2]Collects data on user behaviour and interaction in order to\noptimize the website and make advertisement on the website more relevant.\n\nExpiry: SessionType: Pixel\n\nmuc_adsCollects data on user behaviour and interaction in order to optimize\nthe website and make advertisement on the website more relevant.\n\nExpiry: 400 daysType: HTTP\n\nguest_idCollects data related to the user's visits to the website, such as the\nnumber of visits, average time spent on the website and which pages have been\nloaded, with the purpose of personalising and improving the Twitter service.\n\nExpiry: 400 daysType: HTTP\n\nguest_id_adsCollects information on user behaviour on multiple websites. This\ninformation is used in order to optimize the relevance of advertisement on the\nwebsite.\n\nExpiry: 400 daysType: HTTP\n\nguest_id_marketingCollects information on user behaviour on multiple websites.\nThis information is used in order to optimize the relevance of advertisement\non the website.\n\nExpiry: 400 daysType: HTTP\n\n    * YouTube\n\n15\n\nLearn more about this provider\n\n#-#Pending\n\nExpiry: SessionType: HTML\n\niU5q-!O9@$Registers a unique ID to keep statistics of what videos from YouTube\nthe user has seen.\n\nExpiry: SessionType: HTML\n\nLAST_RESULT_ENTRY_KEYUsed to track user\u2019s interaction with embedded content.\n\nExpiry: SessionType: HTTP\n\nnextIdUsed to track user\u2019s interaction with embedded content.\n\nExpiry: SessionType: HTTP\n\nrequestsUsed to track user\u2019s interaction with embedded content.\n\nExpiry: SessionType: HTTP\n\nyt.innertube::nextIdRegisters a unique ID to keep statistics of what videos\nfrom YouTube the user has seen.\n\nExpiry: PersistentType: HTML\n\nytidb::LAST_RESULT_ENTRY_KEYUsed to track user\u2019s interaction with embedded\ncontent.\n\nExpiry: PersistentType: HTML\n\nYtIdbMeta#databasesUsed to track user\u2019s interaction with embedded content.\n\nExpiry: PersistentType: IDB\n\nyt-remote-cast-availableStores the user's video player preferences using\nembedded YouTube video\n\nExpiry: SessionType: HTML\n\nyt-remote-cast-installedStores the user's video player preferences using\nembedded YouTube video\n\nExpiry: SessionType: HTML\n\nyt-remote-connected-devicesStores the user's video player preferences using\nembedded YouTube video\n\nExpiry: PersistentType: HTML\n\nyt-remote-device-idStores the user's video player preferences using embedded\nYouTube video\n\nExpiry: PersistentType: HTML\n\nyt-remote-fast-check-periodStores the user's video player preferences using\nembedded YouTube video\n\nExpiry: SessionType: HTML\n\nyt-remote-session-appStores the user's video player preferences using embedded\nYouTube video\n\nExpiry: SessionType: HTML\n\nyt-remote-session-nameStores the user's video player preferences using\nembedded YouTube video\n\nExpiry: SessionType: HTML\n\n    * capig.channable.com\n\n1\n\nceeRegisters a unique ID that identifies the user's device during return\nvisits. Used for conversion tracking and to measure the efficacy of online\nads.\n\nExpiry: 3 monthsType: HTTP\n\n    * load.gtm.channable.com\n\n1\n\n_gcl_auUsed by Google AdSense for experimenting with advertisement efficiency\nacross websites using their services.\n\nExpiry: 3 monthsType: HTTP\n\n    * ruler.nyltx.com\n\n2\n\n__rafmDetermines how the user accessed the website. This information is used\nby the website operator in order to measure the efficiency of their marketing.\n\nExpiry: SessionType: HTTP\n\n__raseshContains data on user navigation, interaction and time spent on the\nwebsite and its sub-pages. This data is used to optimise the relevance of\nadvertisements and for statistical purposes.\n\nExpiry: 1028 daysType: HTTP\n\n  * Unclassified cookies are cookies that we are in the process of classifying, together with the providers of individual cookies.\n\n    * VWO\n\n1\n\nLearn more about this provider\n\n_vwo_nls_q_#Pending\n\nExpiry: PersistentType: HTML\n\n    * channable.com\n\n3\n\nchannel_flowPending\n\nExpiry: 400 daysType: HTTP\n\nchannel_flow_firstPending\n\nExpiry: 400 daysType: HTTP\n\nchannel_flow_lastPending\n\nExpiry: 400 daysType: HTTP\n\n    * channable.storylane.io\n\n4\n\neidPending\n\nExpiry: PersistentType: HTML\n\nflv_nhexokeuu5kcPending\n\nExpiry: SessionType: HTML\n\nfp_idPending\n\nExpiry: PersistentType: HTML\n\nsid_nhexokeuu5kcPending\n\nExpiry: SessionType: HTML\n\n    * www.channable.com\n\n3\n\neventValue_high_engaged_userPending\n\nExpiry: SessionType: HTML\n\neventValue_platform_menu_pagesPending\n\nExpiry: SessionType: HTML\n\neventValue_pricing_pageviewPending\n\nExpiry: SessionType: HTML\n\n    * www.flaticon.com\n\n8\n\ng_statePending\n\nExpiry: SessionType: HTTP\n\ngr_lang [x2]Pending\n\nExpiry: SessionType: HTTP\n\ngr_session [x2]Pending\n\nExpiry: SessionType: HTTP\n\ngr_session2Pending\n\nExpiry: SessionType: HTTP\n\nGRIDPending\n\nExpiry: SessionType: HTTP\n\ncsrf_flaticonPending\n\nExpiry: 1 dayType: HTTP\n\nCross-domain consent[#BULK_CONSENT_DOMAINS_COUNT#] [#BULK_CONSENT_TITLE#]\n\nList of domains your consent applies to: [#BULK_CONSENT_DOMAINS#]\n\nCookie declaration last updated on 3/26/24 by Cookiebot\n\n## [#IABV2_TITLE#]\n\n[#IABV2_BODY_INTRO#]\n\n[#IABV2_BODY_LEGITIMATE_INTEREST_INTRO#]\n\n[#IABV2_BODY_PREFERENCE_INTRO#]\n\n[#IABV2_BODY_PURPOSES_INTRO#]\n\n[#IABV2_BODY_PURPOSES#]\n\n[#IABV2_BODY_FEATURES_INTRO#]\n\n[#IABV2_BODY_FEATURES#]\n\n[#IABV2_BODY_PARTNERS_INTRO#]\n\n[#IABV2_BODY_PARTNERS#]\n\nCookies are small text files that can be used by websites to make a user's\nexperience more efficient.\n\nThe law states that we can store cookies on your device if they are strictly\nnecessary for the operation of this site. For all other types of cookies we\nneed your permission.\n\nThis site uses different types of cookies. Some cookies are placed by third\nparty services that appear on our pages.\n\nYou can at any time change or withdraw your consent from the Cookie\nDeclaration on our website.\n\nLearn more about who we are, how you can contact us and how we process\npersonal data in our Privacy Policy.\n\nPlease state your consent ID and date when you contact us regarding your\nconsent.\n\nPowered by Cookiebot by Usercentrics\n\nFree trial\n\nEN\n\nLog in\n\nFree trial\n\nOverview\n\nTech\n\n# How we optimized cluster-wide scheduling for sticky workloads\n\nApril 17, 2024\n\nChannable is a tool for marketers and web shop owners that connects shops to\nmarketplaces, affiliate platforms, and price comparison websites. We download\nproduct data through a feed file or API connection, process it and send the\ntransformed data to any platform. The volume of data can be impressive (1.7\nmillion products are processed per second on average), so we need strategies\nto make all this number crunching imperceptible for our customers.\n\nNot only do we need to process a very large number of products every day, but\nwe also want low latency without unsustainable processing costs. Performance\nis not just a nice-to-have: being able to process great volumes of data is a\nrequirement for our larger customers, and many users appreciate the immediacy\nof changing a rule or setting in Channable and seeing the results within\nseconds.\n\nIn Channable, customers modify their data using IF ... THEN ... ELSE ...\nrules. These rules are sent to one of our rule processing servers. The chosen\nrule processing server applies the rules to the product data and returns the\nprocessed product data for other services to use.\n\nIn this post we describe how we increased the performance of our product feed\nprocessing pipeline by changing how we select the rule processing server for a\ngiven unit of work (job). With our new scheduling system we were able to\nsignificantly reduce waiting times in our cluster, decreasing the 95th\npercentile of the queueing times for jobs by a factor of almost 4.5x.\n\n## Problem statement\n\nPeople who have read our previous blogs will have some idea of the things we\ndo to make the jobs our rule processing servers run as fast as possible. For\noptimal performance and reliability, we scale rule processing horizontally\nacross a set of servers. Our original method of assigning rule processing jobs\nto rule processing servers used a pseudo-random selection of servers based on\nthe project identifier and the set of currently-available rule processing\nservers.\n\nThe selection process was designed to evenly divide the processing workload\nacross the available rule processing servers by generating a preference list\nof servers for each project. This preference list was then used to pick a\nserver to run the project on, by trying each of the servers on the list in\norder and using the first server that responded successfully.\n\nThis approach was chosen to make the rule processing system more robust\nagainst failures where a single server would be unavailable, improving\nreliability and making operations easier. The selection process consisted of\nthe following steps:\n\n  1. Query Consul to retrieve the list of currently-healthy rule processing servers,\n  2. Sort this list of servers alphabetically,\n  3. Shuffle the list of servers using a pseudo-random number generator seeded with the project identifier,\n  4. Pick the first three servers in the resulting shuffled list and try to submit the rule processing job to each server in order. If a server fails to respond, move on to the next server. If all three servers fail to respond, error out.\n\nThe selection process is deterministic: given the same project identifier and\nset of available servers the same preference list is generated every time. The\nreason for using a preference list in this way was that the rule processing\nservers perform caching: if two rule processing jobs for the same project are\nprocessed at the same time on the same server, the rule processing server will\ndeduplicate shared work where possible and e.g. only download the project's\ninput data once. By making each project prefer the same set of servers we\ncould take advantage of the caching without having to explicitly check on\nwhich servers the project was in cache.\n\nWhile the described selection process was easy to implement and did not\nrequire coordination between the various services accessing the rule\nprocessing servers, it also was found to have some significant downsides:\n\n  * The selection process did not take the servers' actual workload into account when assigning jobs to servers. The selection process only checked if rule processing servers were 'healthy' in Consul, i.e. online and not crashed, but not whether the servers were actually able to accept new jobs.\n\nThe selection process would happily assign jobs to servers that were already\nat their processing capacity, which resulted in these jobs having to wait in\nthe server's queue. This would happen even if other servers were idle, which\nis not ideal.\n\n  * The rule processing workload was not evenly distributed among the rule processing servers: while the selection process did distribute the various projects roughly evenly across the available servers, the computational power required for each project varies wildly. This resulted in some rule processing servers often being overloaded, while other servers were idle at the same time.\n\n  * The selection process was not robust against changes in the set of available servers: if any server was added to or removed from the set of available servers, almost all of the project-to-server assignments would change. This behavior was not originally seen as a problem, because when the scheduling algorithm was implemented the set of available servers was relatively stable. Channable later switched a part of the rule processing servers to \"preemptible\" virtual machines, which have a fixed lifetime and are regularly unavailable. The increased change in available servers and thus job-server assignments meant that we could use the cache on the rule processing servers less effectively, which caused longer job runtimes.\n\n## Experimenting with better approaches\n\nAs part of Maarten's Master's thesis project^[1] we evaluated various\nalternative load balancing methods that could offer better performance than\nthe method described above. To evaluate these methods we ran experiments using\nrecorded production data in an isolated experiment environment.\n\nIn the experiment environment we replicated our production workload by\n'replaying' 24 hours of HTTP requests to the rule processing servers. The\nservers the requests were sent to were picked by a custom-built scheduler\nservice, as opposed to the decentralized server selection method used in\nproduction.\n\nFor each of the sent requests we recorded whether the request was successful,\nhow long it took to perform and how much of this time was spent waiting in a\nserver's queue.\n\nTo address the problems described in the previous section, we split our server\nselection process into two distinct requirements:\n\n  1. Overload detection: determining if servers were overloaded and should not receive any work, and\n  2. Server selection: picking a server from the set of servers returned by the overload detection process.\n\nDuring each of the experiments we chose a single overload detection method and\na single server selection method, to evaluate the performance of these two\nmethods combined.\n\n### Overload detection approaches\n\nFor our overload detection method we compared three approaches:\n\n  1. The baseline approach of not doing any overload detection: this approach matched our production approach of \"just send requests until the server isn't available anymore\".\n\n  2. Circuit breaker: cut off requests to a server immediately if it has more than a fixed number of outstanding work items that are not making any progress.\n\n  3. WeChat's DAGOR load-shedding algorithm ^[2]: this algorithm gradually reduces the rate of requests flowing to a server if the server is overloaded, instead of cutting off a server immediately.\n\n### Host selection approaches\n\nFor our host selection we also compared three approaches:\n\n  1. The baseline approach of selecting servers by shuffling the list of servers based on the project identifier.\n\n  2. Just randomly selecting any available server.\n\n  3. 'Cache-aware' host selection. For this approach we kept track of the state of each server's cache, along with the number of rule processing jobs that were being executed on that server. When a host must be selected, the dynamic host selector picks the server where the project is in cache (if any), and otherwise picks the server with the least rule processing jobs in progress.\n\n## Experimental results\n\nIn our results we distinguish between jobs with low and high priorities. Jobs\nwith low priority are triggered by automated processes and comprise the vast\nmajority of the jobs in our system. Jobs with high priority are triggered in\nresponse to user action in the Channable tool, and are executed as quickly as\npossible to show the job's result to the user sooner by pausing execution of\nlow-priority jobs if high-priority jobs exist.\n\nTo evaluate our overload detection methods and host selection methods we\ncompared waiting times and total execution times for the rule processing jobs,\ngrouped by priority.\n\nWe compared these values by looking at the average values, and by looking at\nthe mean and outlier percentiles (90, 95, 99th percentile). Each of the\nresults in the below figures are labeled with the overload detection method\nand host selection method in use for the experiment, respectively.\n\nThe experiments using fully-random host selection are not included in our\nresults: during preliminary testing we found that random host selection\nperformed significantly worse than the baseline host selection method, causing\nthe benchmarking environment to grind to a halt and to be more than an hour\nslower to execute all of the jobs in the benchmark compared to the production\nenvironment. Because of this result we excluded random host selection from\nfurther experimentation and did not bother running a 24-hour experiment with\nit.\n\nWe first show the average job duration (i.e. total processing time) and\nwaiting time:\n\nThe waiting time and job duration of the high priority jobs did not differ too\ngreatly between experiments (though it did improve), but the low-priority\nduration did. Looking at the percentile comparison we can see that this is due\nto the outliers improving significantly:\n\nAfter our experiments we also noticed several other interesting outcomes:\n\n  * The baseline experiment performed worse on average compared to the production environment's performance during the time our experiment data was gathered. This was determined to be due to the experiment environment having different retry behavior than the production environment, which resulted in it stressing the experiment environment more severely than the production environment.\n\n  * Overload detection using the DAGOR algorithm performed slightly worse than the circuit breaker algorithm, while we had expected it to outperform the circuit breaker. On closer inspection we found that the DAGOR algorithm would almost always fully cut off requests to an overloaded server, effectively acting like a delayed circuit breaker. It is possible that DAGOR could outperform a static circuit breaker with more tuning, but we did not investigate further.\n\n  * All approaches, apart from the baseline, greatly reduced the variance in wait times between servers, shown in the graph below. This is to be expected if the worst-case percentiles of the waiting time are reduced, but also nice to see.\n\nThere are of course some caveats because we are using an experiment\nenvironment, most notably that we used static data. The product feed data in\nour experiment environment was taken from a snapshot and did not change over\ntime, which meant that the cache in our rule processing servers was likely\nmore effective than it would have been in the real world.\n\n## Applying the new algorithm in the real world\n\nWhile there are some caveats to our results, a circuit breaker overload\ndetector with cache-aware (dynamic) host selection appeared to be the best\nsolution. We were confident that implementing this type of scheduling in\nproduction would lead to a performance improvement, so that's what we set out\nto do next.\n\nThe following graph shows the 95th percentile of the duration and waiting time\nfor our low-priority rule processing jobs during the eight days before and\nafter the new scheduling algorithm was enabled in our production environment.\n\nWe can see that with the new scheduler active the queueing time drops\ndramatically: in the 24 hours before the scheduler was activated, the 95th\npercentile of the queueing time was 18.42 seconds, while in the 24 hours after\nthe scheduler was activated this metric dropped to 4.11 seconds. This\nimprovement in turn reduces the overall processing time. Nice!\n\nWe also saw an improvement in backlog imbalance, another metric which compares\nthe backlog size of each server against the average backlog size across all\nservers. The backlog imbalance should ideally be close to zero (indicating the\nserver has a backlog close to the average size), and the worst-case backlog\nimbalance also went down once we activated the new scheduling algorithm:\n\nThese improvements in queueing behavior resulted in improvements both for us\nand the end users:\n\n  * End users get to have faster execution for rules and imports.\n  * We get to make better use of our servers, as the workload is distributed in a more effective way. Along with some other improvements in the execution of jobs, it allowed us to save money by deactivating a dedicated server that we used for some specific very heavy tasks. In addition, we now also need fewer general-purpose machines to run our jobs.\n\n## Conclusion\n\nAt Channable, we process very large amounts of data for our customers. Fast\nand reliable processing with minimal delay is important to our customers, and\nusing minimal resources allows us to keep costs down. We hypothesized that the\nmost compute-heavy part of our system could more efficiently scale workloads\nacross servers while lowering the delay in job processing. We tried various\nmethods in an experimental setup and found that the \u2018circuit-breaker, dynamic\u2019\napproach yielded the best results, while being simpler than the DAGOR\nalternative we found in literature. Because of the good experimental results,\nwe implemented this approach in production and improved processing delay while\nrequiring fewer processing resources, saving costs.\n\n1: For more information on the experiments and our rule processing system you\ncan read Maarten's thesis at\nhttps://studenttheses.uu.nl/handle/20.500.12932/43909. \u21a9\n\n2: For more information see: \"Overload Control for Scaling WeChat\nMicroservices\" by Zhou et al., https://arxiv.org/abs/1806.04075 \u21a9\n\nDiego DiverioSoftware Development\n\nMaarten van den BergDevOps\n\n## We are hiring\n\nAre you interested in working at Channable? Check out our vacancy page to see\nif we have an open position that suits you!\n\nApply now\n\n#### Why Channable\n\n  * For retailers\n  * For agencies\n  * For brands\n\n#### Platform\n\n  * Platform\n  * Feed Management\n  * PPC Optimization\n  * Marketplace Integration\n  * Insights & Analytics\n  * Integrations\n  * Pricing\n\n#### Resources\n\n  * Contact\n  * Press\n  * Blog\n  * Newsletter\n  * Tech blog\n  * Success stories\n  * Channacademy\n  * Social responsibility\n\n#### About us\n\n  * Jobs\n  * Status\n  * Terms and conditions\n  * Privacy policy\n  * Data security\n  * Subprocessors\n  * Bug bounty\n  * Cookie policy\n  * Job Applicant Privacy Policy\n\nCapterra\n\nTrustpilot\n\nG2\n\nOMR\n\n", "frontpage": false}
