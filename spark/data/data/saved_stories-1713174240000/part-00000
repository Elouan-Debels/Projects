{"aid": "40036880", "title": "New Method Fine-Tunes LLMs with 10x Less Data, Even Rivals In-Context Learning", "url": "https://paperswithcode.com/paper/token-efficient-leverage-learning-in-large", "domain": "paperswithcode.com", "votes": 1, "user": "lineshogan", "posted_at": "2024-04-15 03:42:13", "comments": 0, "source_title": "Papers with Code - Token-Efficient Leverage Learning in Large Language Models", "source_text": "Token-Efficient Leverage Learning in Large Language Models | Papers With Code\n\n### Subscribe to the PwC Newsletter\n\n##### Join the community\n\nYou need to log in to edit. You can create a new account if you don't have\none.\n\n# Token-Efficient Leverage Learning in Large Language Models\n\n1 Apr 2024 \u00b7 Yuanhao Zeng, Min Wang, Yihang Wang, Yingxia Shao \u00b7\n\nLarge Language Models (LLMs) have excelled in various tasks but perform better\nin high-resource scenarios, which presents challenges in low-resource\nscenarios. Data scarcity and the inherent difficulty of adapting LLMs to\nspecific tasks compound the challenge. To address the twin hurdles, we\nintroduce \\textbf{Leverage Learning}. We present a streamlined implement of\nthis methodology called Token-Efficient Leverage Learning (TELL). TELL\nshowcases the potential of Leverage Learning, demonstrating effectiveness\nacross various LLMs and low-resource tasks, ranging from to tokens. It reduces\ntask data requirements by up to nearly an order of magnitude compared to\nconventional Supervised Fine-Tuning (SFT) while delivering competitive\nperformance. With the same amount of task data, TELL leads in improving task\nperformance compared to SFT. We discuss the mechanism of Leverage Learning,\nsuggesting it aligns with quantization hypothesis and explore its promising\npotential through empirical testing.\n\nPDF Abstract\n\n##\n\nCode\n\nAdd Remove Mark official\n\nLinesHogan/Token-Efficient-Leverage... official\n\n1\n\nLinesHogan/Token-Efficient-Leverage... official\n\n1\n\n##\n\nTasks\n\nAdd Remove\n\nInstruction Following Translation\n\n##\n\nDatasets\n\nHaluEval WMT 2021 - Multilingual Low-Resource Translation for Indo-European\nLanguages\n\n##\n\nResults from the Paper\n\nEdit\n\nSubmit results from this paper to get state-of-the-art GitHub badges and help\nthe community compare results to other papers.\n\n##\n\nMethods\n\nAdd Remove\n\nLeverage Learning \u2022 SFT\n\nContact us on: hello@paperswithcode.com . Papers With Code is a free resource\nwith all data licensed under CC-BY-SA.\n\nTerms Data policy Cookies policy from\n\n", "frontpage": false}
