{"aid": "39979806", "title": "Persistent interaction patterns across social media platforms and over time", "url": "https://www.nature.com/articles/s41586-024-07229-y", "domain": "nature.com", "votes": 3, "user": "andsoitis", "posted_at": "2024-04-09 14:27:53", "comments": 0, "source_title": "Persistent interaction patterns across social media platforms and over time", "source_text": "Persistent interaction patterns across social media platforms and over time | Nature\n\nLoading [MathJax]/jax/output/HTML-CSS/config.js\n\nSkip to main content\n\nThank you for visiting nature.com. You are using a browser version with\nlimited support for CSS. To obtain the best experience, we recommend you use a\nmore up to date browser (or turn off compatibility mode in Internet Explorer).\nIn the meantime, to ensure continued support, we are displaying the site\nwithout styles and JavaScript.\n\nAdvertisement\n\n  * View all journals\n  * Search\n\n## Search\n\nAdvanced search\n\n### Quick links\n\n    * Explore articles by subject\n    * Find a job\n    * Guide to authors\n    * Editorial policies\n\n  * Log in\n\n  * Explore content\n  * About the journal\n  * Publish with us\n\n  * Sign up for alerts\n  * RSS feed\n\nPersistent interaction patterns across social media platforms and over time\n\nDownload PDF\n\nDownload PDF\n\n  * Article\n  * Open access\n  * Published: 20 March 2024\n\n# Persistent interaction patterns across social media platforms and over time\n\n  * Michele Avalle ORCID: orcid.org/0009-0007-4934-2326^1^ na1,\n  * Niccol\u00f2 Di Marco^1^ na1,\n  * Gabriele Etta^1^ na1,\n  * Emanuele Sangiorgio ORCID: orcid.org/0009-0003-1024-3735^2,\n  * Shayan Alipour^1,\n  * Anita Bonetti^3,\n  * Lorenzo Alvisi^1,\n  * Antonio Scala^4,\n  * Andrea Baronchelli^5,6,\n  * Matteo Cinelli ORCID: orcid.org/0000-0003-3899-4592^1 &\n  * ...\n  * Walter Quattrociocchi ORCID: orcid.org/0000-0002-4374-9324^1\n\nNature (2024)Cite this article\n\n  * 15k Accesses\n\n  * 238 Altmetric\n\n  * Metrics details\n\n## Abstract\n\nGrowing concern surrounds the impact of social media platforms on public\ndiscourse^1,2,3,4 and their influence on social dynamics^5,6,7,8,9, especially\nin the context of toxicity^10,11,12. Here, to better understand these\nphenomena, we use a comparative approach to isolate human behavioural patterns\nacross multiple social media platforms. In particular, we analyse\nconversations in different online communities, focusing on identifying\nconsistent patterns of toxic content. Drawing from an extensive dataset that\nspans eight platforms over 34 years\u2014from Usenet to contemporary social\nmedia\u2014our findings show consistent conversation patterns and user behaviour,\nirrespective of the platform, topic or time. Notably, although long\nconversations consistently exhibit higher toxicity, toxic language does not\ninvariably discourage people from participating in a conversation, and\ntoxicity does not necessarily escalate as discussions evolve. Our analysis\nsuggests that debates and contrasting sentiments among users significantly\ncontribute to more intense and hostile discussions. Moreover, the persistence\nof these patterns across three decades, despite changes in platforms and\nsocietal norms, underscores the pivotal role of human behaviour in shaping\nonline discourse.\n\n### Similar content being viewed by others\n\n### Cross-platform social dynamics: an analysis of ChatGPT and COVID-19\nvaccine conversations\n\nArticle Open access 02 February 2024\n\nShayan Alipour, Alessandro Galeazzi, ... Walter Quattrociocchi\n\n### The language of opinion change on social media under the lens of\ncommunicative action\n\nArticle Open access 26 October 2022\n\nCorrado Monti, Luca Maria Aiello, ... Francesco Bonchi\n\n### A social media network analysis of trypophobia communication\n\nArticle Open access 07 December 2022\n\nXanat Vargas Meza & Shinichi Koyama\n\n## Main\n\nThe advent and proliferation of social media platforms have not only\ntransformed the landscape of online participation^2 but have also become\nintegral to our daily lives, serving as primary sources for information,\nentertainment and personal communication^13,14. Although these platforms offer\nunprecedented connectivity and information exchange opportunities, they also\npresent challenges by entangling their business models with complex social\ndynamics, raising substantial concerns about their broader impact on society.\nPrevious research has extensively addressed issues such as polarization,\nmisinformation and antisocial behaviours in online spaces^5,7,12,15,16,17,\nrevealing the multifaceted nature of social media\u2019s influence on public\ndiscourse. However, a considerable challenge in understanding how these\nplatforms might influence inherent human behaviours lies in the general lack\nof accessible data^18. Even when researchers obtain data through special\nagreements with companies like Meta, it may not be enough to clearly\ndistinguish between inherent human behaviours and the effects of the\nplatform\u2019s design^3,4,8,9. This difficulty arises because the data, deeply\nembedded in platform interactions, complicate separating intrinsic human\nbehaviour from the influences exerted by the platform\u2019s design and algorithms.\n\nHere we address this challenge by focusing on toxicity, one of the most\nprominent aspects of concern in online conversations. We use a comparative\nanalysis to uncover consistent patterns across diverse social media platforms\nand timeframes, aiming to shed light on toxicity dynamics across various\ndigital environments. In particular, our goal is to gain insights into\ninherently invariant human patterns of online conversations.\n\nThe lack of non-verbal cues and physical presence on the web can contribute to\nincreased incivility in online discussions compared with face-to-face\ninteractions^19. This trend is especially pronounced in online arenas such as\nnewspaper comment sections and political discussions, where exchanges may\ndegenerate into offensive comments or mockery, undermining the potential for\nproductive and democratic debate^20,21. When exposed to such uncivil language,\nusers are more likely to interpret these messages as hostile, influencing\ntheir judgement and leading them to form opinions based on their beliefs\nrather than the information presented and may foster polarized perspectives,\nespecially among groups with differing values^22. Indeed, there is a natural\ntendency for online users to seek out and align with information that echoes\ntheir pre-existing beliefs, often ignoring contrasting views^6,23. This\nbehaviour may result in the creation of echo chambers, in which like-minded\nindividuals congregate and mutually reinforce shared narratives^5,24,25. These\necho chambers, along with increased polarization, vary in their prevalence and\nintensity across different social media platforms^1, suggesting that the\ndesign and algorithms of these platforms, intended to maximize user\nengagement, can substantially shape online social dynamics. This focus on\nengagement can inadvertently highlight certain behaviours, making it\nchallenging to differentiate between organic user interaction and the\ninfluence of the platform\u2019s design. A substantial portion of current research\nis devoted to examining harmful language on social media and its wider\neffects, online and offline^10,26. This examination is crucial, as it reveals\nhow social media may reflect and amplify societal issues, including the\ndeterioration of public discourse. The growing interest in analysing online\ntoxicity through massive data analysis coincides with advancements in machine\nlearning capable of detecting toxic language^27. Although numerous studies\nhave focused on online toxicity, most concentrate on specific platforms and\ntopics^28,29. Broader, multiplatform studies are still limited in scale and\nreach^12,30. Research fragmentation complicates understanding whether\nperceptions about online toxicity are accurate or misconceptions^31. Key\nquestions include whether online discussions are inherently toxic and how\ntoxic and non-toxic conversations differ. Clarifying these dynamics and how\nthey have evolved over time is crucial for developing effective strategies and\npolicies to mitigate online toxicity.\n\nOur study involves a comparative analysis of online conversations, focusing on\nthree dimensions: time, platform and topic. We examine conversations from\neight different platforms, totalling about 500 million comments. For our\nanalysis, we adopt the toxicity definition provided by the Perspective API, a\nstate-of-the-art classifier for the automatic detection of toxic speech. This\nAPI considers toxicity as \u201ca rude, disrespectful or unreasonable comment\nlikely to make someone leave a discussion\u201d. We further validate this\ndefinition by confirming its consistency with outcomes from other detection\ntools, ensuring the reliability and comparability of our results. The concept\nof toxicity in online discourse varies widely in the literature, reflecting\nits complexity, as seen in various studies^32,33,34. The efficacy and\nconstraints of current machine-learning-based automated toxicity detection\nsystems have recently been debated^11,35. Despite these discussions, automated\nsystems are still the most practical means for large-scale analyses.\n\nHere we analyse online conversations, challenging common assumptions about\ntheir dynamics. Our findings reveal consistent patterns across various\nplatforms and different times, such as the heavy-tailed nature of engagement\ndynamics, a decrease in user participation and an increase in toxic speech in\nlengthier conversations. Our analysis indicates that, although toxicity and\nuser participation in debates are independent variables, the diversity of\nopinions and sentiments among users may have a substantial role in escalating\nconversation toxicity.\n\nTo obtain a comprehensive picture of online social media conversations, we\nanalysed a dataset of about 500 million comments from Facebook, Gab, Reddit,\nTelegram, Twitter, Usenet, Voat and YouTube, covering diverse topics and\nspanning over three decades (a dataset breakdown is shown in Table 1 and\nSupplementary Table 1; for details regarding the data collection, see the\n\u2018Data collection\u2019 section of the Methods).\n\nTable 1 Dataset breakdown\n\nFull size table\n\nOur analysis aims to comprehensively compare the dynamics of diverse social\nmedia accounting for human behaviours and how they evolved. In particular, we\nfirst characterize conversations at a macroscopic level by means of their\nengagement and participation, and we then analyse the toxicity of\nconversations both after and during their unfolding. We conclude the paper by\nexamining potential drivers for the emergence of toxic speech.\n\n## Conversations on different platforms\n\nThis section provides an overview of online conversations by considering user\nactivity and thread size metrics. We define a conversation (or a thread) as a\nsequence of comments that follow chronologically from an initial post. In Fig.\n1a and Extended Data Fig. 1, we observe that, across all platforms, both user\nactivity (defined as the number of comments posted by the user) and thread\nlength (defined as the number of comments in a thread) exhibit heavy-tailed\ndistributions. The summary statistics about these distributions are reported\nin Supplementary Tables 1 and 2.\n\nFig. 1: General characteristics of online conversations.\n\na, The distributions of user activity in terms of comments posted for each\nplatform and each topic. b, The mean user participation as conversations\nevolve. For each dataset, participation is computed for the threads belonging\nto the size interval [0.7\u20131] (Supplementary Table 2). Trends are reported with\ntheir 95% confidence intervals. The x axis represents the normalized position\nof comment intervals in the threads.\n\nFull size image\n\nConsistent with previous studies^36,37 our analysis shows that the macroscopic\npatterns of online conversations, such as the distribution of users/threads\nactivity and lifetime, are consistent across all datasets and topics\n(Supplementary Tables 1\u20134). This observation holds regardless of the specific\nfeatures of the diverse platforms, such as recommendation algorithms and\nmoderation policies (described in the \u2018Content moderation policies\u2019 of the\nMethods), as well as other factors, including the user base and the\nconversation topics. We extend our analysis by examining another aspect of\nuser activity within conversations across all platforms. To do this, we\nintroduce a metric for the participation of users as a thread evolves. In this\nanalysis, threads are filtered to ensure sufficient length as explained in the\n\u2018Logarithmic binning and conversation size\u2019 section of the Methods.\n\nThe participation metric, defined over different conversation intervals (that\nis, 0\u20135% of the thread arranged in chronological order, 5\u201310%, and so on), is\nthe ratio of the number of unique users to the number of comments in the\ninterval. Considering a fixed number of comments c, smaller values of\nparticipation indicate that fewer unique users are producing c comments in a\nsegment of the conversation. In turn, a value of participation equal to 1\nmeans that each user is producing one of the c comments, therefore obtaining\nthe maximal homogeneity of user participation. Our findings show that, across\nall datasets, the participation of users in the evolution of conversations,\naveraged over almost all considered threads, is decreasing, as indicated by\nthe results of Mann\u2013Kendall test\u2014a nonparametric test assessing the presence\nof a monotonic upward or downward tendency\u2014shown in Extended Data Table 1.\nThis indicates that fewer users tend to take part in a conversation as it\nevolves, but those who do are more active (Fig. 1b). Regarding patterns and\nvalues, the trends in user participation for various topics are consistent\nacross each platform. According to the Mann\u2013Kendall test, the only exceptions\nwere Usenet Conspiracy and Talk, for which an ambiguous trend was detected.\nHowever, we note that their regression slopes are negative, suggesting a\ndecreasing trend, even if with a weaker effect. Overall, our first set of\nfindings highlights the shared nature of certain online interactions,\nrevealing a decrease in user participation over time but an increase in\nactivity among participants. This insight, consistent across most platforms,\nunderscores the dynamic interplay between conversation length, user engagement\nand topic-driven participation.\n\n## Conversation size and toxicity\n\nTo detect the presence of toxic language, we used Google\u2019s Perspective API^34,\na state-of-the-art toxicity classifier that has been used extensively in\nrecent literature^29,38. Perspective API defines a toxic comment as \u201cA rude,\ndisrespectful, or unreasonable comment that is likely to make people leave a\ndiscussion\u201d. On the basis of this definition, the classifier assigns a\ntoxicity score in the [0,1] range to a piece of text that can be interpreted\nas an estimate of the likelihood that a reader would perceive the comment as\ntoxic (https://developers.perspectiveapi.com/s/about-the-api-score). To define\nan appropriate classification threshold, we draw from the existing\nliterature^39, which uses 0.6 as the threshold for considering a comment as\ntoxic. A robustness check of our results using different threshold and\nclassification tools is reported in the \u2018Toxicity detection and validation of\nemployed models\u2019 section of the Methods, together with a discussion regarding\npotential shortcomings deriving from automatic classifiers. To further\ninvestigate the interplay between toxicity and conversation features across\nvarious platforms, our study first examines the prevalence of toxic speech in\neach dataset. We then analyse the occurrence of highly toxic users and\nconversations. Lastly, we investigate how the length of conversations\ncorrelates with the probability of encountering toxic comments. First of all,\nwe define the toxicity of a user as the fraction of toxic comments that she/he\nleft. Similarly, the toxicity of a thread is the fraction of toxic comments it\ncontains. We begin by observing that, although some toxic datasets exist on\nunmoderated platforms such as Gab, Usenet and Voat, the prevalence of toxic\nspeech is generally low. Indeed, the percentage of toxic comments in each\ndataset is mostly below 10% (Table 1). Moreover, the complementary cumulative\ndistribution functions illustrated in Extended Data Fig. 2 show that the\nfraction of extremely toxic users is very low for each dataset (in the range\nbetween 10^\u22123 and 10^\u22124), and the majority of active users wrote at least one\ntoxic comment, as reported in Supplementary Table 5, therefore suggesting that\nthe overall volume of toxicity is not a phenomenon limited to the activity of\nvery few users and localized in few conversations. Indeed, the number of users\nversus their toxicity decreases sharply following an exponential trend. The\ntoxicity of threads follows a similar pattern. To understand the association\nbetween the size and toxicity of a conversation, we start by grouping\nconversations according to their length to analyse their structural\ndifferences^40. The grouping is implemented by means of logarithmic binning\n(see the \u2018Logarithmic binning and conversation size\u2019 section of the Methods)\nand the evolution of the average fraction of toxic comments in threads versus\nthe thread size intervals is reported in Fig. 2. Notably, the resulting trends\nare almost all increasing, showing that, independently of the platform and\ntopic, the longer the conversation, the more toxic it tends to be.\n\nFig. 2: Toxicity increases with conversation size.\n\nThe mean fraction of toxic comments in conversations versus conversation size\nfor each dataset. Trends represent the mean toxicity over each size interval\nand their 95% confidence interval. Size ranges are normalized to enable visual\ncomparison of the different trends.\n\nFull size image\n\nWe assessed the increase in the trends by both performing linear regression\nand applying the Mann\u2013Kendall test to ensure the statistical significance of\nour results (Extended Data Table 2). To further validate these outcomes, we\nshuffled the toxicity labels of comments, finding that trends are almost\nalways non-increasing when data are randomized. Furthermore, the z-scores of\nthe regression slopes indicate that the observed trends deviate from the mean\nof the distributions resulting from randomizations, being at least 2 s.d.\ngreater in almost all cases. This provides additional evidence of a remarkable\ndifference from randomness. The only decreasing trend is Usenet Politics.\nMoreover, we verified that our results are not influenced by the specific\nnumber of bins as, after estimating the same trends again with different\nintervals, we found that the qualitative nature of the results remains\nunchanged. These findings are summarized in Extended Data Table 2. These\nanalyses have been validated on the same data using a different threshold for\nidentifying toxic comments and on a new dataset labelled with three different\nclassifiers, obtaining similar results (Extended Data Fig. 5, Extended Data\nTable 5, Supplementary Fig. 1 and Supplementary Table 8). Finally, using a\nsimilar approach, we studied the toxicity content of conversations versus\ntheir lifetime\u2014that is, the time elapsed between the first and last comment.\nIn this case, most trends are flat, and there is no indication that toxicity\nis generally associated either with the duration of a conversation or the\nlifetime of user interactions (Extended Data Fig. 4).\n\n## Conversation evolution and toxicity\n\nIn the previous sections, we analysed the toxicity level of online\nconversations after their conclusion. We next focus on how toxicity evolves\nduring a conversation and its effect on the dynamics of the discussion. The\ncommon beliefs that (1) online interactions inevitably devolve into toxic\nexchanges over time and (2) once a conversation reaches a certain toxicity\nthreshold, it would naturally conclude, are not modern notions but they were\nalso prevalent in the early days of the World Wide Web^41. Assumption 2 aligns\nwith the Perspective API\u2019s definition of toxic language, suggesting that\nincreased toxicity reduces the likelihood of continued participation in a\nconversation. However, this observation should be reconsidered, as it is not\nonly the peak levels of toxicity that might influence a conversation but, for\nexample, also a consistent rate of toxic content. To test these common\nassumptions, we used a method similar to that used for measuring\nparticipation; we select sufficiently long threads, divide each of them into a\nfixed number of equal intervals, compute the fraction of toxic comments for\neach of these intervals, average it over all threads and plot the toxicity\ntrend through the unfolding of the conversations. We find that the average\ntoxicity level remains mostly stable throughout, without showing a distinctive\nincrease around the final part of threads (Fig. 3a (bottom) and Extended Data\nFig. 3). Note that a similar observation was made previously^41, but referring\nonly to Reddit. Our findings challenge the assumption that toxicity\ndiscourages people from participating in a conversation, even though this\nnotion is part of the definition of toxicity used by the detection tool. This\ncan be seen by checking the relationship between trends in user participation,\na quantity related to the number of users in a discussion at some point, and\ntoxicity. The fact that the former typically decreases while the latter\nremains stable during conversations indicates that toxicity is not associated\nwith participation in conversations (an example is shown in Fig. 3a; box plots\nof the slopes of participation and toxicity for the whole dataset are shown in\nFig. 3b). This suggests that, on average, people may leave discussions\nregardless of the toxicity of the exchanges. We calculated the Pearson\u2019s\ncorrelation between user participation and toxicity trends for each dataset to\nsupport this hypothesis. As shown in Fig. 3d, the resulting correlation\ncoefficients are very heterogeneous, indicating no consistent pattern across\ndifferent datasets. To further validate this analysis, we tested the\ndifferences in the participation of users commenting on either toxic or non-\ntoxic conversations. To split such conversations into two disjoint sets, we\nfirst compute the toxicity distribution T_i of long threads in each dataset i,\nand we then label a conversation j in dataset i as toxic if it has toxicity\nt_ij \u2265 \u03bc(T_i) + \u03c3(T_i), with \u03bc(T_i) being mean and \u03c3(T_i) the standard\ndeviation of T_i; all of the other conversations are considered to be non-\ntoxic. After splitting the threads, for each dataset, we compute the Pearson\u2019s\ncorrelation of user participation between sets to find strongly positive\nvalues of the coefficient in all cases (Fig. 3c,e). This result is also\nconfirmed by a different analysis of which the results are reported in\nSupplementary Table 8, in which no significant difference between slopes in\ntoxic and non-toxic threads can be found. Thus, user behaviour in toxic and\nnon-toxic conversations shows almost identical patterns in terms of\nparticipation. This reinforces our finding that toxicity, on average, does not\nappear to affect the likelihood of people participating in a conversation.\nThese analyses were repeated with a lower toxicity classification threshold\n(Extended Data Fig. 5) and on additional datasets (Supplementary Fig. 2 and\nSupplementary Table 11), finding consistent results.\n\nFig. 3: Participation of users is not dependent on toxicity.\n\na, Examples of a typical trend in averaged user participation (top) and\ntoxicity (bottom) versus the normalized position of comment intervals in the\nthreads (Twitter news dataset). b, Box plot distributions of toxicity (n = 25,\nminimum = \u22120.012, maximum = 0.015, lower whisker = \u22120.012, quartile 1 (Q1) = \u2212\n0.004, Q2 = 0.002, Q3 = 0.008, upper whisker = 0.015) and participation (n =\n25, minimum = \u22120.198, maximum = \u22120.022, lower whisker = \u22120.198, Q1 = \u2212 0.109,\nQ2 = \u2212 0.071, Q3 = \u2212 0.049, upper whisker = \u22120.022) trend slopes for all\ndatasets, as resulting from linear regression. c, An example of user\nparticipation in toxic and non-toxic thread sets (Twitter news dataset). d,\nPearson\u2019s correlation coefficients between user participation and toxicity\ntrends for each dataset. e, Pearson\u2019s correlation coefficients between user\nparticipation in toxic and non-toxic threads for each dataset.\n\nFull size image\n\n## Controversy and toxicity\n\nIn this section, we aim to explore why people participate in toxic online\nconversations and why longer discussions tend to be more toxic. Several\nfactors could be the subject matter. First, controversial topics might lead to\nlonger, more heated debates with increased toxicity. Second, the endorsement\nof toxic content by other users may act as an incentive to increase the\ndiscussion\u2019s toxicity. Third, engagement peaks, due to factors such as reduced\ndiscussion focus or the intervention of trolls, may bring a higher share of\ntoxic exchanges. Pursuing this line of inquiry, we identified proxies to\nmeasure the level of controversy in conversations and examined how these\nrelate to toxicity and conversation size. Concurrently, we investigated the\nrelationship between toxicity, endorsement and engagement.\n\nAs shown previously^24,42, controversy is likely to emerge when people with\nopposing views engage in the same debate. Thus, the presence of users with\ndiverse political leanings within a conversation could be a valid proxy for\nmeasuring controversy. We operationalize this definition as follows.\nExploiting the peculiarities of our data, we can infer the political leaning\nof a subset of users in the Facebook News, Twitter News, Twitter Vaccines and\nGab Feed datasets. This is achieved by examining the endorsement, for example,\nin the form of likes, expressed towards news outlets of which the political\ninclinations have been independently assessed by news rating agencies (see the\n\u2018Polarization and user leaning attribution\u2019 section of the Methods). Extended\nData Table 3 shows a breakdown of the datasets. As a result, we label users\nwith a leaning score l \u2208 [\u22121, 1], \u22121 being left leaning and +1 being right\nleaning. We then select threads with at least ten different labelled users, in\nwhich at least 10% of comments (with a minimum of 20) are produced by such\nusers and assign to each of these comments the same leaning score of those who\nposted them. In this setting, the level of controversy within a conversation\nis assumed to be captured by the spread of the political leaning of the\nparticipants in the conversation. A natural way for measuring such a spread is\nthe s.d. \u03c3(l) of the distribution of comments possessing a leaning score: the\nhigher the \u03c3(l), the greater the level of ideological disagreement and\ntherefore controversy in a thread. We analysed the relationship between\ncontroversy and toxicity in online conversations of different sizes. Figure 4a\nshows that controversy increases with the size of conversations in all\ndatasets, and its trends are positively correlated with the corresponding\ntrends in toxicity (Extended Data Table 3). This supports our hypothesis that\ncontroversy and toxicity are closely related in online discussions.\n\nFig. 4: Controversy and toxicity in conversations.\n\na, The mean controversy (\u03c3(l)) and mean toxicity versus thread size (log-\nbinned and normalized) for the Facebook news, Twitter news, Twitter vaccines\nand Gab feed datasets. Here toxicity is calculated in the same conversations\nin which controversy could be computed (Extended Data Table 3); the relative\nPearson\u2019s, Spearman\u2019s and Kendall\u2019s correlation coefficients are also provided\nin Extended Data Table 3. Trends are reported with their 95% confidence\ninterval. b, Likes/upvotes versus toxicity (linearly binned). c, An example\n(Voat politics dataset) of the distributions of the frequency of toxic\ncomments in threads before (n = 2,201, minimum = 0, maximum = 1, lower whisker\n= 0, Q1 = 0, Q2 = 0.15, Q3 = 0.313, upper whisker = 0.769) at the peak (n =\n2,798, minimum = 0, maximum = 0.8, lower whisker = 0, Q1 = 0.125, Q2 = 0.196,\nQ3 = 0.282, upper whisker = 0.513) and after the peak (n = 2,791, minimum = 0,\nmaximum = 1, lower whisker = 0, Q1 = 0.129, Q2 = 0.200, Q3 = 0.282, upper\nwhisker = 0.500) of activity, as detected by Kleinberg\u2019s burst detection\nalgorithm.\n\nFull size image\n\nAs a complementary analysis, we draw on previous results^43. In that study,\nusing a definition of controversy operationally different but conceptually\nrelated to ours, a link was found between a greater degree of controversy of a\ndiscussion topic and a wider distribution of sentiment scores attributed to\nthe set of its posts and comments. We quantified the sentiment of comments\nusing a pretrained BERT model available from Hugging Face^44, used also in\nprevious studies^45. The model predicts the sentiment of a sentence through a\nscoring system ranging from 1 (negative) to 5 (positive). We define the\nsentiment attributed to a comment c as its weighted mean \\\\(s(c)=\\sum\n_{i=1.5}{x}_{i}{p}_{i}\\\\), where x_i \u2208 [1, 5] is the output score from the\nmodel and p_i is the probability associated to that value. Moreover, we\nnormalize the sentiment score s for each dataset between 0 and 1. We observe\nthe trends of the mean s.d. of sentiment in conversations, \\\\(\\bar{\\sigma\n}(s)\\\\), and toxicity are positively correlated for moderated platforms such\nas Facebook and Twitter but are negatively correlated on Gab (Extended Data\nTable 3). The positive correlation observed in Facebook and Twitter indicates\nthat greater discrepancies in sentiment of the conversations can, in general,\nbe linked to toxic conversations and vice versa. Instead, on unregulated\nplatforms such as Gab, highly conflicting sentiments seem to be more likely to\nemerge in less toxic conversations.\n\nAs anticipated, another factor that may be associated with the emergence of\ntoxic comments is the endorsement they receive. Indeed, such positive\nreactions may motivate posting even more comments of the same kind. Using the\nmean number of likes/upvotes as a proxy of endorsement, we have an indication\nthat this may not be the case. Figure 4b shows that the trend in likes/upvotes\nversus comments toxicity is never increasing past the toxicity score threshold\n(0.6).\n\nFinally, to complement our analysis, we inspect the relationship between\ntoxicity and user engagement within conversations, measured as the intensity\nof the number of comments over time. To do so, we used a method for burst\ndetection^46 that, after reconstructing the density profile of a temporal\nstream of elements, separates the stream into different levels of intensity\nand assigns each element to the level to which it belongs (see the \u2018Burst\nanalysis\u2019 section of the Methods). We computed the fraction of toxic comments\nat the highest intensity level of each conversation and for the levels right\nbefore and after it. By comparing the distributions of the fraction of toxic\ncomments for the three intervals, we find that these distributions are\nstatistically different in almost all cases (Fig. 4c and Extended Data Table\n4). In all datasets but one, distributions are consistently shifted towards\nhigher toxicity at the peak of engagement, compared with the previous phase.\nLikewise, in most cases, the peak shows higher toxicity even if compared to\nthe following phase, which in turn is mainly more toxic than the phase before\nthe peak. These results suggest that toxicity is likely to increase together\nwith user engagement.\n\n## Discussion\n\nHere we examine one of the most prominent and persistent characteristics\nonline discussions\u2014toxic behaviour, defined here as rude, disrespectful or\nunreasonable conduct. Our analysis suggests that toxicity is neither a\ndeterrent to user involvement nor an engagement amplifier; rather, it tends to\nemerge when exchanges become more frequent and may be a product of opinion\npolarization. Our findings suggest that the polarization of user\nopinions\u2014intended as the degree of opposed partisanship of users in a\nconversation\u2014may have a more crucial role than toxicity in shaping the\nevolution of online discussions. Thus, monitoring polarization could indicate\nearly interventions in online discussions. However, it is important to\nacknowledge that the dynamics at play in shaping online discourse are probably\nmultifaceted and require a nuanced approach for effective moderation. Other\nfactors may influence toxicity and engagement, such as the specific subject of\nthe conversation, the presence of influential users or \u2018trolls\u2019, the time and\nday of posting, as well as cultural or demographic aspects, such as user\naverage age or geographical location. Furthermore, even though extremely toxic\nusers are rare (Extended Data Fig. 2), the relationship between participation\nand toxicity of a discussion may in principle be affected also by small groups\nof highly toxic and engaged users driving the conversation dynamics. Although\nthe analysis of such subtler aspects is beyond the scope of this Article, they\nare certainly worth investigating in future research.\n\nHowever, when people encounter views that contradict their own, they may react\nwith hostility and contempt, consistent with previous research^47. In turn, it\nmay create a cycle of negative emotions and behaviours that fuels toxicity. We\nalso show that some online conversation features have remained consistent over\nthe past three decades despite the evolution of platforms and social norms.\n\nOur study has some limitations that we acknowledge and discuss. First, we use\npolitical leaning as a proxy for general leaning, which may capture only some\nof the nuances of online opinions. However, political leaning represents a\nbroad spectrum of opinions across different topics, and it correlates well\nwith other dimensions of leaning, such as news preferences, vaccine attitudes\nand stance on climate change^48,49. We could not assign a political leaning to\nusers to analyse controversies on all platforms. Still, those\nconsidered\u2014Facebook, Gab and Twitter\u2014represent different populations and\nmoderation policies, and the combined data account for nearly 90% of the\ncontent in our entire dataset. Our analysis approach is based on breadth and\nheterogeneity. As such, it may raise concerns about potential reductionism due\nto the comparison of different datasets from different sources and time\nperiods. We acknowledge that each discussion thread, platform and context has\nunique characteristics and complexities that might be diminished when\nhomogenizing data. However, we aim not to capture the full depth of every\ndiscussion but to identify and highlight general patterns and trends in online\ntoxicity across platforms and time. The quantitative approach used in our\nstudy is similar to numerous other studies^15 and enables us to uncover these\noverarching principles and patterns that may otherwise remain hidden. Of\ncourse, it is not possible to account for the behaviours of passive users.\nThis entails, for example, that even if toxicity does not seem to make people\nleave conversations, it could still be a factor that discourages them from\njoining them. Our study leverages an extensive dataset to examine the\nintricate relationship between persistent online human behaviours and the\ncharacteristics of different social media platforms. Our findings challenge\nthe prevailing assumption by demonstrating that toxic content, as\ntraditionally defined, does not necessarily reduce user engagement, thereby\nquestioning the assumed direct correlation between toxic content and negative\ndiscourse dynamics. This highlights the necessity for a detailed examination\nof the effect of toxic interactions on user behaviour and the quality of\ndiscussions across various platforms. Our results, showing user resilience to\ntoxic content, indicate the potential for creating advanced, context-aware\nmoderation tools that can accurately navigate the complex influence of\nantagonistic interactions on community engagement and discussion quality.\nMoreover, our study sets the stage for further exploration into the\ncomplexities of toxicity and its effect on engagement within online\ncommunities. Advancing our grasp of online discourse necessitates refining\ncontent moderation techniques grounded in a thorough understanding of human\nbehaviour. Thus, our research adds to the dialogue on creating more\nconstructive online spaces, promoting moderation approaches that are effective\nyet nuanced, facilitating engaging exchanges and reducing the tangible\nnegative effects of toxic behaviour.\n\nThrough the extensive dataset presented here, critical aspects of the online\nplatform ecosystem and fundamental dynamics of user interactions can be\nexplored. Moreover, we provide insights that a comparative approach such as\nthe one followed here can prove invaluable in discerning human behaviour from\nplatform-specific features. This may be used to investigate further sensitive\nissues, such as the formation of polarization and misinformation. The\nresulting outcomes have multiple potential impacts. Our findings reveal\nconsistent toxicity patterns across platforms, topics and time, suggesting\nthat future research in this field should prioritize the concept of\ninvariance. Recognizing that toxic behaviour is a widespread phenomenon that\nis not limited by platform-specific features underscores the need for a\nbroader, unified approach to understanding online discourse. Furthermore, the\nparticipation of users in toxic conversations suggests that a simple approach\nto removing toxic comments may not be sufficient to prevent user exposure to\nsuch phenomena. This indicates a need for more sophisticated moderation\ntechniques to manage conversation dynamics, including early interventions in\ndiscussions that show warnings of becoming toxic. Furthermore, our findings\nsupport the idea that examining content pieces in connection with others could\nenhance the effectiveness of automatic toxicity detection models. The observed\nhomogeneity suggests that models trained using data from one platform may also\nhave applicability to other platforms. Future research could explore further\ninto the role of controversy and its interaction with other elements\ncontributing to toxicity. Moreover, comparing platforms could enhance our\nunderstanding of invariant human factors related to polarization,\ndisinformation and content consumption. Such studies would be instrumental in\ncapturing the drivers of the effect of social media platforms on human\nbehaviour, offering valuable insights into the underlying dynamics of online\ninteractions.\n\n## Methods\n\n### Data collection\n\nIn our study, data collection from various social media platforms was\nstrategically designed to encompass various topics, ensuring maximal\nheterogeneity in the discussion themes. For each platform, where feasible, we\nfocus on gathering posts related to diverse areas such as politics, news,\nenvironment and vaccinations. This approach aims to capture a broad spectrum\nof discourse, providing a comprehensive view of conversation dynamics across\ndifferent content categories.\n\n#### Facebook\n\nWe use datasets from previous studies that covered discussions about\nvaccines^50, news^51 and brexit^52. For the vaccines topic, the resulting\ndataset contains around 2 million comments retrieved from public groups and\npages in a period that ranges from 2 January 2010 to 17 July 2017. For the\nnews topic, we selected a list of pages from the Europe Media Monitor that\nreported the news in English. As a result, the obtained dataset contains\naround 362 million comments between 9 September 2009 and 18 August 2016.\nFurthermore, we collect a total of about 4.5 billion likes that the users put\non posts and comments concerning these pages. Finally, for the brexit topic,\nthe dataset contains around 460,000 comments from 31 December 2015 to 29 July\n2016.\n\n#### Gab\n\nWe collect data from the Pushshift.io archive\n(https://files.pushshift.io/gab/) concerning discussions taking place from 10\nAugust 2016, when the platform was launched, to 29 October 2018, when Gab went\ntemporarily offline due to the Pittsburgh shooting^53. As a result, we collect\na total of around 14 million comments.\n\n#### Reddit\n\nData were collected from the Pushshift.io archive (https://pushshift.io/) for\nthe period ranging from 1 January 2018 to 31 December 2022. For each topic,\nwhenever possible, we manually identified and selected subreddits that best\nrepresented the targeted topics. As a result of this operation, we obtained\nabout 800,000 comments from the r/conspiracy subreddit for the conspiracy\ntopic. For the vaccines topic, we collected about 70,000 comments from the\nr/VaccineDebate subreddit, focusing on the COVID-19 vaccine debate. We\ncollected around 400,000 comments from the r/News subreddit for the news\ntopic. We collected about 70,000 comments from the r/environment subreddit for\nthe climate change topic. Finally, we collected around 550,000 comments from\nthe r/science subreddit for the science topic.\n\n#### Telegram\n\nWe created a list of 14 channels, associating each with one of the topics\nconsidered in the study. For each channel, we manually collected messages and\ntheir related comments. As a result, from the four channels associated with\nthe news topic (news notiziae, news ultimora, news edizionestraordinaria, news\ncovidultimora), we obtained around 724,000 comments from posts between 9 April\n2018 and 20 December 2022. For the politics topic, instead, the corresponding\ntwo channels (politics besttimeline, politics polmemes) produced a total of\naround 490,000 comments between 4 August 2017 and 19 December 2022. Finally,\nthe eight channels assigned to the conspiracy topic (conspiracy bennyjhonson,\nconspiracy tommyrobinsonnews, conspiracy britainsfirst, conspiracy\nloomeredofficial, conspiracy thetrumpistgroup, conspiracy trumpjr, conspiracy\npauljwatson, conspiracy iononmivaccino) produced a total of about 1.4 million\ncomments between 30 August 2019 and 20 December 2022.\n\n#### Twitter\n\nWe used a list of datasets from previous studies that includes discussions\nabout vaccines^54, climate change^49 and news^55 topics. For the vaccines\ntopic, we collected around 50 million comments from 23 January 2010 to 25\nJanuary 2023. For the news topic, we extend the dataset used previously^55 by\ncollecting all threads composed of less than 20 comments, obtaining a total of\nabout 9.5 million comments for a period ranging from 1 January 2020 to 29\nNovember 2022. Finally, for the climate change topic, we collected around 9.7\nmillion comments between 1 January 2020 and 10 January 2023.\n\n#### Usenet\n\nWe collected data for the Usenet discussion system by querying the Usenet\nArchive (https://archive.org/details/usenet?tab=about). We selected a list of\ntopics considered adequate to contain a large, broad and heterogeneous number\nof discussions involving active and populated newsgroups. As a result of this\nselection, we selected conspiracy, politics, news and talk as topic candidates\nfor our analysis. For the conspiracy topic, we collected around 280,000\ncomments between 1 September 1994 and 30 December 2005 from the alt.conspiracy\nnewsgroup. For the politics topics, we collected around 2.6 million comments\nbetween 29 June 1992 and 31 December 2005 from the alt.politics newsgroup. For\nthe news topic, we collected about 620,000 comments between 5 December 1992\nand 31 December 2005 from the alt.news newsgroup. Finally, for the talk topic,\nwe collected all of the conversations from the homonym newsgroup on a period\nthat ranges from 13 February 1989 to 31 December 2005 for around 2.1 million\ncontents.\n\n#### Voat\n\nWe used a dataset presented previously^56 that covers the entire lifetime of\nthe platform, from 9 January 2018 to 25 December 2020, including a total of\naround 16.2 million posts and comments shared by around 113,000 users in about\n7,100 subverses (the equivalent of a subreddit for Voat). Similarly to\nprevious platforms, we associated the topics to specific subverses. As a\nresult of this operation, for the conspiracy topic, we collected about 1\nmillion comments from the greatawakening subverse between 9 January 2018 and\n25 December 2020. For the politics topic, we collected around 1 million\ncomments from the politics subverse between 16 June 2014 and 25 December 2020.\nFinally, for the news topic, we collected about 1.4 million comments from the\nnews subverse between 21 November 2013 and 25 December 2020.\n\n#### YouTube\n\nWe used a dataset proposed in previous studies that collected conversations\nabout the climate change topic^49, which is extended, coherently with previous\nplatforms, by including conversations about vaccines and news topics. The data\ncollection process for YouTube is performed using the YouTube Data API\n(https://developers.google.com/youtube/v3). For the climate change topic, we\ncollected around 840,000 comments between 16 March 2014 and 28 February 2022.\nFor the vaccines topic, we collected conversations between 31 January 2020 and\n24 October 2021 containing keywords about COVID-19 vaccines, namely Sinopharm,\nCanSino, Janssen, Johnson&Johnson, Novavax, CureVac, Pfizer, BioNTech,\nAstraZeneca and Moderna. As a result of this operation, we gathered a total of\naround 2.6 million comments to videos. Finally, for the news topic, we\ncollected about 20 million comments between 13 February 2006 and 8 February\n2022, including videos and comments from a list of news outlets, limited to\nthe UK and provided by Newsguard (see the \u2018Polarization and user leaning\nattribution\u2019 section).\n\n### Content moderation policies\n\nContent moderation policies are guidelines that online platforms use to\nmonitor the content that users post on their sites. Platforms have different\ngoals and audiences, and their moderation policies may vary greatly, with some\nplacing more emphasis on free expression and others prioritizing safety and\ncommunity guidelines.\n\nFacebook and YouTube have strict moderation policies prohibiting hate speech,\nviolence and harassment^57. To address harmful content, Facebook follows a\n\u2018remove, reduce, inform\u2019 strategy and uses a combination of human reviewers\nand artificial intelligence to enforce its policies^58. Similarly, YouTube has\na similar set of community guidelines regarding hate speech policy, covering a\nwide range of behaviours such as vulgar language^59, harassment^60 and, in\ngeneral, does not allow the presence of hate speech and violence against\nindividuals or groups based on various attributes^61. To ensure that these\nguidelines are respected, the platform uses a mix of artificial intelligence\nalgorithms and human reviewers^62.\n\nTwitter also has a comprehensive content moderation policy and specific rules\nagainst hateful conduct^63,64. They use automation^65 and human review in the\nmoderation process^66. At the date of submission, Twitter\u2019s content policies\nhave remained unchanged since Elon Musk\u2019s takeover, except that they ceased\nenforcing their COVID-19 misleading information policy on 23 November 2022.\nTheir policy enforcement has faced criticism for inconsistency^67.\n\nReddit falls somewhere in between regarding how strict its moderation policy\nis. Reddit\u2019s content policy has eight rules, including prohibiting violence,\nharassment and promoting hate based on identity or vulnerability^68,69. Reddit\nrelies heavily on user reports and volunteer moderators. Thus, it could be\nconsidered more lenient than Facebook, YouTube and Twitter regarding enforcing\nrules. In October 2022, Reddit announced that they intend to update their\nenforcement practices to apply automation in content moderation^70.\n\nBy contrast, Telegram, Gab and Voat take a more hands-off approach with fewer\nrestrictions on content. Telegram has ambiguity in its guidelines, which\narises from broad or subjective terms and can lead to different\ninterpretations^71. Although they mentioned they may use automated algorithms\nto analyse messages, Telegram relies mainly on users to report a range of\ncontent, such as violence, child abuse, spam, illegal drugs, personal details\nand pornography^72. According to Telegram\u2019s privacy policy, reported content\nmay be checked by moderators and, if it is confirmed to violate their terms,\ntemporary or permanent restrictions may be imposed on the account^73. Gab\u2019s\nTerms of Service allow all speech protected under the First Amendment to the\nUS Constitution, and unlawful content is removed. They state that they do not\nreview material before it is posted on their website and cannot guarantee\nprompt removal of illegal content after it has been posted^74. Voat was once\nknown as a \u2018free-speech\u2019 alternative to Reddit and allowed content even if it\nmay be considered offensive or controversial^56.\n\nUsenet is a decentralized online discussion system created in 1979. Owing to\nits decentralized nature, Usenet has been difficult to moderate effectively,\nand it has a reputation for being a place where controversial and even illegal\ncontent can be posted without consequence. Each individual group on Usenet can\nhave its own moderators, who are responsible for monitoring and enforcing\ntheir group\u2019s rules, and there is no single set of rules that applies to the\nentire platform^75.\n\n### Logarithmic binning and conversation size\n\nOwing to the heavy-tailed distributions of conversation length (Extended Data\nFig. 1), to plot the figures and perform the analyses, we used logarithmic\nbinning. Thus, according to its length, each thread of each dataset is\nassigned to 1 out of 21 bins. To ensure a minimal number of points in each\nbin, we iteratively change the left bound of the last bin so that it contains\nat least N = 50 elements (we set N = 100 in the case of Facebook news, due to\nits larger size). Specifically, considering threads ordered in increasing\nlength, the size of the largest thread is changed to that of the second last\nlargest one, and the binning is recalculated accordingly until the last bin\ncontains at least N points.\n\nFor visualization purposes, we provide a normalization of the logarithmic\nbinning outcome that consists of mapping discrete points into coordinates of\nthe x axis such that the bins correspond to {0, 0.05, 0.1, ..., 0.95, 1}.\n\nTo perform the part of the analysis, we select conversations belonging to the\n[0.7, 1] interval of the normalized logarithmic binning of thread length. This\ninterval ensures that the conversations are sufficiently long and that we have\na substantial number of threads. Participation and toxicity trends are\nobtained by applying to such conversations a linear binning of 21 elements to\na chronologically ordered sequence of comments, that is, threads. A breakdown\nof the resulting datasets is provided in Supplementary Table 2.\n\nFinally, to assess the equality of the growth rates of participation values in\ntoxic and non-toxic threads (see the \u2018Conversation evolution and toxicity\u2019\nsection), we implemented the following linear regression model:\n\n$${\\rm{p}}{\\rm{a}}{\\rm{r}}{\\rm{t}}{\\rm{i}}{\\rm{c}}{\\rm{i}}{\\rm{p}}{\\rm{a}}{\\rm{t}}{\\rm{i}}{\\rm{o}}{\\rm{n}}={\\beta\n}_{0}+{\\beta }_{1}\\cdot {\\rm{b}}{\\rm{i}}{\\rm{n}}+{\\beta }_{2}\\cdot\n\\,({\\rm{b}}{\\rm{i}}{\\rm{n}}\\cdot\n{\\rm{i}}{\\rm{s}}{\\rm{T}}{\\rm{o}}{\\rm{x}}{\\rm{i}}{\\rm{c}}),$$\n\nwhere the term \u03b2_2 accounts for the effect that being a toxic conversation has\non the growth of participation. Our results show that \u03b2_2 is not significantly\ndifferent from 0 in most original and validation datasets (Supplementary\nTables 8 and 11)\n\n### Toxicity detection and validation of the models used\n\nThe problem of detecting toxicity is highly debated, to the point that there\nis currently no agreement on the very definition of toxic speech^64,76. A\ntoxic comment can be regarded as one that includes obscene or derogatory\nlanguage^32, that uses harsh, abusive language and personal attacks^33, or\ncontains extremism, violence and harassment^11, just to give a few examples.\nEven though toxic speech should, in principle, be distinguished from hate\nspeech, which is commonly more related to targeted attacks that denigrate a\nperson or a group on the basis of attributes such as race, religion, gender,\nsex, sexual orientation and so on^77, it sometimes may also be used as an\numbrella term^78,79. This lack of agreement directly reflects the challenging\nand inherent subjective nature of the concept of toxicity. The complexity of\nthe topic makes it particularly difficult to assess the reliability of natural\nlanguage processing models for automatic toxicity detection despite the\nimpressive improvements in the field. Modern natural language processing\nmodels, such as Perspective API, are deep learning models that leverage word-\nembedding techniques to build representations of words as vectors in a high-\ndimensional space, in which a metric distance should reflect the conceptual\ndistance among words, therefore providing linguistic context. A primary\nconcern regarding toxicity detection models is their limited ability to\ncontextualize conversations^11,80. These models often struggle to incorporate\nfactors beyond the text itself, such as the participant\u2019s personal\ncharacteristics, motivations, relationships, group memberships and the overall\ntone of the discussion^11. Consequently, what is considered to be toxic\ncontent can vary significantly among different groups, such as ethnicities or\nage groups^81, leading to potential biases. These biases may stem from the\nannotators\u2019 backgrounds and the datasets used for training, which might not\nadequately represent cultural heterogeneity. Moreover, subtle forms of toxic\ncontent, like indirect allusions, memes and inside jokes targeted at specific\ngroups, can be particularly challenging to detect. Word embeddings equip\ncurrent classifiers with a rich linguistic context, enhancing their ability to\nrecognize a wide range of patterns characteristic of toxic expression.\nHowever, the requirements for understanding the broader context of a\nconversation, such as personal characteristics, motivations and group\ndynamics, remain beyond the scope of automatic detection models. We\nacknowledge these inherent limitations in our approach. Nonetheless, reliance\non automatic detection models is essential for large-scale analyses of online\ntoxicity like the one conducted in this study. We specifically resort to the\nPerspective API for this task, as it represents state-of-the-art automatic\ntoxicity detection, offering a balance between linguistic nuance and scalable\nanalysis capabilities. To define an appropriate classification threshold, we\ndraw from the existing literature^64, which uses 0.6 as the threshold for\nconsidering a comment to be toxic. This threshold can also be considered a\nreasonable one as, according to the developer guidelines offered by\nPerspective, it would indicate that the majority of the sample of readers,\nnamely 6 out of 10, would perceive that comment as toxic. Due to the\nlimitations mentioned above (for a criticism of Perspective API, see ref.\n^82), we validate our results by performing a comparative analysis using two\nother toxicity detectors: Detoxify (https://github.com/unitaryai/detoxify),\nwhich is similar to Perspective, and IMSYPP, a classifier developed for a\nEuropean Project on hate speech^16 (https://huggingface.co/IMSyPP). In\nSupplementary Table 14, the percentages of agreement among the three models in\nclassifying 100,000 comments taken randomly from each of our datasets are\nreported. For Detoxify we used the same binary toxicity threshold (0.6) as\nused with Perspective. Although IMSYPP operates on a distinct definition of\ntoxicity as outlined previously^16, our comparative analysis shows a general\nagreement in the results. This alignment, despite the differences in\nunderlying definitions and methodologies, underscores the robustness of our\nfindings across various toxicity detection frameworks. Moreover, we perform\nthe core analyses of this study using all classifiers on a further, vast and\nheterogeneous dataset. As shown in Supplementary Figs. 1 and 2, the results\nregarding toxicity increase with conversation size and user participation and\ntoxicity are quantitatively very similar. Furthermore, we verify the stability\nof our findings under different toxicity thresholds. Although the main\nanalyses in this paper use the threshold value recommended by the Perspective\nAPI, set at 0.6, to minimize false positives, our results remain consistent\neven when applying a less conservative threshold of 0.5. This is demonstrated\nin Extended Data Fig. 5, confirming the robustness of our observations across\nvarying toxicity levels. For this study, we used the API support for languages\nprevalent in the European and American continents, including English, Spanish,\nFrench, Portuguese, German, Italian, Dutch, Polish, Swedish and Russian.\nDetoxify also offers multilingual support. However, IMSYPP is limited to\nEnglish and Italian text, a factor considered in our comparative analysis.\n\n### Polarization and user leaning attribution\n\nOur approach to measuring controversy in a conversation is based on estimating\nthe degree of political partisanship among the participants. This measure is\nclosely related to the political science concept of political polarization.\nPolitical polarization is the process by which political attitudes diverge\nfrom moderate positions and gravitate towards ideological extremes, as\ndescribed previously^83. By quantifying the level of partisanship within\ndiscussions, we aim to provide insights into the extent and nature of\npolarization in online debates. In this context, it is important to\ndistinguish between \u2018ideological polarization\u2019 and \u2018affective polarization\u2019.\nIdeological polarization refers to divisions based on political viewpoints. By\ncontrast, affective polarization is characterized by positive emotions towards\nmembers of one\u2019s group and hostility towards those of opposing groups^84,85.\nHere we focus specifically on ideological polarization. The subsequent\ndescription of our procedure for attributing user political leanings will\nfurther clarify this focus. On online social media, the individual leaning of\na user toward a topic can be inferred through the content produced or the\nendorsement shown toward specific content. In this study, we consider the\nendorsement of users to news outlets of which the political leaning has been\nevaluated by trustworthy external sources. Although not without\nlimitations\u2014which we address below\u2014this is a standard approach that has been\nused in several studies, and has become a common and established practice in\nthe field of social media analysis due to its practicality and effectiveness\nin providing a broad understanding of political dynamics on these online\nplatforms^1,43,86,87,88. We label news outlets with a political score based on\nthe information reported by Media Bias/Fact Check (MBFC)\n(https://mediabiasfactcheck.com), integrating with the equivalent information\nfrom Newsguard (https://www.newsguardtech.com/). MBFC is an independent fact-\nchecking organization that rates news outlets on the basis of the reliability\nand the political bias of the content that they produce and share. Similarly,\nNewsguard is a tool created by an international team of journalists that\nprovides news outlet trust and political bias scores. Following standard\nmethods used in the literature^1,43, we calculated the individual leaning of a\nuser l \u2208 [\u22121, 1] as the average of the leaning scores l_c \u2208 [\u22121, 1] attributed\nto each of the content it produced/shared, where l_c results from a mapping of\nthe news organizations political scores provided by MBFC and Newsguard,\nrespectively: [left, centre-left, centre, centre-right, right] to [\u22121, \u2212 0.5,\n0, 0.5, 1], and [far left, left, right, far right] to [\u22121, \u22120.5, 0.5, 1]). Our\ndatasets have different structures, so we have to evaluate user leanings in\ndifferent ways. For Facebook News, we assign a leaning score to users who\nposted a like at least three times and commented at least three times under\nnews outlet pages that have a political score. For Twitter News, a leaning is\nassigned to users who posted at least 15 comments under scored news outlet\npages. For Twitter Vaccines and Gab, we consider users who shared content\nproduced by scored news outlet pages at least three times. A limitation of our\napproach is that engaging with politically aligned content does not always\nimply agreement; users may interact with opposing viewpoints for critical\ndiscussion. However, research indicates that users predominantly share content\naligning with their own views, especially in politically charged\ncontexts^87,89,90. Moreover, our method captures users who actively express\ntheir political leanings, omitting the \u2018passive\u2019 ones. This is due to the lack\nof available data on users who do not explicitly state their opinions.\nNevertheless, analysing active users offers valuable insights into the\ndiscourse of those most engaged and influential on social media platforms.\n\n### Burst analysis\n\nWe used the Kleinberg burst detection algorithm^46 (see the \u2018Controversy and\ntoxicity\u2019 section) to all conversations with at least 50 comments in a\ndataset. In our analysis, we randomly sample up to 5,000 conversations, each\ncontaining a specific number of comments. To ensure the reliability of our\ndata, we exclude conversations with an excessive number of double\ntimestamps\u2014defined as more than 10 consecutive or over 100 within the first 24\nh. This criterion helps to mitigate the influence of bots, which could distort\nthe patterns of human activity. Furthermore, we focus on the first 24 h of\neach thread to analyse streams of comments during their peak activity period.\nConsequently, Usenet was excluded from our study. The unique usage\ncharacteristics of Usenet render such a time-constrained analysis\ninappropriate, as its activity patterns do not align with those of the other\nplatforms under consideration. By reconstructing the density profile of the\ncomment stream, the algorithm divides the entire stream\u2019s interval into\nsubintervals on the basis of their level of intensity. Labelled as discrete\npositive values, higher levels of burstiness represent higher activity\nsegments. To avoid considering flat-density phases, threads with a maximum\nburst level equal to 2 are excluded from this analysis. To assess whether a\nhigher intensity of comments results in a higher comment toxicity, we perform\na Mann\u2013Whitney U-test^91 with Bonferroni correction for multiple testing\nbetween the distributions of the fraction of toxic comments t_i in three\nintensity phases: during the peak of engagement and at the highest levels\nbefore and after. Extended Data Table 4 shows the corrected P values of each\ntest, at a 0.99 confidence level, with H1 indicated in the column header. An\nexample of the distribution of the frequency of toxic comments in threads at\nthe three phases of a conversation considered (pre-peak, peak and post-peak)\nis reported in Fig. 4c.\n\n### Toxicity detection on Usenet\n\nAs discussed in the section on toxicity detection and the Perspective API\nabove, automatic detectors derive their understanding of toxicity from the\nannotated datasets that they are trained on. The Perspective API is\npredominantly trained on recent texts, and its human labellers conform to\ncontemporary cultural norms. Thus, although our dataset dates back to no more\nthan the early 1990s, we provide a discussion on the viability of the\napplication of Perspective API to Usenet and validation analysis. Contemporary\nsociety, especially in Western contexts, is more sensitive to issues of\ntoxicity, including gender, race and sexual orientation, compared with a few\ndecades ago. This means that some comments identified as toxic today,\nincluding those from older platforms like Usenet, might not have been\nconsidered as such in the past. However, this discrepancy does not\nsignificantly affect our analysis, which is centred on current standards of\ntoxicity. On the other hand, changes in linguistic features may have some\nrepercussions: there may be words and locutions that were frequently used in\nthe 1990s that instead appear sparsely in today\u2019s language, making Perspective\npotentially less effective in classifying short texts that contain them. We\ntherefore proceeded to evaluate the impact that such a possible scenario could\nhave on our results. In light of the above considerations, we consider texts\nlabelled as toxic as correctly classified; instead, we assume that there is a\nfixed probability p that a comment may be incorrectly labelled as non-toxic.\nConsequently, we randomly designate a proportion p of non-toxic comments,\nrelabel them as toxic and compute the toxicity versus conversation size trend\n(Fig. 2) on the altered dataset across various p. Specifically, for each\nvalue, we simulate 500 different trends, collecting their regression slopes to\nobtain a null distribution for them. To assess if the probability of error\ncould lead to significant differences in the observed trend, we compute the\nfraction f of slopes lying outside the interval (\u2212|s|,|s|), where s is the\nslope of the observed trend. We report the result in Supplementary Table 9 for\ndifferent values of p. In agreement with our previous analysis, we assume that\nthe slope differs significantly from the ones obtained from randomized data if\nf is less than 0.05.\n\nWe observed that only the Usenet Talk dataset shows sensitivity to small error\nprobabilities, and the others do not show a significant difference.\nConsequently, our results indicate that Perspective API is suitable for\napplication to Usenet data in our analyses, notwithstanding the potential\nlinguistic and cultural shifts that might affect the classifier\u2019s reliability\nwith older texts.\n\n### Toxicity of short conversations\n\nOur study focuses on the relationship between user participation and the\ntoxicity of conversations, particularly in engaged or prolonged discussions. A\npotential concern is that concentrating on longer threads overlooks\nconversations that terminate quickly due to early toxicity, therefore\npotentially biasing our analysis. To address this, we analysed shorter\nconversations, comprising 6 to 20 comments, in each dataset. In particular, we\ncomputed the distributions of toxicity scores of the first and last three\ncomments in each thread. This approach helps to ensure that our analysis\naccounts for a range of conversation lengths and patterns of toxicity\ndevelopment, providing a more comprehensive understanding of the dynamics at\nplay. As shown in Supplementary Fig. 3, for each dataset, the distributions of\nthe toxicity scores display high similarity, meaning that, in short\nconversations, the last comments are not significantly more toxic than the\ninitial ones, indicating that the potential effects mentioned above do not\nundermine our conclusions. Regarding our analysis of longer threads, we notice\nhere that the participation quantity can give rise to similar trends in\nvarious cases. For example, high participation can be achieved because many\nusers take part in the conversation, but also with small groups of users in\nwhich everyone is equally contributing over time. Or, in very large\ndiscussions, the contributions of individual outliers may remain hidden. By\nmeasuring participation, these and other borderline cases may not be distinct\nfrom the statistically highly likely discussion dynamics but, ultimately, this\nlack of discriminatory power does not have any implications on our findings\nnor on the validity of the conclusions that we draw.\n\n### Reporting summary\n\nFurther information on research design is available in the Nature Portfolio\nReporting Summary linked to this article.\n\n## Data availability\n\nFacebook, Twitter and YouTube data are made available in accordance with their\nrespective terms of use. IDs of comments used in this work are provided at\nOpen Science Framework (https://doi.org/10.17605/osf.io/fq5dy). For the\nremaining platforms (Gab, Reddit, Telegram, Usenet and Voat), all of the\nnecessary information to recreate the datasets used in this study can be found\nin the \u2018Data collection\u2019 section.\n\n## Code availability\n\nThe code used for the analyses presented in the Article is available at Open\nScience Framework (https://doi.org/10.17605/osf.io/fq5dy). The repository\nincludes dummy datasets to illustrate the required data format and make the\ncode run.\n\n## References\n\n  1. Cinelli, M., Morales, G. D. F., Galeazzi, A., Quattrociocchi, W. & Starnini, M. The echo chamber effect on social media. Proc. Natl Acad. Sci. USA 118, e2023301118 (2021).\n\nCAS PubMed PubMed Central Google Scholar\n\n  2. Tucker, J. A. et al. Social media, political polarization, and political disinformation: a review of the scientific literature. Preprint at SSRN https://doi.org/10.2139/ssrn.3144139 (2018).\n\n  3. Gonz\u00e1lez-Bail\u00f3n, S. et al. Asymmetric ideological segregation in exposure to political news on Facebook. Science 381, 392\u2013398 (2023).\n\nPubMed ADS Google Scholar\n\n  4. Guess, A. et al. How do social media feed algorithms affect attitudes and behavior in an election campaign? Science 381, 398\u2013404 (2023).\n\nCAS PubMed ADS Google Scholar\n\n  5. Del Vicario, M. et al. The spreading of misinformation online. Proc. Natl Acad. Sci. USA 113, 554\u2013559 (2016).\n\nPubMed PubMed Central ADS Google Scholar\n\n  6. Bakshy, E., Messing, S. & Adamic, L. A. Exposure to ideologically diverse news and opinion on Facebook. Science 348, 1130\u20131132 (2015).\n\nMathSciNet CAS PubMed ADS Google Scholar\n\n  7. Bail, C. A. et al. Exposure to opposing views on social media can increase political polarization. Proc. Natl Acad. Sci. USA 115, 9216\u20139221 (2018).\n\nCAS PubMed PubMed Central ADS Google Scholar\n\n  8. Nyhan, B. et al. Like-minded sources on Facebook are prevalent but not polarizing. Nature 620, 137\u2013144 (2023).\n\nCAS PubMed PubMed Central ADS Google Scholar\n\n  9. Guess, A. et al. Reshares on social media amplify political news but do not detectably affect beliefs or opinions. Science 381, 404\u2013408 (2023).\n\nCAS PubMed ADS Google Scholar\n\n  10. Casta\u00f1o-Pulga\u0155\u0131n, S. A., Su\u00e1rez-Betancur, N., Vega, L. M. T. & L\u00f3pez, H. M. H. Internet, social media and online hate speech. Systematic review. Aggress. Viol. Behav. 58, 101608 (2021).\n\nGoogle Scholar\n\n  11. Sheth, A., Shalin, V. L. & Kursuncu, U. Defining and detecting toxicity on social media: context and knowledge are key. Neurocomputing 490, 312\u2013318 (2022).\n\nGoogle Scholar\n\n  12. Lupu, Y. et al. Offline events and online hate. PLoS ONE 18, e0278511 (2023).\n\nCAS PubMed PubMed Central Google Scholar\n\n  13. Gentzkow, M. & Shapiro, J. M. Ideological segregation online and offline. Q. J. Econ. 126, 1799\u20131839 (2011).\n\nGoogle Scholar\n\n  14. Aichner, T., Gr\u00fcnfelder, M., Maurer, O. & Jegeni, D. Twenty-five years of social media: a review of social media applications and definitions from 1994 to 2019. Cyberpsychol. Behav. Social Netw. 24, 215\u2013222 (2021).\n\nGoogle Scholar\n\n  15. Lazer, D. M. et al. The science of fake news. Science 359, 1094\u20131096 (2018).\n\nCAS PubMed ADS Google Scholar\n\n  16. Cinelli, M. et al. Dynamics of online hate and misinformation. Sci. Rep. 11, 22083 (2021).\n\nPubMed PubMed Central ADS Google Scholar\n\n  17. Gonz\u00e1lez-Bail\u00f3n, S. & Lelkes, Y. Do social media undermine social cohesion? A critical review. Soc. Issues Pol. Rev. 17, 155\u2013180 (2023).\n\nGoogle Scholar\n\n  18. Roozenbeek, J. & Zollo, F. Democratize social-media research\u2014with access and funding. Nature 612, 404\u2013404 (2022).\n\nCAS PubMed Google Scholar\n\n  19. Dutton, W. H. Network rules of order: regulating speech in public electronic fora. Media Cult. Soc. 18, 269\u2013290 (1996).\n\nGoogle Scholar\n\n  20. Papacharissi, Z. Democracy online: civility, politeness, and the democratic potential of online political discussion groups. N. Media Soc. 6, 259\u2013283 (2004).\n\nGoogle Scholar\n\n  21. Coe, K., Kenski, K. & Rains, S. A. Online and uncivil? Patterns and determinants of incivility in newspaper website comments. J. Commun. 64, 658\u2013679 (2014).\n\nGoogle Scholar\n\n  22. Anderson, A. A., Brossard, D., Scheufele, D. A., Xenos, M. A. & Ladwig, P. The \u201cnasty effect:\u201d online incivility and risk perceptions of emerging technologies. J. Comput. Med. Commun. 19, 373\u2013387 (2014).\n\nGoogle Scholar\n\n  23. Garrett, R. K. Echo chambers online?: Politically motivated selective exposure among internet news users. J. Comput. Med. Commun. 14, 265\u2013285 (2009).\n\nGoogle Scholar\n\n  24. Del Vicario, M. et al. Echo chambers: emotional contagion and group polarization on Facebook. Sci. Rep. 6, 37825 (2016).\n\nPubMed PubMed Central ADS Google Scholar\n\n  25. Garimella, K., De Francisci Morales, G., Gionis, A. & Mathioudakis, M. Echo chambers, gatekeepers, and the price of bipartisanship. In Proc. 2018 World Wide Web Conference, 913\u2013922 (International World Wide Web Conferences Steering Committee, 2018).\n\n  26. Johnson, N. et al. Hidden resilience and adaptive dynamics of the global online hate ecology. Nature 573, 261\u2013265 (2019).\n\nCAS PubMed ADS Google Scholar\n\n  27. Fortuna, P. & Nunes, S. A survey on automatic detection of hate speech in text. ACM Comput. Surv. 51, 85 (2018).\n\n  28. Phadke, S. & Mitra, T. Many faced hate: a cross platform study of content framing and information sharing by online hate groups. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems 1\u201313 (Association for Computing Machinery, 2020).\n\n  29. Xia, Y., Zhu, H., Lu, T., Zhang, P. & Gu, N. Exploring antecedents and consequences of toxicity in online discussions: a case study on Reddit. Proc. ACM Hum. Comput. Interact. 4, 108 (2020).\n\n  30. Sipka, A., Hannak, A. & Urman, A. Comparing the language of qanon-related content on Parler, GAB, and Twitter. In Proc. 14th ACM Web Science Conference 2022 411\u2013421 (Association for Computing Machinery, 2022).\n\n  31. Fortuna, P., Soler, J. & Wanner, L. Toxic, hateful, offensive or abusive? What are we really classifying? An empirical analysis of hate speech datasets. In Proc. 12th Language Resources and Evaluation Conference (eds Calzolari, E. et al.) 6786\u20136794 (European Language Resources Association, 2020).\n\n  32. Davidson, T., Warmsley, D., Macy, M. & Weber, I. Automated hate speech detection and the problem of offensive language. In Proc. International AAAI Conference on Web and Social Media 11 (Association for the Advancement of Artificial Intelligence, 2017).\n\n  33. Kolhatkar, V. et al. The SFU opinion and comments corpus: a corpus for the analysis of online news comments. Corpus Pragmat. 4, 155\u2013190 (2020).\n\nPubMed Google Scholar\n\n  34. Lees, A. et al. A new generation of perspective API: efficient multilingual character-level transformers. In KDD'22: The 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining 3197\u20133207 (Association for Computing Machinery, 2022).\n\n  35. Vidgen, B. & Derczynski, L. Directions in abusive language training data, a systematic review: garbage in, garbage out. PLoS ONE 15, e0243300 (2020).\n\nCAS PubMed PubMed Central Google Scholar\n\n  36. Ross, G. J. & Jones, T. Understanding the heavy-tailed dynamics in human behavior. Phys. Rev. E 91, 062809 (2015).\n\nMathSciNet ADS Google Scholar\n\n  37. Choi, D., Chun, S., Oh, H., Han, J. & Kwon, T. T. Rumor propagation is amplified by echo chambers in social media. Sci. Rep. 10, 310 (2020).\n\nCAS PubMed PubMed Central ADS Google Scholar\n\n  38. Beel, J., Xiang, T., Soni, S. & Yang, D. Linguistic characterization of divisive topics online: case studies on contentiousness in abortion, climate change, and gun control. In Proc. International AAAI Conference on Web and Social Media Vol. 16, 32\u201342 (Association for the Advancement of Artificial Intelligence, 2022).\n\n  39. Saveski, M., Roy, B. & Roy, D. The structure of toxic conversations on Twitter. In Proc. Web Conference 2021 (eds Leskovec, J. et al.) 1086\u20131097 (Association for Computing Machinery, 2021).\n\n  40. Juul, J. L. & Ugander, J. Comparing information diffusion mechanisms by matching on cascade size. Proc. Natl Acad. Sci. USA 118, e2100786118 (2021).\n\nCAS PubMed PubMed Central Google Scholar\n\n  41. Fariello, G., Jemielniak, D. & Sulkowski, A. Does Godwin\u2019s law (rule of Nazi analogies) apply in observable reality? An empirical study of selected words in 199 million Reddit posts. N. Media Soc. 26, 14614448211062070 (2021).\n\n  42. Qiu, J., Lin, Z. & Shuai, Q. Investigating the opinions distribution in the controversy on social media. Inf. Sci. 489, 274\u2013288 (2019).\n\nGoogle Scholar\n\n  43. Garimella, K., Morales, G. D. F., Gionis, A. & Mathioudakis, M. Quantifying controversy on social media. ACM Trans. Soc. Comput. 1, 3 (2018).\n\n  44. NLPTown. bert-base-multilingual-uncased-sentiment, huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment (2023).\n\n  45. Ta, H. T., Rahman, A. B. S., Najjar, L. & Gelbukh, A. Transfer Learning from Multilingual DeBERTa for Sexism Identification CEUR Workshop Proceedings Vol. 3202 (CEUR-WS, 2022).\n\n  46. Kleinberg, J. Bursty and hierarchical structure in streams. Data Min. Knowl. Discov. 7, 373\u2013397 (2003).\n\nMathSciNet Google Scholar\n\n  47. Zollo, F. et al. Debunking in a world of tribes. PLoS ONE 12, e0181821 (2017).\n\nPubMed PubMed Central Google Scholar\n\n  48. Albrecht, D. Vaccination, politics and COVID-19 impacts. BMC Publ. Health 22, 96 (2022).\n\nCAS Google Scholar\n\n  49. Falkenberg, M. et al. Growing polarization around climate change on social media. Nat. Clim. Change 12, 1114\u20131121 (2022).\n\n  50. Schmidt, A. L., Zollo, F., Scala, A., Betsch, C. & Quattrociocchi, W. Polarization of the vaccination debate on Facebook. Vaccine 36, 3606\u20133612 (2018).\n\nPubMed Google Scholar\n\n  51. Schmidt, A. L. et al. Anatomy of news consumption on Facebook. Proc. Natl Acad. Sci. USA 114, 3035\u20133039 (2017).\n\nCAS PubMed PubMed Central ADS Google Scholar\n\n  52. Del Vicario, M., Zollo, F., Caldarelli, G., Scala, A. & Quattrociocchi, W. Mapping social dynamics on Facebook: the brexit debate. Soc. Netw. 50, 6\u201316 (2017).\n\nGoogle Scholar\n\n  53. Hunnicutt, T. & Dave, P. Gab.com goes offline after Pittsburgh synagogue shooting. Reuters, www.reuters.com/article/uk-pennsylvania-shooting-gab-idUKKCN1N20QN (29 October 2018).\n\n  54. Valensise, C. M. et al. Lack of evidence for correlation between COVID-19 infodemic and vaccine acceptance. Preprint at arxiv.org/abs/2107.07946 (2021).\n\n  55. Quattrociocchi, A., Etta, G., Avalle, M., Cinelli, M. & Quattrociocchi, W. in Social Informatics (eds Hopfgartner, F. et al.) 245\u2013256 (Springer, 2022).\n\n  56. Mekacher, A. & Papasavva, A. \u201cI can\u2019t keep it up\u201d a dataset from the defunct voat.co news aggregator. In Proc. International AAAI Conference on Web and Social Media Vol. 16, 1302\u20131311 (AAAI, 2022).\n\n  57. Facebook Community Standards, transparency.fb.com/policies/community-standards/hate-speech/ (Facebook, 2023).\n\n  58. Rosen, G. & Lyons, T. Remove, reduce, inform: new steps to manage problematic content. Meta, about.fb.com/news/2019/04/remove-reduce-inform-new-steps/ (10 April 2019).\n\n  59. Vulgar Language Policy, support.google.com/youtube/answer/10072685? (YouTube, 2023).\n\n  60. Harassment & Cyberbullying Policies, support.google.com/youtube/answer/2802268 (YouTube, 2023).\n\n  61. Hate Speech Policy, support.google.com/youtube/answer/2801939 (YouTube, 2023).\n\n  62. How Does YouTube Enforce Its Community Guidelines?, www.youtube.com/intl/enus/howyoutubeworks/policies/community-guidelines/enforcing-community-guidelines (YouTube, 2023).\n\n  63. The Twitter Rules, help.twitter.com/en/rules-and-policies/twitter-rules (Twitter, 2023).\n\n  64. Hateful Conduct, help.twitter.com/en/rules-and-policies/hateful-conduct-policy (Twitter, 2023).\n\n  65. Gorwa, R., Binns, R. & Katzenbach, C. Algorithmic content moderation: technical and political challenges in the automation of platform governance. Big Data Soc. 7, 2053951719897945 (2020).\n\nGoogle Scholar\n\n  66. Our Range of Enforcement Options, help.twitter.com/en/rules-and-policies/enforcement-options (Twitter, 2023).\n\n  67. Elliott, V. & Stokel-Walker, C. Twitter\u2019s moderation system is in tatters. WIRED (17 November 2022).\n\n  68. Reddit Content Policy, www.redditinc.com/policies/content-policy (Reddit, 2023).\n\n  69. Promoting Hate Based on Identity or Vulnerability, www.reddithelp.com/hc/en-us/articles/360045715951 (Reddit, 2023).\n\n  70. Malik, A. Reddit acqui-hires team from ML content moderation startup Oterlu. TechCrunch, tcrn.ch/3yeS2Kd (4 October 2022).\n\n  71. Terms of Service, telegram.org/tos (Telegram, 2023).\n\n  72. Durov, P. The rules of @telegram prohibit calls for violence and hate speech. We rely on our users to report public content that violates this rule. Twitter, twitter.com/durov/status/917076707055751168?lang=en (8 October 2017).\n\n  73. Telegram Privacy Policy, telegram.org/privacy (Telegram, 2023).\n\n  74. Terms of Service, gab.com/about/tos (Gab, 2023).\n\n  75. Salzenberg, C. & Spafford, G. What is Usenet?, www0.mi.infn.it/\u223ccalcolo/Wis usenet.html (1995).\n\n  76. Castelle, M. The linguistic ideologies of deep abusive language classification. In Proc. 2nd Workshop on Abusive Language Online (ALW2) (eds Fi\u0161er, D. et al.) 160\u2013170, aclanthology.org/W18-5120 (Association for Computational Linguistics, 2018).\n\n  77. Tontodimamma, A., Nissi, E. & Sarra, A. E. A. Thirty years of research into hate speech: topics of interest and their evolution. Scientometrics 126, 157\u2013179 (2021).\n\nGoogle Scholar\n\n  78. Sap, M. et al. Annotators with attitudes: how annotator beliefs and identities bias toxic language detection. In Proc. 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (eds. Carpuat, M. et al.) 5884\u20135906 (Association for Computational Linguistics, 2022).\n\n  79. Pavlopoulos, J., Sorensen, J., Dixon, L., Thain, N. & Androutsopoulos, I. Toxicity detection: does context really matter? In Proc. 58th Annual Meeting of the Association for Computational Linguistics (eds Jurafsky, D. et al.) 4296\u20134305 (Association for Computational Linguistics, 2020).\n\n  80. Yin, W. & Zubiaga, A. Hidden behind the obvious: misleading keywords and implicitly abusive language on social media. Online Soc. Netw. Media 30, 100210 (2022).\n\nGoogle Scholar\n\n  81. Sap, M., Card, D., Gabriel, S., Choi, Y. & Smith, N. A. The risk of racial bias in hate speech detection. In Proc. 57th Annual Meeting of the Association for Computational Linguistics (eds Kohonen, A. et al.) 1668\u20131678 (Association for Computational Linguistics, 2019).\n\n  82. Rosenblatt, L., Piedras, L. & Wilkins, J. Critical perspectives: a benchmark revealing pitfalls in PerspectiveAPI. In Proc. Second Workshop on NLP for Positive Impact (NLP4PI) (eds Biester, L. et al.) 15\u201324 (Association for Computational Linguistics, 2022).\n\n  83. DiMaggio, P., Evans, J. & Bryson, B. Have American\u2019s social attitudes become more polarized? Am. J. Sociol. 102, 690\u2013755 (1996).\n\n  84. Fiorina, M. P. & Abrams, S. J. Political polarization in the American public. Annu. Rev. Polit. Sci. 11, 563\u2013588 (2008).\n\nGoogle Scholar\n\n  85. Iyengar, S., Gaurav, S. & Lelkes, Y. Affect, not ideology: a social identity perspective on polarization. Publ. Opin. Q. 76, 405\u2013431 (2012).\n\nGoogle Scholar\n\n  86. Cota, W., Ferreira, S. & Pastor-Satorras, R. E. A. Quantifying echo chamber effects in information spreading over political communication networks. EPJ Data Sci. 8, 38 (2019).\n\n  87. Bessi, A. et al. Users polarization on Facebook and Youtube. PLoS ONE 11, e0159641 (2016).\n\nPubMed PubMed Central Google Scholar\n\n  88. Bessi, A. et al. Science vs conspiracy: collective narratives in the age of misinformation. PLoS ONE 10, e0118093 (2015).\n\nPubMed PubMed Central Google Scholar\n\n  89. Himelboim, I., McCreery, S. & Smith, M. Birds of a feather tweet together: integrating network and content analyses to examine cross-ideology exposure on Twitter. J. Comput. Med. Commun. 18, 40\u201360 (2013).\n\nGoogle Scholar\n\n  90. An, J., Quercia, D. & Crowcroft, J. Partisan sharing: Facebook evidence and societal consequences. In Proc. Second ACM Conference on Online Social Networks, COSN\u203214 13\u201324 (Association for Computing Machinery, 2014).\n\n  91. Mann, H. B. & Whitney, D. R. On a test of whether one of two random variables is stochastically larger than the other. Ann. Math. Stat. 18, 50\u201360 (1947).\n\nMathSciNet Google Scholar\n\nDownload references\n\n## Acknowledgements\n\nWe thank M. Samory for discussions; T. Quandt and Z. Zhang for suggestions\nduring the review process; and Geronimo Stilton and the Hypnotoad for\ninspiring the data analysis and result interpretation. The work is supported\nby IRIS Infodemic Coalition (UK government, grant no. SCH-00001-3391), SERICS\n(PE00000014) under the NRRP MUR program funded by the EU NextGenerationEU\nproject CRESP from the Italian Ministry of Health under the program CCM 2022,\nPON project \u2018Ricerca e Innovazione\u2019 2014-2020, and PRIN Project MUSMA for\nItalian Ministry of University and Research (MUR) through the PRIN 2022CUP\nG53D23002930006 and EU Next-Generation EU, M4 C2 I1.1.\n\n## Author information\n\nAuthor notes\n\n  1. These authors contributed equally: Michele Avalle, Niccol\u00f2 Di Marco, Gabriele Etta\n\n### Authors and Affiliations\n\n  1. Department of Computer Science, Sapienza University of Rome, Rome, Italy\n\nMichele Avalle, Niccol\u00f2 Di Marco, Gabriele Etta, Shayan Alipour, Lorenzo\nAlvisi, Matteo Cinelli & Walter Quattrociocchi\n\n  2. Department of Social Sciences and Economics, Sapienza University of Rome, Rome, Italy\n\nEmanuele Sangiorgio\n\n  3. Department of Communication and Social Research, Sapienza University of Rome, Rome, Italy\n\nAnita Bonetti\n\n  4. Institute of Complex Systems, CNR, Rome, Italy\n\nAntonio Scala\n\n  5. Department of Mathematics, City University of London, London, UK\n\nAndrea Baronchelli\n\n  6. The Alan Turing Institute, London, UK\n\nAndrea Baronchelli\n\nAuthors\n\n  1. Michele Avalle\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  2. Niccol\u00f2 Di Marco\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  3. Gabriele Etta\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  4. Emanuele Sangiorgio\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  5. Shayan Alipour\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  6. Anita Bonetti\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  7. Lorenzo Alvisi\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  8. Antonio Scala\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  9. Andrea Baronchelli\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  10. Matteo Cinelli\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n  11. Walter Quattrociocchi\n\nView author publications\n\nYou can also search for this author in PubMed Google Scholar\n\n### Contributions\n\nConception and design: W.Q., M.A., M.C., G.E. and N.D.M. Data collection: G.E.\nand N.D.M. with collaboration from M.C., M.A. and S.A. Data analysis: G.E.,\nN.D.M., M.A., M.C., W.Q., E.S., A. Bonetti, A. Baronchelli and A.S. Code\nwriting: G.E. and N.D.M. with collaboration from M.A., E.S., S.A. and M.C. All\nof the authors provided critical feedback and helped to shape the research,\nanalysis and manuscript, and contributed to the preparation of the manuscript.\n\n### Corresponding authors\n\nCorrespondence to Matteo Cinelli or Walter Quattrociocchi.\n\n## Ethics declarations\n\n### Competing interests\n\nThe authors declare no competing interests.\n\n## Peer review\n\n### Peer review information\n\nNature thanks Thorsten Quandt, Ziqi Zhang and the other, anonymous,\nreviewer(s) for their contribution to the peer review of this work.\n\n## Additional information\n\nPublisher\u2019s note Springer Nature remains neutral with regard to jurisdictional\nclaims in published maps and institutional affiliations.\n\n## Extended data figures and tables\n\n### Extended Data Fig. 1 General characteristics of online conversations.\n\na. Distributions of conversation length (number of comments in a thread). b.\nDistributions of the time duration (days) of user activity on a platform for\neach platform and each topic. c. Time duration (days) distributions of\nthreads. Colour-coded legend on the side.\n\n### Extended Data Fig. 2 Extremely toxic authors and conversations are rare.\n\na. Complementary cumulative distribution functions (CCDFs) of the toxicity of\nauthors who posted more than 10 comments. Toxicity is defined as usual as the\nfraction of toxic comments over the total of comments posted by a user. b.\nCCDFs of the toxicity of conversations containing more than 10 comments.\nColour-coded legend on the side.\n\n### Extended Data Fig. 3 User toxicity as conversations evolve.\n\nMean fraction of toxic comments as conversations progress. The x-axis\nrepresents the normalized position of comment intervals in the threads. For\neach dataset, toxicity is computed in the thread size interval [0.7\u22121] (see\nmain text and Tab. S2 in SI). Trends are reported with their 95% confidence\ninterval. Colour-coded legend on the side.\n\n### Extended Data Fig. 4 Toxicity is not associated with conversation\nlifetime.\n\nMean toxicity of a. users versus their time of permanence in the dataset and\nb. threads versus their time duration. Trends are reported with their 95%\nconfidence interval and they are reported using a normalized log-binning.\nColour-coded legend on the side.\n\n### Extended Data Fig. 5 Results hold for a different toxicity threshold.\n\nCore analyses presented in the paper repeated employing a lower (0.5) toxicity\nbinary classification threshold. a. Mean fraction of toxic comments in\nconversations versus conversation size, for each dataset (see Fig. 2). Trends\nare reported with their 95% confidence interval. b. Pearson\u2019s correlation\ncoefficients between user participation and toxicity trends for each\ndataset.c. Pearson\u2019s correlation coefficients between users\u2019 participation in\ntoxic and non-toxic thread sets, for each dataset. d. Boxplot of the\ndistribution of toxicity (n = 25, min = \u22120.016, max = 0.020, lower whisker =\n\u22120.005, Q1 = \u2212 0.005, Q_2 = 0.004, Q_3 = 0.012, upper whisker = 0.020) and\nparticipation (n = 25, min = \u22120.198, max = \u22120.022, lower whisker = \u22120.198, Q1\n= \u2212 0.109, Q_2 = \u2212 0.070, Q_3 = \u2212 0.049, upper whisker = \u22120.022) trend slopes\nfor all datasets, as resulting from linear regression. The results of the\nrelative Mann-Kendall tests for trend assessment are shown in Extended Data\nTable 5.\n\nExtended Data Table 1 Results of Mann-Kendall tests applied to participation\nvs normalized comment position\n\nFull size table\n\nExtended Data Table 2 Toxicity versus conversation size\n\nFull size table\n\nExtended Data Table 3 The datasets used in the analysis of controversy\n\nFull size table\n\nExtended Data Table 4 Conversations are more toxic at the peak of activity\n\nFull size table\n\nExtended Data Table 5 Results of Mann-Kendall tests applied to the toxicity vs\nconversation size trends\n\nFull size table\n\n## Supplementary information\n\n### Supplementary Information\n\nSupplementary Information 1\u20134, including details regarding data collection for\nvalidation dataset, Supplementary Figs. 1\u20133, Supplementary Tables 1\u201317 and\nsoftware and coding specifications.\n\n### Reporting Summary\n\n## Rights and permissions\n\nOpen Access This article is licensed under a Creative Commons Attribution 4.0\nInternational License, which permits use, sharing, adaptation, distribution\nand reproduction in any medium or format, as long as you give appropriate\ncredit to the original author(s) and the source, provide a link to the\nCreative Commons licence, and indicate if changes were made. The images or\nother third party material in this article are included in the article\u2019s\nCreative Commons licence, unless indicated otherwise in a credit line to the\nmaterial. If material is not included in the article\u2019s Creative Commons\nlicence and your intended use is not permitted by statutory regulation or\nexceeds the permitted use, you will need to obtain permission directly from\nthe copyright holder. To view a copy of this licence, visit\nhttp://creativecommons.org/licenses/by/4.0/.\n\nReprints and permissions\n\n## About this article\n\n### Cite this article\n\nAvalle, M., Di Marco, N., Etta, G. et al. Persistent interaction patterns\nacross social media platforms and over time. Nature (2024).\nhttps://doi.org/10.1038/s41586-024-07229-y\n\nDownload citation\n\n  * Received: 30 April 2023\n\n  * Accepted: 22 February 2024\n\n  * Published: 20 March 2024\n\n  * DOI: https://doi.org/10.1038/s41586-024-07229-y\n\n### Share this article\n\nAnyone you share the following link with will be able to read this content:\n\nSorry, a shareable link is not currently available for this article.\n\nProvided by the Springer Nature SharedIt content-sharing initiative\n\n### Subjects\n\n  * Mathematics and computing\n  * Social sciences\n\n## Comments\n\nBy submitting a comment you agree to abide by our Terms and Community\nGuidelines. If you find something abusive or that does not comply with our\nterms or guidelines please flag it as inappropriate.\n\nDownload PDF\n\nAdvertisement\n\nNature (Nature) ISSN 1476-4687 (online) ISSN 0028-0836 (print)\n\n## nature.com sitemap\n\n### About Nature Portfolio\n\n  * About us\n  * Press releases\n  * Press office\n  * Contact us\n\n### Discover content\n\n  * Journals A-Z\n  * Articles by subject\n  * Protocol Exchange\n  * Nature Index\n\n### Publishing policies\n\n  * Nature portfolio policies\n  * Open access\n\n### Author & Researcher services\n\n  * Reprints & permissions\n  * Research data\n  * Language editing\n  * Scientific editing\n  * Nature Masterclasses\n  * Research Solutions\n\n### Libraries & institutions\n\n  * Librarian service & tools\n  * Librarian portal\n  * Open research\n  * Recommend to library\n\n### Advertising & partnerships\n\n  * Advertising\n  * Partnerships & Services\n  * Media kits\n  * Branded content\n\n### Professional development\n\n  * Nature Careers\n  * Nature Conferences\n\n### Regional websites\n\n  * Nature Africa\n  * Nature China\n  * Nature India\n  * Nature Italy\n  * Nature Japan\n  * Nature Korea\n  * Nature Middle East\n\n  * Privacy Policy\n  * Use of cookies\n  * Legal notice\n  * Accessibility statement\n  * Terms & Conditions\n  * Your US state privacy rights\n  * Cancel contracts here\n\n\u00a9 2024 Springer Nature Limited\n\nSign up for the Nature Briefing newsletter \u2014 what matters in science, free to\nyour inbox daily.\n\nGet the most important science stories of the day, free in your inbox. Sign up\nfor Nature Briefing\n\n", "frontpage": true}
