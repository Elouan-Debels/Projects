{"aid": "39980977", "title": "The Rise of AI Security Engineers", "url": "https://www.thoughtson.ai/Thoughts/2024/04/08/the-rise-of-ai-security-engineers/", "domain": "thoughtson.ai", "votes": 2, "user": "s_streichsbier", "posted_at": "2024-04-09 16:16:15", "comments": 0, "source_title": "The Rise of AI Security Engineers - Thoughts on AI", "source_text": "The Rise of AI Security Engineers - Thoughts on AI\n\nSkip to content\n\n# The Rise of AI Security Engineers\u00b6\n\nA few days ago at a conference, I asked a room full of engineers three simple\nquestions:\n\n3 Questions\n\n  1. Do you write code that's critical to your company's success?\n  2. Do you worry about hackers exploiting vulnerabilities in your code?\n  3. Are you frustrated by the growing list of security tasks in your backlog?\n\nThe response was a resounding \"yes\" to all three questions, confirming an\ninconvenient truth:\n\nDealing with security is a frustrating experience for engineers.\n\nSecurity is often seen as a necessary evil \u2013 a pesky thing that nobody enjoys\ndealing with, but everyone knows they must. Teams pour countless resources\ninto securing their applications, yet they rarely feel confident in their\nefforts.\n\nDespite the proliferation of security tools and services, the number of\ndisclosed vulnerabilities and successful breaches continues to rise. This\ntrend begs the question:\n\nWhy are our current approaches to security failing?\n\nTo understand the why, we have to look at how application security has evolved\nover the years.\n\n## Previously on \"AppSec\"\u00b6\n\nIf application security were a TV show, we would be at the end of season two.\nThis is what happened so far.\n\n### The Pilot\u00b6\n\nAs software eats the world, companies rush to bring their businesses online.\nThe digital economy booms, but web applications are riddled with\nvulnerabilities, exposing them to attacks. Security experts are few and far\nbetween, and tools to help them are virtually non-existent. They struggle to\nscale security reviews and build tools to keep pace with the growing threat.\n\nCliffhanger: Will teams secure their applications or will hackers prevail?\n\n### Season 1 Recap\u00b6\n\nWe are in the waterfall era, where every release has a dedicated phase to find\nand fix vulnerabilities. Automated tools scan source code but generate a lot\nof false positives, creating extra work. Developers struggle to context\nswitch, understand vulnerabilities, and fix them correctly. Tension between\nsecurity and development teams reaches a boiling point as release deadlines\napproach.\n\nDespite the challenges, a process emerges for conducting security reviews\nbefore releasing software, which only happens a few times a year.\n\nSeason 1 Finale\n\nAs teams adapt to this approach, digital-first, cloud-native companies emerge.\nThey build software iteratively, releasing changes multiple times a month,\nweek, or even daily.\n\nCliffhanger: Will teams be able to \"do security\" at DevOps speed?\n\n### Season 2 Recap\u00b6\n\nAgile and DevOps become the new norm, with security engineers outnumbered 100\nto 1 by developers. Manually reviewing code fast enough is impossible.\nSecurity tools are integrated earlier in the software development lifecycle,\nwith the hope of empowering developers to identify and fix vulnerabilities as\nthey are introduced. However, the security tools slow down builds and\noverwhelm developers with thousands of alerts.\n\nThe security industry responds with innovations to make tools faster, more\ncapable, and better integrated into workflows. Taint tracking, secrets\nverification, and reachability analysis reduce false positives, saving teams\ncountless hours of manual review.\n\nSeason 2 Finale\n\nGenerative AI is used to generate more code than ever before. Teams struggle\nto triage, prioritize, and fix the growing list of vulnerabilities. Hackers\nstart leveraging AI to launch sophisticated, automated attacks.\n\nCliffhanger: Are human defenders a match for this new threat? Unlikely.\n\n### Season 3 Teaser\u00b6\n\nAs we enter the third and final season, the challenges faced by security teams\nand developers are far from resolved. With more software being created than\never and the threat of autonomous AI attackers rising, the question remains:\n\nHow can we secure our applications at scale without causing all the\nfrustration?\n\nThis is where AI Security Engineers are making their appearance, attempting to\nsave the day (and where we end the TV show analogy).\n\n## The Rise of AI Security Engineers\u00b6\n\nGenerative AI has taken the world by storm. The release of GPT-4 was a wake-up\ncall for many, myself included. Large Language Model (LLM) technology has\npushed the boundaries of what can be achieved.\n\nMore and more code is being created with the help of co-pilots. From GitHub\nCopilot completing a few lines of code to Devin, an autonomous agent that can\nsolve engineering tasks on its own. It's easier than ever to create code, even\nwithout being a trained developer.\n\nHowever, the code generated by LLMs is not necessarily secure. The ease of\ncode generation, coupled with the potential security vulnerabilities in AI-\ngenerated code, means that more vulnerabilities are being introduced into\nsoftware. As a result, engineering teams face a rapidly growing backlog of\nvulnerabilities that they need to address. At this inflection point, we must\nask ourselves:\n\nHow can AI be used by defenders in ways that weren't possible before?\n\nWe have seen the first promising applications of generative AI for security.\nLLMs are being used to reduce false positives and generate fixes for\nvulnerabilities, doing so much better than any previous tool.\n\nThe problem is that security engineers and developers still need to do a lot\nof manual work to keep up with all the vulnerabilities. The only solution is\nto increasingly automate this manual work by leveraging AI Security Engineers.\n\n### What are AI Security Engineers?\u00b6\n\nAI Security Engineers are autonomous agents that leverage the power of Large\nLanguage Models (LLMs) to perform security tasks independently. LLMs are\nadvanced AI models that can understand and generate language, enabling AI\nSecurity Engineers to comprehend the intent behind code in a more dynamic and\nhuman-like way.\n\nTraditional security tools rely on static rulesets that generate a high number\nof false positives, flagging issues that aren't actually vulnerabilities. This\nleads to wasted time and resources as teams investigate and address these\nfalse alarms.\n\nIn contrast, AI Security Engineers understand the context and logic behind the\ncode, allowing them to reduce false alerts and increase accuracy. They can\nalso identify complex vulnerabilities that tools miss, such as business logic\nflaws and authorization weaknesses.\n\nA common example is when tools detect cryptography-related issues, such as the\nuse of MD5 or insecure random generators, but require a human to review\nwhether they are used in a security-sensitive context or can be ignored. AI\nSecurity Engineers can more accurately assess the security impact without\nhuman intervention.\n\nAI Security Engineers help developers reduce the growing backlog of\nvulnerabilities by providing tailored remediation advice. They can provide a\ndeveloper with a list of recommended code changes to fix a vulnerability,\nalong with an explanation of why those changes are necessary. The developer\ncan then review the suggestions, make any necessary modifications, and\nimplement the fixes more efficiently.\n\nFurthermore, AI Security Engineers can learn from the feedback and preferences\nthey receive from human security experts and adapt to each organization's\nunique environment. They can prioritize vulnerabilities based on the\norganization's risk appetite and suggest remediation strategies that align\nwith the development team's practices.\n\nAI Security Engineers are designed to augment and support human expertise, not\nreplace it entirely. The goal is to create a synergistic relationship where AI\nand humans work together to achieve better security outcomes.\n\n### Towards Full Autonomy\u00b6\n\nThe benefits of creating fully autonomous AI Security Engineers cannot be\noverstated. I believe that the way we approach security in 5 years will be\nvery different from today. Imagine a future where AI Security Engineers work\ntirelessly behind the scenes, continuously monitoring, analyzing, and\nimproving the security posture of applications without slowing down\ndevelopment.\n\n## Conclusion\u00b6\n\nThe rise of AI Security Engineers represents a pivotal moment in the evolution\nof application security. By leveraging the power of Large Language Models,\nthese autonomous agents have the potential to revolutionize the way we\napproach security. They will enable us to keep pace with the ever-increasing\nspeed of software development and the growing sophistication of cyber threats.\n\nAI Security Engineers need to be accessible to everyone who is building\nsoftware. Autonomous agents equipped with the full context of a business are\nmuch more powerful than autonomous agents with limited context, which for the\nfirst time will put defenders at an advantage over attackers. If we fail to do\nso, and attackers create autonomous AI hackers first, the consequences will be\ndevastating.\n\nIn the next piece, I will dive into the 6 levels of Application Security\nAutomation and present a roadmap for achieving fully autonomous AI Security\nEngineers. Understanding these levels and the path forward is essential for\nanyone looking to stay ahead in this rapidly evolving field.\n\nIf you found this useful, let me know in the comments. Also, follow me on X\nfor more content like this.\n\nJoin the conversation\n\nWhat do you think?\n\n  * Is security something that should only be done by humans?\n  * How much should we automate, where do we draw the line, and why?\n\n## Comments\n\nCopyright \u00a9 2024 Stefan Streichsbier\n\nMade with Material for MkDocs\n\n", "frontpage": false}
