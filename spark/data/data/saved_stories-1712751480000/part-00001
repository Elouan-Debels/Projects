{"aid": "39987480", "title": "The River of Bytes: Elasticsearch Data Transfer Cost Optimization", "url": "https://tech.forter.com/the-river-of-bytes.html", "domain": "forter.com", "votes": 8, "user": "dorony", "posted_at": "2024-04-10 06:16:26", "comments": 0, "source_title": "The River of Bytes", "source_text": "The River of Bytes\n\nForter Tech Blog\n\nsoftware architecture\n\n# The River of Bytes\n\nOmer Hadari April 9, 2024\n\n# The River of Bytes\n\n### TL;DR\n\n  * We have an Elasticsearch cluster that used to move >3PB of data across AWS Availability Zones every month. The entire cluster houses ~100TB of data.\n\n    * That\u2019s like moving 30 clusters\u2019 worth of data between AZs every month!\n  * We did stuff to find out was causing this\n  * We cut it down by 95% which amounts to hundreds of thousands of dollars in annual savings\n\nHopefully you\u2019re now asking yourself \u201cwhat\u2019s stuff\u201d. If so, read on.\n\n### Background\n\n#### What We Found\n\nAt Forter, we have multiple Elasticsearch clusters, all of which are self-\nmanaged and deployed using AWS EC2.\n\nElasticsearch, while a very capable product, is a notoriously finicky beast.\nAmong other metrics you should keep in check if you find yourself responsible\nfor big-ish Elasticsearch clusters is cost.\n\nOne day, while examining the EC2 cost breakdown of one of our bigger\nElasticsearch clusters, we noticed that over 30% of the cost is a result of\nthe UsageType DataTransfer-Regional-Bytes. More specifically, we noticed that\nwe were paying for Elasticsearch Data nodes to transfer 90-120TB of data\nbetween one another every day.\n\nInitial Cost Graph\n\n#### What is This Cost\n\nIn order to increase production resiliency, we deploy critical clusters to 2\nAvailability Zones in the same AWS region. This is how AWS describes\nAvailability Zones:\n\n> Availability Zones consist of one or more discrete data centers, each with\n> redundant power, networking, and connectivity, and housed in separate\n> facilities.\n\nOne of the drawbacks of this approach, is cost. AWS advertises a price of\n$0.01/GB that leaves or enters an availability zone in the same region. That\nis, $0.02/GB because a byte that leaves an AZ doesn\u2019t disappear in the cosmic\nand mysterious dark space between AZs, it enters another AZ. This may seem\nlow, but when Giga becomes Terra or Peta it adds up to quite the pile of\npennies.\n\n#### The Legitimate Portion\n\nInitially, we thought this kind of cost made sense for multi-AZ deployments.\nAfter all, we have to copy every byte we index across zones.\n\nIn order to actually be more resilient, our clusters are configured to make\nsure every document has at least 2 copies, spread across availability zones.\nAn application writes (indexes) a document by connecting to a single node, and\nElasticsearch takes care of copying (replicating) the data to a node in a\ndifferent Availability Zone.\n\n> Elasticsearch has a shard allocation awareness feature. When replication is\n> enabled (each shard has more than 1 copy in the cluster) this feature allows\n> keeping each copy of a shard in a different AZ, by setting an attribute on\n> nodes which denotes the AZ in which they reside. See the diagram below.\n\nES Query Diagram\n\nThere is a problem with this theory however. We observed a cross-AZ data\ntransfer rate of up to 120TB/day. We don\u2019t index this much data. In fact, when\nchecking the growth rate of actual cluster storage, we saw that we only index\naround 10-15 GB every day, including replicas. So what gives?\n\n### The Hunt\n\n#### Where do Bytes Come From?\n\nUsing AWS Cost Explorer, we could see that all of the relevant cross-AZ data\ntransfer cost was tagged as cost of our Elasticsearch data nodes. This means\nthat both the source and the destination of the offending traffic was a data\nnode, i.e. cluster-internal traffic.\n\nAWS doesn\u2019t know or care what bytes stream on their network backbone between\nAvailability Zones, they just count them and bill. We, however, were very much\ninterested in finding out what kind of barrage our data nodes are frantically\nsending one another.\n\nNodes in the same Elasticsearch cluster communicate with each other regularly,\nand they use a dedicated Transport protocol and a dedicated port (9300 by\ndefault). It is used for a very wide variety of internal tasks - from passing\ncopies of documents for replication, distributing a query to all of the\nrelevant shards, updating settings and more.\n\nWe wanted to understand what information the nodes in the cluster are passing\naround between one another, and if we can perhaps make them quiet down.\n\n#### Setting Up\n\nOur objective at this point was to understand what kind of messages were being\ntransferred between data nodes. Are they documents being copied around? Some\nkind of keep-alive? Something else?\n\nAt first we were hopeful that Elasticsearch might offer some kind of\nmonitoring data for Transport requests, similar to the client-level monitoring\n(e.g. query count and index rate). This would have been useful since we could\nhave seen something along the lines of \u201c99% of requests are of type XXX\u201d and\nthen investigated what creates XXX requests. Unfortunately, we were not able\nto find metrics of the sort.\n\n#### Diving In\n\n##### Verification\n\nSo we decided to first verify our assumptions, by looking at the river (flood?\ntsunami?) of data from as close to it as we could get to it. We logged into a\ndata node, and ran the network analysis tool iftop. If you aren\u2019t familiar,\niftop let\u2019s you monitor your network interfaces, and specifically how much\ntraffic moves through them alongside sources and destinations.\n\nWe ran iftop -i <interface> -f 'tcp port 9300' -t -s 10 -L 8 in order to find\nout if the rate of traffic to/from port 9300 explains the amount of cross AZ\ntraffic we observed.\n\nThis is how the output looks like:\n\nWe did the math and indeed, our data nodes were even more chatty than your\naverage Rust enthusiast.\n\n##### Further Down the Network Route\n\nAt this point, my questionable instincts pushed us to try and sniff the data\nwith tcpdump and just look at it to see what it is they are saying to each\nother. Unfortunately for me, Forter Engineering is a respectable and\nresponsible department, and we don\u2019t pass around data in plain-text. Were I to\ntry such a shenanigan I would have been presented with a perfectly useless\nendless stream of encrypted bytes.\n\n> Confession\n>\n> To be honest, it is possible to hack around and read decrypted traffic if\n> you are an admin. I wasted some time using the amazing mitmproxy and managed\n> to sniff the data after all. As you are about to see - this was a redundant\n> detour.\n\n##### Looking for Logs\n\nElasticsearch is written in Java, and a quick look around revealed that it\nuses log4j to control its logs. This meant that by editing a node\u2019s\nlog4j2.properties we could control the verbosity of the logs. log4j allows\ncontrolling separate loggers for separate packages, and configuring log\nverbosity for each on a package/class level. So which logger should be\nmodified, and what classes do we want to look into?\n\nIn order to find the answer - we looked in the thankfully-publicly-available\nElasticsearch code. We found out that transport related code lies in\norg.elasticsearch.transport and its sub-packages. Since we had no idea what\nspecifically creates all the traffic, we decided to go the brutish route and\nenabled TRACE logging for the entire package. We did it by adding the\nfollowing lines to log4j2.properties:\n\n    \n    \n    logger.transport.name = org.elasticsearch.transport logger.transport.level = trace\n\nThen we restarted the node and waited for a bit. Here is an example line we\nsaw in the log:\n\n    \n    \n    [2024-01-31T08:39:55,007][TRACE][o.e.t.TransportLogger ] [192.168.X.X] Netty4TcpChannel{localAddress=/192.168.X.X:9300, remoteAddress=/192.168.Y.Y:56789, profile=default} [length: 1736, request id: 128852428, type: request, version: 7.17.3, action: indices:data/write/bulk[s][r]] READ: 1736B\n\nLet\u2019s break it down: localAdress=/192.168.X.X:9300 is our node\u2019s address in\nport 9300. This is probably the log of another node sending a request to this\nnode\u2019s management port. - remoteAdress=/192.168.Y.Y:56789 is another node\u2019s\naddress in same cluster, and the port it uses. - length - the length in bytes\nof the payload. Interesting, we\u2019ll get back to it. - action:\nindices:data/write/bulk[s] - the type of the request. DING DING DING!\n\nThe action and length parts of the log contain all of the information we need.\nWe can parse the log and find out which action account for the most length.\n\nSo I wrote some hand-made, artisanal bash with no ChatGPT additives (sadly, it\nshows. It spins up an embarrassing amount of processes)\n\n> Horrible Yet Wonderful Bash\n>  \n>  \n>     for action in $(grep -E '(received|sent)'<trace.log | awk '{print $4}' | tr '[' ' ' | cut -d' ' -f3 | sort | uniq); do grep -E $action <trace.log | grep length | sed -E 's/.*(length: [0-9]*).*/\\1+/g' | cut -d' ' -f2 | xargs echo | sed -E 's/\\+$//g' | bc | xargs -I{} echo $action':' '{}'B done | sort -k2 -h -r\n\nAnd got a list of actions, sorted by the cumulative size of their payloads (in\nbytes):\n\n    \n    \n    cluster:monitor/nodes/info: 21086079B indices:data/write/bulk: 6488481B indices:admin/seq_no/global_checkpoint_sync: 1145750B indices:data/read/search: 61445B cluster:monitor/nodes/stats: 44871B indices:monitor/stats: 14651B indices:admin/seq_no/retention_lease_background_sync: 9158B\n\nWeird. We do see what seems to be the indexing action, but it\u2019s in second\nplace. The first place, and by a mile, is cluster:monitor/nodes/info. What is\nthat?\n\nWhen we noticedmonitor/nodes/info we suspected it\u2019s related to Elasticsearch\u2019s\nNode Info API. This API retrieves information about nodes in the cluster. This\nwas still odd, however. Most of the traffic was internal, within the cluster\n(remember - port 9300). And the amounts just didn\u2019t make sense.\n\n##### Looking at Code\n\nIt looked like some kind of node info request was responsible for most of the\ndata. This new information created more questions: - Why is a supposedly\ninnocent metadata request responsible for so much traffic? - What triggers\nthis request, and can we kill it with fire?\n\nKill it With Fire\n\nFirst, in order to verify, we circled back to Elasticsearch\u2019s source code.\nSearching for the log string led us eventually to the implementation of\nTransportNodesInfoAction.\n\nWe could see that it is triggered by an actual call to the Nodes info API. It\nlooked like every time such an API call is made, an equivalent internal call\nis triggered from the node who got the request to all other nodes in the\ncluster. Since the cluster has 112 nodes, each Nodes Info call created 111\nadditional, internal calls.\n\nSo the open questions at this point were: - Whether something external to the\nclusters calls the Node Info API so much - If so, who is it - Does the math\nadd up, because 3PB of data every month is a hell of a lot of info about nodes\n\n##### Closing In\n\nIn order to look into external calls to the Node Info API, we enabled a\ndifferent set of logs:\n\n    \n    \n    logger.http.name = org.elasticsearch.http logger.http.level = trace\n\nAn example log line from the file was:\n\n    \n    \n    [2024-01-31T08:40:03,654][TRACE][o.e.t.TransportLogger ] [192.168.X.X] Netty4TcpChannel{localAddress=/192.168.X.X:9300, remoteAddress=/192.168.Y.Y:58844, profile=default} [length: 10012, request id: 24133878, type: request, version: 7.17.3, action: cluster:monitor/nodes/info[n]] READ: 10012B\n\nGreat, we can extract the address after remoteAddress=/ and see who makes the\nmost monitor/nodes/info[n] actions!\n\nSome more swiftly handcrafted bash:\n\n> Less Horrible and Less Wonderful Bash\n>  \n>  \n>     grep 'action: cluster:monitor/nodes/info' <http-requests.log | sed -E 's/.*remoteAddress=\\/([^:]+).*/\\1/g' | sort | uniq -c | sort -k1 -h -r | head -n10\n\nAnd we got some (redacted) potential addresses of culprits:\n\n    \n    \n    28 192.168...... 26 192.168...... 26 192.168...... 24 192.168...... 24 192.168...... 24 192.168...... 23 192.168...... 23 192.168...... 23 192.168...... 23 192.168......\n\nAll of the top addresses were of instances of the backend of one of our\nservices, which queries this Elasticsearch cluster.\n\nWe did some more back of envelope math. Given the amount of external\nnodes/info requests made by this service, multiplied by the amount of nodes in\nthe cluster, we could explain the amount of cross-AZ traffic we observed!\n\nSo why was this service so addicted to sniffing? > Sniffing in Elasticsearch\nis when the client queries a node in the cluster for a list of available nodes\nto choose from when sending queries.\n\n##### Fix\n\nWe looked in the backend code, and specifically we looked for ES client\ncreation and configuration. We found a few things: 1. By default, new\nconnections were configured to sniff every 5 seconds. 2. Due to the way our\nORM is set up, we create a connection for each index we query. We query over\n10 indices. 3. We explicitly configured sniffing requests to fetch all HTTP\nattributes of every node, instead of the default, much smaller set.\n\nSo across our production, staging and development environments, each with many\ninstances, we got a constant barrage of sniffing requests.\n\nAnd the fixes, in order, were:\n\n  1. We changed the sniffing interval to 60 seconds. That\u2019s not dangerous, since nodes are being replaced in a much slower rate. Clients only need 1 reachable node for every request, and errors trigger a sniffing request anyway.\n  2. We started reusing ES Clients in our ORM\n  3. We changed the client config and are now requesting only the attributes we needed for choosing nodes.\n\nIn the end, client configuration looked something like this:\n\n    \n    \n    const { Client } = require('@elastic/elasticsearch') const client = new Client({ ... sniffInterval: 60000, sniffEndpoint: '_nodes/_all/http?filter_path=nodes.*.attributes.aws_*,nodes.*.roles,nodes.*.host' })\n\n#### Summary\n\nAn internal service was sniffing like mad. 3-PetaBytes-of-node-metadata-mad.\nDespite the lack of readily available logs and metrics, our cloud bill gave us\nmotivation to dive into the river of bytes and dam it.\n\nAfter all fixes were deployed, cross-az Data Transfer rate was reduced from\n3PB/month to less than 0.2PB/Month - a 95% reduction. Still a lot, but much\nless, and can be explained by regular DB operations. Seeing the following\ngraph was very gratifying:\n\nLower Cost Graph\n\nThe moral of the story, if there is one, is that there is no such thing as a\ntrue black-box, especially in your own network and self-hosted services. Even\nif whatever it is you\u2019re using provides no logs or docs, and encrypts traffic\nover-the-wire, it\u2019s your data and your cores.\n\n#### Bandaid Overview\n\nWe could have chosen to put a bandaid on the problem. Here is a short list of\nthings we could have done which would have improved the situation a bit, but\nnot solve the problem.\n\n  1. Compress transport data by changing transport.compress (an ES config). We tried, and it cut message size by ~half - 3PB is a lot, but 1.5PB is still a lot.\n  2. Deploy the cluster in a single AZ - would eliminate cost, but sacrifice resilience.\n\n### Forter Engineering\n\nFollow us on social media to stay up-to-date with cutting edge technologies,\n\n", "frontpage": false}
