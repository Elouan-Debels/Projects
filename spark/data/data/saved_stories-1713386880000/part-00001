{"aid": "40065414", "title": "Watch AI-powered robots play soccer", "url": "https://www.popsci.com/technology/deepmind-robot-soccer/", "domain": "popsci.com", "votes": 1, "user": "gmays", "posted_at": "2024-04-17 14:46:27", "comments": 0, "source_title": "Watch two tiny, AI-powered robots play soccer", "source_text": "Watch AI-powered robots play soccer | Popular Science\n\nSOCIAL\n\nNewsletter Sign-Up\n\n# Watch two tiny, AI-powered robots play soccer\n\nGoogle DeepMind's bipedal bots go head-to-head after years of prep.\n\nBy Andrew Paul | Published Apr 10, 2024 2:00 PM EDT\n\n  * Technology\n\nDeep reinforcement learning allowed a pair of robots to play against one\nanother. Credit: Google DeepMind / Tuomas Haarnoja\n\nSHARE\n\nGoogle DeepMind is now able to train tiny, off-the-shelf robots to square off\non the soccer field. In a new paper published today in Science Robotics,\nresearchers detail their recent efforts to adapt a machine learning subset\nknown as deep reinforcement learning (deep RL) to teach bipedal bots a\nsimplified version of the sport. The team notes that while similar experiments\ncreated extremely agile quadrupedal robots (see: Boston Dynamics Spot) in the\npast, much less work has been conducted for two-legged, humanoid machines. But\nnew footage of the bots dribbling, defending, and shooting goals shows off\njust how good a coach deep reinforcement learning could be for humanoid\nmachines.\n\nWhile ultimately meant for massive tasks like climate forecasting and\nmaterials engineering, Google DeepMind can also absolutely obliterate human\ncompetitors in games like chess, go, and even Starcraft II. But all those\nstrategic maneuvers don\u2019t require complex physical movement and coordination.\nSo while DeepMind can study simulated soccer movements, it hasn\u2019t been able to\ntranslate to a physical playing field\u2014but that\u2019s quickly changing.\n\nTo make the miniature Messi\u2019s, engineers first developed and trained two deep\nRL skill sets in computer simulations\u2014the ability to get up from the ground\nand how to score goals against an untrained opponent. From there, they\nvirtually trained their system to play a full one-on-one soccer matchup by\ncombining these skill sets, then randomly pairing them against partially\ntrained copies of themselves.\n\n[Related: Google DeepMind\u2019s AI forecasting is outperforming the \u2018gold\nstandard\u2019 model.]\n\n\u201cThus, in the second stage, the agent learned to combine previously learned\nskills, refine them to the full soccer task, and predict and anticipate the\nopponent\u2019s behavior,\u201d researchers wrote in their paper introduction, later\nnoting that, \u201cDuring play, the agents transitioned between all of these\nbehaviors fluidly.\u201d\n\nThanks to the deep RL framework, DeepMind-powered agents soon learned to\nimprove on existing abilities, including how to kick and shoot the soccer\nball, block shots, and even defend their own goal against an attacking\nopponent by using its body as a shield.\n\nDuring a series of one-on-one matches using robots utilizing the deep RL\ntraining, the two mechanical athletes walked, turned, kicked, and uprighted\nthemselves faster than if engineers simply supplied them a scripted baseline\nof skills. These weren\u2019t miniscule improvements, either\u2014compared to a non-\nadaptable scripted baseline, the robots walked 181 percent faster, turned 302\npercent faster, kicked 34 percent faster, and took 63 percent less time to get\nup after falling. What\u2019s more, the deep RL-trained robots also showed new,\nemergent behaviors like pivoting on their feet and spinning. Such actions\nwould be extremely challenging to pre-script otherwise.\n\nCredit: Google DeepMind\n\nThere\u2019s still some work to do before DeepMind-powered robots make it to the\nRoboCup. For these initial tests, researchers completely relied on simulation-\nbased deep RL training before transferring that information to physical\nrobots. In the future, engineers want to combine both virtual and real-time\nreinforcement training for their bots. They also hope to scale up their\nrobots, but that will require much more experimentation and fine-tuning.\n\nThe team believes that utilizing similar deep RL approaches for soccer, as\nwell as many other tasks, could further improve bipedal robots movements and\nreal-time adaptation capabilities. Still, it\u2019s unlikely you\u2019ll need to worry\nabout DeepMind humanoid robots on full-sized soccer fields\u2014or in the labor\nmarket\u2014just yet. At the same time, given their continuous improvements, it\u2019s\nprobably not a bad idea to get ready to blow the whistle on them.\n\nAndrew Paul\n\nAndrew Paul is Popular Science's staff writer covering tech news. Previously,\nhe was a regular contributor to The A.V. Club and Input, and has had recent\nwork also featured by Rolling Stone, Fangoria, GQ, Slate, NBC, as well as\nMcSweeney's Internet Tendency. He lives outside Indianapolis.\n\nAI\n\nEngineering\n\nRobots\n\nnews\n\nSports\n\nLike science, tech, and DIY projects?\n\nSign up to receive Popular Science's emails and get the highlights.\n\nLET'S GO\n\nLinks\n\nFollow us\n\nDISCLAIMER(S)\n\nArticles may contain affiliate links which enable us to share in the revenue\nof any purchases made.\n\nRegistration on or use of this site constitutes acceptance of our Terms of\nService.\n\n\u00a9 2024 Recurrent. All rights reserved.\n\n", "frontpage": false}
