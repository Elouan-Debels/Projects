{"aid": "40004619", "title": "So, how much slower are containers?", "url": "https://blog.hathora.dev/benchmarking-container-performance/", "domain": "hathora.dev", "votes": 3, "user": "dsiddharth", "posted_at": "2024-04-11 17:27:47", "comments": 0, "source_title": "So, how much slower are containers?", "source_text": "So, how much slower are containers?\n\nAnnouncing Bare Metal on Hathora\n\nSign in Subscribe\n\n# So, how much slower are containers?\n\n#### Siddharth Dhulipalla\n\nApr 11, 2024 7 min\n\nContainers have become the gold standard of packaging, distributing, and\nrunning applications. The world of gaming, however, has been slow to adopt\ncontainers because performance (both CPU and network) is so important to\nproviding a good player experience. Understandably so, since game servers must\ncomplete intense computations within narrow time windows (less than 16ms for a\n60FPS game) to prevent dropped frames. But now more and more studios are\nstarting to embrace containers.\n\nWe wanted to know: are containers worth the performance cost?\n\nAt Hathora, we\u2019re building a highly scalable orchestration offering for\nrunning dedicated game servers and containers are a core part of our\narchitecture. We\u2019ve seen, first-hand, that containers can work well for modern\ncompetitive games like shooters \u2013 even those with high performance needs. But\nwe wanted to find hard data to fully understand the overhead costs.\n\nWe\u2019ve scoured the internet for data on how containers affect compute and\nnetwork performance. We couldn't find much beyond an IBM research paper from\n2014. So, we decided to do our own tests to figure out how containers stack up\nagainst virtual machines and bare metal. Even if you\u2019re not using containers\nfor game servers, this should still be quite informative!\n\n### tl;dr\n\nCumulative compute performance scores (CPU Mark) between containers and native\nprocesses are remarkably consistent (+/- 0.1%). However on the cloud VM,\nspecific workloads like Physics, Floating Math and Sorting had a higher\nvariance, albeit without a clear winner.\n\nContainer networking added an average latency of 5\u03bcs (microseconds) to the P99\nmeasurement within a data center. When tested over larger distances (500+\nmiles via the Internet), variance between tests of the same configuration was\nhigher than the delta between configurations.\n\nIn our observations, containers introduce no compute tax, and while there is a\nnetwork performance tax, it's so minimal that variance introduced by the\nInternet has an order of magnitude more impact.\n\nRead on if you want to understand how we conducted the testing and the in-\ndepth data.\n\n## Testing Configurations\n\nOur testing environment included both bare metal and cloud VM configurations,\nfeaturing:\n\n  * Bare Metal\n\n    * AMD 4th Gen EPYC 32-core processor, 3.8 ghz with SMT disabled\n    * 128GB RAM\n    * 2TB NVMe disk\n    * 50 Gbit/s NIC\n  * Cloud VM\n\n    * AMD 4th Gen EPYC 32-core processor, 3.7 ghz with SMT disabled\n    * 64GB RAM\n    * 8GB high-performance networked SSD\n    * 12.5 Gbit/s NIC\n\nBoth setups used Ubuntu 22.04 and we tried to keep the number of CPU cores the\nsame. However, because of the limited options for VM configurations, we\ncouldn't keep other specs like RAM, disk, and network card the same. This\nmeans we can't directly compare the bare metal setup to the cloud setup. But,\nwe can still learn a lot about how containers affect each system on their own.\n\n## The Investigation Begins\n\nOur exploration focused on native versus containerized processes, covering\nboth compute and network performance aspects.\n\n### Compute Performance\n\nWe utilized the PassMark PerformanceTest to evaluate a spectrum of\ncomputational tasks across four distinct setups:\n\n  1. Bare Metal\n\n    1. Native processes\n    2. Containerized processes\n  2. Cloud VM\n\n    1. Native processes\n    2. Containerized processes\n\nHere\u2019s what the CPU Mark score (which is an aggregate score across all sub-\ntests) looks like for each testing configuration:\n\nThe outcome was unexpected \u2014 the performance was almost identical! Containers\nran just 0.12% slower than native processes on the physical server and were\nactually 0.08% quicker on the Cloud VM. Such minor differences aren't enough\nto be meaningful, so for all intents and purposes, they performed equally.\n\nNow, let's zoom in on the specific results from the PassMark tests:\n\nThe bare metal results are extremely consistent \u2013 whether the workload is\nrunning inside a container or as a native process, there\u2019s effectively no\ndifference. The cloud VM results are interesting though. There\u2019s a lot more\nvariance between running in a container versus native process. However, which\none is more performant flip flops based on the specific test that\u2019s being run.\nThere\u2019s as much as a 10% swing in either direction which stays true across\nmultiple runs with the same setup. There\u2019s many reasons why this could be the\ncase and we plan to do a follow up piece diving deeper on these differences.\n\nAt the end of the day, containers are just Linux processes with an isolated\nfile system and network (instead of just virtual memory). They run natively on\nthe host kernel with no instruction translation tax, unlike VMs. So it makes\nsense that we didn\u2019t observe any performance difference on the compute front.\n\nA simple architecture representation of bare metal, virtualized, and\ncontainerized processes.\n\nBut containers do introduce an additional layer of processing on the network\nside, so let\u2019s see how network performance stacks up.\n\n### Network Performance\n\nLet\u2019s take a closer look at the exact path a packet takes from when it arrives\nat the network card until the application receives it.\n\nAn illustration of native processes vs. container networking.\n\nWithout containers, there\u2019s minimal processing occurring in Layers 2-4 before\nthe packet gets served up to the application. Once the container networking\ninterface is introduced, there\u2019s a few more hops the packet needs to take\nbefore arriving at the destination. Given these additional hops, it\u2019s pretty\nobvious that there will be a performance tax, so the questions we sought to\nanswer are: how much, and does it matter?\n\nFor the gaming use case, latency and jitter are of utmost importance. So, we\nnarrowed our network performance evaluation to these parameters. Utilizing a\ncustom-built Go client and server, we measured round-trip times across 1\nmillion UDP pings between two nodes within the same data center to minimize\nexternal interference.\n\nWe ran the tests in 3 configurations across both bare metal and cloud VMs:\n\n  1. Go server running as a native process\n  2. Go server inside a container with \u201chost\u201d networking (--network=\"host\" flag in Docker)\n  3. Go server inside a container with Docker networking\n\nHere are the results:\n\nOur measured P99 delta is ~5\u03bcs, (5 millionths of a second). Internet latency\nis typically measured in milliseconds (thousandths of a second), so this is\norders of magnitude smaller than any relevant latency measurement used on a\nglobal scale. To make sense of this, latency is typically a function of the\ndistance between the client and server. When clients are far away, the\nexpected connection latency is high, and vice-versa, latency is small when\nclients are close. The distance between a host and its containerized workload\nis zero, so this overhead only comes from the minuscule extra routing of\ntraffic to a private containerized workspace within the kernel. There is no\npractical difference in client latency between native processes and\ncontainerized processes.\n\nOk, so how does this translate to traffic traversing the internet? We wanted\nto verify that the 5 microsecond delay in the container networking stack\ndoesn\u2019t result in a more meaningful delay when packets need to travel further.\nSo we modified the test slightly to run the client on a Macbook in the office\nwhile running the server in the same 3 modes (native, container networking,\ncontainer with host networking) on the cloud VM.\n\nHere are the results:\n\nThe Internet adds an order of magnitude more variance (measured in\nmilliseconds instead of microseconds) to the latency than what the container\nnetwork interface introduces. We ran each mode 3 times, and the results\nbetween each run vary drastically. In some cases, the container workloads are\nfaster than the fastest native processes, but the likely cause for that is\ncongestion at some other hop along the way.\n\nLastly, we also ran a throughput test to see how much UDP traffic can be\npushed before datagrams start getting dropped. We used iperf3 for the tests,\nrunning the server as a native process and inside a container. The client was\nrun with these args: iperf3 -c $SERVER_IP -u -t 60 -b 25G which attempts to\npush 25Gb/s for 60 seconds to the server.\n\nHere are the results:\n\nSimilar to the other tests, we see that there\u2019s very little impact to\nthroughput as well.\n\nAt the end of the day, it is true that container networking introduces a\nperformance tax to network performance. However, as these results show, the\nimpact is imperceptible and is completely negated by the variance added by the\nInternet.\n\n## What's Next\n\nAt Hathora, we\u2019ve been battle testing containers in one of the most\nperformance intensive use cases \u2013 gaming. Our findings in this deep dive give\nus even more confidence that the performance cost of containers is remarkably\nlow. Despite the critical need for speed in gaming, where even milliseconds\nmatter, our results show that containers hold their ground admirably against\ntraditional setups. The negligible network delay, once a concern with\ncontainer networking, now appears as a minor hiccup rather than a deal-\nbreaker, affirming containers' place in latency-sensitive applications.\n\nHowever, our journey into container performance is far from over. While our\ninitial forays have been promising, showing minimal impact on both compute and\nnetwork fronts, questions linger\u2014especially around the nuanced effects of\nDocker networking on system performance. These initial results serve as a\nstepping stone for what\u2019s more to come. As we continue to peel back the layers\nof container technology, we aim to furnish the gaming and broader tech\ncommunities with concrete data and analysis. The path forward is clear: more\nin-depth exploration and rigorous testing to carve out definitive conclusions\non container overhead, ensuring that as we push the boundaries of what's\npossible in gaming technology, we do so with both eyes open.\n\n### Subscribe to Hathora\n\nDon't miss out on the latest news. Sign up now to get access to the library of\nmembers-only articles.\n\nHathora \u00a9 2024. Powered by Ghost\n\n", "frontpage": false}
