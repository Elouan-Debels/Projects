{"aid": "40067374", "title": "Keypoint Action Tokens Enable In-Context Imitation Learning in Robotics", "url": "https://www.robot-learning.uk/keypoint-action-tokens", "domain": "robot-learning.uk", "votes": 1, "user": "og_kalu", "posted_at": "2024-04-17 17:00:49", "comments": 0, "source_title": "Keypoint Action Tokens", "source_text": "Keypoint Action Tokens\n\ntop of page\n\n# Keypoint Action Tokens Enable In-Context Imitation Learning in Robotics\n\nNorman Di Palo and Edward Johns\n\nPaper\n\nCitation\n\n## Abstract\n\n## Video\n\nWe show that off-the-shelf text-based Transformers, with no additional\ntraining, can perform few-shot in-context visual imitation learning, mapping\nvisual observations to action sequences that emulate the demonstrator's\nbehaviour. To do so, we transform visual observations (inputs) and\ntrajectories of actions (outputs) into sequences of tokens that a text-\npretrained Transformer can ingest and generate, via a framework we call\nKeypoint Action Tokens (KAT). Despite being trained on language, these models\nexcel at translating tokenised visual keypoint observations into action\ntrajectories, performing on par or better than state-of-the-art techniques in\nthe low-data regime. Rather than operating in the language domain, KAT\nleverages text-based Transformers to operate in vision and action domains for\nefficient general imitation learning, indicating promising new avenues for\nrepurposing natural language models for embodied tasks.\n\nKey Idea We repurpose text-pretrained Transformers as sequence-to-sequence\nimitation learning machines, mapping visual inputs to action outputs via our\nproposed Keypoint Action Tokens framework.\n\n## Recording a Demo\n\nIn this video, we illustrate how demonstrations are recorded. Observations are\ntranslated into visual Keypoint Tokens, and the trajectory of poses of the\nend-effector is recorded as a series of Action Tokens, triplets of 3D points\nuniquely defining an end-effector 6D poses. Each demonstration is then added\nto the textual prompt of the Language Model.\n\n## Test-Time Inference\n\nHere, we show the robot's behaviour during testing, after having recorded\nKeypoint Action Tokens for ~10 demos and added them to the LLM textual prompt.\nThen the Keypoint Tokens for the image observed during testing are appended to\nthe prompt, after which the model predicts the Action Tokens autoregressively\nto emulate the behaviour of the demonstrations.\n\n## Videos of Tasks\n\nWe now illustrate how KAT can solve a series of everyday tasks. All the tasks\nshown here were provided with 10 demonstrations, after which the task can then\nbe solved with the objects in novel configurations. We can also observe how\nKAT is robust to visual distractors and change of background in the first two\nvideos.\n\n## Quantitative Results\n\nPlease see our paper for full results, but the two graphs below summarise some\nof our experiments.\n\nOn the left, we show that Keypoint Action Tokens (KAT) outperforms Diffusion\nPolicies, and is comparable to our own version of Diffusion Policies which\nuses our proposed Keypoint Action Tokens to represent observations and actions\n(KeyAct-DP). An important conclusion we can draw here is that when the number\nof demonstrations is small, in-context learning with an LLM performs very\nwell, whereas when the number of demonstrations is large, explicit training on\nthis data, such as with Diffusion Policies, becomes more important.\n\nOn the right, we show that KAT's performance improves as the underlying LLMs\nimprove. This suggests that the improvements and scaling of LLMs will continue\nto lead to improvements in robotics \"for free\", since in KAT we are\nrepurposing LLMs for imitation learning in robotics even though these LLMs\nwere not explicitly trained on robotics data.\n\nbottom of page\n\n", "frontpage": false}
