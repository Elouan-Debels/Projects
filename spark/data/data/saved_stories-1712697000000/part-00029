{"aid": "39978064", "title": "Lessons learned from manually classifying CIFAR-10 (2011)", "url": "http://karpathy.github.io/2011/04/27/manually-classifying-cifar10/", "domain": "karpathy.github.io", "votes": 5, "user": "djoldman", "posted_at": "2024-04-09 10:48:19", "comments": 0, "source_title": "Lessons learned from manually classifying CIFAR-10", "source_text": "Lessons learned from manually classifying CIFAR-10\n\nAndrej Karpathy blog\n\n# Lessons learned from manually classifying CIFAR-10\n\nApr 27, 2011\n\n### CIFAR-10\n\n> Note, this post is from 2011 and slightly outdated in some places.\n\nStatistics. CIFAR-10 consists of 50,000 training images, all of them in 1 of\n10 categories (displayed left). The test set consists of 10,000 novel images\nfrom the same categories, and the task is to classify each to its category.\nThe state of the art is currently at about 80% classification accuracy (4000\ncentroids), achieved by Adam Coates et al. (PDF). This paper achieved the\naccuracy by using whitening, k-means to learn many centroids, and then using a\nsoft activation function as features.\n\nState of the Art performance. By the way, running their method with 1600\ncentroids gives 77% classification accuracy. If you set the clusters to be\nrandom the accuracy becomes 70%, and if you set the clusters to be random\npatches from the training set, the accuracy goes up to 74%. It seems like the\nwhole purpose of k-means is to nicely spread out the clusters around the data.\nI\u2019m guessing that the 70% random clusters performance might be because many of\nthe clusters are relatively too far away from data manifolds, and never become\nactivated \u2013 it\u2019s as if you had much fewer clusters to begin with.\n\nHuman Accuracy. Over the weekend I wanted to see what kind of classification\naccuracy a human would achieve on this dataset. I set out to write some quick\nMATLAB code that would provide the interface to do this. It showed one image\nat a time and allowed me to press a key from 0-9 indicating my belief about\nits class category. My classification accuracy ended up at about 94% on 400\nimages. Why not 100%? Because some images are really unfair! To give you an\nidea, here are some questionable images from CIFAR-10:\n\n> CIFAR-10 human accuracy is approximately 94%\n\n### Observations\n\nA few observations I derived from this exercise:\n\n  * The objects within classes in this dataset can be extremely varied. For example the \u201cbird\u201d class contains many different types of bird (both big birds and small). Not only are there many types of bird, but the occur at many possible magnifications, all possible angles and all possible poses. Sometimes only parts of the bird are shown. The poses problem is even worse for the dog/cat category, because these animals occur at many many different types of poses, and sometimes only the head is shown. Or left part of the body, etc.\n\n  * My classification method felt strangely dichotomous. Sometimes you can clearly see the animal or object and classify it based very highly-informative distinct parts (for example, you find ears of a cat). Other times, my recognition was purely based on context and the overall cues in the image such as the colors.\n\n  * The CIFAR-10 dataset is too small to properly contain examples of everything that it is asking for in the test set. I base this conclusion at least on my multiple ways of visualizing the nearest image in the training set.\n\n  * I don\u2019t quite understand how Adam Coates et al. perform so well on this dataset (80%) with their method. My guess is that it works along the following lines: looking at the image squinting your eyes you can almost always narrow down the category to about 2 or 3. The final disambiguation probably comes from finding very good specific informative patches (like a patch of some kind of fur, or pointy ear part, etc.). The k-means dictionary must be catching these cases and the SVM likely picks up on them.\n\n  * My impression from this exercise is that it will be hard to go above 80%, but I suspect improvements might be possible up to range of about 85-90%, depending on how wrong I am about the lack of training data. (2015 update: Obviously this prediction was way off, with state of the art now in 95%, as seen in this Kaggle competition leaderboard. I\u2019m impressed!)\n\nI encourage people to try this for themselves (see my code, above), as it is\nvery interesting and fun! I have trouble exactly articulating what I learned,\nbut overall I feel like I gained more intuition for image classification tasks\nand more appreciation for the difficulty of the problem at hand.\n\nFinally, here is an example of my debugging interface:\n\nThe Matlab code used to generate these results can be found here\n\n  * Andrej Karpathy blog\n\n  * karpathy\n  * karpathy\n\nMusings of a Computer Scientist.\n\n", "frontpage": true}
