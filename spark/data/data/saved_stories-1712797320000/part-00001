{"aid": "39994476", "title": "Mixtral-8x22B on HuggingFace", "url": "https://huggingface.co/mistral-community/Mixtral-8x22B-v0.1", "domain": "huggingface.co", "votes": 18, "user": "milliondreams", "posted_at": "2024-04-10 19:01:01", "comments": 0, "source_title": "mistral-community/Mixtral-8x22B-v0.1 \u00b7 Hugging Face", "source_text": "mistral-community/Mixtral-8x22B-v0.1 \u00b7 Hugging Face\n\nHugging Face\n\n#\n\nmistral-community\n\n/\n\nMixtral-8x22B-v0.1\n\nText Generation Transformers Safetensors 5 languages mixtral Mixture of\nExperts Inference Endpoints text-generation-inference\n\nModel card Files Files and versions Community\n\n5\n\nEdit model card\n\n# Mixtral-8x22B\n\n> Kudos to @v2ray for converting the checkpoints and uploading them in\n> transformers compatible format. Go give them a follow!\n\nConverted to HuggingFace Transformers format using the script here.\n\nThe Mixtral-8x22B Large Language Model (LLM) is a pretrained generative Sparse\nMixture of Experts.\n\n## Run the model\n\n    \n    \n    from transformers import AutoModelForCausalLM, AutoTokenizer model_id = \"mistral-community/Mixtral-8x22B-v0.1\" tokenizer = AutoTokenizer.from_pretrained(model_id) model = AutoModelForCausalLM.from_pretrained(model_id) text = \"Hello my name is\" inputs = tokenizer(text, return_tensors=\"pt\") outputs = model.generate(**inputs, max_new_tokens=20) print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n\nBy default, transformers will load the model in full precision. Therefore you\nmight be interested to further reduce down the memory requirements to run the\nmodel through the optimizations we offer in HF ecosystem:\n\n### In half-precision\n\nNote float16 precision only works on GPU devices\n\n### Lower precision using (8-bit & 4-bit) using bitsandbytes\n\n### Load the model with Flash Attention 2\n\n## Notice\n\nMixtral-8x22B-v0.1 is a pretrained base model and therefore does not have any\nmoderation mechanisms.\n\n# The Mistral AI Team\n\nAlbert Jiang, Alexandre Sablayrolles, Alexis Tacnet, Antoine Roux, Arthur\nMensch, Audrey Herblin-Stoop, Baptiste Bout, Baudouin de Monicault,Blanche\nSavary, Bam4d, Caroline Feldman, Devendra Singh Chaplot, Diego de las Casas,\nEleonore Arcelin, Emma Bou Hanna, Etienne Metzger, Gianna Lengyel, Guillaume\nBour, Guillaume Lample, Harizo Rajaona, Jean-Malo Delignon, Jia Li, Justus\nMurke, Louis Martin, Louis Ternon, Lucile Saulnier, L\u00e9lio Renard Lavaud,\nMargaret Jennings, Marie Pellat, Marie Torelli, Marie-Anne Lachaux, Nicolas\nSchuhl, Patrick von Platen, Pierre Stock, Sandeep Subramanian, Sophia Yang,\nSzymon Antoniak, Teven Le Scao, Thibaut Lavril, Timoth\u00e9e Lacroix, Th\u00e9ophile\nGervet, Thomas Wang, Valera Nemychnikova, William El Sayed, William Marshall.\n\nDownloads last month\n\n    31\n\nSafetensors\n\nModel size\n\n141B params\n\nTensor type\n\nBF16\n\n\u00b7\n\n", "frontpage": true}
