{"aid": "40040483", "title": "Realists, Pioneers, Skeptics and Futurists: The AI Hot Take Matrix", "url": "https://tidalseries.com/mapping-ai-hot-takes/", "domain": "tidalseries.com", "votes": 1, "user": "tidalseries", "posted_at": "2024-04-15 13:44:32", "comments": 0, "source_title": "Realists, Pioneers, Skeptics and Futurists: The AI Hot Take Matrix", "source_text": "Realists, Pioneers, Skeptics and Futurists: The AI Hot Take Matrix\n\nSign in Subscribe\n\nBy Curtis Collicutt in Matrix Mappings \u2014 Apr 15, 2024\n\n# Realists, Pioneers, Skeptics and Futurists: The AI Hot Take Matrix\n\nI had four pieces of AI content decaying in browser tabs, and they were each\nstarkly different in terms of their feel for where AI is headed. I realised\nthat in order to understand what these articles meant for the future of AI, I\nneeded to map these positions out.\n\nAI Hot Take Matrix\n\nThere is a never-ending stream of articles, podcasts and takes on the latest\nadvances in artificial intelligence. I have participated in the AI resurgence\nas best I can, and over the past few weeks I found myself with four different\nbrowser tabs lingering, each containing a different piece of AI content.\nHowever, each of these \"takes on AI\" had a completely different spin, a\ndifferent feel to it, and I began to wonder what I was supposed to do with\nthis starkly differentiated information. Based on these takes, AI could be\nanything from complete rubbish (actually worse than not having it at all), to\nthe undoing of the world, to forcing governments to introduce universal basic\nincome as the AI does all our work for us.\n\nI realised that in order to understand what these articles meant for the\nfuture of AI, I needed to map out the positions and put them on a graph or a\nmatrix, and that's exactly what I did.\n\nFirst, let's explore the \"Hot Take Matrix\" itself.\n\n## The Hot Take Matrix\n\nBecause I had four articles by four quite different people with four\ncompletely different takes, I decided a graph might work out the best, where I\ncould map them into four quadrants. What I came up with is below, which I'll\ndescribe in a bit more detail.\n\nAI Hot Take Matrix\n\n### X-Axis: Level of Optimism About AI\n\nThis axis looks at the overall impact of AI on society, from the pessimistic\nand destructive end (left) to the optimistic views that see beneficial\noutcomes (increased productivity, solving complex problems such as disease,\netc.) on the right.The left side may have a high P(doom) rating.\n\n### Y-Axis: Degree of Confidence in AI\n\nHere we assess confidence in the technology as a whole, whether the AI's\noutput is useful and 'truthful' (right-hand side), to the extent that it is\nnot useful due to its tendency to make things up rather than answer\nnegatively. Can AI actually solve problems? Can we wire up agents to do our\nwork for us? Will it truly be helpful? Could it really replace humans for some\nwork and would we trust it to do so?\n\n### Realists\n\nRealists find current AI practical and are likely to build solutions and\ncompanies based on what is possible today; what can be done now. They find\nwhat it can do today very valuable, but because AI is a fast-moving target,\nthey are unlikely to adopt new tools and workflows, knowing that they will\nsoon be obsolete. They take a simplistic and sensible approach to AI. They may\nnot believe that AI will change society in a major way, but rather that it is\ngood, if not great, at certain things that add value. They will cut through\nthe hype and use AI to do real work.\n\n### Pioneers\n\nPioneers may have companies that are creating the \"picks and shovels\" for the\nAI revolution. They will often have extreme beliefs about where AI is going.\nHowever, they may have qualms about how powerful AI could become and how\nquickly it will do so. They may feel that they are in a difficult position\nbecause they feel that AI has the ability to do as much harm as good, but\nstrangely believe they have little power over the industry itself.\n\n### Skeptics\n\nSkeptics have reservations about both the development of AI technology and its\nability to have a positive impact on society. They often point to unresolved\nissues such as ethical concerns, potential unemployment, copyright issues or\nother negative outcomes. Overall, they may feel that generative AI simply\npredicts the next word; that it does what it does through little more than\nadvanced statistics. They may believe in AGI but that it is decades away. They\nmay feel that we are heading for a new AI winter and that the AI related\nfinancial market is headed for a collapse.\n\n### Futurists\n\nFuturists often have incredibly hopeful visions of the impact of AI, seeing\nits potential to solve major societal challenges. They use AI as much as\npossible and see massive changes on the horizon. They tend to use the\ntechnology rather than build it directly, instead using it as much as possible\nto enable their own work. They also want to teach others how to use it. They\nare the adopters of GenAI. However, they may still take practical approaches\nto AI and be unwilling to use it in certain situations, such as writing.\n\n## Meet Today's Crew\n\nHere's a few examples of these personas base on some AI related content that's\ncome across my desk over the last couple of weeks.\n\n### The Realist - Ken Kantzer\n\n> Are we going to achieve AGI? No. Not with this transformers + the data of\n> the internet + $XB infrastructure approach.\n\nKen Kantzer had a great post about his experiences after half a billion GPT\ntokens. He is building a company, Truss, that uses GenAI to do document\nanalysis. In the course of his work, he's come up with a number of practical\nlessons he's learned. Overall, he feels that when using OpenAI's ChatGPT 4,\nless is more. Less prompting. Less tools. That \"context windows are a\nmisnomer\".\n\nIs he optimistic? Yes, but from a practical point of view.\n\n> Is GPT-4 actually useful, or is it all marketing? It is 100% useful. This is\n> the early days of the internet still. Will it fire everyone? No. Primarily,\n> I see this lowering the barrier of entry to ML/AI that was previously only\n> available to Google. - https://kenkantzer.com/lessons-after-a-half-billion-\n> gpt-tokens/\n\nClearly he is a realist, someone who is using these tools to build a business\nand does not have the resources to build a dedicated in-house AI team, but one\nof his points is that with OpenAI he doesn't need to.\n\n### The Pioneer - Dario Amodei\n\nI heavily recommend listening to the entire Ezra Klein Podcast with Amodei. As\nwell, in order to understand what thinks about AI, it would be a good idea to\nreview Anthropic's Responsible Scaling Policy.\n\nBelow we can see that Amodei believes that things are going to happen very,\nvery quickly in the world of AI, where we've got months\u2013not years, not\ndecades, but months\u2013before we see massive, world-altering change. Note where\nKlein starts swearing.\n\nAmodei is a proponent of \"scaling laws\", which in high level terms means that\nthe more information and computing power we feed into these models, the more\npowerful they become.\n\n> I and my colleagues were among the first to run what are called scaling\n> laws, which is basically studying what happens as you vary the size of the\n> model, its capacity to absorb information, and the amount of data that you\n> feed into it. And we found these very smooth patterns. And we had this\n> projection that, look, if you spend $100 million or $1 billion or $10\n> billion on these models, instead of the $10,000 we were spending then,\n> projections that all of these wondrous things would happen, and we imagined\n> that they would have enormous economic value. -\n> https://www.nytimes.com/2024/04/12/podcasts/transcript-ezra-klein-\n> interviews-dario-amodei.html\n\nAmodei also suggests that A.S.L level 3 could be very soon, where we have a\nconsiderable increase in the risk of the use of AI, even to the point of it\nbeing autonomous.\n\n> ASL-3 refers to systems that substantially increase the risk of catastrophic\n> misuse compared to non-AI baselines (e.g. search engines or textbooks) OR\n> that show low-level autonomous capabilities. -\n> https://www.anthropic.com/news/anthropics-responsible-scaling-policy\n\n### The Futurist - Ethan Mollick\n\n> The current best estimates of the rate of improvement in Large Language\n> Models show capabilities doubling ever 5 to 14 months.\n\n  * Article/Podcast - https://www.nytimes.com/2024/04/02/opinion/ezra-klein-podcast-ethan-mollick.html and https://www.oneusefulthing.org/p/what-just-happened-what-is-happening\n  * https://twitter.com/emollick\n  * Substack - https://www.oneusefulthing.org/\n  * Book - https://www.moreusefulthings.com/book\n\nMollick is very positive about the current state of AI, and only thinks its is\ngoing to get better.\n\nHere he talks about how we don't know what LLMs can or can't do, and how they\ncan solve surprisingly difficult problems if we keep at it.\n\n> In fact, it is often very hard to know what these models can\u2019t do, because\n> most people stop experimenting when an approach doesn\u2019t work. Yet careful\n> prompting can often make an AI do something that seemed impossible. For\n> example, this weekend, programmer Victor Taelin made a bet. He argued that\n> \"GPTs will NEVER solve the A::B problem\" because they can\u2019t learn how to\n> reason over new information. He offered $10,000 to anyone who could get an\n> AI to solve the problem in the image below (he called it a \u201cbraindead\n> question that most children should be able to read, learn and solve in a\n> minute,\u201d though I am not sure it is that simple, but you should try it.) -\n> https://www.oneusefulthing.org/p/what-just-happened-what-is-happening\n\nAs well, he works with every model and understands which ones are working best\nfor him at this time.\n\n> The biggest capability set right now is GPT-4, so if you do any math or\n> coding work, it does coding for you. It has some really interesting\n> interfaces. That\u2019s what I would use \u2014 and because GPT-5 is coming out,\n> that\u2019s fairly powerful. And Google is probably the most accessible, and\n> plugged into the Google ecosystem. So I don\u2019t think you can really go wrong\n> with any of these. Generally, I think Claude 3 is the most likely to freak\n> you out right now. And GPT-4 is probably the most likely to be super useful\n> right now. - https://www.oneusefulthing.org/p/what-just-happened-what-is-\n> happening\n\nInteresting how he suggests that Claude 3 is capable of freaking people out,\nbut Kantzer doesn't think it adds much value over ChatGPT 4. Who's right?\n\n### The Skeptic - Gary Marcus\n\n> If we really have changed regimes, from rapid progress to diminishing\n> returns, and hallucinations and stupid errors do linger, LLMs may never be\n> ready for prime time. Instead, as I warned in August, we may well be in for\n> a correction. In the most extreme case, OpenAI\u2019s $86 billion valuation could\n> look in hindsight like a WeWork moment for AI.\n\nI subscribe to Gary Marcus' substack. It's a bit depressing for me to read\nsometimes, but I keep at it to try to have a varied and skeptical view on AI.\n\nEven just looking at the titles of his posts we can get a good feel for where\nhe stands.\n\n  * \"Evidence that LLMs are reaching a point of diminishing returns \u2014 and what that might mean\" - This one is particularly interesting because he is refuting some information that Ethan Mollick provides.\n  * \"Superhuman AGI is not nigh\"\n  * \"$10 million says we won\u2019t see human-superior AGI by the end of 2025\"\n  * \"Breaking news: Scaling will never get us to AGI \"\n\nIn the post that I'm specifically referencing here, the one where he refutes\nsome statements by Mollick, he says the below.\n\n> The study Mollick linked to doesn\u2019t actually show what he claims. If you\n> read it carefully, it says literally nothing about capabilities improving.\n> It shows that models are getting more efficient in terms of the\n> computational resources they require to get to given level of performance,\n> \u201cthe level of compute needed to achieve a given level of performance has\n> halved roughly every 8 months, with a 95% confidence interval of 5 to 14\n> months.\u201d But (a) past performance doesn\u2019t always predict future performance,\n> and (b) most of the data in the study are old, none from this year. -\n> https://garymarcus.substack.com/p/evidence-that-llms-are-reaching-a\n\nAgain...who is right? Who can see the future?\n\n## Hot Take Matrix\n\nI'll keep updating this matrix as I come across new and interesting \"hot\ntakes.\" We're all just making bets on the future of AI here!\n\n\ud83d\udc4a\n\nThanks for reading! Please forward on to your friends and colleagues.\n\n## Sign up for Tidal Series by Curtis Collicutt\n\nMaking hi-tech human-centric\n\nNo spam. Unsubscribe anytime.\n\n### Subscribe to Tidal Series by Curtis Collicutt\n\nDon\u2019t miss out on the latest issues. Sign up now to get access to the library\nof members-only issues.\n\njamie@example.com\n\nSubscribe\n\nTidal Series by Curtis Collicutt \u00a9 2024\n\nPowered by Ghost\n\n", "frontpage": false}
