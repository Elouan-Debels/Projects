{"aid": "39984209", "title": "Building Reliable Systems Out of Unreliable Agents", "url": "https://www.rainforestqa.com/blog/building-reliable-systems-out-of-unreliable-agents", "domain": "rainforestqa.com", "votes": 52, "user": "fredsters_s", "posted_at": "2024-04-09 21:01:39", "comments": 7, "source_title": "Building reliable systems out of unreliable agents | Rainforest QA", "source_text": "Building reliable systems out of unreliable agents | Rainforest QA\n\nThe Rainforest Blog\n\nLog in\n\nTalk to us\n\nBlog\n\nEngineering\n\n# Building reliable systems out of unreliable agents\n\nApril 3, 2024\n\nMaciej Gryka\n\nContents\n\nToggle\n\nIf you\u2019ve tried building real-world features with AI, chances are that you\u2019ve\nexperienced reliability issues. It\u2019s common knowledge that AI makes for great\ndemos, but... questionable products. After getting uncannily correct answers\nat first, you get burned on reliability with some wild output and decide you\ncan\u2019t make anything useful out of that.\n\nWell, I\u2019m here to tell you that there\u2019s hope. Even though AI agents are not\nreliable, it is possible to build reliable systems out of them.\n\nThese learnings come from a years-long process of creating a QA AI. While\nbuilding, we found a process that worked pretty well for us and we\u2019re sharing\nit here. As a summary, it consists of these high-level steps:\n\n  * Write simple prompts to solve your problem\n  * Use that experience to build an eval system to do prompt engineering and improve performance in a principled way\n  * Deploy your AI system with good observability, and use that signal to keep gathering examples and improving your evals\n  * Invest in Retrieval Augmented Generation (RAG)\n  * Fine-tune your model using the data you gathered from earlier steps\n\nHaving worked on this problem for a while, I think these are the best\npractices every team should adopt. But there\u2019s an additional approach we came\nup with that gave us a breakthrough in reliability, and it might be a good fit\nfor your product, too:\n\n  * Use complementary agents\n\nThe principle behind this step is simple: it\u2019s possible to build systems of\ncomplementary agents that work much more reliably than a single agent. More on\nthat later.\n\nBefore we jump in, a note on who this is for: you don\u2019t need much AI\nexperience to follow the process we lay out here. In fact, most of our team\nwhile building our QA agent didn\u2019t have previous AI or ML experience. A solid\nsoftware engineering background, however, is super helpful \u2014 a sentiment\nechoed by well-known people in the industry. Particularly, thinking deeply\nabout how to test what you\u2019re building and constantly finding ways to optimize\nyour workflow are really important.\n\nContents\n\nToggle\n\n## Start with simple prompts\n\nThe most obvious way to start using an LLM to solve a problem is simply asking\nit to do the thing in your own words. This approach often works well enough at\nthe beginning to give you hope, but starts falling down as soon as you want\nany reliability. The answers you get might be mostly correct, but not good\nenough for production. And you\u2019ll quickly notice scenarios where the answers\nare consistently wrong.\n\nThe best LLMs today are amazing generalists, but not very good specialists \u2014\nand generally, you want specialists to solve your business problems. They need\nto have enough general knowledge to not be tedious, but at the same time they\nneed to know how exactly to handle the specifics in the gray areas of your\nproblem space.\n\nLet\u2019s take a trivial example: you want to get a list of ingredients needed to\nprepare different things to eat. You start with the first thing that comes to\nmind:\n\nThis is good \u2014 the list of ingredients is right there. But there\u2019s also a\nbunch of other stuff that you don\u2019t need. For example, you might use the same\nknife for both jars and not care for being lectured about knife hygiene. You\ncan fiddle with the prompt pretty easily:\n\nBetter, but there\u2019s still some unnecessary commentary there. Let\u2019s have\nanother shot:\n\nOK, I guess that\u2019ll do. Now we just need to make it JSON so we can integrate\nit with the rest of our product. Also, just to be safe, let\u2019s run it a few\ntimes to make sure it\u2019s reliable:\n\nAs you can see, we\u2019re reliably getting JSON out, but it\u2019s not consistent: the\ncapitalization of the keys and what\u2019s included in each value varies. These\nspecific problems are easy to deal with and even this might be good enough for\nyour use case, but we\u2019re still far from the reliable and reproducible behavior\nwe\u2019re looking for.\n\n### Minimum necessary product integration\n\nAt this point, it\u2019s time to integrate a minimal version of your \u201cAI component\u201d\nwith your product, so the next step is to start using the API instead of the\nconsole. Grab your favorite LLM-provider client (or just use their API \u2014 there\nare some good reasons to stick with HTTP) and integrate it into your product\nin the most minimal way possible. The point is to start building out the\ninfrastructure, knowing that the results won\u2019t be great yet.\n\nSome cheat codes at this point, based on my experience:\n\n  * If you use mypy, you might be tempted to use strongly-typed inputs when interacting with the client (e.g., the OpenAI client), but my advice would be: don\u2019t. While I like having mypy around, it\u2019s much easier to work with plain dictionaries to build your messages and you\u2019re not risking a lot of bugs.\n  * In my experience, it\u2019s a good idea to set temperature=0.0 in all your model calls if you care about reliability. You still won\u2019t get perfect reproducibility, but it\u2019s usually the best place to start your explorations.\n  * If you\u2019re thinking about using a wrapper like instructor to get structured data out of the LLM: it\u2019s really cool and makes some use-cases very smooth, but also makes your code a little less flexible. I\u2019d usually start without it and then bring it in at a later point, once I\u2019m confident in the shape of my data.\n\n## Use an eval system to do prompt engineering\n\nThe first thing you should try after the naive \u201cask a simple question\u201d\napproach is prompt engineering. While the phrase is common, I don\u2019t think many\npeople have an accurate definition of what \u201cprompt engineering\u201d actually\nmeans, so let\u2019s define it first.\n\nWhen I say \u201cprompt engineering,\u201d I mean something like, \u201citerative improvement\nof a prompt based on measurable success criteria\u201d, where \u201citerative\u201d and\n\u201cmeasurable success criteria\u201d are the key phrases. (I like this post from last\nyear as an early attempt to define this.) The key is to have some way of\ndetermining whether an answer you get from an LLM is correct and then\nmeasuring correctness across examples to give you a number you can compare\nover time.\n\nCreate an evaluation loop so you have a way of checking any change you make,\nand then make that loop as fast as it can be so you can iterate effectively.\nFor an overview of how to think about your eval systems, see this excellent\nblog post.\n\n### Evaluating when there are multiple correct answers\n\nThis whole procedure rhymes with \u201ctesting\u201d and \u201ccollecting a validation set,\u201d\nexcept for any given question there might be multiple correct answers, so it\u2019s\nnot obvious how to do this. Here are some situations and how to deal with\nthem:\n\n  * It\u2019s possible to apply deterministic transformations on the output of the LLM before you compare it to the \u201cknown good\u201d answer. An example might be the capitalization issue from earlier, or maybe you only ever want three first words as an output. Running these transformations will make it trivial to compare what you get with what you expect.\n  * You might be able to use some heuristics to validate your output. E.g., if you\u2019re working on summarization, you might be able to say something like \u201cto summarize this story accurately, the following words are absolutely necessary\u201d and then get away with doing string matching on the response.\n  * Maybe you need the output in a certain format. E.g., you\u2019re getting function calls or their arguments from an LLM or you\u2019re expecting country codes or other well-defined outputs. In these cases, you can validate what you get against a known schema and retry in case of errors. In practice, you can use something like instructor \u2014 if you\u2019re comfortable with the constraints it imposes, including around code flexibility \u2014 and then you\u2019re left with straightforwardly comparing structured data.\n  * In a true Inception fashion, you might want to use a simpler, smaller, and cheaper LLM to evaluate the outputs of your big-LLM-using-component. Comparing two differently-written, but equivalent lists of ingredients is an easy task even for something like GPT 3.5 Turbo. Just keep in mind: even thought it\u2019s pretty reliable, you\u2019re now introducing some flakiness into your test suite. Trade-offs!\n  * To evaluate an answer, you might have to \u201cexecute\u201d the entire set of instructions the agent gives you and check if you\u2019ve reached the goal. This is more complex and time-consuming, but sometimes the only way. For example, we often ask an agent to achieve a goal inside a browser by outputting a series of instructions that might span multiple screens. The only way for us to check its answer is to execute the instructions inside Playwright and run some assertions on the final state.\n\n### Building your validation set\n\nOnce you have your evaluation loop nailed down, you can build a validation set\nof example inputs and the corresponding outputs you\u2019d like the agent to\nproduce.\n\nAs you evaluate different strategies and make changes to your prompts, it\u2019s\nideal if you have a single metric to compare over time. Something like \u201c%\nprompts answered correctly\u201d is the most obvious, but something like\nprecision/recall might be more informative depending on your use case.\n\nIt\u2019s also possible you won\u2019t be able to use a single metric if you\u2019re\nevaluating fuzzy properties of your answers, in which case you can at least\nlook at what breaks after each change and make a judgment call.\n\n### Prompt engineering tricks\n\nNow\u2019s your chance to try all the prompt engineering tricks you\u2019ve heard about.\nLet\u2019s cover some of them!\n\nFirst of all, provide all the context you\u2019d need to give to an intelligent\nhuman operator who\u2019s unfamiliar with the nuances and requirements of the task.\nThis is a necessary (but not sufficient!) condition to making your system\nwork. E.g., if you know you absolutely always want some salted butter under\nyour peanut butter, you need to include that information in your prompt.\n\nIf you\u2019re not getting the correct responses, ask the agent to think step-by-\nstep before providing the actual answer. This can be a little tricky if you\u2019re\nexpecting structured data out \u2014 you\u2019ll have to somehow give the agent a way to\ndo some reasoning before it makes any consequential decisions and locks itself\nin.\n\nE.g., if you use the tool-calling API, which is really nifty, you might be\ntempted to tell the agent to do some chain-of-thought reasoning by having a\nJSON schema similar to this:\n\n    \n    \n    [ { \"type\": \"function\", \"function\": { \"name\": \"scoop_out\", \"description\": \"scoop something out\", \"parameters\": { \"type\": \"object\", \"properties\": { \"chain_of_thought\": { \"type\": \"string\", \"description\": \"the reasoning for this action\" }, \"jar\": { \"type\": \"string\", \"description\": \"the jar to scoop out of\" }, \"amount\": { \"type\": \"integer\", \"description\": \"how much to scoop out\" } }, \"required\": [\"chain_of_thought\", \"jar\",\"amount\"] } } }, { \"type\": \"function\", \"function\": { \"name\": \"spread\", \"description\": \"spread something on a piece of bread\", \"parameters\": { \"type\": \"object\", \"properties\": { \"chain_of_thought\": { \"type\": \"string\", \"description\": \"the reasoning for this action\" }, \"substance\": { \"type\": \"string\", \"description\": \"what to spread\" } }, \"required\": [\"chain_of_thought\", \"substance\"] } } } ]\n\nUnfortunately, this will make the agent output the function name before it\nproduces the chain-of-thought, locking it into the early decision and\ndefeating the whole point. One way to get around this is to pass in the schema\nwith the available functions, but ask the agent to output a JSON that wraps\nthe function spec, similar to this:\n\n    \n    \n    You are an assistant helping prepare food. Given a dish name, respond with a JSON object of the following structure: {{ \"chain_of_thought\": \"Describe your reasoning for the next action\", \"function_name\": \"scoop_out\" | \"spread\", \"function_args\": {{ < appropriate arguments for the function > }} }}\n\nThis has fewer guarantees, but works well enough in practice with GPT 4 Turbo.\n\nGenerally, chain-of-thought has a speed/cost vs. accuracy trade-off, so pay\nattention to your latencies and token usage in addition to correctness.\n\nAnother popular trick is few-shot prompting. In many cases, you\u2019ll get a\nnoticeable bump in performance if you include a few examples of questions and\ntheir corresponding answers in your prompt, though this isn\u2019t always feasible.\nE.g., your actual input might be so large that including more than a single\n\u201cshot\u201d in your prompt isn\u2019t practical.\n\nFinally, you can try offering the agent a bribe or telling it something bad\nwill happen if it answers incorrectly. We didn\u2019t find these tactics worked for\nus, but they might for you \u2014 they\u2019re worth trying, assuming you trust your\neval process.\n\nEvery trick from the ones listed above will change things: some things will\nhopefully get better, but it\u2019s likely you\u2019ll break something else at the same\ntime. It\u2019s really important you have a representative set of examples that you\ncan work with, so commit to spending time on building it.\n\n## Improve with observability\n\nAt this point, you\u2019ll likely have something that\u2019s good enough to deploy as an\nalpha-quality product. Which you should absolutely do as soon as you can so\nyou can get real user data and feedback.\n\nIt\u2019s important you\u2019re open about where your system is in terms of robustness,\nbut it\u2019s impossible to overstate the value of users telling you what they\nthink. It\u2019s tempting to get everything working correctly before opening up to\nusers, but you\u2019ll hit a point of diminishing returns without real user\nfeedback. This is an absolutely necessary ingredient to making your system\nbetter over time \u2014 don\u2019t skip it.\n\nBefore release, just make sure your observability practices are solid. From my\nperspective, this basically means logging all of your LLM input/output pairs\nso you can a) look at them and learn what your users need and b) label them\nmanually to build up your eval set.\n\nThere a many options to go with here, from big monitoring providers you might\nalready be using to open-source libraries that help you trace your LLM calls.\nSome, like openllmetry and openinference even use the OpenTelemetry under the\nhood, which seems like a great idea. I haven\u2019t seen a tool focused on labeling\nthe data you\u2019ve gathered and turning it into a validation set, however, which\nis why we built our own solution: store some JSON files in S3 and have a web\ninterface to look at and label them. It doesn\u2019t have as many bells and\nwhistles as off-the-shelf options, but it\u2019s enough for what we need at the\nmoment.\n\n## Invest in RAG\n\nOnce you\u2019ve exhausted all your prompt-engineering tricks and you feel like\nyou\u2019re out of ideas and your performance is at a plateau, it might be time to\ninvest in a RAG pipeline. Roughly, RAG is runtime prompt engineering where you\nbuild a system to dynamically add relevant things to your prompt before you\nask the agent for an answer.\n\nAn example might be answering questions about very recent events. This isn\u2019t\nsomething LLMs are good at, because they\u2019re not usually retrained to include\nthe latest news. However, it\u2019s relatively straightforward to run a web search\nand include some of the most relevant news articles in your prompt before\nasking the LLM to give you the answer. If you have relevant data of your own\nyou can leverage, it\u2019s likely to give you another noticeable improvement.\n\nAnother example from our world: we\u2019ve got an agent interacting with the UI of\nan application based on plain-English prompts. We also have more than ten\nyears worth of data from our clients writing testing prompts in English and\nour crowd of human testers executing those instructions. We can use this data\nto tell the agent something like \u201cit looks like most human testers executing\nsimilar tasks clicked on button X and then typed Y into field Z\u201d to guide it.\n\nRetrieval is great and very powerful, but it has real trade-offs, complexity\nbeing the main one. Again, there are many options: you can roll your own\nsolution, use an external provider, or have some combination of the two (e.g.,\nusing OpenAI\u2019s embeddings and storing the vectors in your Postgres instance).\n\nA particular library that looked great (a little more on the do-it-yourself\nend of the spectrum) is RAGatouille, but I wasn\u2019t able to make it work and\ngave up after a couple of hours. In the end, we used BigQuery to get data out,\nOpenAI for producing embeddings, and Pinecone for storage and nearest-neighbor\nsearch because that was the easiest way for us to deploy something without\nsetting up a lot of new infrastructure. Pinecone makes it very easy to store\nand search through embeddings with their associated metadata to augment your\nprompts.\n\nThere\u2019s more we can do here \u2014 we didn\u2019t evaluate any alternative embedding\nengines, we only find top-3 related samples, and get limited data out of those\nsamples currently. Looking at alternative embeddings, including more samples,\nand getting more details information about each sample is something we plan to\nlook at in the future.\n\nYou can spend quite a while on this level of the ladder. There\u2019s a lot of room\nfor exploration. If you exhaust all the possibilities and ways of building\nyour pipeline, still aren\u2019t getting good enough results and can\u2019t think any\nmore sources of useful data, it\u2019s time to consider fine-tuning.\n\n## Fine-tune your model\n\nNow we\u2019re getting to the edges of the known universe. If you\u2019ve done\neverything above: created an eval system, shipped an AI product, observed it\nrunning in production, got real user data, and even have a useful RAG\npipeline, then congratulations! You\u2019re on the bleeding edge of applied AI.\n\nWhere to go from here is unclear. Fine-tuning a model based on the data you\u2019ve\ngathered so far seems like the obvious choice. But beware \u2014 I\u2019ve heard\nconflicting opinions in the industry about the merits of fine-tuning relative\nto the effort required.\n\nIt seems like it should work better, but there are unresolved practical\nmatters: OpenAI only allows you to fine-tune older models, and Anthropic is\nkind of promising to make it available soon with a bunch of caveats.\n\nFine-tuning and hosting your own models is a whole different area of\nexpertise. Which model do you choose as the base? How do you gather data for\nfine-tuning? How do you evaluate any improvements? And so on. In the case of\nself-hosted models, I\u2019d caution against hoping to save money vs. hosted\nsolutions \u2014 you\u2019re very unlikely to have the expertise and the economies of\nscale to get there, even if you choose smaller models.\n\nMy advice would be to wait a few months for the dust to settle a bit before\ninvesting here, unless you\u2019ve tried everything else already and still aren\u2019t\ngetting good-enough results. We haven\u2019t had to do this so far because we still\nhaven\u2019t exhausted all the possibilities mentioned above, so we\u2019re postponing\nthe increase in complexity.\n\n## Use complementary agents\n\nFinally, I want to share a trick that might be applicable to your problem, but\nis sort of independent of the whole process described above \u2014 you can apply it\nat any of the stages I\u2019ve described.\n\nIt involves a bit of a computation vs. reliability trade-off: it turns out\nthat in many situations it\u2019s possible to throw more resources at a problem to\nget better results. The only question is: can you find a balance that\u2019s both\nfast and cheap enough while being accurate enough?\n\nYou\u2019ll often feel like you\u2019re playing whack-a-mole when trying to fix specific\nproblems with your LLM prompts. For instance, I often find there\u2019s a tension\nbetween creating the correct high-level plan of execution and the ability to\nprecisely execute it. This reminded me of the idea behind \u201cpioneers, settlers,\nand city planners\u201d: different people have different skills and approaches and\nthrive in different situations. It\u2019s rare for a single person to both have a\ngood grand vision and to be able to precisely manage the vision\u2019s execution.\n\nOf course, LLMs aren\u2019t people, but some of their properties make the analogy\nwork. While it\u2019s difficult to prompt your way to an agent that always does the\nright thing, it\u2019s much easier to plan what\u2019s needed and create a team of\nspecialists that complement each other.\n\nI\u2019ve seen a similar approach called an \u201censemble of agents,\u201d but I prefer\n\u201ccomplementary agents\u201d for this approach because it highlights that the agents\nare meaningfully different and support each other in ways that identical\nagents couldn\u2019t.\n\nFor example, to achieve a non-obvious goal, it helps to create a high-level\nplan first, before jumping into the details. While creating high-level plans,\nit\u2019s useful to have a very broad and low-resolution view of the world without\ngetting bogged down by details. Once you have a plan, however, executing each\nsubsequent step is much easier with a narrow, high-resolution view of the\nworld. Specific details can make or break your work, and at the same time,\nseeing irrelevant information can confuse you. How do we square this circle?\n\nOne answer is creating teams of complementary agents to give each other\nfeedback. LLMs are pretty good at correcting themselves if you tell them what\nthey got wrong, and it\u2019s not too difficult to create a \u201cverifier\u201d agent that\nchecks specific aspects of a given response.\n\nAn example conversation between \u201chigh level planner\u201d and \u201cverifier\u201d agents\nmight look something like the following:\n\n    \n    \n    Planner OK, we need to make a PB&J sandwich! To do that, we need to get some peanut butter, some jelly and some bread, then take out a plate and a knife. Verifier Cool, that sounds good. Planner OK, now take the peanut butter and spread it on the bread. Verifier (noticing there's no slice of bread visible) Wait, I can't see the bread in front of me, you can't spread anything on it because it's not there. Planner Ah, of course, we need to take the slice of bread out first and put it on a plate. Verifier Yep, that seems reasonable, let's do it.\n\nThe two agents complement each other, and neither can work on its own. Nobody\nis perfect, but we can build a reliable system out of flawed pieces if we\u2019re\nthoughtful about it.\n\n### How we\u2019re using complementary agents\n\nThis is exactly what we did for our testing agents: there\u2019s a planner and a\nverifier. The planner knows the overall goal and tries to achieve it. It\u2019s\ncreative and can usually find a way to get to the goal even if it isn\u2019t\nimmediately obvious. E.g., if you ask it to click on a product that\u2019s not on\nthe current page, it\u2019ll often use the search functionality to look for the\nproduct. But sometimes the planner is too optimistic and wants to do things\nthat seem like they should be possible, but in fact aren\u2019t.\n\nFor example, it might want to click on a \u201cPay now\u201d button on an e-commerce\ncheckout page, because the button should be there, but it\u2019s just below the\nfold and not currently visible. In such cases, the verifier (who doesn\u2019t know\nthe overall goal and is only looking at the immediate situation) can correct\nthe planner and point out that the concrete task we\u2019re trying to do right now\nisn\u2019t possible.\n\n## Final notes\n\nPutting all this together again, we now have a pretty clear process for\nbuilding LLM-based products that deal with their inherent unreliability.\n\nMaking something usable in the real world still isn\u2019t a well-explored area,\nand things are changing really quickly, so you can\u2019t follow well-explored\npaths. It\u2019s basically applied research, rather than pure engineering. Still,\nhaving a clear process to follow while you\u2019re iterating will make your life\neasier and allow you to set the right expectations at the start.\n\nThe process I\u2019ve described should follow a step-by-step increase in\ncomplexity, starting with naive prompting and finally doing RAG and possibly\nfine-tuning. At each step, evaluate whether the increased complexity is worth\nit \u2014 you might get good-enough results pretty early, depending on your use\ncase. I bet getting through the first four steps will be enough to handle most\nof your problems.\n\nKeep in mind that this is going to be a very iterative process \u2014 there\u2019s no\nway to design and build AI systems in a single try. You won\u2019t be able to\npredict what works and how your users will bend your tool, so you absolutely\nneed their feedback. You\u2019ll build the first, inadequate version, use it,\nnotice flaws, improve it, release it more widely, etc. And if you\u2019re\nsuccessful and build something that gets used, your prize will be more\niterations and improvements. Yay!\n\nAlso, don\u2019t sweat having a single, optimizable success metric too much. It\u2019s\nthe ideal scenario, but it might take you a while to get to that point.\nDespite having a collection of tests and examples, we still rely \u201cvibe checks\u201d\nwhen evaluating whether a new prompt version is an improvement. Just counting\npassing examples might not be enough if some examples are more important than\nothers.\n\nFinally, try the \u201ccomplementary agents\u201d trick to work around weaknesses you\nnotice. It\u2019s often very difficult to make a single agent do the right thing\nreliably, but detecting the wrong thing so you can retry tends to be easier.\n\n### What\u2019s next for us\n\nThere\u2019s still a bunch of things that we\u2019re planning to work on in the coming\nmonths, so our product won\u2019t be static. I don\u2019t expect, however, to deviate\nmuch from the process we described here. We\u2019re continuously speaking with our\ncustomers and monitoring how our product is being used and finding edge cases\nto fix. We\u2019re also certainly not at the global optimum when it comes to\nreliability, speed, and cost, so we\u2019ll continue experimenting with alternative\nmodels, providers, and ways of work.\n\nSpecifically, I\u2019m really intrigued by the latest Anthropic models (e.g., what\ncan we usefully do with a small model like Haiku, which still has vision\ncapabilities?) and I\u2019m deeply intrigued by the ideas DSPy is promoting. I\nsuspect there are some unrealized wins in the way we structure our prompts.\n\n### Related articles\n\nAugust 30, 2022\n\nThree Ways to Draw Pie Charts without any JavaScript\n\nIn this post, we share three different approaches to drawing pie charts using\nonly HTML and CSS.\n\nAugust 2, 2022\n\nThe engineering hiring process at Rainforest QA\n\nHere's the exact hiring process we use to evaluate candidates for the\nengineering team here at Rainforest QA.\n\nApril 16, 2021\n\nHunting a race condition in the Android 10 Emulator\n\nMarch 5, 2021\n\nDiscrete optimization for on-call scheduling\n\n### Find more bugs, faster, without adding headcount\n\nRainforest gives your team QA superpowers.\n\nSpend more time building product, not bashing bugs.\n\nTalk to us\n\nTalk to Sales\n\n###### Features\n\nCrowdsourced Testing Browser Testing Mobile Testing Regression Testing\n\n###### Customer Stories\n\nCase Studies\n\n###### Support\n\nHelp Center Blog Change Log Service Status\n\n###### Developers\n\nRainforest API Rainforest CLI CircleCI GitHub\n\n###### Company\n\nAbout Careers Terms of Service Privacy Policy Security White Hat Policy\nCookies\n\n###### Integrations\n\nSlack Jira Microsoft Teams\n\n###### Compare Us\n\nCodeless Test Automation Tools Selenium Alternative Cypress Alternative Testim\nAlternative\n\n###### Certifications And Awards\n\n\u00a9 2024 Rainforest QA, Inc. \u201cRainforest\u201d is a registered U.S. & E.U. trademark.\nUnauthorized use is prohibited.\n\n", "frontpage": true}
