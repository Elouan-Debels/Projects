{"aid": "40000924", "title": "The theory of Proximal Policy Optimisation implementations", "url": "https://salmanmohammadi.github.io/content/ppo/", "domain": "salmanmohammadi.github.io", "votes": 1, "user": "desideratum", "posted_at": "2024-04-11 11:16:32", "comments": 0, "source_title": "The theory of Proximal Policy Optimization implementations", "source_text": "The theory of Proximal Policy Optimization implementations\n\nLoading [MathJax]/extensions/TeX/AMSmath.js\n\n# The theory of Proximal Policy Optimization implementations\n\n## Prelude\n\nThe aim of this post is to share my understanding of some of the conceptual\nand theoretical background behind implementations of the Proximal Policy\nAlgorithm (PPO) reinforcement learning (RL) algorithm. PPO is widely used due\nto its stability and sample efficiency - popular applications include beating\nthe Dota 2 world champions and aligning language models. While the PPO paper\nprovides quite a general and straightforward overview of the algorithm, modern\nimplementations of PPO use several additional techniques to achieve state-of-\nthe-art performance in complex environmentsProcgen, Karle Cobbe et al. Atari,\nOpenAI Gymnasium . You might discover this if you try to implement the\nalgorithm solely based on the paper. I try and present a coherent narrative\nhere around these additional techniques.\n\nI\u2019d recommend reading parts one, two, and three of SpinningUp if you\u2019re new to\nreinforcement learning. There\u2019s a few longer-form educational resources that\nI\u2019d recommend if you\u2019d like a broader understanding of the field- A (Long)\nPeek into Reinforcement Learning, Lilian Weng - Artificial Intelligence: A\nModern Approach, Stuart Russell and Peter Norvig - Reinforcement Learning: An\nIntroduction, Sutton and Barto CS285 at UC Berkeley, Deep Reinforcement\nLearning, Sergey Levine , but this isn\u2019t comprehensive. You should be familiar\nwith common concepts and terminology in RLLilian Weng\u2019s list of RL notation is\nvery useful here . For clarity, I\u2019ll try to spell out any jargon I use here.\n\n## Recap\n\n### Policy Gradient Methods\n\nPPO is an on-policy reinforcement learning algorithm. It directly learns a\nstochastic policy function parameterised by \u03b8 representing the likelihood of\naction a in state s, \u03c0\u03b8(a\u2223s). Consider that we have some differentiable\nfunction, J(\u03b8), which is a continuous performance measure of the policy \u03c0\u03b8. In\nthe simplest case, we have J(\u03b8)=E\u03c4\u223c\u03c0\u03b8[R(\u03c4)], which is known as the returnThe\nreturn represents the sum of rewards achieved over some time frame. This can\nbe over a fixed timescale, i.e. the finite-horizon return, or over all time,\ni.e. the infinite-horizon return. over a trajectoryA trajectory, \u03c4, (also\nknown as an episode or rollout) describes a sequence of interactions between\nthe agent and the environment. , \u03c4. PPO is a kind of policy gradient method-\nPolicy Gradient Algorithms, Lilian Weng - Policy Gradients, CS285 UC Berkeley,\nLecture 5, Sergey Levine which directly optimizes the policy parameters \u03b8\nagainst J(\u03b8). The policy gradient theorem shows that:\n\n\u2207\u03b8J(\u03b8)=E[t=0\u2211inf\u2207\u03b8ln\u03c0\u03b8(at\u2223st)Rt]\n\nIn other words, the gradient of our performance measure J(\u03b8) with respect to\nour policy parameters \u03b8 points in the direction of maximising the return Rt.\nCrucially, this shows that we can estimate the true gradient using an\nexpectation of the sample gradient - the core idea behind the\nREINFORCEReinforcement Learning: An Introduction, 13.3 REINFORCE: Monte Carlo\nPolicy Gradient, Sutton and Barto algorithm. This is great. This expression\nhas the more general form which substitutes Rt for some lower-variance\nestimator of the total expected reward, \u03a6Section 2, High Dimensional\nContinuous Control Using Generalized Advantage Estimation, Schulman et. al.\n2016 :\n\n\u2207\u03b8J(\u03b8)=E[t=0\u2211inf\u2207\u03b8ln\u03c0\u03b8(at\u2223st)\u03a6t](1)\n\nModern implementations of PPO make the choice of \u03a6t=A\u03c0(st,at), the advantage\nfunction. This function estimates the advantage of a particular action in a\ngiven state over the expected value of following the policy, i.e. how much\nbetter is taking this action in this state over all other actions? Briefly\ndescribed here, the advantage function takes the form\n\nA\u03c0(s,a)=Q\u03c0(s,a)\u2212V\u03c0(s)\n\nwhere V(s) is the state-value function, and Q(s,a) is the state-action -value\nfunction, or Q-functionThe value function, V(s), describes the expected return\nfrom starting in state s. Similarly the state-action value function, Q(s,a),\ndescribes the expected return from starting in state s and taking action a.\nSee also Reinforcement Learning: An Introduction, 3.7 Value Functions, Sutton\nand Barto . I\u2019ve found it easier to intuit the nuances of PPO by following the\nnarrative around its motivations and predecessor. PPO iterates on the Trust\nRegion Policy Optimization (TRPO) method which constrains the objective\nfunction with respect to the size of the policy update. The TRPO objective\nfunction is defined asI\u2019m omitting ln from ln\u03c0(at\u2223st) for brevity from here\non. - Proximal Policy Optimization Algorithms, Section 2.2, Schulman et al. -\nTrust Region Policy Optimization, Schulman et al. for further details on the\nconstraint. :\n\nJ(\u03b8)=subject toE[\u03c0\u03b8old(at,st)\u03c0\u03b8(at,st)At]E[KL(\u03c0\u03b8old\u2223\u2223\u03c0\u03b8)]\u2264\u03b4\n\nWhere KL is the Kullback-Liebler divergence (a measure of distance between two\nprobability distributions), and the size of policy update is defined as the\nratio between the new policy and the old policy:\n\nr(\u03b8)=\u03c0\u03b8old(at,st)\u03c0\u03b8(at,st)\n\nPolicy gradient methods optimise policies through (ideally small) iterative\ngradient updates to parameters \u03b8. The old policy, \u03c0\u03b8old(at,st), is the one\nused to generate the current trajectory, and the new policy, \u03c0\u03b8(at,st) is the\npolicy currently being optimisedNote: at the start of a series of policy\nupdate steps, we have \u03c0\u03b8old(at,st)=\u03c0\u03b8(at,st), so r(\u03b8)=1. . If the advantage is\npositive, then the new policy becomes greedier relative to the old policyThe\nnew policy will place higher density on actions relative to the old policy,\ni.e. \u03c0\u03b8(at,st)>\u03c0\u03b8old(at,st). , and we have r(\u03b8)>1 - the inverse applies for\nnegative advantage, where we have r(\u03b8)<1\\. The core principle of TRPO (and\nPPO) is to prevent large parameter updates which occur due to variance in\nenvironment and training dynamics, thus helping to stabilise optimisation.\nUpdating using the ratio between old and new policies in this way allows for\nselective reinforcement or penalisation of actions whilst grounding updates\nrelative to the original, stable policyConsider optimising our policy using\neqn. 1 - without normalising the update w.r.t. the old policy, updates to the\npolicy can lead to catastrophically large updates. .\n\n### PPO\n\nPPO modifies the TRPO objective function by constraining the objective\nfunction:\n\nJ(\u03b8)=E[min(\u03c0\u03b8old(at,st)\u03c0\u03b8(at,st)At,clip(\u03c0\u03b8old(at,st)\u03c0\u03b8(at,st),1\u2212\u03b5,1+\u03b5)At]\n\nTo break this somewhat dense equation down, we can substitute our earlier\nexpression r(\u03b8) in:\n\nJ(\u03b8)=E[min(r(\u03b8)At,clip(r(\u03b8),1\u2212\u03b5,1+\u03b5)At)]\n\nThe clip term constrains the policy ratio, r, to lie between 1\u2212\u03b5 and 1+\u03b5\u03b5 is\nusually set to \u223c0.1. . In this manner, the objective function disincentives\ngreedy policy updates in the direction of improving the objective. When the\nnew policy places lower density on actions compared to the previous policy,\ni.e. may be more conservative, the advantage update is smaller. When the new\npolicy places higher density on actions compared to the previous policy i.e.\nis greedier, the advantage update is also smaller. This is why PPO is\nconsidered to place a lower pessimistic bound on policy updates. The policy is\nonly updated by 1+\u03b5 or 1\u2212\u03b5, depending on whether the advantage function is\npositive or negative, respectively.\n\nSo far, we\u2019ve introduced the concept of policy gradients, objective functions\nin RL, and the core concepts behind PPO. Reinforcement learning algorithms\nplace significant emphasis in reducing variance during\noptimisationStochasticity in environment dynamics, delayed rewards, and\nexploration-exploitation tradeoffs all contribute to unstable training. . This\nbecomes apparent in estimating the advantage function which relies on rewards\nobtained during a trajectory. Practical implementations of policy gradient\nalgorithms take additional steps to trade variance for bias here by also\nestimating an on-policy state-value function V\u03c6\u03c0(st), which is the expected\nreturn an agent receives from starting in state st, and following policy \u03c0\nthereafter. Jointly learning a value function and policy function in this way\nis known as the Actor-Critic framework- Reinforcement Learning: An\nIntroduction, 6.6 Actor-Critic Methods, Sutton and Barto - A (Long) Peek into\nReinforcement Learning, Value Function, Lilian Weng .\n\n### Value Learning and Actor-Critics\n\nValue-function learning involves approximating the future rewards of following\na policy from a current state. The value function is learned alongside the\npolicy, and the simplest method is to minimise a mean-squared-error objective\nagainst the discounted returnThe discounted return down-weights rewards\nobtained in the future by an exponential discount factor \u03b3t, i.e. rewards in\nthe distant future aren\u2019t worth as much as near-term rewards. ,\nR(\u03c4)=t=0\u2211inf\u03b3trt:\n\n\u03c6\u2217=arg min\u03c6Est,Rt\u223c\u03c0[(V\u03c6(st)\u2212\u03b3tr(st))2]\n\nWe can now use this to learn a lower-variance estimator of the advantage\nfunctionThe on-policy Q-function is defined as the expected reward of taking\naction at in state st, and following policy \u03c0 thereafter:\nQ\u03c0(s,a)=E[rt+\u03b3V\u03c0(st+1)] :\n\nA^\u03c0(s,a)=Est+1[Q\u03c0(st,at)\u2212V\u03c6\u03c0(st)]=Est+1[rt+\u03b3V\u03c6\u03c0(st+1)\u2212V\u03c6\u03c0(s)]\n\nWe end up with an estimator of the advantage function that solely relies on\nsamples rewards and our learned approximate value function. In fact, our\nexpression shows that the advantage function can be estimated with the\ntemporal-differenceReinforcement Learning: An Introduction, 6. Temporal-\nDifference Learning, Sutton and Barto residual error, \u03b4t, of the value\nfunction:\n\nV\u03c6(st)V\u03c6(st)where\u03b4t\u2190V\u03c6(st)+\u03b1(rt+\u03b3V\u03c6(st+1)\u2212V\u03c6(st))\u2190V\u03c6(st)+\u03b1\u03b4t=rt+\u03b3V\u03c6(st+1)\u2212V\u03c6(st)\n\n### Generalised Advantage Estimation (GAE)\n\nThere\u2019s one more thing we can do to reduce variance. The current advantage\nestimator estimates reward by samples collected from a single trajectory.\nHowever, there is a huge amount of variance in the possible trajectories that\nmay evolve from a given state. These trajectories may look similar in the\nshort-term (except for policies early in the optimisation process which are\nfar more random), but longer-term rewards can vary significantly. We could\nconsider a lower-variance estimate of the advantage by sampling trajectories -\nonce again trading variance for bias. This is the central idea behind n-step\nreturns- DeepMimic Supplementary A, Peng et al. - Reinforcement Learning: An\nIntroduction, 7.1 n-step TD Prediction, Sutton and Barto . Consider the term\n\u03b4t in our estimation of the advantage function. we take the initial reward\nobserved from the environment, rt, then bootstrap future estimated discounted\nrewards, and subtract a baseline estimated value function for the stateDaniel\nTakeshi\u2019s post on using baselines to reduce variance of gradient estimates is\nuseful here. . For a single time-step, this can be denoted as:\n\nAt(1)^=rt+\u03b3V\u03c6\u03c0(st+1)\u2212V\u03c6\u03c0(s)=\u03b4tV\u03c6\n\nWhat if we sample rewards from multiple timesteps, and then estimate the\nfuture discounted rewards from then on? Let\u2019s denote At^(k) as follows:\n\nAt^(1)At^(2)At^(k)At^(\u221e)=rt+\u03b3V\u03c6\u03c0(st+1)\u2212V\u03c6\u03c0(s)=rt+\u03b3rt+1+\u03b32V\u03c6\u03c0(st+2)\u2212V\u03c6\u03c0(s)=rt+\u03b3rt+1+...+rt+k\u22121+\u03b3kV\u03c6\u03c0(st+k)\u2212V\u03c6\u03c0(s)=rt+\u03b3rt+1++\u03b32rt+2+...\u2212V\u03c6\u03c0(s)=\u03b4tV\u03c6=\u03b4tV\u03c6+\u03b3\u03b4t+1V\u03c6=\u03b4tV\u03c6+\u03b3\u03b4t+1V\u03c6+\u03b32\u03b4t+1V\u03c6=l=0\u2211k\u22121\u03b3l\u03b4t+1V\u03c6\n\nObserve that for k=\u221e we recover an unbiased, high-variance expectation of the\ninfinite-horizon discounted return, minus our baseline estimate value\nfunction. GAE- High Dimensional Continuous Control Using Generalized Advantage\nEstimation, Schulman et. al. 2016 - Notes on the Generalized Advantage\nEstimation Paper, Daniel Takeshi introduces a discount factor, \u03bb\u2208[0,1], to\ntake an exponentially weighted average over every k-th step estimator. Using\nnotation from the paperThe identity 1\u2212k1=1+k+k2+k3+... for \u2223k\u2223<1 is useful to\nnote here. , we can derive a generalized advantage estimator for cases where\n0<\u03bb<1:\n\nAt^\u03b3\u03bb=(1\u2212\u03bb)(At^(1)+\u03bbAt^(2)+\u03bb2At^(3)+...)=(1\u2212\u03bb)(\u03b4tV\u03c6+\u03bb(\u03b4tV\u03c6+\u03b3\u03b4t+1V\u03c6)+\u03bb2(\u03b4tV\u03c6+\u03b3\u03b4t+1V\u03c6+\u03b32\u03b4t+2V\u03c6)+...)=(1\u2212\u03bb)(\u03b4tV\u03c6(1+\u03bb+\u03bb2+...)+\u03b3\u03b4t+1V\u03c6(\u03bb+\u03bb2+\u03bb3+...)+...)=(1\u2212\u03bb)(\u03b4tV\u03c6(1\u2212\u03bb1)+\u03b3\u03b4tV\u03c6(1\u2212\u03bb\u03bb)+\u03b32\u03b4tV\u03c6(1\u2212\u03bb\u03bb2))=l=1\u2211\u221e(\u03b3\u03bb)l\u03b4t+1V\u03c6\n\nGreat. As you may have noticed, there\u2019s two special cases for this expression\n- \u03bb=0, and \u03bb=1:\n\nAt^\u03b3\u22170At^\u03b3\u22171=At^(1)=rt+\u03b3V\u03c6\u03c0(st+1)\u2212V\u03c6\u03c0(s)=\u03b4t=l=1\u2211\u221e\u03b3l\u03b4t+1V\u03c6=(rt+\u03b3V\u03c6(st+1)\u2212V\u03c6(st))+\u03b3(rt+1+\u03b3V\u03c6(st+2)\u2212V\u03c6(st+1))+\u03b32(rt+2+\u03b3V\u03c6(st+3)\u2212V\u03c6(st+2))+...=rt+\u03b3V\u03c6(st+1)\u2212V\u03c6(st)+\u03b3rt+1+\u03b32V\u03c6(st+2)\u2212\u03b3V\u03c6(st+1)+\u03b32rt+2+\u03b33V\u03c6(st+3)\u2212\u03b32V\u03c6(st+2)+...=l=1\u2211\u221e\u03b3lrt+1\u2212V\u03c6(st)\n\nWe see that \u03bb=0 obtains the original biased low-variance actor-critic\nadvantage estimator. For \u03bb=1, we obtain a low-bias, high-variance advantage\nestimator, which is simply the discounted return minus our baseline estimated\nvalue function. For \u03bb\u2208(0,1), we obtain an advantage estimator which allows\ncontrol of the bias-variance tradeoff. The authors note that the two\nparameters, \u03b3 and \u03bb, control variance in different ways. Lower values of \u03b3\ndiscount future rewards, which will always result in a biased estimate of the\nreturn, since there\u2019s an implicit prior that future rewards are less valuable.\nThe authors note that \u03b3 controls the strength of this prior regardless of how\naccurate our value function is. On the other hand, since n-step returns are\nunbiased estimators of the advantage function, lower values of \u03bb reduce\nvariance when the value function is accurate. In other words, when\nV\u03c6(st)=V(st) and for 0<\u03bb<1 , we obtain an unbiased estimator of the advantage\nfunction.\n\n### Pseudocode\n\nTying everything together, we can show the general process of updating our\npolicy and value function using PPO for a single trajectory of fixed length N:\n\n> Given policy estimator \u03c0\u03b8, value function estimator V\u03c6, \u03b3, \u03bb, N time-steps.\n>\n\n>> For t=1,2,...,N:\n\n>>\n\n>>> Run policy \u03c0\u03b8 in environment and collect rewards, observations, actions,\nand value function estimates rt,st,ar,vt where vt=V\u03c6(st)\n\n>>\n\n>> Compute \u03b4tV\u03c6=rt+\u03b3vt+1\u2212vt for all t.\n\n>>\n\n>> Compute generalized advantage estimate, At^=l=0\u2211N(\u03b3\u03bb)t\u03b4t for all t.\n\n>>\n\n>> Sample \u03c0\u03b8(at,st) log-probabilities from stored actions and states.\n\n>>\n\n>> Optimise \u03b8 using J(\u03b8), the PPO objectiveThis is usually done over M\nminibatch steps for M\u2264N. \u03c0\u03b8old is fixed as the initial policy at the start of\nthe trajectory, and \u03c0\u03b8 is taken as the policy at the current optimisation\nstep. .\n\n>>\n\n>> Optimise \u03c6 using L=(V\u03c6(st)\u2212\u03b3trt)2 using stored rt and stSimilarly to the\npolicy optimisation step, this is also done over M steps. V\u03c6 is taken as the\nvalue function at the current optimisation step, i.e. value estimates are\nbootstrapped. .\n\n>\n> ... repeat!\n\nFeedback and corrections are very welcomed and appreciated. I\u2019ll follow this\npost with implementation details soon. For now, I\u2019d recommend the excellent\nresource by Shengyi Huang on reproducing PPO implementations from popular RL\nlibraries. If you\u2019re able to implement the policy update from the PPO paper,\nhopefully there\u2019s enough detail here that you\u2019re able to reproduce other\nimplementations of PPO. Thank you so much for reading.\n\nThe theory of Proximal Policy Optimization implementations - April 11, 2024 -\nsalman mohammadi\n\nmy github credit to clayh53 for this theme.\n\nget in touch at firstname.lastname at outlook dot com\n\n", "frontpage": false}
