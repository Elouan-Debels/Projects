{"aid": "40066945", "title": "Ampere Readies 256-Core CPU Beast, Awaits the AI Inference Wave", "url": "https://www.nextplatform.com/2024/04/16/ampere-readies-256-core-cpu-beast-awaits-the-ai-inference-wave/", "domain": "nextplatform.com", "votes": 3, "user": "rbanffy", "posted_at": "2024-04-17 16:28:33", "comments": 0, "source_title": "Ampere Readies 256-Core CPU Beast, Awaits The AI Inference Wave", "source_text": "Ampere Readies 256-Core CPU Beast, Awaits The AI Inference Wave\n\nLatest\n\n  * [ April 16, 2024 ] Ampere Readies 256-Core CPU Beast, Awaits The AI Inference Wave Compute\n  * [ April 15, 2024 ] Los Alamos Pushes The Memory Wall With \u201cVenado\u201d Supercomputer HPC\n  * [ April 12, 2024 ] AWS Hedges Its Bets With Nvidia GPUs And Homegrown AI Chips Compute\n  * [ April 11, 2024 ] Looking To Adopt Generative AI Within Your Organization? AI\n  * [ April 10, 2024 ] With MTIA v2 Chip, Meta Can Do AI Inference, But Not Training AI\n  * [ April 10, 2024 ] Gelsinger: With Gaudi 3 and Xeon 6, AI Workloads Will Come Our Way AI\n  * [ April 9, 2024 ] Google Joins The Homegrown Arm Server CPU Club Compute\n  * [ April 9, 2024 ] With Gaudi 3, Intel Can Sell AI Accelerators To The PyTorch Masses AI\n\n# Ampere Readies 256-Core CPU Beast, Awaits The AI Inference Wave\n\nApril 16, 2024 Timothy Prickett Morgan Compute 1\n\nHow many cores is enough for server CPUs? All that we can get, and then some.\n\nFor the past two decades, the game in compute engines has been to try to pack\nas many cores and additional functionality as possible into a socket and make\nthe overall system price/performance come down per unit of power consumed and\nheat dissipated.\n\nThe first dual-core processors entered the datacenter in 2001 before Denard\nscaling of chip clock speeds more or less ceased around four years later,\nwhich is the last free ride in architectural enhancements that chip architects\nhad. Moore\u2019s Law was still going strong back then, but was clearly entering\nmiddle age as the cost of transistors for each manufacturing node kept getting\nsmaller and smaller, but at a decreasing rate. The cost per transistor started\ngoing up rather than down around the 10 nanometer barrier, and it will\ncontinue for the foreseeable future until we find an alternative to CMOS chip\netching. Which probably means as long as any of us of a certain age will care.\n\nAnd so, we want more and more cores on our compute engines, and the socket\nfull of chiplets is becoming the motherboard, like a black hole sucking the\nsurrounding components into it, because anything that can keep the signaling\ninside the socket increases computational and economic efficiency even if the\nmove to chiplets is creating all kinds of havoc with power and thermals. The\ninterconnects are eating an increasing share of the socket power budget, but\nmoving to chiplets increases yield and therefore lowers manufacturing costs\nand allows a kind of flexibility that we think the industry wants. Why should\nyour compute engine socket only come with components from one chip maker? Your\nmotherboard never did.\n\nIt is with that backdrop and the desire to create better compute engines for\nAI inference and other more traditional workloads that Ampere Computing is\nhinting at future AmpereOne compute engines to come.\n\nAmpere Computing started out using the X-Gene 3 cores created by Applied\nMicro, the original custom Arm server chip maker from the prior decade that\nformed a part of the foundation of Ampere when it was founded back in early\n2018. We are smart-aleks here at The Next Platform, and we created an\naugmented Ampere Computing Arm server chip roadmap that included a codename of\n\u201cMagneto\u201d for those original eMAG chips from 2019, and we have put names on\nthe homegrown cores used in the more recent AmpereOne processors as well as on\nthe AmpereOne chip generations so we don\u2019t all go mad trying to talk about the\ncompany\u2019s chip lineup.\n\nHere is our original story on the Ampere roadmap from back in May 2022, and\njust as a refresher here is that roadmap because it is relevant to the\nconversation we just had with Jeff Wittich, chief product officer at Ampere\nComputing, to get an update on progress on the AmpereOne roadmap.\n\nOn this roadmap, the dates shown correlate approximately to the time between\nwhen the chips come back from the foundry and start sampling to early adopter\ncustomers. They tend to ship in volumes about a year later, which is common\nwith all kinds of ASICs like switch chips. We call this timing, when the feeds\nand speeds are available and the chips are close to shipping in volume, the\n\u201claunch\u201d of the chip.\n\nThe 192 core \u201cSiryn\u201d AmpereOne-1 chip, which is based on what we are calling\nthe A1 core, which is etched in 5 nanometer processes from Taiwan\nSemiconductor Manufacturing Co, and which is the first homegrown Arm core from\nAmpere Computing, was launched in May 2023. (We detailed it here.) That is the\nreal Ampere Computing codename, but we are the ones calling it the AmpereOne-1\nso we can differentiate it from successive AmpereOne chips. Anyway, that\n192-core Siryn chip has eight DDR5 memory channels, and like the Graviton3 Arm\nserver CPU designs from Amazon Web Services, the AmpereOne-1 design puts all\nof the compute cores on a single monolithic die and then wraps memory\ncontroller and I/O controller chiplets around the outside. There are SKUs of\nthe AmpereOne-1 that have 136, 144, 160, 176, and 192 cores active, with power\ndraw ranging from 200 watts to 350 watts; cores run at 3 GHz.\n\nWittich reminded us on our call, which was ostensibly about AI inferencing on\nCPUs, that an updated AmpereOne chip was due later this year with twelve\nmemory channels. In our roadmap, this is called the \u201cPolaris\u201d chip \u2013 that is\nour codename for it \u2013 and it is using an A2 core with more performance and\nmore features than the A1 core.\n\nThat AmpereOne-2, as we are also calling it, will have a 33 percent increase\nin DDR5 memory controllers, and depending on the speed and capacity of the\nmemory supported, about a third more capacity and possibly 40 percent or even\n50 percent more memory bandwidth. The AmpereOne-1 had DDR5 memory running at\n4.8 GHz, but you can get DIMMs that run as high as 6 GHz or 7.8 GHz. If you\ndon\u2019t mind the heat, with a dozen memory controllers running at 7.8 GHz, the\nbandwidth on this AmpereOne-2 chip, which is etched with an enhanced 5\nnanometer process from TSMC, could go up by a factor of 2.25X. We think Ampere\nComputing will shoot for 6.4 GHz DDR5 memory and double the bandwidth per\nsocket, which will help make its case for doing AI inferencing on the CPU.\n\nThat boost in memory controllers is probably setting the stage for what we are\ncalling the AmpereOne-3 chip, which is our name for it and which is etched in\n3 nanometer (3N to be precise) processes from TSMC. We think this will be\nusing a modified A2+ core. Wittich confirmed to us that a future AmpereOne\nchip was in fact using the 3N process and was at TSMC right now being etched\nas it moves towards its launch. And then he told us that this future chip\nwould have 256 cores. He did not get into chiplet architectures, but did say\nthat Ampere Computing was using the UCI-Express in-socket variant of PCI-\nExpress as its chiplet interconnect for future designs.\n\n\u201cWe have been moving pretty fast on the on the compute side,\u201d Wittich tells\nThe Next Platform. \u201cThis design has got about a lot of other cloud features in\nit \u2013 things around performance management to get the most out of all of those\ncores. In each of the chip releases, we are going to be making what would\ngenerally be considered generational changes in the CPU core. We are adding a\nlot in every single generation. So you are going to see more performance, a\nlot more efficiency, a lot more features like security enhancements, which all\nhappen at the microarchitecture level. But we have done a lot to ensure that\nyou get great performance consistency across all of the AmpereOnes. We are\nalso taking a chiplet approach with this 256-core design, which is another\nstep as well. Chiplets are a pretty big part of our overall strategy.\u201d\n\nWe think there is a good chance that this future AmpereOne-3 chip \u2013 we need a\nMarvel universe code name if you want to suggest it \u2013 is actually going to be\na two-chiplet design on the cores, with 128 cores per chiplet. It could be a\nfour-chiplet design with 64 cores per chiplet.\n\nIt looks like AWS has two chiplets making up the Graviton4, using the\n\u201cDemeter\u201d V2 cores from Arm Ltd, with 48 cores per compute chiplet. The\nGraviton4 has a dozen DDR5 memory controllers, by the way. If AWS can break up\nits monolithic compute die, so can Ampere Computing.\n\nWe also think that AmpereOne-3 will support PCI-Express 6.0 I/O controllers,\nthe bandwidth of which is also important for AI inference workloads.\n\nThere is also a high likelihood that the AmpereOne-3 will have fatter vectors\n\u2013 AmpereOne-1 has two 128-bit vectors, like a Neoverse N1 and N2 core did, but\nwe think that will need to be doubled up to four 128-bit vectors, or a true\ntensor core matrix math unit will have to be added to the cores or as an\nadjunct to the core tiles. There are a lot of possibilities, but we know one\nthing: Ampere Computing is hell bent on capturing as much AI inference on its\nCPUs as is technically feasible.\n\nJust as the large language models came in and transformed (pun intended)\neverything about the AI market and made it very much real for everyone, and\njust as Intel and IBM were adding matrix math units to their respective Xeon 5\n(formerly Xeon SP) and Power10 processors, we made an argument about how AI\ninference would remain largely on the CPU, even though it sure didn\u2019t look\nlike it at the time when GPT-4 needed 8 or 16 GPUs to host models to do\ninference with reasonable sub-200 millisecond response times, and today we are\nlooking at around 32 GPUs as the GPT-4 model grows in parameter size.\n\nOur argument then is only amplified now that GPUs are so expensive and scarce:\nIt is hard to be free. And it makes sense for AI inference to be running in\nthe same place that the applications themselves are running.\n\nIn the tests above, the machines are all single socket instances either\nrunning on the Oracle or AWS clouds (left side) or using OEM iron running in a\ntesting lab (right side).\n\nThe DLRM is a deep learning recommendation engine from Meta Platforms; BERT is\nan earlier natural language transformer from Google; Whisper is an automatic\nspeech recognition system from OpenAI; and ResNet-50 is the quintessential\nimage recognition model that got this whole machine learning AI ball rolling.\n\nOn the left side, the inferences per dollar spent are pretty close between an\n80-core Altra CPU from Ampere Computing and an Nvidia T4 GPU accelerator\nexcept with the OpenAI Whisper automatic speech recognition system, where the\nAltra CPU just blows away the T4 and also makes the Intel \u201cIce Lake\u201d Xeon and\nAWS Graviton3 chips look pretty bad.\n\nOn the right, we are just measuring inferences per second for the four\nworkloads, and this time, it compares a 128-core Altra Max M128-30 against a\n40-core Intel Xeon SP-8380 and a 64-core AMD Epyc 7763. Obviously, there are\nmore recent Intel and AMD processors, which cram more cores into a socket. But\nAmpere is starting its ramp of its 192-core AmpereOne A192-32X, which has\nabout twice the integer performance as the Altra Max M128-30 and probably 1.5X\nmore performance on the vector units and possibly, if there have been\narchitectural changes, as much as 2X more. And we presume the 256-core chip\nthat will launch next year will have a big jump in integer and vector\nperformance.\n\nHere is an interesting comparison using the OpenAI Whisper speech to text\nplatform, rated in transactions per second:\n\nUsing the whisper-medium model, an Altra Max M128-30 can do 375,415\ntransactions per second (TPS), and an Nvidia \u201cHopper\u201d H100 GPU can do 395,500\nTPS.\n\nIf you cap the racks at 15 kilowatts \u2013 which is reasonable in an enterprise\ndatacenter \u2013 then 2,400 servers using the Altra Max CPUs in 60 racks costs\n$17.2 million and delivers 901 million TPS. That works out to a\nprice/performance of $18,979 per million TPS. At the same 60 racks of space at\n15 kilowatts per rack, that is only one DGX H100 per rack, which ain\u2019t that\nmuch. Those 60 DGX H100s would cost $27.3 million and deliver only 190 million\nTPS, but cost $143,684 per million TPS. The CPU inference has 7.6X better\nprice/performance on Whisper. You can put four DGXs in each rack and buy 240\nof them, for a total of $109.2 million, and drive 760 million TPS, but the\nprice/performance doesn\u2019t change. We suspect the Whisper model is using FP16\nhalf precision data formats and processing, and by moving to FP8, Nvidia could\ncut that gap in half. But half the gaps is still a 3.8X difference in bang for\nthe buck in favor of the Ampere Computing Arm CPU.\n\nWhen Nvidia delivers FP4 processing on inference with the \u201cBlackwell\u201d B100\nGPUs and delivers 5X the raw performance on inference and the AmpereOne-3\ncomes out with 256 cores and perhaps delivers 3X to 4X the inference\nperformance (it depends on what the AmpereOne-3 looks like), Nvidia will close\nthe gap some.\n\nWe will have to wait and see. But don\u2019t count the CPU out just yet in the\nenterprise datacenter running modest-sized AI models with tens to hundreds of\nbillions of parameters.\n\nHaving said all that, we would love to see comparisons of the 192-core\nAmpereOne-2 against AMD \u201cGenoa\u201d Epyc 9004s, against AMD \u201cBergamo\u201d Epyc 9004s,\nagainst Intel \u201cSapphire Rapids\u201d Xeon SPs, and against Nvidia Hopper H200s with\nfatter memory than the H100 has. The Intel \u201cSierra Forest\u201d Xeon 6 processors,\ndue in the second quarter of this year, can scale to 288 cores, but do not\nhave AVX-512 vector or AMX matrix math units, and are essentially useless for\nAI inference.\n\n#### Sign up to our Newsletter\n\nFeaturing highlights, analysis, and stories from the week directly from us to\nyour inbox with nothing in between. Subscribe now\n\n### Related Articles\n\nCompute\n\n### VMware Partners Its Way Deeper into Cloud, Edge, And AI\n\nOctober 6, 2021 Jeffrey Burt Compute 0\n\nSoftware maker VMware has always been about tight partnerships with other tech\nvendors. When you are middleware between hardware and operating systems, you\nsort of have no choice. You need to support a diverse set of hardware below\nand a rich of operating systems and applications above. During its early ...\n\nCompute\n\n### Intel To Broaden FPGA Lineup And Make Them At Home\n\nSeptember 27, 2022 Timothy Prickett Morgan Compute 2\n\nBack in 2015, when Intel was flush with cash thanks to a near-monopoly from\nX86 datacenter compute, it shelled out an incredible $16.7 billion to acquire\nFPGA maker Altera because a few hyperscalers and cloud builders were monkeying\naround with offloading whole chunks of CPU compute to FPGAs to create ...\n\nCompute\n\n### Nvidia Unifies AI Compute With \u201cAmpere\u201d GPU\n\nMay 14, 2020 Timothy Prickett Morgan Compute 11\n\nThe in-person GPU Technical Conference held annually in San Jose may have been\ncanceled in March thanks to the coronavirus pandemic, but behind the scenes\nNvidia kept on pace with the rollout of its much-awaited \u201cAmpere\u201d GA100 GPU,\nwhich is finally being unveiled today. All of the feeds and speeds ...\n\n#### 1 Comment\n\n  1. Slim Albert says:\n\nApril 17, 2024 at 5:23 am\n\nLooks like a great riposte to the Sierra Frostcake (eh-eh-eh!), especially\nwith those vector units. In analogy to Poseidon V3\u2019s 2\u00d764-core CSS dies (and\nthe 2026 SiPearl Rhea3, in 2-3 nm), I think that 4\u00d764-core dies might be the\nticket for AmpereOne-3. It would make it easier to produce SKUs with 64, 128,\n192, and 256 cores. Keeping all cores fed could be a slight challenge though \u2014\nmaybe some 6.4+ GHz channel-doubling DDR5 MCR DIMMs (if that\u2019s what MCR does)\nwould be useful here. PCIe 6.0/CXL 3.0 is also definitely the way to go. A\ncool roadmap from Ampere!\n\nReply\n\n### Leave a Reply Cancel reply\n\nThis site uses Akismet to reduce spam. Learn how your comment data is\nprocessed.\n\n###### About\n\nThe Next Platform is published by Stackhouse Publishing Inc in partnership\nwith the UK\u2019s top technology publication, The Register.\n\nIt offers in-depth coverage of high-end computing at large enterprises,\nsupercomputing centers, hyperscale data centers, and public clouds. Read\nmore...\n\n###### Newsletter\n\nFeaturing highlights, analysis, and stories from the week directly from us to\nyour inbox with nothing in between. Subscribe now\n\nAll Content Copyright The Next Platform\n\n", "frontpage": true}
