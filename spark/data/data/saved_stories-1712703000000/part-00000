{"aid": "39981295", "title": "A 1064 parameter neural network interpolates 450x2 real numbers", "url": "https://www.youtube.com/watch?v=VqbqRnP7zmU", "domain": "youtube.com", "votes": 1, "user": "ofou", "posted_at": "2024-04-09 16:47:54", "comments": 0, "source_title": "A 1064 parameter neural network with sine activation interpolates 450x2 real numbers.", "source_text": "A 1064 parameter neural network with sine activation interpolates 450x2 real\nnumbers. - YouTube\n\nA 1064 parameter neural network with sine activation interpolates 450x2 real\nnumbers.\n\n2x\n\nIf playback doesn't begin shortly, try restarting your device.\n\n\u2022\n\nUp next\n\nLive\n\nUpcoming\n\nPlay Now\n\nYou're signed out\n\nVideos you watch may be added to the TV's watch history and influence TV\nrecommendations. To avoid this, cancel and sign in to YouTube on your\ncomputer.\n\nShare\n\nAn error occurred while retrieving sharing information. Please try again\nlater.\n\n0:00\n\n0:00 / 2:13\u2022Watch full video\n\n\u2022\n\nNaN / NaN\n\nBack\n\nDE\n\n## Description\n\nA 1064 parameter neural network with sine activation interpolates 450x2 real\nnumbers.\n\n22Likes\n\n620Views\n\nApr 72024\n\nThis is a visualization of the training of the neural network. The red dots\nshow the actual values that the neural network outputs while the blue dots\nshow the values that the network is trained to produce as an output. Our\nneural network has sine activation. The sine function is an unusual activation\nfunction for neural networks, and it is difficult to make the sine activation\nuseful in machine learning since there are plenty of alternatives that are\nmore suitable for whichever task one would like the network to perform.\nNevertheless, in this animation, we demonstrate that neural networks with sine\nactivation can be used to interpolate through a collection of random (or non-\nrandom) data points even better than networks with your typical activation\nfunctions. Neural networks are not only good at memorizing data, but neural\nnetworks are also good at interpolating data in the sense that there is\ntypically a quite small gap between the number of parameters that a neural\nnetwork has and the number of parameters required to interpolate the training\ndata, and networks with sine activation are especially good at interpolating\nover data. The neural network is of the form\nChain(SkipConnection(Dense(mn,mn,sin),+),SkipConnection(Dense(mn,mn,sin),+),SkipConnection(Dense(mn,mn,sin),+),Dense(mn,2))\nwhere mn=18. The network has 3*mn^2+mn*2=3*18^2+36=1008 parameters for its\nweight matrices and 3*mn+2=56 parameters for the bias vectors. This means that\nthe network has a total of 1064 parameters. The neural network N is trained so\nthat N(x_j)=y_j where j is in the set {1,....,450} and where the pairs\n(x_j,y_j) are generated at random. With enough training (and with enough\nprecision for the arithmetic), one can make the loss level arbitrarily small.\nThis means that 85 percent (900/1064) of the networks parameters are strictly\nrequired for interpolation regardless of the network architecture. The weights\nof the network were initialized near zero (except for the final weight\nmatrix). Due to the low weight initialization of the neural network, the\nnetwork trained slowly up until 1:10. The loss level for (x_j,y_j) is the\ndistance between N(x_j) and y_j squared. The loss level indicated on the top\nof the visualization is the mean loss level. The notion of a neural network is\nnot my own. I am simply making animations that highlight some of the\nproperties of neural networks especially with regards to safety. Neural\nnetworks with sine activation are good for memorizing data and interpolating\ndata, but these neural networks are not good for much else since networks with\nsine activation are good at memorizing and interpolating but bad at learning.\nUnless otherwise stated, all algorithms featured on this channel are my own.\nYou can go to https://github.com/sponsors/jvanname to support my research on\nmachine learning algorithms. I am also available to consult on the use of safe\nand interpretable AI for your business. I am designing machine learning\nalgorithms for AI safety such as LSRDRs. In particular, my algorithms are\ndesigned to be more predictable and understandable to humans than other\nmachine learning algorithms, and my algorithms can be used to interpret more\ncomplex AI systems such as neural networks. With more understandable AI, we\ncan ensure that AI systems will be used responsibly and that we will avoid\ncatastrophic AI scenarios. There is currently nobody else who is working on\nLSRDRs, so your support will ensure a unique approach to AI safety.\n\nShow less Show more\n\n### Joseph Van Name\n\n953 subscribers\n\nVideos\n\nAbout\n\nVideos\n\nAbout\n\n## Comments 7\n\nTop comments\n\nNewest first\n\nNaN / NaN\n\n# A 1064 parameter neural network with sine activation interpolates 450x2 real\nnumbers.\n\nJoseph Van Name\n\nJoseph Van Name\n\n953 subscribers\n\n<__slot-el>\n\n<__slot-el>\n\nDownload\n\n620 views 1 day ago\n\n620 views \u2022 Apr 7, 2024\n\nShow less\n\nThis is a visualization of the training of the neural network. The red dots\nshow the actual values that the neural network outputs while the blue dots\nshow the values that the network is trained to produce as an output.\n......more\n\n...more\n\n### Joseph Van Name\n\n953 subscribers\n\nVideos\n\nAbout\n\nVideos\n\nAbout\n\nShow less\n\n# A 1064 parameter neural network with sine activation interpolates 450x2 real\nnumbers.\n\n620 views620 views\n\nApr 7, 2024\n\nDownload\n\n### A 1344 parameter 10 layer deep neural network successfully interpolating\n450x2 real numbers.\n\nJoseph Van Name\n\nJoseph Van Name\n\n\u2022\n\n\u2022\n\n1.6K views 1 day ago\n\nNew\n\n", "frontpage": false}
