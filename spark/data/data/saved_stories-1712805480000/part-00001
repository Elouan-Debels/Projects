{"aid": "39995807", "title": "RealmDreamer: Text-Driven 3D Scene Generation", "url": "https://realmdreamer.github.io/", "domain": "realmdreamer.github.io", "votes": 2, "user": "billconan", "posted_at": "2024-04-10 21:16:35", "comments": 0, "source_title": "RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth Diffusion", "source_text": "RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth\nDiffusion\n\n## RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth\nDiffusion\n\n  * Jaidev Shriram ^1*\n  * Alex Trevithick ^1*\n  * Lingjie Liu ^2\n  * Ravi Ramamoorthi ^1\n\n^1University of California, San Diego, ^2University of Pennsylvania\n\n* equal contribution\n\nPaper Arxiv Code (Soon)\n\n# We generate large, explorable 3D scenes from a text-description\n\n### with just pretrained 2D diffusion models\n\n# Abstract\n\nWe introduce RealmDreamer, a technique for generation of general forward-\nfacing 3D scenes from text descriptions. Our technique optimizes a 3D Gaussian\nSplatting representation to match complex text prompts. We initialize these\nsplats by utilizing the state-of-the-art text-to-image generators, lifting\ntheir samples into 3D, and computing the occlusion volume. We then optimize\nthis representation across multiple views as a 3D inpainting task with image-\nconditional diffusion models. To learn correct geometric structure, we\nincorporate a depth diffusion model by conditioning on the samples from the\ninpainting model, giving rich geometric structure. Finally, we finetune the\nmodel using sharpened samples from image generators. Notably, our technique\ndoes not require training on any scene-specific dataset and can synthesize a\nvariety of high-quality 3D scenes in different styles, consisting of multiple\nobjects. Its generality additionally allows 3D synthesis from a single image.\n\n# Results\n\nRGB Depth\n\nA bear sitting in a classroom with a hat on, realistic, 4k image, high detail\n\n## Inpainting priors are great for occlusion reasoning\n\nUsing text conditioned 2D diffusion models for 3D scene generation is tricky\ngiven the lack of 3D consistency across different samples. We mitigate this by\nleveraging 2D inpainting priors as novel view estimators instead. By rendering\nan incomplete 3D model and inpainting unknown regions, we learn to generate\nconsistent 3D scenes.\n\n## Image to 3D\n\nWe show that our technique can generate 3D scenes from a single image. This is\na challenging task as it requires the model to hallucinate the missing\ngeometry and texture in the scene. We do not require training on any scene-\nspecific dataset.\n\nInput Image\n\n\"The Brandenburg Gate in Berlin, large stone gateway with series of columns\nand a sculpture of a chariot and horses on stop, clear sky, 4k image,\nphotorealistic\"\n\nInput Image\n\n\"A minimal conference room, with a long table, a screen on the wall and a\nwhiteboard, 4k image, photorealistic, sharp\"\n\n# How?\n\n### Step 1: Generate a Prototype\n\nWe start by generating a cheap 2D prototype of the 3D scene from a text\ndescription using a pretrained text-to-image generator. Given the desired\nimage, we lift its content into 3D using monocular depth estimator, before\ncomputing the occlusion volume. This serves as the initialization for a 3D\nGaussian Splatting (3DGS) representation.\n\n### Step 2: Inpaint Missing Regions\n\nThe generated 3D scene is incomplete and contains missing regions. To fill\nthem in, we leverage a 2D inpainting diffusion model and optimize the splats\nto match its output over multiple views. An additional depth distillation loss\non sampled images ensure the inpainted regions are geometrically plausible.\n\n### Step 3: Refine the Scene\n\nFinally, we refine the 3D model to improve the cohesion between inpainted\nregions and the prototype by using a vanilla text-to-image diffusion model. An\nadditional sharpness filter ensures the generated samples are more detailed.\n\n# Related Work\n\nThere are many related works that have influenced our technique:\n\n  * Dreamfusion, Score Jacobian Chaining, and ProlificDreamer pioneer pretrained 2D Diffusion Models for 3D generation.\n  * Text2Room shows the effectiveness of iterative approaches for indoor scene synthesis.\n\nThere are also some concurrent work that tackle scene generation or use\ninpainting models for similar applications:\n\n  * NeRFiller: Completing Scenes via Generative 3D Inpainting\n  * Inpaint3D: 3D Scene Content Generation using 2D Inpainting Diffusion\n  * SceneWiz3D: Towards Text-guided 3D Scene Composition\n  * Text2NeRF: Text-Driven 3D Scene Generation with Neural Radiance Fields\n\n# Citation\n\nIf you find our work interesting, please consider citing us!\n\n    \n    \n    @article{shriram2024realmdreamer, title={RealmDreamer: Text-Driven 3D Scene Generation with Inpainting and Depth Diffusion}, author={Jaidev Shriram and Alex Trevithick and Lingjie Liu and Ravi Ramamoorthi}, journal={arXiv}, year={2024} }\n\n# Acknowledgements\n\nWe thank Jiatao Gu and Kai-En Lin for early discussions, Aleksander Holynski\nand Ben Poole for later discussions. We thank Michelle Chiu for video design\nhelp. This work was supported in part by an NSF graduate Fellowship, ONR grant\nN00014-23-1-2526, NSF CHASE-CI Grants 2100237 and 2120019, gifts from Adobe,\nGoogle, Qualcomm, Meta, the Ronald L. Graham Chair, and the UC San Diego\nCenter for Visual Computing.\n\nThe website template was borrowed from Reconfusion, Micha\u00ebl Gharbi, and Ref-\nNeRF.\n\n", "frontpage": false}
