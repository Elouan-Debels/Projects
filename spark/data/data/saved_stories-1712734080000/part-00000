{"aid": "39986064", "title": "AI-First Process Automation with LLMs/Action/Multimodal/Visual Language Models", "url": "https://github.com/OpenAdaptAI/OpenAdapt", "domain": "github.com/openadaptai", "votes": 2, "user": "9woc", "posted_at": "2024-04-10 01:26:01", "comments": 0, "source_title": "GitHub - OpenAdaptAI/OpenAdapt: AI-First Process Automation with Large [Language (LLMs) / Action (LAMs) / Multimodal (LMMs)] / Visual Language (VLMs)) Models", "source_text": "GitHub - OpenAdaptAI/OpenAdapt: AI-First Process Automation with Large\n[Language (LLMs) / Action (LAMs) / Multimodal (LMMs)] / Visual Language\n(VLMs)) Models\n\nSkip to content\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nOpenAdaptAI / OpenAdapt Public\n\n  * Notifications\n  * Fork 61\n  * Star 327\n\nAI-First Process Automation with Large [Language (LLMs) / Action (LAMs) /\nMultimodal (LMMs)] / Visual Language (VLMs)) Models\n\nwww.openadapt.ai\n\n### License\n\nMIT license\n\n327 stars 61 forks Branches Tags Activity\n\nStar\n\nNotifications\n\n# OpenAdaptAI/OpenAdapt\n\nThis commit does not belong to any branch on this repository, and may belong\nto a fork outside of the repository.\n\n26 Branches\n\n33 Tags\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\nabrichrdocs(README): System Overview -> Alpha Architecturedd1c29e \u00b7\n\n## History\n\n746 Commits  \n  \n### .github\n\n|\n\n### .github\n\n| chore: modify install script to execute on user's repo and branch  \n  \n### alembic\n\n|\n\n### alembic\n\n| fix(video): add migration script  \n  \n### assets\n\n|\n\n### assets\n\n| Merge branch 'main' into share-magic-wormhole  \n  \n### install\n\n|\n\n### install\n\n| fix(install): add vs redistributable installation in powershell script (  \n  \n### openadapt\n\n|\n\n### openadapt\n\n| fix(app): use default logo for tray icon  \n  \n### tests\n\n|\n\n### tests\n\n| feat: video  \n  \n### .env.example\n\n|\n\n### .env.example\n\n| Add .env.example and generate env in config.py  \n  \n### .flake8\n\n|\n\n### .flake8\n\n| fix: modify flake8 config (#429)  \n  \n### .gitignore\n\n|\n\n### .gitignore\n\n| feat: video  \n  \n### .pre-commit-config.yaml\n\n|\n\n### .pre-commit-config.yaml\n\n| chore: add preview option to black pre-commit hook and update README (#...  \n  \n### CHANGELOG.md\n\n|\n\n### CHANGELOG.md\n\n| 0.16.2  \n  \n### CONTRIBUTING.md\n\n|\n\n### CONTRIBUTING.md\n\n| chore: add pypi action and oa-atomacos and oa-pynput packages (#456)  \n  \n### LICENSE\n\n|\n\n### LICENSE\n\n| Update LICENSE (#505)  \n  \n### README.md\n\n|\n\n### README.md\n\n| docs(README): System Overview -> Alpha Architecture  \n  \n### SETUP.md\n\n|\n\n### SETUP.md\n\n| docs: update install section of README.md (#344)  \n  \n### alembic.ini\n\n|\n\n### alembic.ini\n\n| add alembic LICENCE requirements.txt setup.py; fix README  \n  \n### permissions_in_macOS.md\n\n|\n\n### permissions_in_macOS.md\n\n| chore: format changes with black, and google python docstring, instal...  \n  \n### poetry.lock\n\n|\n\n### poetry.lock\n\n| feat: video  \n  \n### pyproject.toml\n\n|\n\n### pyproject.toml\n\n| 0.16.2  \n  \n### setup.py\n\n|\n\n### setup.py\n\n| chore: format changes with black, and google python docstring, instal...  \n  \n## Repository files navigation\n\nJoin us on Discord\n\nRead our Architecture document\n\nJoin the Discussion on the Request for Comments\n\nSee also:\n\n  * https://github.com/OpenAdaptAI/SoM\n  * https://github.com/OpenAdaptAI/pynput\n  * https://github.com/OpenAdaptAI/atomacos\n\n# OpenAdapt: AI-First Process Automation with Large Multimodal Models (LMMs).\n\nOpenAdapt is the open source software adapter between Large Multimodal Models\n(LMMs) and traditional desktop and web Graphical User Interfaces (GUIs).\n\n### Enormous volumes of mental labor are wasted on repetitive GUI workflows.\n\n### Foundation Models (e.g. GPT-4, ACT-1) are powerful automation tools.\n\n### OpenAdapt connects Foundation Models to GUIs:\n\nEarly demo: https://www.loom.com/share/9d77eb7028f34f7f87c6661fb758d1c0 (more\ncoming soon!)\n\nWelcome to OpenAdapt! This Python library implements AI-First Process\nAutomation with the power of Large Multimodal Modals (LMMs) by:\n\n  * Recording screenshots and associated user input\n  * Aggregating and visualizing user input and recordings for development\n  * Converting screenshots and user input into tokenized format\n  * Generating synthetic input via transformer model completions\n  * Generating task trees by analyzing recordings (work-in-progress)\n  * Replaying synthetic input to complete tasks (work-in-progress)\n\nThe goal is similar to that of Robotic Process Automation, except that we use\nLarge Multimodal Models instead of conventional RPA tools.\n\nThe direction is adjacent to Adept.ai, with some key differences:\n\n  1. OpenAdapt is model agnostic.\n  2. OpenAdapt generates prompts automatically by learning from human demonstration (auto-prompted, not user-prompted). This means that agents are grounded in existing processes, which mitigates hallucinations and ensures successful task completion.\n  3. OpenAdapt works with all types of desktop GUIs, including virtualized (e.g. Citrix) and web.\n  4. OpenAdapt is open source (MIT license).\n\n## Install\n\nInstallation Method| Recommended for| Ease of Use  \n---|---|---  \nScripted| Non-technical users| Streamlines the installation process for users\nunfamiliar with setup steps  \nManual| Technical Users| Allows for more control and customization during the\ninstallation process  \n  \n### Installation Scripts\n\n#### Windows\n\n  * Press Windows Key, type \"powershell\", and press Enter\n  * Copy and paste the following command into the terminal, and press Enter (If Prompted for User Account Control, click 'Yes'):\n    \n        Start-Process powershell -Verb RunAs -ArgumentList '-NoExit', '-ExecutionPolicy', 'Bypass', '-Command', \"iwr -UseBasicParsing -Uri 'https://raw.githubusercontent.com/OpenAdaptAI/OpenAdapt/main/install/install_openadapt.ps1' | Invoke-Expression\"\n\n#### MacOS\n\n  * Download and install Git and Python 3.10\n  * Press Command+Space, type \"terminal\", and press Enter\n  * Copy and paste the following command into the terminal, and press Enter:\n    \n        /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/OpenAdaptAI/OpenAdapt/HEAD/install/install_openadapt.sh)\"\n\n### Manual Setup\n\nPrerequisite:\n\n  * Python 3.10\n  * Git\n  * Tesseract (for OCR)\n\nFor the setup of any/all of the above dependencies, follow the steps SETUP.md.\n\nInstall with Poetry :\n\n    \n    \n    git clone https://github.com/OpenAdaptAI/OpenAdapt.git cd OpenAdapt pip3 install poetry poetry install poetry shell alembic upgrade head pytest\n\n### Permissions\n\nSee how to set up system permissions on macOS here.\n\n## Usage\n\n### Shell\n\nRun this in every new terminal window once (while inside the OpenAdapt root\ndirectory) before running any openadapt commands below:\n\n    \n    \n    poetry shell\n\nYou should see the something like this:\n\n    \n    \n    % poetry shell Using python3.10 (3.10.13) ... (openadapt-py3.10) %\n\nNotice the environment prefix (openadapt-py3.10).\n\n### Record\n\nCreate a new recording by running the following command:\n\n    \n    \n    python -m openadapt.record \"testing out openadapt\"\n\nWait until all three event writers have started:\n\n    \n    \n    | INFO | __mp_main__:write_events:230 - event_type='screen' starting | INFO | __mp_main__:write_events:230 - event_type='action' starting | INFO | __mp_main__:write_events:230 - event_type='window' starting\n\nType a few words into the terminal and move your mouse around the screen to\ngenerate some events, then stop the recording by pressing CTRL+C.\n\nCurrent limitations:\n\n  * recording should be short (i.e. under a minute), as they are somewhat memory intensive, and there is currently an open issue describing a possible memory leak\n  * the only touchpad and trackpad gestures currently supported are pointing the cursor and left or right clicking, as described in this open issue\n\n### Visualize\n\nVisualize the latest recording you created by running the following command:\n\n    \n    \n    python -m openadapt.visualize\n\nThis will open a scrollable window that looks something like this:\n\nFor a browser-based visualization, run:\n\n    \n    \n    python -m openadapt.deprecated.visualize\n\nThis will open up a tab in your browser that looks something like this:\n\n### Playback\n\nYou can play back the recording using the following command:\n\n    \n    \n    python -m openadapt.replay NaiveReplayStrategy\n\nOther replay strategies include:\n\n  * StatefulReplayStrategy: Proof-of-concept which uses the OpenAI GPT-4 API with prompts constructed via OS-level window data.\n\nSee https://github.com/OpenAdaptAI/OpenAdapt/tree/main/openadapt/strategies\nfor a complete list. More ReplayStrategies coming soon! (see Contributing).\n\n## Features\n\n### State-of-the-art GUI understanding via Segment Anything in High Quality:\n\n### Industry leading privacy (PII/PHI scrubbing) via AWS Comprehend, Microsoft\nPresidio and Private AI:\n\n### Decentralized and secure data distribution via Magic Wormhole:\n\n### Detailed performance monitoring via pympler and tracemalloc:\n\n### System Tray Icon and Client GUI App (work-in-progress)\n\n### And much more!\n\n## \ud83d\ude80 Open Contract Positions at OpenAdapt.AI\n\nWe are thrilled to open new contract positions for developers passionate about\npushing boundaries in technology. If you're ready to make a significant\nimpact, consider the following roles:\n\n#### Frontend Developer\n\n  * Responsibilities: Develop and test key features such as process visualization, demo booking, app store, and blog integration.\n  * Skills: Proficiency in modern frontend technologies and a knack for UI/UX design.\n\n#### Machine Learning Engineer\n\n  * Role: Implement and refine process replay strategies using state-of-the-art LLMs/LMMs. Extract dynamic process descriptions from extensive process recordings.\n  * Skills: Strong background in machine learning, experience with LLMs/LMMs, and problem-solving aptitude.\n\n#### Software Engineer\n\n  * Focus: Enhance memory optimization techniques during process recording and replay. Develop sophisticated tools for process observation and productivity measurement.\n  * Skills: Expertise in software optimization, memory management, and analytics.\n\n#### Technical Writer\n\n  * Focus: Maintaining OpenAdapt repositories\n  * Skills: Passion for writing and/or documentation\n\n### \ud83d\udd0d How to Apply\n\n  * Step 1: Submit an empty Pull Request to OpenAdapt or OpenAdapt.web. Format your PR title as [Proposal] <your title here>\n  * Step 2: Include a brief, informal outline of your approach in the PR description. Feel free to add any questions you might have.\n  * Need Clarifications? Reach out to us on Discord.\n\nWe're looking forward to your contributions. Let's build the future \ud83d\ude80\n\n## Contributing\n\n### Notable Works-in-progress (incomplete, see\nhttps://github.com/OpenAdaptAI/OpenAdapt/pulls and\nhttps://github.com/OpenAdaptAI/OpenAdapt/issues/ for more)\n\n  * Video Recording Hardware Acceleration (help wanted)\n  * Audio Narration (help wanted)\n  * Chrome Extension (help wanted)\n  * Gemini Vision (help wanted)\n\n### Replay Problem Statement\n\nOur goal is to automate the task described and demonstrated in a Recording.\nThat is, given a new Screenshot, we want to generate the appropriate\nActionEvent(s) based on the previously recorded ActionEvents in order to\naccomplish the task specified in the Recording.task_description and narrated\nby the user in AudioInfo.words_with_timestamps, while accounting for\ndifferences in screen resolution, window size, application behavior, etc.\n\nIf it's not clear what ActionEvent is appropriate for the given Screenshot,\n(e.g. if the GUI application is behaving in a way we haven't seen before), we\ncan ask the user to take over temporarily to demonstrate the appropriate\ncourse of action.\n\n### Data Model\n\nThe data model consists of the following entities:\n\n  1. Recording: Contains information about the screen dimensions, platform, and other metadata.\n  2. ActionEvent: Represents a user action event such as a mouse click or key press. Each ActionEvent has an associated Screenshot taken immediately before the event occurred. ActionEvents are aggregated to remove unnecessary events (see visualize.)\n  3. Screenshot: Contains the PNG data of a screenshot taken during the recording.\n  4. WindowEvent: Represents a window event such as a change in window title, position, or size.\n\n### API\n\nYou can assume that you have access to the following functions:\n\n  * create_recording(\"doing taxes\"): Creates a recording.\n  * get_latest_recording(): Gets the latest recording.\n  * get_events(recording): Returns a list of ActionEvent objects for the given recording.\n\nSee GitBook Documentation for more.\n\n### Instructions\n\nJoin us on Discord. Then:\n\n  1. Fork this repository and clone it to your local machine.\n  2. Get OpenAdapt up and running by following the instructions under Setup.\n  3. Look through the list of open issues at https://github.com/OpenAdaptAI/OpenAdapt/issues and once you find one you would like to address, indicate your interest with a comment.\n  4. Implement a solution to the issue you selected. Write unit tests for your implementation.\n  5. Submit a Pull Request (PR) to this repository. Note: submitting a PR before your implementation is complete (e.g. with high level documentation and/or implementation stubs) is encouraged, as it provides us with the opportunity to provide early feedback and iterate on the approach.\n\n### Evaluation Criteria\n\nYour submission will be evaluated based on the following criteria:\n\n  1. Functionality : Your implementation should correctly generate the new ActionEvent objects that can be replayed in order to accomplish the task in the original recording.\n\n  2. Code Quality : Your code should be well-structured, clean, and easy to understand.\n\n  3. Scalability : Your solution should be efficient and scale well with large datasets.\n\n  4. Testing : Your tests should cover various edge cases and scenarios to ensure the correctness of your implementation.\n\n### Submission\n\n  1. Commit your changes to your forked repository.\n\n  2. Create a pull request to the original repository with your changes.\n\n  3. In your pull request, include a brief summary of your approach, any assumptions you made, and how you integrated external libraries.\n\n  4. Bonus: interacting with ChatGPT and/or other language transformer models in order to generate code and/or evaluate design decisions is encouraged. If you choose to do so, please include the full transcript.\n\n## Troubleshooting\n\nMacOS: if you encounter system alert messages or find issues when making and\nreplaying recordings, make sure to set up permissions accordingly.\n\nIn summary (from https://stackoverflow.com/a/69673312):\n\n  1. Settings -> Security & Privacy\n  2. Click on the Privacy tab\n  3. Scroll and click on the Accessibility Row\n  4. Click +\n  5. Navigate to /System/Applications/Utilities/ (or wherever Terminal.app is installed)\n  6. Click okay.\n\n## Developing\n\n### Generate migration (after editing a model)\n\n    \n    \n    alembic revision --autogenerate -m \"<msg>\"\n\n### Pre-commit Hooks\n\nTo ensure code quality and consistency, OpenAdapt uses pre-commit hooks. These\nhooks will be executed automatically before each commit to perform various\nchecks and validations on your codebase.\n\nThe following pre-commit hooks are used in OpenAdapt:\n\n  * check-yaml: Validates the syntax and structure of YAML files.\n  * end-of-file-fixer: Ensures that files end with a newline character.\n  * trailing-whitespace: Detects and removes trailing whitespace at the end of lines.\n  * black: Formats Python code to adhere to the Black code style. Notably, the --preview feature is used.\n  * isort: Sorts Python import statements in a consistent and standardized manner.\n\nTo set up the pre-commit hooks, follow these steps:\n\n  1. Navigate to the root directory of your OpenAdapt repository.\n\n  2. Run the following command to install the hooks:\n\n    \n    \n    pre-commit install\n\nNow, the pre-commit hooks are installed and will run automatically before each\ncommit. They will enforce code quality standards and prevent committing code\nthat doesn't pass the defined checks.\n\n### Status Checks\n\nWhen you submit a PR, the \"Python CI\" workflow is triggered for code\nconsistency. It follows organized steps to review your code:\n\n  1. Python Black Check : This step verifies code formatting using Python Black style, with the --preview flag for style.\n\n  2. Flake8 Review : Next, Flake8 tool thoroughly checks code structure, including flake8-annotations and flake8-docstrings. Though GitHub Actions automates checks, it's wise to locally run flake8 . before finalizing changes for quicker issue spotting and resolution.\n\n# Submitting an Issue\n\nPlease submit any issues to https://github.com/OpenAdaptAI/OpenAdapt/issues\nwith the following information:\n\n  * Problem description (please include any relevant console output and/or screenshots)\n  * Steps to reproduce (please help others to help you!)\n\n## About\n\nAI-First Process Automation with Large [Language (LLMs) / Action (LAMs) /\nMultimodal (LMMs)] / Visual Language (VLMs)) Models\n\nwww.OpenAdapt.AI\n\n### Topics\n\npython transformers process-mining process-automation huggingface huggingface-\ntransformers gpt-4 large-language-models gpt-4-api segment-anything large-\nmultimodal-models gpt-4-vision-preview gpt4-turbo gpt4-vision large-action-\nmodel\n\n### Resources\n\nReadme\n\n### License\n\nMIT license\n\nActivity\n\nCustom properties\n\n### Stars\n\n327 stars\n\n### Watchers\n\n6 watching\n\n### Forks\n\n61 forks\n\nReport repository\n\n## Releases 33\n\nv0.16.2 Latest\n\nMar 8, 2024\n\n\\+ 32 releases\n\n## Packages 0\n\nNo packages published\n\n## Contributors 14\n\n## Languages\n\n  * Python 96.6%\n  * PowerShell 2.5%\n  * Other 0.9%\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": false}
