{"aid": "39980732", "title": "HairFastGAN: Shape and Color Transfer for Hairstyle", "url": "https://github.com/AIRI-Institute/HairFastGAN", "domain": "github.com/airi-institute", "votes": 1, "user": "panyanyany", "posted_at": "2024-04-09 15:51:02", "comments": 0, "source_title": "GitHub - AIRI-Institute/HairFastGAN: Official Implementation for \"HairFastGAN: Realistic and Robust Hair Transfer with a Fast Encoder-Based Approach\"", "source_text": "GitHub - AIRI-Institute/HairFastGAN: Official Implementation for \"HairFastGAN:\nRealistic and Robust Hair Transfer with a Fast Encoder-Based Approach\"\n\nSkip to content\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nAIRI-Institute / HairFastGAN Public\n\n  * Notifications\n  * Fork 6\n  * Star 78\n\nOfficial Implementation for \"HairFastGAN: Realistic and Robust Hair Transfer\nwith a Fast Encoder-Based Approach\"\n\n### License\n\nMIT license\n\n78 stars 6 forks Branches Tags Activity\n\nStar\n\nNotifications\n\n# AIRI-Institute/HairFastGAN\n\nThis commit does not belong to any branch on this repository, and may belong\nto a fork outside of the repository.\n\n1 Branch\n\n0 Tags\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\nmaximkmsean codesfc6d9e4 \u00b7\n\n## History\n\n2 Commits  \n  \n### datasets\n\n|\n\n### datasets\n\n| Initial commit  \n  \n### docs/assets\n\n|\n\n### docs/assets\n\n| Initial commit  \n  \n### losses\n\n|\n\n### losses\n\n| Initial commit  \n  \n### models\n\n|\n\n### models\n\n| sean codes  \n  \n### scripts\n\n|\n\n### scripts\n\n| Initial commit  \n  \n### utils\n\n|\n\n### utils\n\n| Initial commit  \n  \n### .gitignore\n\n|\n\n### .gitignore\n\n| sean codes  \n  \n### LICENSE\n\n|\n\n### LICENSE\n\n| Initial commit  \n  \n### README.md\n\n|\n\n### README.md\n\n| Initial commit  \n  \n### hair_swap.py\n\n|\n\n### hair_swap.py\n\n| Initial commit  \n  \n### main.py\n\n|\n\n### main.py\n\n| Initial commit  \n  \n### poetry.lock\n\n|\n\n### poetry.lock\n\n| Initial commit  \n  \n### pyproject.toml\n\n|\n\n### pyproject.toml\n\n| Initial commit  \n  \n### requirements.txt\n\n|\n\n### requirements.txt\n\n| Initial commit  \n  \n## Repository files navigation\n\n# HairFastGAN: Realistic and Robust Hair Transfer with a Fast Encoder-Based\nApproach\n\n> Our paper addresses the complex task of transferring a hairstyle from a\n> reference image to an input photo for virtual hair try-on. This task is\n> challenging due to the need to adapt to various photo poses, the sensitivity\n> of hairstyles, and the lack of objective metrics. The current state of the\n> art hairstyle transfer methods use an optimization process for different\n> parts of the approach, making them inexcusably slow. At the same time,\n> faster encoder-based models are of very low quality because they either\n> operate in StyleGAN's W+ space or use other low-dimensional image\n> generators. Additionally, both approaches have a problem with hairstyle\n> transfer when the source pose is very different from the target pose,\n> because they either don't consider the pose at all or deal with it\n> inefficiently. In our paper, we present the HairFast model, which uniquely\n> solves these problems and achieves high resolution, near real-time\n> performance, and superior reconstruction compared to optimization problem-\n> based methods. Our solution includes a new architecture operating in the FS\n> latent space of StyleGAN, an enhanced inpainting approach, and improved\n> encoders for better alignment, color transfer, and a new encoder for post-\n> processing. The effectiveness of our approach is demonstrated on realism\n> metrics after random hairstyle transfer and reconstruction when the original\n> hairstyle is transferred. In the most difficult scenario of transferring\n> both shape and color of a hairstyle from different images, our method\n> performs in less than a second on the Nvidia V100.\n\nThe proposed HairFast framework allows to edit a hairstyle on an arbitrary\nphoto based on an example from other photos. Here we have an example of how\nthe method works by transferring a hairstyle from one photo and a hair color\nfrom another.\n\n## Prerequisites\n\nYou need following hardware and python version to run our method.\n\n  * Linux\n  * NVIDIA GPU + CUDA CuDNN\n  * Python 3.10\n  * PyTorch 1.13.1+\n\n## Installation\n\n  * Clone this repo:\n\n    \n    \n    git clone https://github.com/AIRI-Institute/HairFastGAN cd HairFastGAN\n\n  * Download all pretrained models:\n\n    \n    \n    git clone https://huggingface.co/AIRI-Institute/HairFastGAN cd HairFastGAN && git lfs pull && cd .. mv HairFastGAN/pretrained_models pretrained_models mv HairFastGAN/input input rm -rf HairFastGAN\n\n  * Setting the environment\n\nOption 1 [recommended], install Poetry and then:\n\n    \n    \n    poetry install\n\nOption 2, just install the dependencies in your environment:\n\n    \n    \n    pip install -r requirements.txt\n\n## Inference\n\nYou can use main.py to run the method, either for a single run or for a batch\nof experiments.\n\n  * An example of running a single experiment:\n\n    \n    \n    python main.py --face_path=6.png --shape_path=7.png --color_path=8.png \\ --input_dir=input --result_path=output/result.png\n\n  * To run the batch version, first create an image triples file (face/shape/color):\n\n    \n    \n    cat > example.txt << EOF 6.png 7.png 8.png 8.png 4.jpg 5.jpg EOF\n\nAnd now you can run the method:\n\n    \n    \n    python main.py --file_path=example.txt --input_dir=input --output_dir=output\n\n  * You can use HairFast in the code directly:\n\n    \n    \n    from hair_swap import HairFast, get_parser # Init HairFast hair_fast = HairFast(get_parser().parse_args([])) # Inference result = hair_fast(face_img, shape_img, color_img)\n\nSee the code for input parameters and output formats.\n\n  * Alternatively, you can use our Colab Notebook to prepare the environment, download the code, pretrained weights, and allow you to run experiments with a convenient form.\n\n## Scripts\n\nThere is a list of scripts below, see arguments via --help for details.\n\nPath| Description  \n---|---  \nscripts/fid_metric.py| Metrics calculation  \nscripts/rotate_gen.py| Dataset generation for rotate training  \nscripts/blending_gen.py| Dataset generation for blending training  \nscripts/pp_gen.py| Dataset generation for post processing training  \nscripts/rotate_train.py| Rotate training  \nscripts/blending_train.py| Blending training  \nscripts/pp_train.py| Post processing training  \n  \n## Training\n\nFor training, you need to generate a dataset and then run the scripts for\ntraining. See the scripts section above.\n\nWe use Weights & Biases to track experiments. Before training, you should put\nyour W&B API key into the WANDB_KEY environment variable.\n\n## Method diagram\n\nOverview of HairFast: first, the images go through an Embedding module where\nwe obtain various latent representations and segmentation masks. Next, we\ntransfer the desired hairstyle shape using Alignment module and the desired\nhair color using Blending module. The last step we do Post Processing to bring\nback the lost details of the original image where they are needed.\n\n## Repository structure\n\n    \n    \n    . \u251c\u2500\u2500 \ud83d\udcc2 datasets # Implementation of torch datasets for inference \u251c\u2500\u2500 \ud83d\udcc2 docs # Folder with method diagram and teaser \u251c\u2500\u2500 \ud83d\udcc2 models # Folder containting all the models \u2502 \u251c\u2500\u2500 ... \u2502 \u251c\u2500\u2500 \ud83d\udcc4 Embedding.py # Implementation of Embedding module \u2502 \u251c\u2500\u2500 \ud83d\udcc4 Alignment.py # Implementation of Alignment module \u2502 \u251c\u2500\u2500 \ud83d\udcc4 Blending.py # Implementation of Blending module \u2502 \u251c\u2500\u2500 \ud83d\udcc4 Encoders.py # Implementation of encoder architectures \u2502 \u2514\u2500\u2500 \ud83d\udcc4 Net.py # Implementation of basic models \u2502 \u251c\u2500\u2500 \ud83d\udcc2 losses # Folder containing various loss criterias for training \u251c\u2500\u2500 \ud83d\udcc2 scripts # Folder with various scripts \u251c\u2500\u2500 \ud83d\udcc2 utils # Folder with utility functions \u2502 \u251c\u2500\u2500 \ud83d\udcdc poetry.lock # Records exact dependency versions. \u251c\u2500\u2500 \ud83d\udcdc pyproject.toml # Poetry configuration for dependencies. \u251c\u2500\u2500 \ud83d\udcdc requirements.txt # Lists required Python packages. \u251c\u2500\u2500 \ud83d\udcc4 hair_swap.py # Implementation of the HairFast main class \u2514\u2500\u2500 \ud83d\udcc4 main.py # Script for inference\n\n## References & Acknowledgments\n\nThe repository was started from Barbershop.\n\nThe code CtrlHair, SEAN, HairCLIP, FSE, E4E and STAR was also used.\n\n## Citation\n\nIf you use this code for your research, please cite our paper:\n\n    \n    \n    @misc{nikolaev2024hairfastgan, title={HairFastGAN: Realistic and Robust Hair Transfer with a Fast Encoder-Based Approach}, author={Maxim Nikolaev and Mikhail Kuznetsov and Dmitry Vetrov and Aibek Alanov}, year={2024}, eprint={2404.01094}, archivePrefix={arXiv}, primaryClass={cs.CV} }\n\n## About\n\nOfficial Implementation for \"HairFastGAN: Realistic and Robust Hair Transfer\nwith a Fast Encoder-Based Approach\"\n\n### Topics\n\nimage-processing image-editing generative-adversarial-network face-editing\nstylegan2\n\n### Resources\n\nReadme\n\n### License\n\nMIT license\n\nActivity\n\nCustom properties\n\n### Stars\n\n78 stars\n\n### Watchers\n\n4 watching\n\n### Forks\n\n6 forks\n\nReport repository\n\n## Releases\n\nNo releases published\n\n## Contributors 2\n\n  * maximkm Maxim Nikolaev\n  * DmitryZykin\n\n## Languages\n\n  * Python 90.6%\n  * Cuda 6.7%\n  * C++ 2.5%\n  * Other 0.2%\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": false}
