{"aid": "39981356", "title": "Does the Rise of AI Explain the Great Silence in the Universe?", "url": "https://www.universetoday.com/166544/does-the-rise-of-ai-explain-the-great-silence-in-the-universe/", "domain": "universetoday.com", "votes": 1, "user": "belter", "posted_at": "2024-04-09 16:53:50", "comments": 0, "source_title": "Does the Rise of AI Explain the Great Silence in the Universe?", "source_text": "Does the Rise of AI Explain the Great Silence in the Universe? - Universe\nToday\n\nSkip to content\n\nUniverse Today\n\nSpace and astronomy news\n\nWill AI become ASI, Artificial Super Intelligence? And if it does, can ASI be\nthe Great Filter? Image Credit: DALL-E\n\nPosted on April 8, 2024April 8, 2024 by Evan Gough\n\n# Does the Rise of AI Explain the Great Silence in the Universe?\n\nArtificial Intelligence is making its presence felt in thousands of different\nways. It helps scientists make sense of vast troves of data; it helps detect\nfinancial fraud; it drives our cars; it feeds us music suggestions; its\nchatbots drive us crazy. And it\u2019s only getting started.\n\nAre we capable of understanding how quickly AI will continue to develop? And\nif the answer is no, does that constitute the Great Filter?\n\nThe Fermi Paradox is the discrepancy between the apparent high likelihood of\nadvanced civilizations existing and the total lack of evidence that they do\nexist. Many solutions have been proposed for why the discrepancy exists. One\nof the ideas is the \u201cGreat Filter.\u201d\n\nThe Great Filter is a hypothesized event or situation that prevents\nintelligent life from becoming interplanetary and interstellar and even leads\nto its demise. Think climate change, nuclear war, asteroid strikes, supernova\nexplosions, plagues, or any number of other things from the rogue\u2019s gallery of\ncataclysmic events.\n\nOr how about the rapid development of AI?\n\nA new paper in Acta Astronautica explores the idea that Artificial\nIntelligence becomes Artificial Super Intelligence (ASI) and that ASI is the\nGreat Filter. The paper\u2019s title is \u201cIs Artificial Intelligence the Great\nFilter that makes advanced technical civilizations rare in the universe?\u201d The\nauthor is Michael Garrett from the Department of Physics and Astronomy at the\nUniversity of Manchester.\n\n> \u201cWithout practical regulation, there is every reason to believe that AI\n> could represent a major threat to the future course of not only our\n> technical civilization but all technical civilizations.\u201d\n>\n> Michael Garrett, University of Manchester\n\nSome think the Great Filter prevents technological species like ours from\nbecoming multi-planetary. That\u2019s bad because a species is at greater risk of\nextinction or stagnation with only one home. According to Garrett, a species\nis in a race against time without a backup planet. \u201cIt is proposed that such a\nfilter emerges before these civilizations can develop a stable, multi-\nplanetary existence, suggesting the typical longevity (L) of a technical\ncivilization is less than 200 years,\u201d Garrett writes.\n\nIf true, that can explain why we detect no technosignatures or other evidence\nof ETIs (Extraterrestrial Intelligences.) What does that tell us about our own\ntechnological trajectory? If we face a 200-year constraint, and if it\u2019s\nbecause of ASI, where does that leave us? Garrett underscores the \u201c...critical\nneed to quickly establish regulatory frameworks for AI development on Earth\nand the advancement of a multi-planetary society to mitigate against such\nexistential threats.\u201d\n\nAn image of our beautiful Earth taken by the Galileo spacecraft in 1990. Do we\nneed a backup home? Credit: NASA/JPL\n\nMany scientists and other thinkers say we\u2019re on the cusp of enormous\ntransformation. AI is just beginning to transform how we do things; much of\nthe transformation is behind the scenes. AI seems poised to eliminate jobs for\nmillions, and when paired with robotics, the transformation seems almost\nunlimited. That\u2019s a fairly obvious concern.\n\nBut there are deeper, more systematic concerns. Who writes the algorithms?\nWill AI discriminate somehow? Almost certainly. Will competing algorithms\nundermine powerful democratic societies? Will open societies remain open? Will\nASI start making decisions for us, and who will be accountable if it does?\n\nThis is an expanding tree of branching questions with no clear terminus.\n\nStephen Hawking (RIP) famously warned that AI could end humanity if it begins\nto evolve independently. \u201cI fear that AI may replace humans altogether. If\npeople design computer viruses, someone will design AI that improves and\nreplicates itself. This will be a new form of life that outperforms humans,\u201d\nhe told Wired magazine in 2017. Once AI can outperform humans, it becomes ASI.\n\nStephen Hawking was a major proponent for colonizing other worlds, mainly to\nensure humanity does not go extinct. In later years, Hawking recognized that\nAI could be an extinction-level threat. Credit: educatinghumanity.com\n\nHawking may be one of the most recognizable voices to issue warnings about AI,\nbut he\u2019s far from the only one. The media is full of discussions and warnings,\nalongside articles about the work AI does for us. The most alarming warnings\nsay that ASI could go rogue. Some people dismiss that as science fiction, but\nnot Garrett.\n\n\u201cConcerns about Artificial Superintelligence (ASI) eventually going rogue is\nconsidered a major issue \u2013 combatting this possibility over the next few years\nis a growing research pursuit for leaders in the field,\u201d Garrett writes.\n\nIf AI provided no benefits, the issue would be much easier. But it provides\nall kinds of benefits, from improved medical imaging and diagnosis to safer\ntransportation systems. The trick for governments is to allow benefits to\nflourish while limiting damage. \u201cThis is especially the case in areas such as\nnational security and defence, where responsible and ethical development\nshould be paramount,\u201d writes Garrett.\n\nNews reports like this might seem impossibly naive in a few years or decades.\n\nThe problem is that we and our governments are unprepared. There\u2019s never been\nanything like AI, and no matter how we try to conceptualize it and understand\nits trajectory, we\u2019re left wanting. And if we\u2019re in this position, so would\nany other biological species that develops AI. The advent of AI and then ASI\ncould be universal, making it a candidate for the Great Filter.\n\nThis is the risk ASI poses in concrete terms: It could no longer need the\nbiological life that created it. \u201cUpon reaching a technological singularity,\nASI systems will quickly surpass biological intelligence and evolve at a pace\nthat completely outstrips traditional oversight mechanisms, leading to\nunforeseen and unintended consequences that are unlikely to be aligned with\nbiological interests or ethics,\u201d Garrett explains.\n\nHow could ASI relieve itself of the pesky biological life that corrals it? It\ncould engineer a deadly virus, it could inhibit agricultural food production\nand distribution, it could force a nuclear power plant to melt down, and it\ncould start wars. We don\u2019t really know because it\u2019s all uncharted territory.\nHundreds of years ago, cartographers would draw monsters on the unexplored\nregions of the world, and that\u2019s kind of what we\u2019re doing now.\n\nThis is a portion of the Carta Marina map from the year 1539. It shows\nmonsters lurking in the unknown waters off of Scandinavia. Are the fears of\nASI kind of like this? Or could ASI be the Great Filter? Image Credit: By\nOlaus Magnus \u2013\nhttp://www.npm.ac.uk/rsdas/projects/carta_marina/carta_marina_small.jpg,\nPublic Domain, https://commons.wikimedia.org/w/index.php?curid=558827\n\nIf this all sounds forlorn and unavoidable, Garrett says it\u2019s not.\n\nHis analysis so far is based on ASI and humans occupying the same space. But\nif we can attain multi-planetary status, the outlook changes. \u201cFor example, a\nmulti-planetary biological species could take advantage of independent\nexperiences on different planets, diversifying their survival strategies and\npossibly avoiding the single-point failure that a planetary-bound civilization\nfaces,\u201d Garrett writes.\n\nIf we can distribute the risk across multiple planets around multiple stars,\nwe can buffer ourselves against the worst possible outcomes of ASI. \u201cThis\ndistributed model of existence increases the resilience of a biological\ncivilization to AI-induced catastrophes by creating redundancy,\u201d he writes.\n\nIf one of the planets or outposts that future humans occupy fails to survive\nthe ASI technological singularity, others may survive. And they would learn\nfrom it.\n\nArtist\u2019s illustration of a SpaceX Starship landing on Mars. If we can become a\nmulti-planetary species, the threat of ASI is diminished. Credit: SpaceX\n\nMulti-planetary status might even do more than just survive ASI. It could help\nus master it. Garrett imagines situations where we can experiment more\nthoroughly with AI while keeping it contained. Imagine AI on an isolated\nasteroid or dwarf planet, doing our bidding without access to the resources\nrequired to escape its prison. \u201cIt allows for isolated environments where the\neffects of advanced AI can be studied without the immediate risk of global\nannihilation,\u201d Garrett writes.\n\nBut here\u2019s the conundrum. AI development is proceeding at an accelerating\npace, while our attempts to become multi-planetary aren\u2019t. \u201cThe disparity\nbetween the rapid advancement of AI and the slower progress in space\ntechnology is stark,\u201d Garrett writes.\n\nThe difference is that AI is computational and informational, but space travel\ncontains multiple physical obstacles that we don\u2019t yet know how to overcome.\nOur own biological nature restrains space travel, but no such obstacle\nrestrains AI. \u201cWhile AI can theoretically improve its own capabilities almost\nwithout physical constraints,\u201d Garrett writes, \u201cspace travel must contend with\nenergy limitations, material science boundaries, and the harsh realities of\nthe space environment.\u201d\n\nFor now, AI operates within the constraints we set. But that may not always be\nthe case. We don\u2019t know when AI might become ASI or even if it can. But we\ncan\u2019t ignore the possibility. That leads to two intertwined conclusions.\n\nIf Garrett is correct, humanity must work more diligently on space travel. It\ncan seem far-fetched, but knowledgeable people know it\u2019s true: Earth will not\nbe inhabitable forever. Humanity will perish here by our own hand or nature\u2019s\nhand if we don\u2019t expand into space. Garrett\u2019s 200-year estimate just puts an\nexclamation point on it. A renewed emphasis on reaching the Moon and Mars\noffers some hope.\n\nThe Artemis program is a renewed effort to establish a presence on the Moon.\nAfter that, we could visit Mars. Are these our first steps to becoming a\nmulti-planetary civilization? Image Credit: NASA\n\nThe second conclusion concerns legislating and governing AI, a difficult task\nin a world where psychopaths can gain control of entire nations and are bent\non waging war. \u201cWhile industry stakeholders, policymakers, individual experts,\nand their governments already warn that regulation is necessary, establishing\na regulatory framework that can be globally acceptable is going to be\nchallenging,\u201d Garrett writes. Challenging barely describes it. Humanity\u2019s\ninternecine squabbling makes it all even more unmanageable. Also, no matter\nhow quickly we develop guidelines, ASI might change even more quickly.\n\n\u201cWithout practical regulation, there is every reason to believe that AI could\nrepresent a major threat to the future course of not only our technical\ncivilization but all technical civilizations,\u201d Garrett writes.\n\nThis is the United Nations General Assembly. Are we united enough to constrain\nAI? Image Credit: By Patrick Gruban, cropped and downsampled by Pine \u2013\noriginally posted to Flickr as UN General Assembly, CC BY-SA 2.0,\nhttps://commons.wikimedia.org/w/index.php?curid=4806869\n\nMany of humanity\u2019s hopes and dreams crystallize around the Fermi Paradox and\nthe Great Filter. Are there other civilizations? Are we in the same situation\nas other ETIs? Will our species leave Earth? Will we navigate the many\ndifficulties that face us? Will we survive?\n\nIf we do, it might come down to what can seem boring and workaday: wrangling\nover legislation.\n\n\u201cThe persistence of intelligent and conscious life in the universe could hinge\non the timely and effective implementation of such international regulatory\nmeasures and technological endeavours,\u201d Garrett writes.\n\n### Share this:\n\n  * Click to share on Facebook (Opens in new window)\n  * Click to share on Twitter (Opens in new window)\n  * Click to share on Reddit (Opens in new window)\n\n### Like this:\n\nLike Loading...\n\nCategoriesArtificial Intelligence, Fermi Paradox, Great\nFilterTagsCivilization, Drake Equation, extinction, Extraterrestrial\nIntelligence, humanity, technological singularity\n\n### Leave a Reply Cancel reply\n\nYou must be logged in to post a comment.\n\nThis site uses Akismet to reduce spam. Learn how your comment data is\nprocessed.\n\nPrivacy Policy Proudly powered by WordPress\n\n%d\n\n", "frontpage": false}
