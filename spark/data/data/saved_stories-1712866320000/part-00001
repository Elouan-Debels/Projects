{"aid": "40002346", "title": "Scaling Laws for Data Filtering \u2013 Data Curation Cannot Be Compute Agnostic", "url": "https://arxiv.org/abs/2404.07177", "domain": "arxiv.org", "votes": 1, "user": "kmdupree", "posted_at": "2024-04-11 14:10:57", "comments": 0, "source_title": "Scaling Laws for Data Filtering -- Data Curation cannot be Compute Agnostic", "source_text": "[2404.07177] Scaling Laws for Data Filtering -- Data Curation cannot be\nCompute Agnostic\n\nSkip to main content\n\nWe gratefully acknowledge support from the Simons Foundation, member\ninstitutions, and all contributors. Donate\n\n> cs > arXiv:2404.07177\n\n# Computer Science > Machine Learning\n\narXiv:2404.07177 (cs)\n\n[Submitted on 10 Apr 2024]\n\n# Title:Scaling Laws for Data Filtering -- Data Curation cannot be Compute\nAgnostic\n\nAuthors:Sachin Goyal, Pratyush Maini, Zachary C. Lipton, Aditi Raghunathan, J.\nZico Kolter\n\nView a PDF of the paper titled Scaling Laws for Data Filtering -- Data\nCuration cannot be Compute Agnostic, by Sachin Goyal and 4 other authors\n\nView PDF HTML (experimental)\n\n> Abstract:Vision-language models (VLMs) are trained for thousands of GPU\n> hours on carefully curated web datasets. In recent times, data curation has\n> gained prominence with several works developing strategies to retain 'high-\n> quality' subsets of 'raw' scraped data. For instance, the LAION public\n> dataset retained only 10% of the total crawled data. However, these\n> strategies are typically developed agnostic of the available compute for\n> training. In this paper, we first demonstrate that making filtering\n> decisions independent of training compute is often suboptimal: the limited\n> high-quality data rapidly loses its utility when repeated, eventually\n> requiring the inclusion of 'unseen' but 'lower-quality' data. To address\n> this quality-quantity tradeoff (\\texttt{QQT}), we introduce neural scaling\n> laws that account for the non-homogeneous nature of web data, an angle\n> ignored in existing literature. Our scaling laws (i) characterize the\n> \\textit{differing} 'utility' of various quality subsets of web data; (ii)\n> account for how utility diminishes for a data point at its 'nth' repetition;\n> and (iii) formulate the mutual interaction of various data pools when\n> combined, enabling the estimation of model performance on a combination of\n> multiple data pools without ever jointly training on them. Our key message\n> is that data curation \\textit{cannot} be agnostic of the total compute that\n> a model will be trained for. Our scaling laws allow us to curate the best\n> possible pool for achieving top performance on Datacomp at various compute\n> budgets, carving out a pareto-frontier for data curation. Code is available\n> at this https URL.\n\nComments:| Published at CVPR 2024  \n---|---  \nSubjects:| Machine Learning (cs.LG)  \nCite as:| arXiv:2404.07177 [cs.LG]  \n(or arXiv:2404.07177v1 [cs.LG] for this version)  \nhttps://doi.org/10.48550/arXiv.2404.07177arXiv-issued DOI via DataCite  \n  \n## Submission history\n\nFrom: Sachin Goyal [view email] [v1] Wed, 10 Apr 2024 17:27:54 UTC (342 KB)\n\nFull-text links:\n\n## Access Paper:\n\nView a PDF of the paper titled Scaling Laws for Data Filtering -- Data\nCuration cannot be Compute Agnostic, by Sachin Goyal and 4 other authors\n\n  * View PDF\n  * HTML (experimental)\n  * TeX Source\n  * Other Formats\n\nview license\n\nCurrent browse context:\n\ncs.LG\n\n< prev | next >\n\nnew | recent | 2404\n\nChange to browse by:\n\ncs\n\n### References & Citations\n\n  * NASA ADS\n  * Google Scholar\n  * Semantic Scholar\n\na export BibTeX citation Loading...\n\n## BibTeX formatted citation\n\n\u00d7\n\nData provided by:\n\n### Bookmark\n\n# Bibliographic and Citation Tools\n\nBibliographic Explorer (What is the Explorer?)\n\nLitmaps (What is Litmaps?)\n\nscite Smart Citations (What are Smart Citations?)\n\n# Code, Data and Media Associated with this Article\n\nCatalyzeX Code Finder for Papers (What is CatalyzeX?)\n\nDagsHub (What is DagsHub?)\n\nGotit.pub (What is GotitPub?)\n\nPapers with Code (What is Papers with Code?)\n\nScienceCast (What is ScienceCast?)\n\n# Demos\n\nReplicate (What is Replicate?)\n\nHugging Face Spaces (What is Spaces?)\n\nTXYZ.AI (What is TXYZ.AI?)\n\n# Recommenders and Search Tools\n\nInfluence Flower (What are Influence Flowers?)\n\nConnected Papers (What is Connected Papers?)\n\nCORE Recommender (What is CORE?)\n\nIArxiv Recommender (What is IArxiv?)\n\n  * Author\n  * Venue\n  * Institution\n  * Topic\n\n# arXivLabs: experimental projects with community collaborators\n\narXivLabs is a framework that allows collaborators to develop and share new\narXiv features directly on our website.\n\nBoth individuals and organizations that work with arXivLabs have embraced and\naccepted our values of openness, community, excellence, and user data privacy.\narXiv is committed to these values and only works with partners that adhere to\nthem.\n\nHave an idea for a project that will add value for arXiv's community? Learn\nmore about arXivLabs.\n\nWhich authors of this paper are endorsers? | Disable MathJax (What is MathJax?)\n\n  * About\n  * Help\n\n  * Contact\n  * Subscribe\n\n  * Copyright\n  * Privacy Policy\n\n  * Web Accessibility Assistance\n  * arXiv Operational Status Get status notifications via email or slack\n\n", "frontpage": false}
