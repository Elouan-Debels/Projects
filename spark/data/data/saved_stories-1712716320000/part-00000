{"aid": "39983910", "title": "Making an RISC-V OS (Part 3): Managing free memory", "url": "https://traxys.me/riscv_os_buddy.html", "domain": "traxys.me", "votes": 9, "user": "traxys", "posted_at": "2024-04-09 20:30:33", "comments": 0, "source_title": "Making an RISC-V OS (Part 3): Managing free memory", "source_text": "Making an RISC-V OS (Part 3): Managing free memory\n\n# Making an RISC-V OS (Part 3): Managing free memory\n\n## 09 April 2024\n\n1 - Where is my RAM, and how much of it have I?\n\n1.1 - Looking at the QEMU Devicetree\n\n1.2 - Parsing the DTB\n\n1.3 - The early allocator\n\n1.4 - Reserved memory\n\n2 - Allocating (physically contiguous) memory\n\n2.1 - A quick aside on fragmentation\n\n2.2 - Organizing the RAM into free lists\n\n2.3 - Managing buddy allocations\n\nSee previous part on handling the transition from physical to virtual memory.\n\n## Where is my RAM, and how much of it have I?\n\nWhen we left last time our kernel had booted up in virtual memory, but we\nstill had no idea what our surroundings looked like. The only things we know\nabout are the sections that have been allocated by the linker in our ELF\nbinary, but those are statically defined, they don't adapt to our hardware.\n\nWe need to find a way to get a list of all the devices & memory regions we\nhave access to, with their properties & physical locations. In x86 machines\nthis is often done through ACPI. On RISC-V, as well as ARM, this is often done\nthrough Devicetrees.\n\nDevice trees are a data structure that is populated by a low-level firmware\n(or it could even be hard-coded somewhere in the address-space) that contain\nall the information on the system. Device trees have two main representations:\n\n  * A textual format (.dts) that is human editable.\n  * A binary format (.dtb) that is space efficient and easily parsable by machines.\n\nIn order to gather our system specs we are going to rely on the dtb structure\npopulated by QEMU before the system boot. This structure is passed in the a1\nregister, and OpenSBI forwards it to us.\n\n### Looking at the QEMU Devicetree\n\nWe can dump the QEMU device tree blob by running qemu-system-riscv64 -M\nvirt,dumpdtb=virt.dtb. It is then possible to convert it to a textual format\nby running dtc -I dtb -O dts virt.dtb >> virt.dts.\n\nHere is a small extract from the Devicetree with the properties that we want\nto query for now:\n\n    \n    \n    /dts-v1/ / { #address-cells = <0x02>; #size-cells = <0x02>; compatible = \"riscv-virtio\"; model = \"riscv-virtio,qemu\"; /* some nodes ... */ memory@80000000 { device_type = \"memory\"; reg = <0x00 0x80000000 0x00 0x80000000>; }; /* ... more nodes */ };\n\nThe RAM location in physical memory and its length is coded in the reg\nproperty of the memory@80000000 node. We simply need to parse this device tree\nwhen booting, and we would be able to initialize the rest of the memory.\n\n### Parsing the DTB\n\nThe Devicetree specification PDF describes in the 5th section the format of\nthe Devicetree.\n\nIn summary a device tree is divided in two main sections:\n\n  * The struct section, containing the hierarchy of the device tree.\n  * The strings section, containing the value of numerous properties.\n\nThe main structure to be parsed is the struct section, as the strings section\nis only pointed to. The struct section in organised in nested arrays, each\narray is composed of the following parts:\n\n  * A header indicating the start of a node\n  * A number of properties of the current object\n  * A number of sub arrays (each of exactly the same format)\n  * An end of node marker\n\nThis means that to get the offset of the second child of a node the entire\nfirst child tree must be parsed. In order to query the Devicetree without\nneeding to re-parse the entire tree each time we can use a new structure of\nthe form:\n\n    \n    \n    #[derive(Debug)] pub struct DeviceTreeNode<'a, 'd> { pub name: &'d str, // This is a type reprensenting each possible property pub props: Vec<'a, DtProp<'a, 'd>>, pub children: Vec<'a, DeviceTreeNode<'a, 'd>>, }\n\nThe only issue with this representation is that it requires an allocator, but\nwe are trying to find the RAM, so we don't have any free memory right now!\n\nNote that all integers are stored big-endian! Most tokens also need to be 32\nbit aligned. There exists a NOP token that needs to be skipped. All those\nrestrictions make the code a bit more complex than it seems with the\ndescription, so I won't show include the parsing function in this post, but it\nis available on the git repository\n\n### The early allocator\n\nIn order to parse the dtb we are going to create a small section in the binary\nusing the following assembly snippet:\n\n    \n    \n    .section .data .global _early_heap .align 12 _early_heap: .rep 1024 * 1024 .byte 0 .endr\n\nWe can then access this zone with the following code:\n\n    \n    \n    const EARLY_HEAP_LEN: usize = 1024 * 1024; extern \"C\" { #[link_name = \"_early_heap\"] static mut EARLY_HEAP: u8; } fn kmain(...) -> ! { /* ... See previous part ... */ let early_heap: &[u8] = unsafe { core::slice::from_raw_parts_mut( addr_of_mut!(EARLY_HEAP) as *mut MaybeUninit<u8>, EARLY_HEAP_LEN, ) }; /* ... parse the dtb ... */ }\n\nUsing this block of memory we have 1 MiB to do whatever we want! We are going\nto implement a really simple bump allocator with this memory. A bump allocator\nlooks like this:\n\nWe have a current pointer inside the buffer that indicates when next to\nallocate. The memory between start and current is in use, and can't be freed.\nWhenever we need to allocate some memory we can bump the current pointer\naccording to the amount we need, and use that block.\n\nIt is possible to improve a bit the bump allocator by implementing two\noptimizations:\n\n  * If are trying to free the last block (meaning the end of the block is equal to current) then we can decrement current by the length of the block. This allows to handle efficiently short-lived objects.\n  * If we try to re-allocate some memory, and it is the last piece of memory we allocated we can simply bump the current pointer to increase as required the allocation size (or decrease it!). This allows to efficiently implement growable arrays (Vec) where we append the items sequentially, and never touch them again.\n\nUsing this allocator we can implement Vec that re-allocate themselves with\ntwice the capacity whenever they are full. This allows us to parse easily the\ndtb described in the previous section!\n\n### Reserved memory\n\nThe memory section is not the only useful node to read, there is a second node\nthat is important to locate the RAM: reserved-memory. OpenSBI will reserve\nsome of the RAM for itself, and our OS is not allowed to touch it. This is\nimplemented using Machine level permissions, if we try to access that memory\nthe CPU will trap.\n\nThis means that we need to combine the information from the memory field with\nthe information from the reserved-memory field in order to find the effective\nmemory.\n\nRight now Hades only handles a single physical memory block, and uses all\nmemory after the last reserved-memory field.\n\n## Allocating (physically contiguous) memory\n\nNow that we know where our physical memory lies we can start to write some\ncode to know what RAM is free, and what RAM is allocated. We are going to\nwrite a buddy allocator for getting pages, as it is a quite simple allocator\nthat has nice guarantees on external fragmentation. In order to simplify the\nimplementation we are going to choose the largest allocation size, above which\nwe won't be able to allocate contiguous memory. This is a bit arbitrary, but\nwe are going to choose pages or 16 MiB.\n\n### A quick aside on fragmentation\n\nWhen talking about allocators fragmentation is one of the most important\nthings we need to think about. There are two kinds of fragmentation that are\nimportant to handle:\n\n  * Internal fragmentation\n  * External fragmentation\n\nExternal fragmentation measures the \"contiguousness\" of memory. If an\nallocator has 16 GiB available, but can't allocate more than 1 KiB of\ncontiguous memory because all the memory is split up, then we would say that\nthe external fragmentation is very large.\n\nInternal fragmentation measures the amount of \"wasted\" space. An allocator can\ngive a bigger amount of RAM to a request than requested. For example if our\nallocation is a page (4 KiB), and the user asks for 4 B we would waste most of\nthe space!\n\nSee the Wikipedia page for more details on fragmentation.\n\n### Organizing the RAM into free lists\n\nThe first step in implementing a buddy allocator is to define free lists for\neach of the allocation sizes we support. A free list is simply a linked list\nwhere we only pop or push to the head of the list. This allows fast access to\nfree elements, if all our elements are fungible.\n\nSo we can start our page allocator as follows:\n\n    \n    \n    #[derive(Clone, Copy)] struct FreeList { head: Option<&'static Page>, } #[derive(Clone, Copy)] struct FreePageState { next: Option<&'static Page>, } pub union PageState { free: FreePageState, } #[derive(Debug)] #[non_exhaustive] pub struct Page { pub state: UnsafeCell<PageState>, /// Represents the status of allocations in the buddy allocator allocated: AtomicU16, } pub struct PageAllocator { free: [FreeList; MAX_ORDER + 1], // MAX_ORDER is 12 pages: &'static [Page], } static PAGE_ALLOCATOR: SpinLock<PageAllocator> = SpinLock::new(PageAllocator { free: [FreeList { head: None }; MAX_ORDER + 1], pages: &[], });\n\nWe need to initialize the PageAllocator with a pointer to a memory region that\nwill store our bookkeeping for the pages (the array of Page). In our OS we are\ngoing to put this at the start of RAM (as found in the dtb). Each entry in the\nPageAllocator.pages field represents the corresponding 4 KiB page from the\nremaining of RAM.\n\nThe Page structs are linked together using the Page.state.free.next field.\nThis field is only valid when the page is not currently allocated. Because we\nmust use one Page struct for every page in the system it is important to keep\nthe struct as small as possible.\n\nBecause there is no way to construct a PageAllocator, as the fields are\nprivate, the safety invariants on the PageState rely on the PAGE_ALLOCATOR\nstructure being only accessible through a SpinLock. This allows us to ensure\nthat while we hold the lock we can be sure that there can't be another user\nthat can access unallocated pages, and as such it is safe to take a (single!)\nmutable reference on the Page.state field.\n\nWe are then going to create a init function, that will assign physical memory\nthe to PAGE_ALLOCATOR static:\n\n    \n    \n    fn align(v: usize, exp: usize) -> usize { assert!(exp.is_power_of_two()); (v + exp - 1) & !(exp - 1) } /// SAFETY: `start` must point to currently _unused_ physical memory /// /// This function may only be called once pub unsafe fn init(start: usize, mut length: usize) { static INIT: AtomicBool = AtomicBool::new(false); assert!(!INIT.load(atomic::Ordering::Relaxed)); let max_page_size = PAGE_SIZE * (1 << MAX_ORDER); let start_aligned = align(start, max_page_size); // Remove start pages that are not aligned length -= start_aligned - start; // Remove end pages that don't fit in a max_page_size length -= length % max_page_size; // We know know exactly how many 2**12 contiguous pages blocks are available on the system let max_page_count = length / max_page_size; let page_count = length / PAGE_SIZE; let page_info_size = page_count * core::mem::size_of::<Page>(); let required_pages_for_page_struct = page_info_size.div_ceil(max_page_size); // We reserve the start of RAM for our array of Page let pages: &[Page] = unsafe { core::slice::from_raw_parts( (start_aligned + RAM_VIRTUAL_START as usize) as *const _, page_count, ) }; // We iterate over each of the pages in the start of a 2**12 block of pages, // and add it to the max order freelist let mut free_pages = [FreeList { head: None }; MAX_ORDER + 1]; for i in (required_pages_for_page_struct..max_page_count).rev() { let page_idx = i * (1 << MAX_ORDER); free_pages[MAX_ORDER].add(&pages[page_idx]); } let mut allocator = PAGE_ALLOCATOR.lock(); allocator.free = free_pages; allocator.pages = pages; INIT.store(true, atomic::Ordering::Relaxed); }\n\n### Managing buddy allocations\n\nTo represent a page allocation we are going to use a PageAllocation structure,\nthat acts as a RAII handle:\n\n    \n    \n    pub struct PageAllocation(&'static [Page]); impl Deref for PageAllocation { type Target = [Page]; fn deref(&self) -> &Self::Target { self.0 } } impl Drop for PageAllocation { fn drop(&mut self) { PAGE_ALLOCATOR.lock().free(self.0); } } pub fn alloc_pages(order: usize) -> Option<PageAllocation> { if order > MAX_ORDER { return None; } let pages = PAGE_ALLOCATOR.lock().alloc(order)?; Some(PageAllocation(pages)) }\n\nWe are now going to describe how to allocate & free memory with a buddy\nallocator!\n\n#### Page allocation\n\nAllocations with a buddy allocator are quite simple: When allocating an order\nwe look if we have any available memory in the free list at index of our\nPAGE_ALLOCATOR. If a page is available then we remove it from the free list,\nand return it. If no page is available we try to allocate a page of order ,\nand then split it in two parts (the two buddies!), adding the second half to\nthe free list of index and returning the first half.\n\nNote that this is a recursive algorithm, and it terminates whenever we can\nallocate a page at a given order because the free list is not empty, or when\nthe free list at index MAX_ORDER is empty, and we try to pop from it.\n\nHere is the code to implement this algorithm:\n\n    \n    \n    impl PageAllocator { fn alloc(&mut self, order: usize) -> Option<&'static [Page]> { match self.free[order].head.take() { Some(h) => { self.free[order].head = unsafe { (*h.state.get()).free.next }; h.allocated.fetch_or(1 << order, atomic::Ordering::Relaxed); Some(&self.pages[h.index(self.pages)..h.index(self.pages) + (1 << order)]) } None => { if order == MAX_ORDER { None } else { let larger = self.alloc(order + 1)?; let buddy = &larger[larger.len() / 2]; self.free[order].add(buddy); larger[0] .allocated .fetch_or(1 << order, atomic::Ordering::Relaxed); Some(&larger[0..larger.len() / 2]) } } } } }\n\n#### Freeing allocations\n\nFreeing allocations is conceptually simple, but requires a bit more code to\nimplement. The main idea is that when we free a page of order we try to see if\nthe buddy (meaning the part that was split up during the allocation) is free\ntoo. If both are free then we can merge the two pages of order to create a\npage of order .\n\nWe can then recursively free the page of order , until we arrive at a page of\norder MAX_ORDER, where we don't have buddies anymore.\n\nThe code that does this is the following:\n\n    \n    \n    impl PageAllocator { fn free(&mut self, pages: &'static [Page]) { // Because order is a power of two this is guaranteed to give the log2 let order = pages.len().trailing_zeros() as usize; // Mark the page as free let prev = pages[0] .allocated .fetch_and(!(1 << order), atomic::Ordering::Relaxed); assert!(prev & (1 << order) != 0); // If it's a max order page no merging to be done if order < MAX_ORDER { /// Calculate the index of the page using the address of pages[0] let index = pages[0].index(self.pages); // Is this the first page or the second page? // // There exists a trick to find the buddy with a bit manipulation if sizes are a // power of two, but as I don't exactly understand it I have not implemented it let (first_idx, buddy_index) = match index % (1 << (order + 1)) { 0 => (index, index + (1 << order)), _ => { let first = index - (1 << order); (first, first) } }; let buddy = &self.pages[buddy_index]; // Find if the buddy is currently allocated let buddy_allocation = buddy.allocated.load(atomic::Ordering::Relaxed); if buddy_allocation & (1 << order) == 0 { // Remove the buddy from it's freelist. Because we use a simply linked list we need // to iterate over the list to it. // I may implement a doubly linked list at a further date to alleviate this, but // it is much more unsafe to do in rust self.free[order] .remove(buddy) .expect(\"buddy was not in the free list while it was un-allocated\"); buddy .allocated .fetch_and(!(1 << order), atomic::Ordering::Relaxed); // Recursively free the aggregated large page return self.free(&self.pages[first_idx..first_idx + (1 << (order + 1))]); } } self.free[order].add(&pages[0]); } }\n\nWith this we can now handle allocations & de-allocations of blocks of pages,\nup to 16 MiB. Note while the buddy allocator handles external fragmentation\nwell (due to us always re-aggregating the memory), our allocator has issues\nwith internal fragmentation. Indeed, even if we need to allocate a single byte\nwe need to allocate a 4 KiB page! This is often solved using slab allocators,\nbut we won't be implementing one right now, as we try to keep the complexity\nto a minimum if we don't encounter any issues.\n\n###### Back to index\n\nMade with Verin The source code is licensed MIT. The website content is\nlicensed CC BY NC SA 4.0.\n\n", "frontpage": true}
