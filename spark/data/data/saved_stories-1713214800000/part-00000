{"aid": "40041406", "title": "Do we need to store all that telemetry?", "url": "https://mattklein123.dev/2024/04/10/do-you-need-to-store-that-telemetry/", "domain": "mattklein123.dev", "votes": 51, "user": "mklein123", "posted_at": "2024-04-15 14:57:23", "comments": 13, "source_title": "Do you *really* need to store all that telemetry?", "source_text": "Do you *really* need to store all that telemetry?\n\n  * Home\n  * About\n  * Appearances\n  * Writing\n\nPrevious post Next post Back to top Share post\n\n  1. 1\\. Reasons for emitting telemetry\n  2. 2\\. Are there truly unique problems?\n  3. 3\\. How does adding a control plane help?\n  4. 4\\. What about local storage?\n  5. 5\\. Putting it all together\n\n# Do you *really* need to store all that telemetry?\n\nMatt Klein\n\n2024-04-10\n\nIn my last post I talked about why modern observability has become so\nexpensive. At the end of the post I posit a question: What if by default we\nnever send any telemetry at all?\n\nIn this post I\u2019m going to talk specifically about the perceived need of\nstoring all possible telemetry data. Is this a real need? Or is this an idea\nthat has been drilled into us for so long that we think there\u2019s no other\npossible way of doing it? If we can break away from the disconcerting idea\nthat all possible telemetry is not stored by default, what new paradigms and\ncost models for observing and debugging systems become possible?\n\n## Reasons for emitting telemetry\n\nBefore diving into the question of whether we really need to store telemetry,\nit\u2019s first important to break down the reasons why we do store telemetry.\nTelemetry emission can be generally placed into three discrete categories:\n\n  1. Telemetry used for debugging, monitoring, and general observability: When people mention telemetry in the observability context, this is typically the category that they are thinking about. Emitting logs, metrics, and traces to be used for alarming on anomalous conditions as well as debugging hard to understand issues. The end goal is to prevent downtime, continuously improve performance, and make customers happy.\n  2. Telemetry required for compliance and auditing: Security and compliance requirements may force us to store specific pieces of information for our applications (audit trails, network access logs, etc.). This class of telemetry generally never has to be queried live so can be sent to relatively cheap cold storage and queried later if absolutely necessary.\n  3. Telemetry required for business intelligence: Established businesses have core metrics that they track for decision making. Telemetry (whether in the form of traditional metrics, analytics events, or something else) is emitted by applications and tracked in business intelligence dashboards.\n\nFor the purpose of the remainder of this post I will only consider the first\ncategory. Telemetry required for compliance, auditing, or business\nintelligence is very important though out of scope due to its required storage\nand is thus exempt from the thought exercise: can we disable everything by\ndefault?\n\n## Are there truly unique problems?\n\nWhen presented with the idea of disabling all telemetry by default, the most\ncommon skeptical response is: \u201cWhat if I need to debug a problem that just\nhappened?\u201d This is a perfectly reasonable response and I admit that the idea\nof disabling all telemetry by default is likely to be deeply disconcerting to\nsome.\n\nLet\u2019s start by breaking down how telemetry helps with observability of\nsystems:\n\n  1. Metrics provide a cost effective summarization of application events and can be used for understanding the overall shape of system behavior.\n  2. Logs/events provide a more detailed trail of exact application behavior at higher relative cost to metrics. Logs are great for digging into the details of what led up to a specific event.\n  3. Traces provide a parent/child grouping of events and timings either within a single process or across multiple processes/distributed nodes. Traces are great for understanding exact relationships between system components.\n\nIn my personal experience as an engineer working on very large scale systems\nfor nearly 25 years, I understand the fear of disabling telemetry by default,\nbut I also believe that there is no problem that happens once and never\nhappens again. This is the critical point. If problems are not truly unique\nand are likely to happen again, can\u2019t we take steps to catch the issue the\nnext time around with targeted telemetry specific to that problem? Said\nanother way, once a problem is known, can we instruct the system to collect\nadditional information and provide it to us the next time the problem occurs?\n\nOne might then naturally ask, \u201cif I disable all telemetry by default how would\nI know about problems in the first place?\u201d While the terms APM, monitoring,\nobservability, etc. have become muddled due to vendor marketing, it is\nimportant to differentiate the act of \u201cmonitoring\u201d a system from the act of\ndebugging. The ideas presented in this post do not aim to change the methods\nof \u201cmonitoring\u201d for problems: we are notified of problems via customer\ncomplaints, alert conditions on metrics or event counts (that are sent all the\ntime so that we can run alert queries), in newer systems possibly anomaly\ndetection, and so on. A different way of thinking about \u201coff by default\u201d is\nthat all signals become opt-in vs. opt-out. I.e., the data needed to run an\nalert query would be opted-in by default. This post is talking about what\nhappens after we are notified of a problem.\n\nI admit that the scale of the systems that I have worked on have strongly\nshaped my views on this topic. Between working on high performance computing\nin AWS EC2, to building Twitter\u2019s edge proxy, to creating Envoy at Lyft, the\norder of magnitude of requests per second across the entire system and on each\nnode has always meant that capturing detailed logging by default was never\npractical purely from an overhead perspective. (By default, Envoy logs at Info\nlevel and at that level emits nearly no logs after startup, a pattern I have\ncome to use on all systems that I build, for better or worse.)\n\nInstead, I have relied on metrics for large scale debugging and monitoring of\nthese systems. While this has been a successful approach overall (Envoy after\nall is run by thousands of organizations and is considered a highly reliable\npiece of software), it\u2019s not without issues. Taking Envoy as specific example\n(even though the same idea applies to other systems I have worked on):\n\n  1. Envoy emits so many metrics that the cost of the metrics themselves becomes prohibitive for many organizations, to the point that many operators statically disable a portion of Envoy metrics by default. (As an aside this disabling mechanism has exposed many bugs over the years because the metrics themselves have been erroneously used for control logic \u2013 but that is an interesting story for a different day!)\n  2. When problems do crop up that require more intensive debugging, the lack of explicit logging at all times can lead to a prolonged debugging process.\n\nOn the topic of the prolonged debugging process, what does that look like\nusing traditional telemetry production techniques?\n\n  1. Attempt a local reproduction. If this is easily possible, using debug and trace logs to root cause the issue is usually relatively easy.\n  2. If that fails (or in parallel), start with code inspection to try to manually intuit what might have happened.\n  3. If local reproduction and manual intuition fails, we are then left with the painful process of manual code changes, possibly across many deployments, to add telemetry breadcrumbs to aid in catching the issue the next time it happens and root causing it.\n\nThankfully (3) above doesn\u2019t happen often, but it does happen, necessitating\npainfully involved debugging experiences.\n\nBuilding these types of systems over many years and debugging them has led me\nto think that there must be a more efficient way. And if there is a more\nefficient way for debugging and observing systems like Envoy, can those ideas\nbe applied to systems at large in such a way to both improve the debugging and\nobservation experience but also reduce cost along the way?\n\n## How does adding a control plane help?\n\nFor 30 years how telemetry is produced has not changed: we define all of the\ndata points that we need ahead of time and ship them out of the origin\nprocess, typically at large expense. If we apply the control plane / data\nplane split to observability telemetry production we can fundamentally change\nthe status quo for the first time in three decades via getting real-time\naccess to the data we need to debug without having to store and pay for all of\nit:\n\n  1. Enable/disable metrics, logs, and events at the source.\n  2. Filter telemetry at the source.\n  3. Live stream telemetry at the source to a web portal or CLI without storing it anywhere along the way!\n\nBeyond simply enabling/disabling/filtering at the source, we can add\nintelligence to our telemetry production points and do so much more. We can\nhave the control plane send instructions on a specific sequence of events to\nmatch on (effectively a finite state machine sent from control plane to data\nplane), followed by a set of actions to take when that sequence of events\noccurs. Actions could be to dump extra information, take profiles, take\nscreenshots on mobile apps, and so on.\n\nContinuing with the Envoy example in the previous section, instead of having\nto painfully make code changes and deploy them over a long period of time, if\nall of the built-in metrics, debug, and trace logs could be controlled\ndynamically with very specific match conditions and actions, think about how\nmuch shorter the debugging cycle would be. If we assume that problems are\nnever unique and will happen again, we can start doing iterative debugging in\nreal-time against a fleet of running instances, and likely vastly reduce the\ntime required to root cause an issue.\n\nAnother way of thinking about it is that this model makes debug information\navailable with immediacy relative to the criticality of the issue as defined\nby the number of occurrences. Meaning, during an incident the wait time for\nrecurrence will be close to zero, while the wait time might be longer for very\nuncommon things that are likely to be considered less critical.\n\nThis approach is powerful across multiple dimensions:\n\n  1. By targeting specific conditions, operators can avoid wading through volumes of information that are not relevant to their observation of the system, significantly reducing cognitive load.\n  2. Operators can decide in real-time what is the best observation method. Would they like to see actual log lines? Would they like to see synthetic metric aggregates of log lines? Would they like to see a subset of explicit metrics?\n  3. Not surprisingly, because the data needed for investigation is enabled and nothing more, the value of the transported and stored data approaches 100%, making the ROI of the more limited dataset a great value proposition.\n\nAs long as operators are able to accept the idea that problems will occur\nagain, with the right system they can catch them the next time around!\n\n## What about local storage?\n\nAlong with real-time control, we can also add local storage of telemetry data\nin an efficient circular buffer. Typically, local storage is cheap and\nunderutilized, allowing for \u201cfree\u201d storage of a finite amount of historical\ndata, that wraps automatically. Local storage provides the ability to \u201ctime\ntravel\u201d when a particular event is hit. How many times have we logged an error\nonly to realize that it\u2019s impossible to figure out the real source of the\nerror without the preceding debug/trace logs?\n\nA possibly obvious tradeoff of this system is that the lookback period is\nbounded, with the number of seconds of data available a function of the buffer\nsize and the data rate. I still think that the benefits of the circular buffer\nin terms of cost efficiency outweigh the downsides of limited historical\nretention.\n\nWhen coupled with the matching and actions described in the previous section,\ndumping the local buffer becomes a specific action to take, along with many\nother possible actions.\n\nI will note that this is not a new idea: the Apollo spacecraft guidance\ncomputer had a system to store recently executed instructions in a circular\nbuffer and dump them when a problem occurred to ease debugging. Similar\ndebugging tools have been implemented for many years in embedded devices and\nother systems. Circular buffers have also been used in modern observability\nsystems as part of centralized trace aggregation systems. The key difference\nis moving local storage all the way to the edge where it is easier to scale\nand coupling it with a control plane, which unlocks the ability to deploy\ndynamic queries across many targets that can result in history being dumped to\nease debugging.\n\nImagining the combination of local storage and real-time control being used to\ndebug Envoy is what started me down this entire path in the first place!\n\n## Putting it all together\n\nThe paradigms used to emit telemetry have remained unchanged for many years.\nTo this end, engineers are very used to sending data, and expecting it to be\nthere in case they might need it.\n\nAdding a control plane, local storage, and not sending any data by default is\na drastic change to how engineers think about observability. Some engineers\nfind these ideas deeply disconcerting, and fear that the data might be needed\nso it should be sent no matter what.\n\nBy starting from a default of no exported telemetry, engineers have the\nability to \u201cchange the game\u201d in multiple ways:\n\n  1. Because the major cost of telemetry production is what happens to data after it leaves the process, not the production within a process, we can free developers from thinking about cost at all. Emit as many metrics, logs, events, traces, etc. as desired. It\u2019s effectively free!\n  2. Real-time control via the control plane allows telemetry to be enabled and disabled on-demand, whether to temporarily debug an issue, permanently generate (synthetic) metrics to populate dashboards and alerts, etc. Ad-hoc investigations can lead to dynamic production of telemetry, solely for the purpose of solving the issue at hand, before being disabled again.\n  3. Critical telemetry needed for auditing and/or monitoring can be sent by default by request of the operator. Furthermore, the definition of critical telemetry can be changed without the need for any code changes or deployments.\n\nIt\u2019s true that changing how engineers and operators think about telemetry\nproduction and consumption is a tall order, but in my opinion the benefits of\nthis change far outweigh the challenges of relearning fundamental techniques:\nnamely, vastly decreased cost and vastly increased fidelity for helping to\nunderstand and fix product issues that are actively problematic.\n\nI founded bitdrift last year with the singular purpose of moving away from 30\nyear old telemetry production paradigms and tackling this problem head on. Is\nit indeed possible to not send any telemetry by default and still successfully\nobserve large scale production systems?\n\nSince we launched Capture publicly a few months ago it\u2019s been super\ninteresting to start talking to potential customers; the responses we have\nreceived vary widely, ranging from those that immediately buy into the idea of\noff by default to those that are inherently skeptical, asking the same\n(completely reasonable!) question posed above: \u201cBut what if I really need to\nlook at some of that data after the fact?\u201d\n\nI continue to be extremely excited by the idea of what we can build if we\nstart from the position of accepting that bugs will happen again, not sending\nany telemetry by default, and proceeding from there.\n\nSo, say it with me: \u201cNo, I don\u2019t really need to store all of that telemetry!\u201d\nOr, \u201cJust say no to the logging industrial complex!\u201d\n\n  * Home\n  * About\n  * Appearances\n  * Writing\n\n  1. 1\\. Reasons for emitting telemetry\n  2. 2\\. Are there truly unique problems?\n  3. 3\\. How does adding a control plane help?\n  4. 4\\. What about local storage?\n  5. 5\\. Putting it all together\n\nMenu TOC Share Top\n\nCopyright \u00a9 2020-2024 Matt Klein\n\n", "frontpage": true}
