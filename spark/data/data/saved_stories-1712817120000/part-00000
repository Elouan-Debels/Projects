{"aid": "39997205", "title": "Scaling Farcaster", "url": "https://github.com/farcasterxyz/protocol/discussions/163", "domain": "github.com/farcasterxyz", "votes": 2, "user": "martialg", "posted_at": "2024-04-11 00:29:39", "comments": 0, "source_title": "Scaling Farcaster \u00b7 farcasterxyz/protocol \u00b7 Discussion #163", "source_text": "Scaling Farcaster \u00b7 farcasterxyz/protocol \u00b7 Discussion #163 \u00b7 GitHub\n\nSkip to content\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nfarcasterxyz / protocol Public\n\n  * Notifications\n  * Fork 169\n  * Star 1.8k\n\n# Scaling Farcaster #163\n\nvarunsrin started this conversation in FIP Stage 1: Ideas\n\nScaling Farcaster #163\n\nvarunsrin\n\nApr 10, 2024 \u00b7 0 comments\n\nReturn to top\n\nDiscussion options\n\n##\n\nvarunsrin\n\nApr 10, 2024\n\nMaintainer\n\nOriginal comment in English -\n\n# Problem\n\nOver the last three months, Farcaster grew 16x from 5,000 to 80,000 daily\nusers.Stress on the network has increased proportionally. We\u2019re unlikely to\nkeep up with another 10x in growth. Each day, our nodes (hubs) are seeing:\n\n  * 80,000 daily users\n  * 4.4M messages\n  * 1.7 GiB of state growth\n  * 45.5 - 70 GiB of network traffic\n\n# Goals\n\nFarcaster should handle 1 million daily users and make sure that:\n\n  1. A hub costs no more than $1000 / month to operate.\n  2. Messages arrive in < 10 seconds, 90% of the time.\n  3. 99% of hubs are \u201cin sync\u201d, according to some health metric.\n  4. 99.9% of data can replicate to an external database in < 1s.\n\nIt\u2019s important we achieve this without major architectural changes if\nfeasible, and want to hold onto the following invariants:\n\n  * A hub should store all state on the network (no sharding)\n  * Consensus should happen through CRDT\n  * Sync should happen through libp2p gossip + out of band sync\n\nIf you're new to Farcaster, check out the Hub overview video for more context.\n\n# Proposal\n\nThere are three areas we need to examine \u2014 state management, which validates\nnew messages and updates a hub's state, sync which converges all hubs to the\nsame global state and replication which copies data to an external data store.\n\n## State\n\nWhen a user says \u201cHello World!\u201d, a new cast message is created and added to\nthe global network state through a hub. The message's validity is checked and\nthen it is appended to the hub's local disk and gossiped out to the broader\nnetwork.A user must pay yearly rent (storage fees) to keep their state live on\nthe network. Renting a unit of storage lets a user store a specific number of\nmessages of each type. The average size of a message is ~ 300 bytes.\n\n### Problems\n\n  1. Global state is growing daily by 1.67 GiB and 4.39M messages. At 80k DAU, that's ~21 KiB and 55 messages of growth per user per day. At 1M users, we\u2019d expect to see 21 GiB of state growth per day or 7.5 TB of state growth per year.\n  2. Messages are added at an average rate of 50/s. At 1M users, we\u2019d expect 1000/s with spikes up to 10,000/s. We can handle this today but only by using NVMe disks because our system is very latency sensitive. This needs to be rewritten because NVMe disks have limited total capacity relative to general SSDs.\n\nStorage problems aren't hair on fire, but if growth continues at this pace\nwe'll likely need to deal with them in the next 6-12 months.\n\n### Metrics\n\nThe following data should be available on public dashboards:\n\n  1. Daily State Growth \u2014 how much state was added each day?\n  2. Throughput Stats \u2014 what was the peak and average messages per second in the last week?\n\nWe should also periodically run the following benchmarks:\n\n  1. Analysis of size and counts for messages for users (min, average, median, p95, p99, max)\n  2. Writes per second \u2014 can we commit 10,000 messages per second under ideal conditions?\n\n### Proposed Changes\n\n  1. Investigate latency sensitivity \u2014 is nvme truly a requirement? Run the benchmarks on our ideal system and build the dashboards.\n  2. Hub hardware limits \u2014 we increase the minimum storage requirement for starting a new hub to some dynamic multiple of the latest storage growth to prevent them from shutting down. As long as we are under $1000/month we should be fine.\n  3. Compression \u2014 A message is roughly 300 bytes but we use 400 bytes to store it on disk. Using compression or changing indices might reduce this by 50% (200 bytes).\n  4. Reducing limits \u2014 Changing storage limits for each unit to be 50% smaller would probably cause a 3-10% decrease in total state growth on the network.\n  5. Raising fees \u2014 Increasing storage costs would slow down the rate at which new users are added which would give us more headroom to grow.\n\n## Sync\n\nA new hub will do a snapshot sync to download messages up to the last day, and\ngossip sync to stream all messages going forward. It will also do a \"diff\nsync\" with another hub to download messages from the current day, which were\nnot in the snapshot. This diff sync is also re-run periodically to catch\nmessages that were dropped due to lossy gossip or other downtime.Sync can be\nthought of as a three-phase process:\n\n  1. Snapshot sync \u2014 a fast sync that brings a hub close to current state.\n  2. Gossip sync \u2014 a fast, lossy sync that fetches new messages via libp2p.\n  3. Diff sync \u2014 a slow, deterministic sync fetches any missing messages periodically.\n\n### Problems\n\n  1. Gossip sync consumes ~700 KiB/s on hubs while state is growing by ~ 20 KiB/s. That\u2019s a 35x overhead, and should be closer to 10x.\n  2. Hubs are exhibiting odd sync behavior on the margins - some hubs have more messages than expected while others are falling behind perpetually. The root cause is still unclear.\n  3. Diff sync can only handle 70-100 msg/s at peak and if a hub goes offline for a short while, it will never catch up the current state.\n  4. Hubs sometimes run into rate limits on blockchain nodes, likely due to using a free plan. This breaks sync silently and they diverge from the rest of the network until they are re-synced.\n\n### Metrics\n\nThe following data should be available on dashboards:\n\n  1. Message Delay \u2014 p90 for messages going from one hub to another, ideally <10s\n  2. Health \u2014 how many hubs are within 0.1% of the latest network state? ideally 99%+\n\nThe following data should be periodically benchmarked:\n\n  1. Gossip overhead \u2014 the ratio of gossip network traffic to state growth (target: 12:1)\n  2. Catchup speed \u2014 time take to catch up on the last 12 hours of sync (target: 1 hour)\n\n### Proposed Changes\n\n  1. Add bundles to reduce gossip throughput.\n  2. Find the root cause for hubs getting out of sync.\n  3. Throw clearer errors when blockchain nodes are unreadable instead of failing sync.\n  4. Improve diff sync so that it approaches 1k messages / second.\n  5. Explore new sync model using events streams to complement diff sync.\n\n## Replication\n\nWhen hubs receive messages they are stored locally in rocksdb. Apps will need\nto transfer this data to a higher level store like Postgres to be able to run\nqueries on it efficiently. Farcaster has a library called shuttle which helps\ncopy messages into a database table and keep it in sync.\n\n### Problems\n\n  1. Shuttle is still in alpha, and needs to be shipped. Once its out, we might have a better POV on what needs to improve.\n\n### Metrics\n\n  1. Replication Delay \u2014 p90 for messages going a hub to a replicated db, ideally < 1s\n\n## System Requirements\n\nA hub should be capable of being run on commodity cloud hardware for < $1000 /\nmonth while successfully replicating to a postgres database. For today\u2019s hubs\nwe recommend provisioning at least the following:| Resource| Amount  \n---|---  \nCPU| 4  \nRAM| 16 GB  \nStorage| 500 GB  \nBandwidth| 1 Gbps  \n  \nThis cost a little over $150/month on [latitude.sh](http://latitude.sh) today.\nLarger providers like AWS and GCP tend to be a bit more expensive.\n\nWe expect our requirements at 10x to be closer to:\n\nResource| Amount  \n---|---  \nCPU| 16  \nRAM| 128 GB  \nStorage| 20 TB  \nBandwidth| 10 Gbps  \n  \nBeta Was this translation helpful? Give feedback.\n\nYou must be logged in to vote\n\n## Replies: 0 comments\n\nSign up for free to join this conversation on GitHub. Already have an account?\nSign in to comment\n\nCategory\n\nFIP Stage 1: Ideas\n\nLabels\n\nNone yet\n\n1 participant\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": false}
