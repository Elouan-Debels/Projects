{"aid": "40001983", "title": "The Rise of Audio AR", "url": "https://www.dbreunig.com/2024/04/10/the_rise_of_audio_ar.html", "domain": "dbreunig.com", "votes": 1, "user": "dbreunig", "posted_at": "2024-04-11 13:35:36", "comments": 0, "source_title": "The Rise of Audio AR", "source_text": "The Rise of Audio AR | Drew Breunig\n\nThe Rise of Audio AR | Drew Breunig\n\ndbreunig.com\n\nApr 10, 2024\n\n# The Rise of Audio AR\n\n#### The Right Information, At The Right Time, Without Looking\n\nPhoto by Justin Lynham\n\nFor a bit there, virtual reality (VR) and augmented reality (AR) were being\nheralded as the next big thing.\n\nMeta went all-in, spending billions on Oculus R&D annually and eventually\nrebranding the company to signal their focus. Microsoft\u2019s Hololens had plenty\nof buzz, Magic Leap was the darling of the tech press and investors, Sony\nshipped millions of PSVR units, and Apple was rumored to be working on\nsomething big.\n\nBut the hype never materialized.\n\nOculus has yet to find its killer app and no one\u2019s quite sure what to do with\nApple\u2019s Vision Pro. ChatGPT arrived and AI quickly became the next big thing,\nbumping VR/AR to the back of the line.\n\nEven Meta seems to talk more about LLaMA than AR these days.\n\nBut quietly the pieces have been coming together for a different kind of AR:\nAudio Augmented Reality (AAR). Thanks to the rise of smart headphones,\nimproved voice recognition and generation, AI language models, and better\ncontextual data, AAR is set to slowly but surely seep into our daily lives.\nAAR will deliver the right information, at the right time without requiring us\nto change our focus. It\u2019s a monumental shift in how we interact with\ntechnology, and it\u2019s coming sooner than you think.\n\n#### The Pioneers of Audio AR\n\nThe concept of Audio AR isn\u2019t entirely new. Microsoft\u2019s Soundscape and\nFourSquare\u2019s MarsBot were two early attempts at delivering an Audio AR\nexperience.\n\nMicrosoft\u2019s Soundscape was developed primarily as an accessibility tool for\nthe visually impaired. As users walked through a city, spatial audio pings\nwould notify them of surrounding points of interest, from laundromats to\nrestaurants. It demonstrated the potential of using spatial audio and\nlocation-based alerts to augment reality. However, in dense urban\nenvironments, it created an overwhelming cacophony of sounds. And in suburban\nareas, it was too sparse to be very useful.\n\nFourSquare\u2019s MarsBot took a more personalized, data-driven approach. It\nleveraged FourSquare\u2019s extensive database of places, their ratings, and\nreviews to predict and recommend locations a user might find interesting. But\nMarsBot only thrived in a high-density environment like New York City. It was\nan experiment \u2013 one with great thinking about what it means to be an audio-\nfirst AR app \u2013 that never really landed. It struggled to be truly passive,\ndemanding too much user attention to filter through irrelevant alerts.\n\nThese apps, and experiments like them, broke important ground and showed Audio\nAR could be used for more than simple turn-by-turn navigation. Soundscape\nshowed the power of spatial audio and location-based notifications for\nwayfinding. MarsBot pioneered using personalized data and experimented with\nmore humane notifications. But it seems only a very specific set of nerds like\nmyself took notice. Their ideas lay dormant for a few years, waiting for the\nright enabling technologies to arrive.\n\n#### The Arrival of Enabling Technologies\n\nSeveral key technologies have emerged in recent years, enabling a new\ngeneration of Audio AR applications:\n\n  1. Smart wireless headphones like Apple\u2019s AirPods have seen massive adoption, putting an always-available, hands-free audio interface in many people\u2019s ears. They deliver high-quality sound and give developers access to a range of controls and sensor data, from volume to head movement to biometrics. Built-in microphones and simple controls make it easy to trigger assistants and audio apps.\n\n  2. Voice recognition has made huge leaps thanks to improved noise cancellation, language models, and edge computing. Talking to virtual assistants now feels almost as natural as conversing with a person. You can speak to them casually without having to modify your speech. Speech-to-text is now fast and accurate, without the need for a network connection.\n\n  3. Text-to-speech engines can now generate human-like voices with realistic inflection and intonation. The latest models are hard to distinguish from a real human, especially in short exchanges. This allows for the fluid generation of dynamic content.\n\n  4. Large language models can engage in open-ended conversation, answer follow-up questions, and even take actions on the user\u2019s behalf. They translate imperfect and inconsistent voice commands into programmatic actions enabling a more complex interface without a screen. Further, LLMs can be used as decision engines for what content to surface and when.\n\n  5. Rich location and context data allow Audio AR apps to deeply understand the user\u2019s environment and current situation. This includes detailed place data, real-time weather and traffic, calendar and messaging data, and more. It\u2019s impossible to serve the right bit of information at precisely the right time without this data.\n\nPutting it all together, we now can deliver highly contextual and personalized\naudio content and interactions to users as they go about their day. The\nchallenge now moves from building enabling technologies to building the UX.\n\n#### The Problem to Be Solved\n\nThe core UX challenge for Audio AR applications is delivering the right\ninformation at the right time.\n\nToo much irrelevant information becomes overwhelming. Too little and the\nexperience isn\u2019t very useful. The sweet spot is frustratingly narrow.\n\nTo illustrate this challenge, let\u2019s first establish a starting point with an\nAudio AR app and use case that works well: audio tours.\n\nA few weeks ago, I traveled to Ghent, Belgium for an Overture Maps Foundation\nmeeting (speaking of improved access to location data...). Having some time to\nexplore the first morning, I checked out an audio tour of the city on\nVoiceMap. I have no idea how good other tours are, but this tour was\nperfection. So good, it suggested the potential of Audio AR.\n\nVoice Map\u2019s tours work by using your phone\u2019s GPS to trigger audio descriptions\ntied to specific waypoints. As you walk, the narrator gently directs you where\nto go next, providing landmarks where the next segment of the tour will pick\nup. Voice Map helps creators time their content to specific route segments,\n(in this case) achieving a relaxed yet seamless experience as you stroll. If\nyou pause to duck into a cafe to sit for a moment, the next cue isn\u2019t\ntriggered and the audio pauses. Walk outside and the content picks back up.\nThe interface was my headphones and my location, that\u2019s it.\n\nBut, to borrow a term from game design, I was on rails.\n\nThe content and notifications were perfect because I\u2019d purchased and started\nthe tour. Voice App knew where I was going and the information I wanted. For a\nsmall slice of Ghent, it was the perfect Audio AR experience. I started\nwondering what it would take to cover the entire city in content and cues,\nenabling me to walk wherever I wanted. But even then, I\u2019d turn the tour off if\nI was taking a path I\u2019d already walked or if I just didn\u2019t want to be\ndisturbed for a while. Even with content coverage, cues would get tricky\nquickly and require more than just my current location. Time of day, my\ncalendar, my past location history \u2013 all of this would figure into how the\ncontent should be surfaced.\n\nCreating an always-on, ambient experience that proactively surfaces relevant\ninformation is quite the challenge.\n\nAfter the tour, I looked up MarsBot to see if it was still available. It\u2019s\nnot, but Denis Crowley, FourSquare\u2019s founder and MarBot\u2019s creator, is taking\nanother bite at the apple with a new company called BeeBot. BeeBot leverages\nLLMs to help with content creation and determining when to push an audio\nnotification. Crowley was recently interviewed on Alex Kantrowitz\u2019s Big\nTechnology Podcast.\n\nLike MarsBot and SoundScape, Crowley\u2019s approach with BeeBot is to \u201cpush\u201d\ncontent to the user. This is undoubtedly the hard mode of Audio AR. Not only\ndo you have to figure out what content to push, but you have to figure out\nwhen to push it.\n\nFor their entry into Audio AR, Meta has taken the opposite approach.\n\nPhoto by Phil Nickinson for Digital Trends\n\n#### A Glimpse of the Future\n\nMeta\u2019s Wayfarer sunglasses are perhaps the best Audio AR product currently on\nthe market. They combine the enabling technologies \u2013 smart headphones, voice\nrecognition and synthesis, LLMs, and context data \u2013 into a single, cohesive\nproduct.\n\nDespite this, they\u2019re kinda flying under the radar. Before I had my pair, I\nknew one person who used them...and they work at Meta. If you, like me, aren\u2019t\nfamiliar with Meta\u2019s Wayfarers here\u2019s a quick rundown. They look nearly\nidentical to Ray Ban\u2019s iconic Wayfarer glasses, with a bit more heft on their\narms. They have built-in speakers that do a great job of delivering spatial\naudio so only you can hear it; there\u2019s no bass, but it\u2019s great for spoken\nword. They have cameras that let you take pictures and short videos of your\nPOV. And if you join the beta program, you can access Meta\u2019s voice multi-model\nAI. If you want to know more about what you\u2019re looking at, just say \u201cHey\nMeta,\u201d and ask.\n\nTo be sure, there are rough edges. The assistant refuses to answer many types\nof questions and is occasionally wrong. The interface is invisible, forcing\nusers to result to trial and error.\n\nDespite this, Meta\u2019s Wayfarers succeed because they use a \u201cpull\u201d UX model:\nusers have to explicitly ask for information. The guesswork of knowing when to\npush content is eliminated; a welcome decision for a product whose rough edges\nare still very apparent (hence the beta program). Contextual data is still\nused to inform the assistant\u2019s response \u2013 both with the user\u2019s location and\nthe camera\u2019s view \u2013 but the user controls the timing.\n\nAnd when everything clicks, Meta\u2019s Wayfarers are a glimpse of the future.\n\nPhoto by Trusted Reviews\n\n#### Focusing on a Specific Context Sessions\n\nThere\u2019s a hybrid approach to the UX worth considering: focusing on a specific\ncontext in a session that a user proactively engages. During this period the\napp can push content with confidence that it\u2019s relavent. This design pattern\nis put to excellent use by Apple\u2019s Fitness app.\n\nIf you keep an AirPod in your ear while using the Fitness app, you\u2019ll receive\npacing information, heart rate feedback, and more. While cycling, the Fitness\napp will pipe notifications into one ear, letting me keep my phone in my bag.\nBy engaging a specific context, the push notifications are all relevant. The\nsurface area of the UX is limited, allowing for a more controlled experience.\n\nBeyond fitness, this model could work well for a host of contexts where the\nuser opts into a context-specific session. Voice Maps could blanket a city\nwith generated content and allow users to engage an exploration mode, inviting\nthe app to push content frequently. A cooking app could push instructions and\nset timers, following the start of a recipe session. A shopping app could\nprovide product rankings and reviews as one picks up items in store, once a\nshopping session begins.\n\nFocusing on actively engaged contexts is a great way to chip away at Audio\nAR\u2019s problem to be solved. The app gets to push with abandon, but only when\nthe user turns it on. Developers get to focus on specific contextual data to\nobtain and consider and limited set of notification triggers.\n\n#### What We Need for the Mainstream\n\nTo fully realize the potential of Audio AR we need to build the tools and\nobtain the data needed to turn our context into an invisible interface.\n\nIn the three examples discussed above \u2013 Voice Map, Meta\u2019s Wayfarers, and Apple\nFitness \u2013 we both see the potential of and challenges facing Audio AR. Each\nproduct is awkward in its own way, but their stumbles often have to do with\naccess to contextual data.\n\nIf you go off the rails of a Voice Map tour, the app has nothing to say. Its\nknowledge of the world is limited to the path prepared by a tour creator; it\ncan\u2019t help you explore.\n\nMeta\u2019s Wayfarers can\u2019t answer any questions regarding my calendar or messages\nbecause it doesn\u2019t have access to that data. And Siri, who can access those\nthings, can\u2019t be invoked from the glasses.\n\nApple Fitness doesn\u2019t suggest I head home if I have a meeting soon or it\u2019s\nabout to rain. The data is there, but the contextual domain Fitness cares\nabout is limited to my workout.\n\nIt feels like we\u2019re back at the beginning of the Web 2.0 era, before the\nCambrian Era of APIs that allowed our tools to talk to each other. Assistant\ninterfaces present themselves as having vast capabilities, yet they stumble if\nneeding anything from another platform.\n\nFor Audio AR to take off beyond contextual sessions, we need open standards\nfor interfacing with headphones, sharing contextual information, and invoking\nvoice assistants. Recent updates to Apple\u2019s APIs for AirPods and Android\u2019s\nAwareness API are steps in the right direction, but we\u2019re still a long way\nfrom the ideal foundation. (Hopefully fear of antitrust spurs better\nhabits...)\n\nUntil we get to this stable foundation, we\u2019ll be chipping away at the UX\nproblem of Audio AR \u2013 delivering the right information at the right time \u2013 in\nspecific context sessions.\n\n#### Our Guy in the Chair\n\nThe tech industry has the habit of getting behind new technology use cases\nwhen they have a solid pop culture reference. Ready Player One was more\nvaluable as a touchstone for Meta\u2019s VR positioning than it ever was as a\nmovie. The number of big data or business intelligence companies who sold\ntheir wares by referencing Minority Report or The Dark Knight is too high to\ncount.\n\nAs ChatGPT arrived and AI is having its moment, the tech industry has leaned\nheavily on Her to bring to life the vision of a conversational AI. And while\nthis might seem fitting to Audio AR, I think we can do better.\n\nPeople don\u2019t want a conversational relationship with an AI assistant. People\nwant a guy in the chair.\n\nPerhaps the best example of the \u201cguy in the chair\u201d trope is Ving Rhames as\nLuther Stickell in the Mission Impossible series. Luther is the tech guy who\nstays back at the base, monitoring the team\u2019s progress and providing them with\nthe information they need to succeed. He\u2019s the one who can see the big\npicture, who can guide the hero to success. He\u2019s the one who can deliver the\nright information at the right time, without the hero having to look away from\nthe task at hand.\n\nThe assistant in Her is an anomaly. The guy or gal in the chair trope is\nconstant and pervasive in our media. It\u2019s in Mission Impossible, The Matrix,\nApollo 13, The Martian, and the Batman, Spiderman, and Avengers series. In\nvideogames it\u2019s a staple, pivotal to the design of Metal Gear Solid, Portal,\nHalf-Life, and Mass Effect. The examples listed on the TV Tropes page are\nseemingly endless.\n\nWe want a guy in the chair, delivering the right information at the right\ntime, without us having to look away.\n\nAudio AR represents a major shift in how we interact with digital information.\nBy moving the interface from our screens to our ears, it allows us to stay\nheads-up and engaged with the world around us. It has the potential to make\ntechnology feel less intrusive, more ambient, and less constrained by the form\nfactors of our devices.\n\nBut to get there we need to solve some key interaction challenges. We need\nopen standards for integrating with headphones and understanding context. We\nneed thoughtful design that delivers the right information at the right time.\n\nThe enabling technologies are already in place. Headphones, voice interfaces,\nand AI have reached a level of maturity where Audio AR is practical. And we\u2019re\nseeing compelling products \u2013 not just experiments \u2013 like Meta\u2019s Wayfarer\nglasses.\n\nThe age of Audio AR is already here. It\u2019s just a matter of time before it\nbecomes mainstream.\n\n2024\n\n", "frontpage": false}
