{"aid": "40065862", "title": "Future of Humanity Institute shuts down", "url": "https://www.futureofhumanityinstitute.org", "domain": "futureofhumanityinstitute.org", "votes": 52, "user": "rdl", "posted_at": "2024-04-17 15:14:08", "comments": 15, "source_title": "Future of Humanity Institute", "source_text": "Future of Humanity Institute\n\nMenu\n\nStreet Address\n\nCity, State, Zip\n\nPhone Number\n\nYour Custom Text Here\n\n(2005\u20132024)\n\nEstablished in 2005, initially for a 3-year period, the Future of Humanity\nInstitute was a multidisciplinary research group at Oxford University. It was\nfounded by Prof Nick Bostrom and brought together a select set of researchers\nfrom disciplines such as philosophy, computer science, mathematics, and\neconomics to study big-picture questions for human civilization, attempting to\nshield them from ordinary academic pressures and create an organizational\nculture conducive to creativity and intellectual progress.\n\nDuring its 19-year existence, the team at FHI made a series of research\ncontributions that helped change our conversation about the future and\ncontributed to the creation of several new fields and paradigms. FHI was\ninvolved in the germination of a wide range of ideas including existential\nrisk, effective altruism, longtermism, AI alignment, AI governance, global\ncatastrophic risk, grand futures, information hazards, the unilateralist\u2019s\ncurse, and moral uncertainty. It also did significant work on anthropics,\nhuman enhancement ethics, systemic risk modeling, forecasting and prediction\nmarkets, the search for extraterrestrial intelligence, and on the attributes\nand strategic implications of key future technologies. One major contribution\nwas in showing that it was even possible to do rigorous research on big\npicture questions about humanity\u2019s future.\n\nOver time FHI faced increasing administrative headwinds within the Faculty of\nPhilosophy (the Institute\u2019s organizational home). Starting in 2020, the\nFaculty imposed a freeze on fundraising and hiring. In late 2023, the Faculty\nof Philosophy decided that the contracts of the remaining FHI staff would not\nbe renewed. On 16 April 2024, the Institute was closed down.\n\nOver the course of its nineteen years, FHI inspired the emergence of a vibrant\necosystem of organizations where the kinds of questions that FHI investigated\ncan be explored. FHI alumni will continue to research these questions both\nwithin Oxford and at other places around the world. Topics that once struggled\nto eke out a precarious existence at the margins of a single philosophy\ndepartment are now pursued by leading AI labs, government agencies,\nnonprofits, and specialized academic research centers (with many more in the\nprocess of creation).\n\n## Resources\n\n  * FHI\u2019s final tech report \u2014 an oral history of the institute\n\n  * A collection of FHI\u2019s technical reports and other online pieces\n\n  * Historical snapshots of FHI\u2019s official website at the Internet Archive\n\n  * An extensive list of articles by FHI (or mentioning FHI) via Google Scholar\n\n## FHI Books\n\n  * Global Catastrophic Risks, Nick Bostrom and Milan \u0106irkovi\u0107 (eds.), 2008.\n\n  * Human Enhancement, Julian Savulescu and Nick Bostrom (eds.), 2009.\n\n  * Radical Abundance: How a Revolution in Nanotechnology Will Change Civilization, Eric Drexler, 2013.\n\n  * Superintelligence: Paths, Dangers, Strategies, Nick Bostrom, 2014.\n\n  * The Precipice: Existential Risk and the Future of Humanity, Toby Ord, 2020.\n\n  * Moral Uncertainty, William MacAskill, Krister Bykvist, and Toby Ord, 2020.\n\n  * Deep Utopia: Life and Meaning in a Solved World, Nick Bostrom, 2024.\n\n## Key FHI Papers\n\n### Existential and Catastrophic Risk\n\n  * Existential risks: analyzing human extinction scenarios and related hazards, Nick Bostrom, 2002 (pre-FHI).\n\n  * Astronomical waste: the opportunity cost of delayed technological development, Nick Bostrom, 2003 (pre-FHI).\n\n  * How unlikely is a doomsday catastrophe? Max Tegmark and Nick Bostrom, 2005.\n\n  * What is a singleton? Nick Bostrom, 2005.\n\n  * Where are they? Why I hope the search for extraterrestrial life finds nothing, Nick Bostrom, 2008.\n\n  * Probing the improbable: methodological challenges for risks with low probabilities and high stakes. Toby Ord, Rafaela Hillerbrand, and Anders Sandberg, 2010.\n\n  * Anthropic shadow: observation selection effects and human extinction risks, Milan \u0106irkovi\u0107, Anders Sandberg, and Nick Bostrom, 2010.\n\n  * Information hazards, Nick Bostrom, 2011.\n\n  * Existential risk prevention as global priority, Nick Bostrom, 2013.\n\n  * How much could refuges help us recover from a global catastrophe? Nick Beckstead, 2015.\n\n  * Existential Risk and Existential Hope: Definitions, Owen Cotton-Barratt and Toby Ord, 2015.\n\n  * The unilateralist\u2019s curse and the case for a principle of conformity, Nick Bostrom, Thomas Douglas, and Anders Sandberg, 2016.\n\n  * An upper bound for the background rate of human extinction, Andrew Snyder-Beattie, Toby Ord, and Michael Bonsall, 2019.\n\n  * The vulnerable world hypothesis, Nick Bostrom, 2019.\n\n  * The lifespan of civilizations: do societies \u201cage,\u201d or Is collapse just bad luck? Anders Sandberg, 2023.\n\n### AI Safety\n\n  * Thinking inside the box: controlling and using an oracle AI, Stuart Armstrong, Anders Sandberg, and Nick Bostrom, 2012.\n\n  * The superintelligent will: Motivation and instrumental rationality in advanced artificial agents, Nick Bostrom, 2012.\n\n  * Safely interruptible agents, Laurent Orseau and Stuart Armstrong, 2016.\n\n  * Future progress in artificial intelligence: a survey of expert opinion, Vincent M\u00fcller and Nick Bostrom, 2016.\n\n  * Modeling agents with probabilistic programs, Owain Evans et al., 2017.\n\n  * When will AI exceed human performance? Evidence from AI experts, Katja Grace et al., 2018.\n\n  * Reframing superintelligence: comprehensive AI services as general intelligence, Eric Drexler, 2019.\n\n  * Truthful AI: developing and governing AI that does not lie, Owain Evans et al., 2021.\n\n### AI Governance\n\n  * Racing to the precipice: a model of artificial intelligence development, Stuart Armstrong, Carl Shulman, and Nick Bostrom, 2013.\n\n  * Strategic implications of openness in AI development, Nick Bostrom, 2017.\n\n  * AI governance: a research agenda, Allan Dafoe, 2018.\n\n  * The malicious use of artificial intelligence: Forecasting, prevention, and mitigation, Miles Brundage et al., 2018.\n\n  * Beyond privacy trade-offs with structured transparency, Andrew Trask et al., 2020.\n\n  * The windfall clause: distributing the benefits of AI for the common good, Cullen O\u2019Keefe et al., 2020.\n\n  * Institutionalizing ethics in AI through broader impact requirements, Carina Prunkl et al., 2021.\n\n  * International Control of Powerful Technology: Lessons from the Baruch Plan for Nuclear Weapons, Waqar Zaidi and Allan Dafoe, 2021.\n\n  * Lessons from the development of the atomic bomb, Toby Ord. 2022.\n\n### Digital Minds\n\n  * Quantity of experience: brain-duplication and degrees of consciousness, Nick Bostrom, 2006.\n\n  * Propositions concerning digital minds and society, Nick Bostrom and Carl Shulman, 2022\n\n  * Consciousness in artificial intelligence: insights from the science of consciousness, Patrick Butlin et al., 2023.\n\n### Biological Risk\n\n  * Information hazards in biotechnology, Gregory Lewis et al., 2019.\n\n  * The biosecurity benefits of genetic engineering attribution, Gregory Lewis et al., 2020.\n\n  * High-risk human-caused pathogen exposure events from 1975-2016, David Manheim and Gregory Lewis, 2021.\n\n  * Inferring the effectiveness of government interventions against COVID-19, Jan Brauner et al. 2021.\n\n### Human Enhancement\n\n  * The fable of the dragon tyrant, Nick Bostrom, 2005.\n\n  * The reversal test, Nick Bostrom and Toby Ord, 2006.\n\n  * The wisdom of nature: an evolutionary heuristic for human enhancement, Nick Bostrom and Anders Sandberg, 2009.\n\n  * The superintelligent will: motivation and instrumental rationality in advanced artificial agents, Nick Bostrom, 2012.\n\n  * Embryo selection for cognitive enhancement: curiosity or game-changer? Carl Shulman and Nick Bostrom, 2014.\n\n### Moral Uncertainty\n\n  * Statistical normalization methods in interpersonal and intertheoretic comparisons. William MacAskill, Owen Cotton-Barratt, and Toby Ord, 2020.\n\n  * Why maximize expected choice-worthiness? William MacAskill and Toby Ord, 2020.\n\n### Effective Altruism\n\n  * The moral imperative towards cost-effectiveness in global health, Toby Ord, 2013.\n\n  * Global poverty and the demands of morality, Toby Ord, 2014.\n\n  * Moral trade, Toby Ord, 2015.\n\n### Grand Futures\n\n  * Eternity in six hours: Intergalactic spreading of intelligent life and sharpening the Fermi paradox, Stuart Armstrong and Anders Sandberg, 2013.\n\n  * That is not dead which can eternal lie: the aestivation hypothesis for resolving Fermi's paradox. Anders Sandberg, Stuart Armstrong, and Milan \u0106irkovi\u0107, 2016.\n\n  * Dissolving the Fermi paradox, Anders Sandberg, Eric Drexler, and Toby Ord, 2018.\n\n  * The edges of our universe. Toby Ord, 2021.\n\n  * The timing of evolutionary transitions suggests intelligent life is rare, Andrew Snyder-Beattie et al., 2021.\n\n### Longtermism\n\n  * Shaping humanity\u2019s longterm trajectory. Toby Ord, 2023.\n\n  * The Lindy effect, Toby Ord, 2023.\n\n### Ethical Theory\n\n  * Beyond action: applying consequentialism to decision making and motivation, Toby Ord, 2009.\n\n  * Pascal\u2019s mugging, Nick Bostrom, 2009.\n\n  * Infinite ethics, Nick Bostrom, 2011.\n\n### Other Topics\n\n  * Are you living In a computer simulation?, Nick Bostrom, 2003 (pre-FHI).\n\n  * Whole brain emulation: a roadmap, Anders Sandberg and Nick Bostrom, 2008.\n\n  * Crucial considerations and wise philanthropy, Nick Bostrom, 2014.\n\n", "frontpage": true}
