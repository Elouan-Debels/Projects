{"aid": "39991661", "title": "Dr ChatGPT: How different prompts impact health answer correctness", "url": "https://aclanthology.org/2023.emnlp-main.928/", "domain": "aclanthology.org", "votes": 1, "user": "belter", "posted_at": "2024-04-10 15:15:13", "comments": 0, "source_title": "Dr ChatGPT tell me what I want to hear: How different prompts impact health answer correctness", "source_text": "Dr ChatGPT tell me what I want to hear: How different prompts impact health\nanswer correctness - ACL Anthology\n\n## Dr ChatGPT tell me what I want to hear: How different prompts impact health\nanswer correctness\n\nBevan Koopman, Guido Zuccon\n\n##### Abstract\n\nThis paper investigates the significant impact different prompts have on the\nbehaviour of ChatGPT when used for health information seeking. As people more\nand more depend on generative large language models (LLMs) like ChatGPT, it is\ncritical to understand model behaviour under different conditions, especially\nfor domains where incorrect answers can have serious consequences such as\nhealth. Using the TREC Misinformation dataset, we empirically evaluate ChatGPT\nto show not just its effectiveness but reveal that knowledge passed in the\nprompt can bias the model to the detriment of answer correctness. We show this\noccurs both for retrieve-then-generate pipelines and based on how a user\nphrases their question as well as the question type. This work has important\nimplications for the development of more robust and transparent question-\nanswering systems based on generative large language models. Prompts, raw\nresult files and manual analysis are made publicly available at\nhttps://github.com/ielab/drchatgpt-health_prompting.\n\nAnthology ID:\n\n    2023.emnlp-main.928\nVolume:\n\n    Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing\nMonth:\n\n    December\nYear:\n\n    2023\nAddress:\n\n    Singapore\nEditors:\n\n    Houda Bouamor, Juan Pino, Kalika Bali\nVenue:\n\n    EMNLP\nSIG:\n\nPublisher:\n\n    Association for Computational Linguistics\nNote:\n\nPages:\n\n    15012\u201315022\nLanguage:\n\nURL:\n\n    https://aclanthology.org/2023.emnlp-main.928\nDOI:\n\n    10.18653/v1/2023.emnlp-main.928\nBibkey:\n\nCite (ACL):\n\n    Bevan Koopman and Guido Zuccon. 2023. Dr ChatGPT tell me what I want to hear: How different prompts impact health answer correctness. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 15012\u201315022, Singapore. Association for Computational Linguistics.\nCite (Informal):\n\n    Dr ChatGPT tell me what I want to hear: How different prompts impact health answer correctness (Koopman & Zuccon, EMNLP 2023)\nCopy Citation:\n\nPDF:\n\n    https://aclanthology.org/2023.emnlp-main.928.pdf\n\nPDF Cite Search\n\n##### Export citation\n\n  * BibTeX\n  * MODS XML\n  * Endnote\n  * Preformatted\n\n    \n    \n    @inproceedings{koopman-zuccon-2023-dr, title = \"Dr {C}hat{GPT} tell me what {I} want to hear: How different prompts impact health answer correctness\", author = \"Koopman, Bevan and Zuccon, Guido\", editor = \"Bouamor, Houda and Pino, Juan and Bali, Kalika\", booktitle = \"Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing\", month = dec, year = \"2023\", address = \"Singapore\", publisher = \"Association for Computational Linguistics\", url = \"https://aclanthology.org/2023.emnlp-main.928\", doi = \"10.18653/v1/2023.emnlp-main.928\", pages = \"15012--15022\", abstract = \"This paper investigates the significant impact different prompts have on the behaviour of ChatGPT when used for health information seeking. As people more and more depend on generative large language models (LLMs) like ChatGPT, it is critical to understand model behaviour under different conditions, especially for domains where incorrect answers can have serious consequences such as health. Using the TREC Misinformation dataset, we empirically evaluate ChatGPT to show not just its effectiveness but reveal that knowledge passed in the prompt can bias the model to the detriment of answer correctness. We show this occurs both for retrieve-then-generate pipelines and based on how a user phrases their question as well as the question type. This work has important implications for the development of more robust and transparent question-answering systems based on generative large language models. Prompts, raw result files and manual analysis are made publicly available at \\url{https://github.com/ielab/drchatgpt-health_prompting}.\", }\n\nDownload as File\n\n    \n    \n    <?xml version=\"1.0\" encoding=\"UTF-8\"?> <modsCollection xmlns=\"http://www.loc.gov/mods/v3\"> <mods ID=\"koopman-zuccon-2023-dr\"> <titleInfo> <title>Dr ChatGPT tell me what I want to hear: How different prompts impact health answer correctness</title> </titleInfo> <name type=\"personal\"> <namePart type=\"given\">Bevan</namePart> <namePart type=\"family\">Koopman</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">author</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Guido</namePart> <namePart type=\"family\">Zuccon</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">author</roleTerm> </role> </name> <originInfo> <dateIssued>2023-12</dateIssued> </originInfo> <typeOfResource>text</typeOfResource> <relatedItem type=\"host\"> <titleInfo> <title>Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</title> </titleInfo> <name type=\"personal\"> <namePart type=\"given\">Houda</namePart> <namePart type=\"family\">Bouamor</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">editor</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Juan</namePart> <namePart type=\"family\">Pino</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">editor</roleTerm> </role> </name> <name type=\"personal\"> <namePart type=\"given\">Kalika</namePart> <namePart type=\"family\">Bali</namePart> <role> <roleTerm authority=\"marcrelator\" type=\"text\">editor</roleTerm> </role> </name> <originInfo> <publisher>Association for Computational Linguistics</publisher> <place> <placeTerm type=\"text\">Singapore</placeTerm> </place> </originInfo> <genre authority=\"marcgt\">conference publication</genre> </relatedItem> <abstract>This paper investigates the significant impact different prompts have on the behaviour of ChatGPT when used for health information seeking. As people more and more depend on generative large language models (LLMs) like ChatGPT, it is critical to understand model behaviour under different conditions, especially for domains where incorrect answers can have serious consequences such as health. Using the TREC Misinformation dataset, we empirically evaluate ChatGPT to show not just its effectiveness but reveal that knowledge passed in the prompt can bias the model to the detriment of answer correctness. We show this occurs both for retrieve-then-generate pipelines and based on how a user phrases their question as well as the question type. This work has important implications for the development of more robust and transparent question-answering systems based on generative large language models. Prompts, raw result files and manual analysis are made publicly available at https://github.com/ielab/drchatgpt-health_prompting.</abstract> <identifier type=\"citekey\">koopman-zuccon-2023-dr</identifier> <identifier type=\"doi\">10.18653/v1/2023.emnlp-main.928</identifier> <location> <url>https://aclanthology.org/2023.emnlp-main.928</url> </location> <part> <date>2023-12</date> <extent unit=\"page\"> <start>15012</start> <end>15022</end> </extent> </part> </mods> </modsCollection>\n\nDownload as File\n\n    \n    \n    %0 Conference Proceedings %T Dr ChatGPT tell me what I want to hear: How different prompts impact health answer correctness %A Koopman, Bevan %A Zuccon, Guido %Y Bouamor, Houda %Y Pino, Juan %Y Bali, Kalika %S Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing %D 2023 %8 December %I Association for Computational Linguistics %C Singapore %F koopman-zuccon-2023-dr %X This paper investigates the significant impact different prompts have on the behaviour of ChatGPT when used for health information seeking. As people more and more depend on generative large language models (LLMs) like ChatGPT, it is critical to understand model behaviour under different conditions, especially for domains where incorrect answers can have serious consequences such as health. Using the TREC Misinformation dataset, we empirically evaluate ChatGPT to show not just its effectiveness but reveal that knowledge passed in the prompt can bias the model to the detriment of answer correctness. We show this occurs both for retrieve-then-generate pipelines and based on how a user phrases their question as well as the question type. This work has important implications for the development of more robust and transparent question-answering systems based on generative large language models. Prompts, raw result files and manual analysis are made publicly available at https://github.com/ielab/drchatgpt-health_prompting. %R 10.18653/v1/2023.emnlp-main.928 %U https://aclanthology.org/2023.emnlp-main.928 %U https://doi.org/10.18653/v1/2023.emnlp-main.928 %P 15012-15022\n\nDownload as File\n\n##### Markdown (Informal)\n\n[Dr ChatGPT tell me what I want to hear: How different prompts impact health\nanswer correctness](https://aclanthology.org/2023.emnlp-main.928) (Koopman &\nZuccon, EMNLP 2023)\n\n  * Dr ChatGPT tell me what I want to hear: How different prompts impact health answer correctness (Koopman & Zuccon, EMNLP 2023)\n\n##### ACL\n\n  * Bevan Koopman and Guido Zuccon. 2023. Dr ChatGPT tell me what I want to hear: How different prompts impact health answer correctness. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 15012\u201315022, Singapore. Association for Computational Linguistics.\n\nACL materials are Copyright \u00a9 1963\u20132024 ACL; other materials are copyrighted\nby their respective copyright holders. Materials prior to 2016 here are\nlicensed under the Creative Commons Attribution-NonCommercial-ShareAlike 3.0\nInternational License. Permission is granted to make copies for the purposes\nof teaching and research. Materials published in or after 2016 are licensed on\na Creative Commons Attribution 4.0 International License.\n\nThe ACL Anthology is managed and built by the ACL Anthology team of\nvolunteers.\n\nSite last built on 10 April 2024 at 02:27 UTC with commit 4be344a.\n\n", "frontpage": false}
