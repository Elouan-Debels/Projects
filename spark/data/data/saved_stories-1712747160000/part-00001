{"aid": "39987134", "title": "An incomplete list of software development metrics", "url": "https://dan.turnerhallow.co.uk/an-incomplete-list-of-software-development-metrics.html", "domain": "turnerhallow.co.uk", "votes": 1, "user": "kqr", "posted_at": "2024-04-10 05:04:25", "comments": 0, "source_title": "An incomplete list of software development metrics", "source_text": "Forkcasting -- An incomplete list of software development metrics\n\nForkcasting\n\nAs clear as a puddle of mud\n\n# An incomplete list of software development metrics\n\n2024-Apr-07\n\nIt's a list!\n\nThere have been a couple of posts on Statistical Process Control (SPC)\nrecently. The most common objections are that you can't measure software\ndevelopment, that software development is too intrinsically variable for these\ntechniques to be worthwhile, or the measures have no effect on outcomes that\nmatter (e.g., Free cash flow (FCF), return on invested capital (RoIC), etc.).\n\nHere's an incomplete list of measurements that might apply to software systems\nand their associated development processes:\n\n  * Team size\n  * Team aggregate years of experience\n  * Average years of experience\n  * Count services owned by the team\n  * Time since last deployment to a service\n  * Time since last commit to a service\n  * Count of tickets in the backlog\n  * Count incomplete estimated tickets\n  * Count of unestimated tickets\n  * Age of oldest ticket in backlog\n  * Age of oldest ticket in $Stage (in-progress, review, to deploy)\n  * Count of tickets in $Stage\n  * Test non-commentary source statements (NCSS) total\n  * Test NCSS added per PR\n  * Non-test NCSS total\n  * Non-test NCSS added per PR\n  * Test cyclomatic complexity\n  * Non-test cyclomatic complexity\n  * PR age\n  * Time between PR ready for review and PR merge\n  * PR comment count\n  * PR count\n  * Time between PR merge and deploy\n  * Build duration\n  * Deployment duration\n  * Count of commits deployed per deployment\n  * PRs per story\n  * Story cycle time\n  * Story cycle time broken down by story point estimate\n  * Story lead time\n  * Story ticket word count\n  * Story comment count\n  * Stories complete per week\n  * Story points complete per week (if tracking story points)\n  * Scrum meeting dev-time (grooming, planning, review, daily stand-up, etc.)\n  * non-Scrum meetings dev-time\n  * Non-Scrum meeting count\n  * Tickets completed without estimate (if estimating)\n  * Tickets completed with estimate (if estimating)\n  * Dev-time spent per ticket estimate (if estimating)\n  * Within-team estimate noise (if using unambiguous estimates and getting multiple estimates for each ticket)\n  * Estimate mean squared error (if using unambiguous estimates, e.g. ticket duration, calendar date.)\n  * Estimate/Actual ratio (if using unambiguous estimates)\n  * Tickets in progress per developer (not done or in backlog)\n  * Count of tickets \"resurrected\" from \"done\"\n  * Count of tests failed\n  * Time to repair failed test\n  * Test duration\n  * Deployment count\n  * Deployment automatically rolled-back count\n  * Deployment manually rolled-back count\n  * Statement coverage\n  * Branch coverage\n  * t-way coverage\n  * Mutation coverage\n  * Tests per API method\n  * Count of ticket $Type (bug, task, story, ...) in each $Stage (incl. backlog)\n  * Tickets priority counts (if tracking priority)\n  * Paging alert count\n  * Non-paging alert count\n  * Paging alert response time\n  * Non-paging alert response time\n  * Time to resolve a page alert\n  * Time to resolve a non-paging alert\n\nThere's also the usual system monitoring suspects: cloud provider costs, error\ncount, request counts, latency percentiles, disk IOPS, count inodes used, days\nto TLS expiry, dependency request count, dependency error count, dependency\nlatency, retry count, etc.\n\nThere's also the DORA metrics, which I think I've somewhat duplicated above.\n\nI think that this list conclusively disproves the claim that we can't measure\nsoftware development.\n\nI'd also argue that this weakens the claim that we can't measure things that\naffect the outputs we care about. There's a lot of possible metrics here. It\nwould be surprising if none affect FCF, RoIC, etc.\n\nI don't think this list helps much with the claim that software development is\ntoo intrinsically variable for SPC to apply. I think you need to try it out in\nyour context to know if that's true for you. If it is true, the next question\nis: are you sure it's intrinsically high-variance? What did you try to change\nthe process' variance?\n\nSome of the metrics are hard to measure, and some may need operational\ndefinitions. For example, what exactly is an \"API method\", does it matter if\nit is publicly accessible?\n\nThere's one final objection that this doesn't cover: The quality of the\nsoftware process is irrelevant compared to the product and positioning. I\ndon't believe this; I think they're different skills. A team with fast,\nreliable execution and a good sense for the market will probably to outperform\na team who only has a good sense for the market. There's also the lifetime of\nthe software after you've launched it (bug fixes, improvements, etc.) to\nconsider. A team that can deliver faster and more reliably probably delivers\nbetter RoIC.\n\nThank you to both kqr for the original inspiration, and Cedric Chin for\nhelping me reflect on how I've seen others measure software development.\n\nDan Turner Licensed under CC BY-NC-SA 4.0\n\nRSS\n\n", "frontpage": false}
