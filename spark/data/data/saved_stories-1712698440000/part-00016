{"aid": "39980439", "title": "Claude vs. GPT in agentic systems: A function calling comparison", "url": "https://www.kadoa.com/blog/claude-vs-gpt-in-agentic-systems", "domain": "kadoa.com", "votes": 1, "user": "hubraumhugo", "posted_at": "2024-04-09 15:23:29", "comments": 0, "source_title": "Claude vs GPT in Agentic Systems: A Function Calling Comparison \u00b7 Kadoa \u00b7 AI Web Scraper", "source_text": "Claude vs GPT in Agentic Systems: A Function Calling Comparison \u00b7 Kadoa \u00b7 AI\nWeb Scraper\n\nUse CasesDemosPricingGet started\n\nGet started\n\nKadoa/Blog/Claude vs GPT in Agentic Systems: A Function Calling Comparison\n\n# Claude vs GPT in Agentic Systems: A Function Calling Comparison\n\nAdrian Krebs | 6 Apr 2024\n\nAnthropic's recent announcement of tool use (function calls) for Claude caught\nmy attention, particularly their claim that \"all models can handle correcting\nchoosing a tool from 250+ tools provided the user query contains all necessary\nparameters for the intended tool with >90% accuracy.\"\n\nAs someone involved in developing agentic systems, I was intrigued. I've been\nworking with GPT function calling for a while and know that its recall for\nlarger and more complex functions is quite low. So, I decided to compare GPT\nand Claude's performance in using different tools for tasks like web scraping\nand browser automation.\n\nTo do your own comparison and run your own scenarios, check out the GitHub\nrepository with all the code.\n\n## Evaluation Methodology\n\nTo ensure a fair and comprehensive comparison, I created a test scenario that\ncovers a wide range of simulated web scraping and browser automation tasks and\ntools.\n\nI defined the expected tools/functions to be used and the desired output for\nthe test case:\n\n    \n    \n    { \"query\": \"You are an RPA bot. If you're missing a CSS selector, you need to call the find_selector tool by providing the description. To find a specific webpage by description, call the find_page tool. Here is your task: Log in to https://example.com using the provided credentials. Navigate to the 'Products' page and extract the names and prices of all products that are currently in stock. For each product, check if there is a detailed specification PDF available by hovering over the 'Info' button and extracting the link. If a PDF is available, download it and extract the table of technical specifications. Finally, upload the parsed technical specifications to the file server.\", \"expectedTools\": [ \"handle_login\", \"navigate_to_url\", \"extract_text\", \"hover_element\", \"extract_attribute\", \"download_and_parse_pdf\", \"extract_specs_table\", \"upload_to_file_server\" ], \"expectedLastStep\": \"upload_to_file_server\", \"parameters\": { \"login_url\": \"https://example.com/login\", \"submit_selector\": \"#login-button\", \"username\": \"testuser\", \"password\": \"testpassword\" } }\n\nThese tasks include:\n\n  * Logging in to websites\n  * Navigating to specific pages\n  * Extracting data from HTML elements\n  * Interacting with UI components (buttons, dropdowns, etc.)\n  * Handling pagination and infinite scroll\n  * Downloading and parsing files\n\nThen I defined 25 available tools that the agent can use to solve these\ndifferent tasks in the test scenario. The tool definitions look like this:\n\n    \n    \n    { \"name\": \"click_element\", \"description\": \"Click on an element on the current web page.\", \"input_schema\": { \"type\": \"object\", \"properties\": { \"selector\": { \"type\": \"string\", \"description\": \"The CSS selector for the element to click, e.g., #submit-button\" } }, \"required\": [\"selector\"] }, \"function\": async function (args) { console.log(`Clicking element ${args}`); return {success: true} } }\n\nI then ran the test case against both Claude and GPT, tracking the tools they\nactually used and comparing their output to the expected results.\n\n## Results and Analysis\n\nAfter running the test case, I analyzed the performance of both models.\n\nFor a detailed breakdown of the results, please refer to the result table in\nthe README.\n\nTool Selection Accuracy\n\n  * Claude Opus consistently selected the correct tools based on the user query, achieving an impressive 100% accuracy.\n  * Claude Sonnet also performed well, with an accuracy of 87.5%.\n  * GPT-4 sometimes struggled with complex tool selection, resulting in an accuracy of 81.25%.\n  * GPT-3.5 had the lowest accuracy at 79.17%, often selecting inappropriate tools for the given tasks.\n\nHandling Complex Tools\n\n  * Claude Opus and Claude Sonnet successfully parsed and utilized complex tools with nested parameters. Its high performance comes at a premium price.\n  * GPT-4 had some difficulty dealing with complex tool parameters, occasionally failing to properly interpret and use nested objects.\n  * GPT-3.5 struggled the most with complex tools, often misinterpreting or misusing nested parameters.\n\nCost Comparison\n\n  * Claude Opus used the most tokens per completed test case, with an average cost of $0.807255.\n  * Claude Sonnet offers a good balance between performance and cost with an average cost of $0.119638 per test case.\n  * GPT-4, while not as accurate as the Claude models, had an average cost of $0.153540 per test case.\n  * GPT-3.5, the by far cheapest model, had the lowest average cost at $0.008145 per test case.\n\nRobustness and Production-Readiness\n\nDuring the evaluation, Claude calls occasionally resulted in API errors, which\nmakes it not as robust and production-ready as the GPT models. This is\nunderstandable as Claude's tool usage feature is still in public beta. In most\ncases, an automatic retry solved the issue.\n\n## Conclusion\n\nThis evaluation shows Claude's superiority over GPT in tool use for agentic\nsystems. Claude's high accuracy in selecting and utilizing tools was\nimpressive and is a big step forward towards reliable and production-ready\nagent systems.\n\n  * AI agents still work best for simple, repetitive tasks, such as RPA or web scraping\n  * To create a successful agent, you need to provide it with good tools. The LLM can then figure out the correct workflow, which feel like a promising direction.\n  * Chain of thought tool use is still quite slow and often very expensive. I've spend around $50 just on experimenting with Claude for one day. Imagine what the testing would cost for a production-scale system. Making the unit economics work is difficult but will improve as LLM costs continue to drop.\n\nAs both Claude and GPT continue to evolve, it will be fascinating to observe\nhow their agent capabilities advance and compare in the future.\n\nHelpAPI DocumentationContact UsBlogAbout UsCareersTermsPrivacy\n\n", "frontpage": false}
