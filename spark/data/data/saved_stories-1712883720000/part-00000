{"aid": "40005579", "title": "Interview with Google Cloud CEO Thomas Kurian on Google's Enterprise AI Strategy", "url": "https://stratechery.com/2024/an-interview-with-google-cloud-ceo-thomas-kurian-about-googles-enterprise-ai-strategy/", "domain": "stratechery.com", "votes": 17, "user": "kjhughes", "posted_at": "2024-04-11 18:59:48", "comments": 2, "source_title": "An Interview with Google Cloud CEO Thomas Kurian About Google\u2019s Enterprise AI Strategy", "source_text": "An Interview with Google Cloud CEO Thomas Kurian About Google\u2019s Enterprise AI\nStrategy \u2013 Stratechery by Ben Thompson\n\nSkip to content\n\n# Stratechery by Ben Thompson\n\nOn the business, strategy, and impact of technology.\n\n# By Ben Thompson\n\nAbout Ben Follow via Email/RSS Twitter\n\n# Stratechery Plus\n\nAbout Stratechery Plus Subscribe Member Forum\n\n# Account\n\n  * Member\n\n  * Delivery Preferences\n  * Manage Account\n  * Sign Out\n\n  * Log In\n  * Sign Up\n\nLoading\n\n# Explore Stratechery\n\n  * Concepts\n  * Companies\n  * Topics\n\n# Archives\n\n  * Articles\n  * Updates\n  * Interviews\n  * Years in Review\n\n# An Interview with Google Cloud CEO Thomas Kurian About Google\u2019s Enterprise\nAI Strategy\n\nPosted onThursday, April 11, 2024Thursday, April 11, 2024 Author by Ben\nThompson\n\nGood morning,\n\nThis week\u2019s Stratechery Interview is with Google Cloud CEO Thomas Kurian.\nKurian joined Google to lead the company\u2019s cloud division in 2018; prior to\nthat he was President of Product Development at Oracle, where he worked for 22\nyears.\n\nIn this interview (which was conducted last night after I had already\npublished Gemini 1.5 and Google\u2019s Nature), we discuss Kurian\u2019s keynote at this\nweek\u2019s Google Cloud Next conference, which was almost completely focused on\nAI. To that end, we cover Google Cloud\u2019s strategy, why AI offers a reset in\ncompetition, and how the company can win in the enterprise space broadly. As\nyesterday\u2019s Article \u2014 and this Interview \u2014 make clear, I do think that Google\nis very well placed in this area; one certainly gets the impression that\nKurian thinks so as well.\n\nAs a reminder, all Stratechery content, including interviews, is available as\na podcast; click the link at the top of this email to add Stratechery to your\npodcast player.\n\nOn to the Interview:\n\n### An Interview with Google Cloud CEO Thomas Kurian About Google\u2019s Enterprise\nAI Strategy\n\nThis interview is lightly edited for clarity.\n\nTopics: Google\u2019s Cloud Strategy | An \u201cOpen\u201d Platform | Data Gravity and GTM | Infrastructure and One Million Tokens | Challenges and Opportunities\n\n### Google\u2019s Cloud Strategy\n\nThomas Kurian, welcome back to Stratechery.\n\nThomas Kurian: Thank you for having me.\n\nSo I\u2019m going to give you the floor to start. If someone comes to you and says,\n\u201cHey, Thomas. What was Google Cloud Next about this year? You did it a few\nmonths early, what is the takeaway you wanted folks to have from the keynote?\u201d\n\nTK: The focus of our product strategy is what we\u2019re explaining to clients at\nthe keynote, and the product strategy is fairly simple. We are seeing\nsignificant acceleration in how customers want to use digital tools and AI to\ntransform their core business. To enable that, we offer two important things.\nFirst, as people have moved AI out of proof-of-concepts into production\ndeployments across many different parts of their enterprise, they want not to\nchoose a model, but they want a platform, and the platform needs certain\ncharacteristics. The platform needs to provide a set of services to tune\nmodels, to connect them with their enterprise systems, to be able to delegate\ntasks to the model, to measure the quality of the model, to test it, to deploy\nit, monitor it. Our platform provides that.\n\nIn addition, we have an open architecture, which gives them the ability to use\nthese services, but across a range of different models. Some from Google,\nobviously Gemini, but also from partners.\n\nWe announced in addition that Gemini has made some major advances. We\nintroduced the million context window, we showed a number of places where it\u2019s\nbeing used by customers, we\u2019ve done a significant update to Imagen, which is\nour image model, and we\u2019ve integrated it across our portfolio. When I say\nacross our portfolio to assist people in doing things in Workspace, for\nexample, writing, email, creating documents, but just as much as assisting in\ntheir existing workflow, we also showed through the introduction of Google\nVids, a brand new experience that you can create using generative AI, which is\nstorytelling.\n\nSimilarly, we brought it across our platform in Google Cloud. We started with\ncoding, but we added operations for people who want to operate a cloud. We\nadded the capability for analysis of data and in each step as well as cyber.\nAnd in each step, it is meant to open up how people can talk to their data,\nopen up how quickly teams can do cybersecurity analysis by having a real\nexpert work alongside them.\n\nSo one big piece was you need as an enterprise to choose a platform if you\u2019re\ngoing to use AI at scale, and that platform needs to be open, but it also\nneeds to be vertically optimized because if you\u2019re going to use it across your\nenvironments and across your systems and processes, you need it to be cost\nefficient and scale and we know how to do that at Google.\n\nSo we introduced a number of new types of advances in our AI and other\nsystems. We introduced a new Axion processor, we introduced a number of new\nadvances with Intel in traditional classical computing, but we also introduced\na variety of new things in our AI systems. TPU v5, some new Nvidia systems,\nand then most importantly, most people think it\u2019s just about the chip, it\u2019s\nnot about the chip, it\u2019s the system you build with the chip, and there are\nmany things that we do that are super advantageous. One example being the new\nParallelstore that we introduced to provide a 10x improvement in the way that\ndata loads off disk into a model when it\u2019s being trained, which is super\nimportant if you\u2019re going to large-scale training or inferencing.\n\nSo these are some of the advances. We\u2019ve also made the ability to now deploy\nthese on extraordinarily large clusters at scale and because we provide an\nassortment of different kinds of accelerators, which are optimized for many\ndifferent kinds of training and serving needs, we\u2019re seeing a lot of pickup\nand it makes it much more efficient for people from a cost-performance-latency\npoint-of-view.\n\nWell, that\u2019s a good overview. I think you previewed a lot of the stuff that I\nwant to get into, but you did mention that, \u201cPeople are moving out of proof-\nof-concept into actually doing products\u201d. Is that actually happening? What are\nthe actual use cases that companies are actually rolling out broadly as\nopposed to doing experiments on what might be possible?\n\nTK: Broad-brush, Ben, we can break it into four major categories. One category\nis streamlining internal processes within the organization, streamlining\ninternal processes. In finance, you want to automate accounts receivable,\ncollections, and cashflow prediction. In human resources, you want to automate\nyour human help desk as well as improve the efficiency with which you can do\nbenefits matching, for example. In procurement and supply chain, you want for\nexample, look at all my suppliers, their contracts with me and tell me which\nones have indemnification and warranty protection, so I can drive more volume\nto those that give me indemnification and warranties and less to those that\ndon\u2019t. For example, these are all practical cases we have customers live in\ndeployment with.\n\nSecond is transforming the customer experience. Transforming the customer\nexperiences, how you market, how you merchandise, how you do commerce, how you\ndo sales and service. An example is what Mercedes-Benz CEO Ola K\u00e4llenius\ntalked about how they\u2019re building a completely new experience for the way that\nthey market and sell and service their vehicles.\n\nThird is that some people are integrating it into their products, and when I\nsay re-imagining their products, re-imagining their core products using AI. We\nhad two examples of companies who are in the devices space. One is Samsung and\nthe other one is Oppo, and they\u2019re re-imagining the actual device itself using\nAI with all the multimodality that we provide.\n\nThere are quite a few companies now re-thinking that if a model can change the\nway that I see it, that I can process multimodal information. For example, in\nmedia we have people saying, \u201cIf your model can read as much information as it\ncan, can it take a long movie and shrink it into highlights? Can I take a\nsports recording of the NCAA basketball final and say, \u2018find me all the\nhighlights by this particular player\u2019?\u201d and not have to have a human being sit\nthere and splice the video, but have it do it and I can create the highlights\nreel really quickly. So there are lots of people re-imagining the product\nofferings that they have.\n\nAnd finally, there are some people saying, \u201cWith the cost efficiency of this,\nI can change how I enter a brand new market because, for example, I can do\npersonalized offers in a market where I may not have a physical presence, but\nI can do much higher conversion rate for customers with online marketing and\nadvertising because now I can do highly tailored campaigns because the cost of\ncreating the content is much lower.\u201d So broad-brush, streamline the core\nprocesses and back office, transform the customer experience and it doesn\u2019t\nmean call centers or chatbots, it can be actually transferring the product\nitself, transforming the nature of the product you build and enter new\nmarkets.\n\nIs it fair to say then when you talk about, \u201cMoving from proof-of-concept to\nactual production\u201d, or maybe that\u2019s not the words you used, but people are\nsaying, \u201cOkay, we\u2019re going to build this\u201d because this stuff\u2019s not showing up\nyet, in the real world. Is it the case that, \u201cWe see that this could be\nvaluable, now we\u2019re in\u201d, and that\u2019s why you\u2019re emphasizing the platform choice\nnow because they\u2019ve committed to AI broadly, and now it\u2019s like, \u201cWhere are we\ngoing to build it\u201d?\n\nTK: We have people experimenting, but we also have people actually live\ndeployment and directing traffic. Orange, the telecom company, was talking\nabout how many customers they\u2019re handling online, Discover Financial was\ntalking about how their agents are actually using a search and AI tools to\ndiscover information from policy and procedure documents live. So there are\npeople actually literally running true traffic through these systems and\nactually using them to handle real customer workload.\n\nAre you seeing the case in a lot of in customers, or maybe you\u2019re hearing from\npotential customers, that AI is rolling out, if that\u2019s the right word, in an\nemployee arbitrage situation? Where there\u2019s individual employees that are\ntaking on themselves to use these tools and they are personally benefiting\nfrom the increased productivity \u2014 maybe they\u2019re doing less work or maybe\nthey\u2019re getting more done \u2014 and the companies want to capture that more\nsystematically. Is that a theme that you\u2019re seeing?\n\nTK: We\u2019re seeing three flavors. Flavor one is a company has, we\u2019re going to\ntry eight or nine, what they call customer journeys or use cases, we\u2019re going\nto pick the three that we see as the maximum return, meaning value and value\ndoes not mean cost savings always. It could be, for example, we have one who\nis handling 1 million calls a day through our customer service system. Now a\nmillion calls a day, if you think about it, Ben, an average person can do\nabout 250 calls a day, that\u2019s a certain volume in an eight-hour day. If you\nhandled a million, that is a lot of people, so the reality is that several of\nthem were not being answered and people never called because the wait time was\nso long. So in that case, it was not about cost savings, it\u2019s the fact that\nthey\u2019re getting able to reach many more customers than they could do before.\nSo that\u2019s one. One part is people saying, \u201cI have a bunch of scenarios, I\u2019m\ngoing to pick the three\u201d, and in many cases, they\u2019re actually augmenting\nsomething they\u2019re doing or doing something they couldn\u2019t do before, that\u2019s\nscenario one.\n\nScenario two was I have, for example, there\u2019s a large insurance company that\u2019s\nworking with us. Today, when they do claims and risk calculation, it takes a\nlong time to handle the claims and the risk, particularly the risk\ncalculation, because there\u2019s thousands of pages of documents, there\u2019s a lot of\nspreadsheets going back and forth. They put it into Gemini and it was able to\nrun the calculations much, much more quickly. So second is I\u2019m picking a very\nhigh value use case for my organization, which is the core function, and I\u2019m\ngoing to implement it because I can get a real competitive advantage. In their\ncase, it\u2019s the fact that they can both get more accurate scoring on the risk\nand they can also do a much more accurate job, faster job in responding.\n\nAnd the third scenario is what you said. \u201cHey, we\u2019ve got a bunch of people,\nwe\u2019re going to give it to a certain number of developers\u201d. For example, our\ncoding tool, \u201cThey are going to test it, they say it helps me generate much\nbetter unit tests, it helps me write better quality code\u201d. Wayfair\u2019s CTO was\ntalking about what their experience is, and then they say, \u201cLet\u2019s go broadly\u201d,\nso all three patterns are being seen.\n\n### An \u201cOpen\u201d Platform\n\nThis was a Google Cloud event broadly, and most of Google Cloud\u2019s business\ntoday is some combination of Workspace and actual cloud computing. However,\nthe entire presentation was about AI. I mean, maybe not the entire, you had a\ncouple Google Meet updates and things like that, but by and large, I think\nthat\u2019s a fair characterization. What does that say about Google\u2019s priorities?\nIf I\u2019m just a regular Google Cloud customer, I\u2019m like, \u201cYeah, AI is good and\ncool, but what about my managed databases?\u201d, or whatever it might be?\n\nTK: Just to be frank, Ben, we have 30,000 people here, and they have hundreds\nof sessions, so a certain set of sessions are on AI, but there\u2019s lots of\nsessions on other topics, I wouldn\u2019t over-index on what you saw in the\nkeynote.\n\nWe do see though that customers, they\u2019re looking increasingly at when they\npick a cloud partner, they\u2019re looking at it as not as a point in time, but\nthey\u2019re looking at it as, \u201cWho\u2019s going to help me transform my business?\u201d, and\nthe basis of how they thought about it in 2007, \u201908, \u201909, was all about, \u201cHow\ndo they help me either go faster by building apps or reduce my cost of data\ncenters by allowing me to lift and shift workloads?\u201d. Now, they\u2019re thinking\nabout it in a different way. They\u2019re looking at it as, \u201cCan I use AI to\ntransform my business? Who\u2019s got the best platform and tools to help me do\nthat?\u201d.\n\nOnce we get them to use the AI platform and tools, it does drag in many of our\nother services. You need good data to feed the model, and so many people use\nour analytics system, BigQuery, to clean up the data, to handle the\nsegmentation of it, et cetera. Other people say, \u201cI want to feed it from an\noperational databases\u201d, like AlloyDB, or Spanner, or one of our operational\ndatabases. We get a lot of people saying, \u201cI want to keep the output of the\nmodel safe, secure. I also want to see if people are analyzing, trying to\nfigure out a way to attack my systems\u201d, so we sell them our cyber tools. So AI\nis not just AI by itself, it typically is a collection of things that you need\nin order to do AI well.\n\nIs there an interpretation of this sort of idea, AI almost as a tip-of-the-\nspear aspect, along with the messaging you really drilled down at the end? You\nmentioned it already on this talk about being \u201copen\u201d. Open can mean lots of\nthings, but I interpreted it as, \u201cYou can access whatever you need from us at\nany level\u201d. Does that open a sort of extent to, \u201cLook, if you already have\ncloud computing elsewhere, if you have lots of data elsewhere, we\u2019re going to\nmake it super easy to plug into us because maybe you want our AI tools, and we\nhope you come over with everything else, but we\u2019re going to work regardless\u201d?\nIs that one of the ways to think about the open framing?\n\nTK: Open is three elements. First, definitely that. So today, if you\u2019re\nrunning your applications on another cloud, or if you\u2019re using a SaaS\napplication, like a Salesforce, a Workday, an SAP system, et cetera, and\nyou\u2019re wondering, \u201cCan I use Google\u2019s AI but connect it into my existing\napplication?\u201d \u2014 yes, you can, and you don\u2019t need to actually move anything\nover to us in order to do that. So that\u2019s one, and it allows us to use AI to\nserve every customer, not just existing GCP customers.\n\nThe second thing, when we say open, it means people can choose models for the\nright purpose in their organization. So for example, we have people who say,\n\u201cLook, I love Gemini, I\u2019m going to use it in my customer-facing function and\nmy back office function, but I am in this particular line of business, and I\nwant to derive my own model for that purpose because I\u2019m integrating it into\nmy own product experience, and I really need to control the model weights\u201d. In\nwhich case, they may either use our open source, or they can use Mistral or\nsome other open source.\n\nBut in the past, they\u2019ve always had to choose. If you chose a model, you had\nto choose a toolset, and that doesn\u2019t make any sense because all the common\nthings you need to do, do you need to tune it? Do you need to ground it? Do\nyou need to integrate it into your existing data store to do retrieval,\naugmented generation? Those are all things that our platform provides, so\nthat\u2019s the second piece that we do.\n\nThe third piece is we also connect the dots for them into other services and\nGCP if they want to and so it allows us to attract new customers, it allows us\nto be open, to allow them the choice of using a variety of different models to\nserve their own needs organization-wide, while having a common way to manage,\nmonitor, and improve the models, and, it also allows us to offer some other\nservices along with it.\n\nI\u2019m curious about overall, is this sort of a reboot of a growth strategy? In\nthe context of Google Cloud results, last year in Q3, growth slowed pretty\nsignificantly from 28 to 22%, margins contracted, which reversed a very long\ntrend. Even before we get to the AI connection to that or this platform, what\nhappened last year? What was going on there?\n\nTK: Clearly, it was not a structural thing, because if you look at our Q4 and\nour results, it bumped up sharply, and we\u2019re very confident about the future.\nWe are the only cloud provider, if you go back to 2019 to now, and if you look\nat growing top-line and operating income, no one, not even the one who has the\nlargest share, has grown top-line and operating income for as long as we have.\n\nIt\u2019s definitely not a reboot of the strategy. We started this thing called\nmulticloud, which is, \u201cDon\u2019t be locked into a single cloud provider, allow\npeople to use a choice of cloud provider, allow them to choose the best cloud\nprovider for the task\u201d. So analytics is an area where we did particularly\nwell, we did really well in certain areas, like certain kinds of legacy\nsystems, migrating them, our systems ran them really well because we can\nhandle scale-up in a different way than other people did, and so it allowed us\nto open a lot of doors.\n\nWhen AI came along, the first cycle was everybody thinking, \u201cI have to pick a\nmodel,\u201d and the model changes every three weeks and so our point was, \u201cYou\u2019re\nchasing the wrong thing when you think about picking a model, what you need to\ndo is to think about a platform\u201d, because you need to integrate it into your\nheterogeneity and think about the platform first and the model second, and\nmake sure the platform supports a collection of models, because you may choose\nthe latest one from anybody, and so that\u2019s the nature of it.\n\nWell, that\u2019s the bit though as to why I\u2019m getting to the question of a reboot,\nbecause I think this idea of you\u2019re going to handle, you can have your\nmulticloud, that makes sense given your competitive position in the market,\nbeing third place. Do you see AI, though, in all this talk about, \u201cYou need to\nchoose a platform? Sure, our platform\u2019s going to be open, you can use it\nanywhere\u201d \u2014 but do you see this as a wedge to be like, \u201cOkay, this is a reboot\nbroadly for the industry as far as cloud goes, and sure, your data may be in\nAWS, or in Azure, or whatever it might be, but if you have a platform going\nforward, you should start with us\u201d? Then maybe we\u2019ll look up in ten, fifteen\nyears, and all the center of gravity shifted to wherever the platforms are?\n\nTK: For sure. I mean, it\u2019s a change in the way that people make purchase\ndecisions, right? Ten years ago, you were worried about commodity computing,\nand you were like, \u201cWho\u2019s going to give me the lowest cost for compute, and\nthe lowest cost for storage, and the lowest cost for networking?\u201d. Now the\nbasis of competition has changed and we have a very strong position, given our\ncapability both at the top, meaning offering a platform, offering models, et\ncetera, and building products that have long integrated models.\n\nJust as an example, Ben, integrating a model into a product is not as easy as\npeople think; Gmail has been doing that since 2015. On any daily basis, there\nare over 500 million operations a day that we run and to do it well, when a\npartner talked about the fact that 75% of people who generate an image for\nslides actually end up presenting it, it\u2019s because we have paid a lot of\nattention over the years on how to integrate it.\n\nSo we play at the top of the stack, and we have the infrastructure and scale\nto do it really well from a cost, performance, and global scale that changes\nthe nature of the competition. So we definitely see this, as you said, as a\nreset moment for how customers thinking of choosing their cloud decision.\n\nIf you\u2019re talking about a lot of choices about models, and customers were\nover-indexed on choosing the correct model, that implies that models are maybe\na commodity, and that we\u2019ve seen with GPT-4 prices are down something like 90%\nsince release. Is that a trend you anticipate continuing, and is it something\nthat you want to push and actually accelerate?\n\nTK: Models \u2014 whether they\u2019re a commodity or not, time will tell, these are\nvery early innings. All we\u2019re pointing out is every month, there\u2019s a new model\nfrom a new player, and the existing models get better on many different\ndimensions. It\u2019s like trying to pick a phone based on a camera, and the\ncamera\u2019s changing every two weeks, right? Is that the basis on which you want\nto make your selection?\n\nWell, but if you make that basis, then you might be locked into the operating\nsystem.\n\nTK: That\u2019s right, and so that\u2019s why we say you should choose an open platform,\nand you should be able to use a collection of different models, because it\u2019s\nchanging, and don\u2019t lock into a particular operating system at a time when the\napplications on top of it are changing, to use your analogy.\n\nWhy is your platform open as compared to others? Microsoft has announced you\ncan use other models, not just OpenAI models. Amazon is sort of, to the extent\nyou can ascertain a strategy, it\u2019s like, \u201cLook, we\u2019re not committing to\nanything, you could do whatever you want.\u201d Why do you feel comfortable saying,\n\u201cNo, we\u2019re the open one,\u201d and they\u2019re not?\n\nTK: Well, first of all, the completeness of our platform; Vertex has a lot\nmore services than you can get with the other platforms. Secondly, in order to\nimprove a platform, you have to have your own model, because there\u2019s a bunch\nof things you do when you engineer services with that model.\n\nI\u2019ll give you a really basic example. You use a model, you decide to ground\nthe answers. Grounding improves quality, but can also introduce latency. How\ndo you make sure that when you\u2019re grounding, you\u2019re not serially post-\nprocessing a model\u2019s answer to add latency? Unless you have your own model,\nyou wouldn\u2019t even get to that. So because we have our own model, we\u2019re able to\nengineer these things, but we make them available as services with other\nmodels, so you can use enterprise grounding as a very specific example. There\nare lots of customers using it with Mistral and with Llama and with Anthropic.\n\nSecond thing, we are not just offering models, but we\u2019re actually helping the\nthird party go to customers with us. I met a lot of customers today jointly\nwith [CEO] Dario [Amodei] from Anthropic, and it\u2019s a commitment to make sure\nwe\u2019re not just giving you our infrastructure, we\u2019re not just training,\nintegrating a model into Vertex, we\u2019re not just making it a first-class model,\nbut we\u2019re actually bringing it to clients together.\n\nI think that\u2019s what we mean by open. One of the other players has no models of\ntheir own, so naturally they\u2019re offering a bunch of models, and the other\nplayer has outsourced their model development to a third party.\n\n### Data Gravity and GTM\n\nHow do you think about LLMs in the context of enterprise? You mentioned this\ngrounding bit. I think in the consumer space, it\u2019s like, \u201cWow, I can look up\nanswers\u201d, but from an enterprise perspective, I would imagine, I\u2019m actually\ncurious how often people bring up concerns about hallucinations. Maybe they\u2019re\neven overstated to an extent. In a lot of the demos, LLMs were more user\ninterfaces to existing data. Is that the framing that this is mostly\nmanifesting in? Or are there other use cases? I mean, there\u2019s the, \u201cWrite\nemail for me\u201d, but do you see it primarily as an interface to data, in the\nenterprise use case?\n\nTK: There\u2019s a bunch of people who are using it to do Q&A with their data, we\ncall it Open Book Q&A. Think of it as I have a bunch of data, I want to\nsummarize it and ask it questions, and it\u2019s not necessarily look in the raw\ndata, but summarize it and ask questions. There\u2019s certainly a collection of it\nbut there\u2019s also people who are automating a process, and automating a\nprocesses like get the output of a model, I feed into another model, and drive\na process with it. Do you see what I mean?\n\nYep.\n\nTK: They use function calling, like I give a model a question, it generates an\nanswer, I put it into a function, the function does a bunch of things, then it\ncalls another model, that next model takes it on.\n\nThink of it as when you\u2019re doing software engineering or coding, I\u2019ll use that\nas an example, one way you can use the model is, \u201cIntrospect my code base and\nsee if there\u2019s a security vulnerability\u201d, like if I have a VAS vulnerability\npattern in my code base, that\u2019s the equivalent of the question and answer on\nyour data. Second is, \u201cHey, if you find it, fix it, and here is the pattern I\nwant you to follow to fix it, and then run the scan to see if it still exists\nand then compile it and deploy it. When you deploy it, I want to run an A/B\ntest.\u201d It\u2019s that second thing we see a lot of people now doing, which is\nautomating a process, and there can be a number of different levels of\nsophistication of that process.\n\nHow important to achieving Q&A with your data or doing some of these processes\n\u2014 Is it essential that that data be on GCP or how does that work with external\ndata? Because I think the framing and the advantage, say, to your competitor\nwith none of their own models, is, well, they have all the data, and so data\nhas a lot of gravity, it will pull people to that because models are a\ncommodity, they can do XYZ.\n\nTK: We support data from four types of environments, an on-premise data\ncenter, an application like a software-as-a-service application. Your customer\nservice data could be in Salesforce, and Salesforce may not be running at all\non GCP. It could be on another cloud provider, or it could be in a vector\nstore.\n\nIs the goal that that doesn\u2019t matter? Or is it just a reality that it\u2019s going\nto work better the more that it\u2019s on Google?\n\nTK: You don\u2019t have to be on Google to use it, first. And secondly, the big\nthing we say though is, there are tools you need to do to make sure that\nthere\u2019s a lot of basic things we suggest to people to make sure they think\nabout it.\n\nOne example is how you handle access control permissions on your data because\nmodels don\u2019t have the ability to handle access control rules. You can put it\nin a function after you\u2019ve asked the model set of questions, but that\u2019s post-\nprocessing. There are techniques to add access control, something that\nengineers call ACLs \u2014 [Access Control Lists] on your data, so we suggest they\nthink about that and we give them design patterns.\n\nSecond, we ask them to measure the tests on the system to make sure that the\nsystem has predictable latency. Because a model can send a request, and if\nyour system has huge queue, sometimes it responds quickly, sometimes it takes\na long time, the model gets confused whether it\u2019s waiting for an answer or\nnot, that\u2019s the second thing.\n\nThe third thing that we typically ask them to work through is, there are tools\nto evaluate the quality of the data and when I say the quality of the data, to\nmake sure you\u2019re not in a situation where it\u2019s garbage in, garbage out. By\nquality, we\u2019re not talking just about structural quality, making sure there\nare no null sets and things like that, but also semantically, it has a correct\nmeaning, because if you ask it for revenue and you are looking at invoice\nrevenue but not GAAP revenue, you\u2019ll get an answer on invoice revenue and you\nneed to make sure it\u2019s correctly defined whether you mean GAAP revenue or\ninvoice revenue. Those are examples, and we provide blueprints to customers to\ndo it.\n\nWhere are the decisions made about this? Especially when you\u2019re talking about\nthe context of a platform and the way it interacts, is this at the CTO, CIO,\nCEO level where these choices are going to be made? Or is this something that\nengineering leads or program manager leads can say, let\u2019s use the Vertex AI\nplatform \u2014 where are you thinking about the go-to-market there?\n\nTK: Typically, the decisions are made jointly between a business unit leader\nand an IT leader. Sometimes the scenario is so important to the company, it\u2019s\nthe CEO driving it because it could be a real big change to the business model\nof the company. Often it\u2019s a head of a line of business and the CIO or the\nCTO.\n\nThis is kind of a nerdy question, but I\u2019m curious about how costs are\nallocated for all this sort of AI work. I assume that Google Cloud is paying\nfor its own share of servers, but there\u2019s a lot of shared R&D and model\ndevelopment that\u2019s shared with Google proper. Is Google Cloud just on the cost\nof serving side of things? I\u2019m curious \u2014 like I said, it\u2019s very nerdy \u2014 I\u2019m\njust curious how you interact with Google proper given there\u2019s so much shared\ndevelopment here.\n\nTK: First of all, our global infrastructure for Google is shared between Cloud\nand the rest of Google, you know that it\u2019s been true for a long time.\n\nYeah.\n\nTK: So from the point of the physical infrastructure, we share the\ninfrastructure with all the product units, including with the Google DeepMind\nteam that builds the models. On how the costs are allocated, I\u2019ll just tell\nyou, we are reported as a segment and we pay a fair share.\n\nThis kind of goes back to the commodity sort of question. Is Google Cloud and\nthis AI stuff, are you looking to win by being better? Or is there a bit where\nyou can really leverage that infrastructure advantage? Of course you should be\nsharing resources, that\u2019s the huge advantage that you have to not just win on\nfeatures but also win on cost and price and TCO and all those sorts of things.\n\nTK: Two things, if I may, Ben. First of all, we are winning because we\u2019re\nbetter. Every customer who has chosen us had the choice to choose anyone else,\nwe win because we have great products, including in AI. Second, one of the\nimportant elements of using AI at scale is cost, but there are other elements\nalong with cost. There\u2019s latency. There is, for example, simple things like,\n\u201cHow many times do I need to ask a model the question to get the correct\nanswer?\u201d, because every time you go to the model, you\u2019re passing in a set of\ntokens and there\u2019s a set of costs. It\u2019s also a customer experience issue. You\nsee what I mean?\n\nYep.\n\nTK: On all those things, there are many, many things we\u2019ve done to sort out\nperformance, scale, latency, et cetera. I mean, as a very simple example, to\nget the million tokens to work, there\u2019s changes you need in the way that you\ndesign the model. There\u2019s changes what you do with the serving stock, et\ncetera. Because we\u2019ve got expertise in both, we\u2019re able to do those things.\n\n### Infrastructure and One Million Tokens\n\nHow important is that million context window in the story you are telling? My\nperception is, there\u2019s a lot of stuff you could do if you build a lot of\ninfrastructure around it, whether it be RAG or other implementations, but it\nfeels like with Gemini 1.5 there are jack-of-all-trades possibilities that\nseem to open up to a much greater extent, and there\u2019s a bit where, you had\nthat compliance bit, the statements of work and they had to compare it to the\n100-page compliance document. I got some comments like, \u201cMaybe companies\nshouldn\u2019t have 100-page compliance notebooks or whatever it might be\u201d, but the\nreality is, that\u2019s the case, the world has that. My perception of the keynote\nis, that was the killer feature, that seemed to undergird everything. Was that\nthe correct perception?\n\nTK: Yeah, there are two reasons. Just to be perfectly clear, Ben, the long\ncontext window allows you to do three things that are important. First of all,\nwhen you look at high definition video, for example, and other modalities, and\njust imagine you\u2019re dumping a high definition video in and you want to create\nout of the NCAA final, which just happened, the highlight reel but you don\u2019t\nwant to specify every attribute about what you want spliced into the highlight\nreel. The model has to digest it and because it has to process it, it\u2019s a\nfairly dense representation of the video because there are objects, there are\npeople moving, there are actions, like I\u2019m throwing a pass. They could be, I\nhave my name on the back of my t-shirt, there could be a score like, \u201cWhen did\nthey change from 24 to 26 points? Did they score three pointers?\u201d, so there\nare many, many, many dimensions. So reasoning becomes a lot better when you\ncan take a lot more context, that\u2019s one, and it\u2019s particularly true of\nmodality.\n\nThe second is, today people don\u2019t use models to maintain state or memory,\nmeaning they ask it a question, the next time they think, \u201cHey, it may not\nremember\u201d, so when you\u2019re able to maintain a longer context, you can maintain\nmore state, and therefore you can do richer and richer things rather than just\ntalk back-and-forth with a very simplistic interface. You see what I mean?\n\nThe third thing is, there are certainly complex scenarios, it\u2019s the\nunfortunate reality, there\u2019s lots of policies and procedure books that are\neven longer than what we showed, and so there are scenarios like that that we\nhave to be able to deal with. But in the longer term, the real breakthrough is\nthe following. Context length, if you can decouple the capabilities of the\nmodel and the latency to serve a model from the context length, then you can\nfundamentally change how quickly you can scale a model.\n\nIs this ultimately, from your perspective, a question of infrastructure, and\nthat just leans into Google\u2019s biggest advantage?\n\nTK: It\u2019s a question of global infrastructure, but also optimizations at every\nlayer in the infrastructure, which we can co-engineer with DeepMind.\n\nRight. This gets at the integration point of your own model.\n\nTK: Yeah, and then the second thing is the expertise in Google DeepMind to use\nthe infrastructure. If I can give an example, it\u2019s sort of like, years and\nyears ago, when Google built its global infrastructure and they acquired\nYouTube, they could instantly scale video distribution globally, and if you go\nback in time, there were many companies in the video business when Google\nacquired YouTube, but they didn\u2019t have that scalable infrastructure. In the\nend, I think you\u2019ve seen our success with it because we can do global\ndistribution. This is a similar notion, let\u2019s say, a few years on.\n\nYeah, that fits my thinking on the matter and actually to me this was what was\nencouraging about that keynote, was it felt like there is an aspect, of\ncourse, in any product development, you need to understand your customer,\nunderstand their use case, but I think there\u2019s a bit where companies need to\nunderstand themselves, and what I got from that keynote and what I felt\nencouraging in a way I haven\u2019t seen in other Google presentations in the Post-\nChatGPT Era, I\u2019ll call it the Modern AI Era, was a real, \u201cWe know who we are\nand what we\u2019re good at and here\u2019s ways we\u2019re going to manifest that\nadvantage\u201d. I got that very distinctly, which I thought was a positive sign.\n\nTK: Thank you.\n\n### Challenges and Opportunities\n\nOne of the areas where I think Google really dropped the ball, this goes back\nten years or whatever it is, particularly around Workspace, was building an\necosystem. I think I\u2019ve asked you about this before, and the fact that it was\nkind of Microsoft Lite that, \u201cWe\u2019re going to do everything\u201d, but Microsoft\u2019s\nvery good at that and there\u2019s all these other products in Silicon Valley that\nare best in breed, but no one was tying them together, there wasn\u2019t the glue\nto have everything work together. Does any of those lessons or learnings, or\nmaybe I\u2019m right or wrong, apply to this next era? How is Google thinking about\npartnering with other startups or with other entities as far as being able to\nglue different stuff together? Because that is a little bit in contrast to the\nstory you\u2019re telling about, \u201cLook, we have real integration advantages by\ndoing a lot of stuff ourselves\u201d. How are you thinking about the balance there?\n\nTK: We definitely are working with an ecosystem, and the reason is the\nfollowing. We\u2019ve always said the fact that we\u2019re integrating things doesn\u2019t\nmean we\u2019re a closed system. So just recently, since you mentioned Workspace,\nwe have integrations to automate workflow for people. People eventually want\nto use a collaboration tool, not to just collaborate, but they actually wanted\nto automate workflow. So we\u2019ve done work with many partners, and there were\nseveral at this event from Canva to HubSpot to DocuSign to Adobe and several\nothers where we\u2019ve actually done the detail work to make that workflow totally\nseamless, you\u2019re going to continue seeing us do more of it.\n\nSundar Pichai mentioned in his video greeting, he emphasized the number of AI\nstartups, and particularly AI unicorns using Google Cloud. To go back to the\nreboot idea, do you view the AI Era as a restart in terms of capturing the\nnext generation of companies? I mean, obviously, AWS had a huge advantage here\nas far as general cloud computing, the entire mobile app ecosystem was by and\nlarge built on AWS. In the enterprise era, you have to deal with what\u2019s there,\nwhat they\u2019ve already dealt with, you have to have the integrations. Do you see\nyourself as having this as a big focus, \u201cWe\u2019re going to own this era of\nstartups\u201d?\n\nTK: Yes. And by the way, every one of those startups is being pursued by the\nother two, and the fact that 90% of the unicorns and 60% of all AI-funded\nstartups, up in each case by ten points in eight months, and they are the most\ndiscerning ones. I mean, just to be frank, the unicorns, for them, it is the\nreally biggest cost of goods sold in their P&L.\n\nSo what\u2019s the driver there?\n\nTK: The efficiency of our infrastructure.\n\nThey don\u2019t have all the legacy data and legacy things, so you\u2019ll feel like on\nan even playing field without that legacy, no one can touch you?\n\nTK: We are not looking to raise a trillion dollars to build a supercomputer\nbecause we\u2019ve been building one for ten years, and it\u2019s our fifth generation\nof building one of those, so we\u2019re not learning on the job for the first time.\n\nGoogle is, enterprise customers or startups, whoever it might be, they\u2019re\nbuying infrastructure or they\u2019re wanting to use models. Is there any concern\nabout Google in the consumer space? I mean, one of the challenges with\nbecoming an answer engine as opposed to a search engine is you\u2019re giving one\nanswer, and there\u2019s a lot of fighting over what that answer should be. You\u2019re\nnot offloading that to the user. Are you feeling any blow back or worries\nabout just Google being a culture war flash point or Gemini, or whatever it\nmight be? Or has that been relatively immaterial?\n\nTK: That has been completely immaterial.\n\nIs it an asset?\n\nTK: We are known as a company with great engineering prowess. You asked, Ben,\nwhat\u2019s the nature of Google? Google at the heart of it is a great engineering\ncompany. We build great products, we know how to build the most scalable\ninfrastructure in the world. As people choose to do AI, they want a partner\nwho has been there, done that, and unlike people who are for the first time\nworking with a third-party model and for the first time trying to introduce\ntheir products, we\u2019ve built models for years and we\u2019ve integrated our products\nfor years and we\u2019ve run it for years on a scalable infrastructure. So every\ncompany has issues, we\u2019re proud of who we are and customers are proud to work\nwith us.\n\nI\u2019ve been impressed. I\u2019ve told you this last time you were on here. I\u2019ve been\nreally impressed with the work you\u2019ve done with Google Cloud. And I\u2019m curious,\nas you look back on your tenure, to what extent have you succeeded by building\nan organization that\u2019s independent from Google as opposed to integrated with\nGoogle? What\u2019s the push and pull there, particularly given that the enterprise\ngo-to market and the support needs and all those things are very distinct from\na many billion users consumer product?\n\nTK: We have different needs, we have shared values. So we have shared values\non many things, treating our employees fairly, aspiring to excellence, making\nsure that our teams work effectively, empathetically, and collaboratively with\nthe rest of Google. At the end of the day, all of our models are served on the\ninfrastructure our team builds. When you use search, it runs on the\ninfrastructure that our team delivers. When you use YouTube, it runs on the\nnetwork that our team builds. So we work very well with the rest of Google.\n\nAt the same time, the necessity to work in an enterprise context requires us\nto have some unique things. As an example, we have many functions that the\nrest of Google doesn\u2019t have. They don\u2019t need professional services, they don\u2019t\nneed certain kinds of commercial attorneys, they don\u2019t need certain kinds of\nsystems that we do. Even the sales team works differently than the way that\nour sales team has to do because we sell to technology departments while they\nsell to chief marketing officers, so there are differences in what we do.\n\nAt the same time, we\u2019ve earned the respect of the rest of Google by the hard\nwork we\u2019ve done to win customers and grow the business. And we\u2019re very proud.\n\nAnd I think you should be. Is there a future where we look back in decades and\nwe\u2019re like, \u201cOh yeah, Google, the AI infrastructure company, they started as a\nsearch engine, believe it or not\u201d? Is there a potential where what your\nbuilding is actually in this new era what comes to matter most?\n\nTK: To us, five years ago, nobody gave us a shot, and to be frank, I was told\nit doesn\u2019t matter at all. One way to look at it is, \u201cAre we working on\nsomething that\u2019s important?\u201d, another way to look at it is, \u201cIt\u2019s all upside\u201d,\nand we took the latter. And today it matters, there\u2019s no question about it.\nWhen we look at the investor community, when we look at the customer\ncommunity, when we look at the relationships that Google has now, because we\nboth serve the technology team and the marketing team in many companies, it\ndefinitely matters. Will it be the most important thing? Time will tell. I\u2019ve\nlearned to never make a forecast in technology.\n\n(laughing) I think you make lots of forecasts in your day-to-day job, but\nyou\u2019re not going to share them with me, which is totally fine. Thomas, thanks\nfor coming back on. I thought it was a good keynote. There\u2019s a bit about AI\nwhere we don\u2019t have to discuss on here, it\u2019s not your area, that proposes\nfundamental challenges for the consumer product from a business model\nperspective, from the answer perspective, but at the same time, it\u2019s like a\nhand-in-glove fit with the cloud and enterprise and what you\u2019re doing, and it\nseems to me that you seem to recognize and are capitalizing on the same thing.\n\nTK: Thank you so much for your time, Ben. Good to talk to you.\n\nThis Daily Update Interview is also available as a podcast. To receive it in\nyour podcast player, visit Stratechery.\n\nThe Daily Update is intended for a single recipient, but occasional forwarding\nis totally fine! If you would like to order multiple subscriptions for your\nteam with a group discount (minimum 5), please contact me directly.\n\nThanks for being a supporter, and have a great day!\n\n### Share\n\n  * Facebook\n  * Twitter\n  * LinkedIn\n  * Email\n\n### Related\n\nGemini 1.5 and Google\u2019s NatureWednesday, April 10, 2024\n\nThe Value Chain ConstraintTuesday, February 26, 2019\n\nHow Google Is Challenging AWSWednesday, November 30, 2016\n\n# By Ben Thompson\n\nAbout Ben Follow via Email/RSS Twitter\n\n# Stratechery Plus\n\nAbout Stratechery Plus Subscribe Member Forum\n\n# Account\n\n  * Member\n\n  * Delivery Preferences\n  * Manage Account\n  * Sign Out\n\n  * Log In\n  * Sign Up\n\nLoading\n\n# Explore Stratechery\n\n  * Concepts\n  * Companies\n  * Topics\n\n# Archives\n\n  * Articles\n  * Updates\n  * Interviews\n  * Years in Review\n\n# Subscriber\u2019s Daily Update\n\nThursday, April 11, 2024\n\n# An Interview with Google Cloud CEO Thomas Kurian About Google\u2019s Enterprise\nAI Strategy\n\nTuesday, April 9, 2024\n\n# Personal Day\n\nMonday, April 8, 2024\n\n# The Status of Just Walk Out, TSMC Gets CHIPS Act Grant\n\nWednesday, April 3, 2024\n\n# An Interview with Benedict Evans About Regulation and AI\n\nOn the business, strategy, and impact of technology. \u00a9 Stratechery LLC 2024 | Terms of Service | Privacy Policy\n\n# Search results\n\nFiltersShow filters\n\nSort by:\n\n\u2022\u2022\n\n## No results found\n\n## Filter options\n\nSearch powered by Jetpack\n\n", "frontpage": true}
