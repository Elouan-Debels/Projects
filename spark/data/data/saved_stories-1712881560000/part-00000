{"aid": "40005209", "title": "When RAG runs out of steam, use schema extraction and analytics with Sycamore", "url": "https://www.aryn.ai/post/when-rag-runs-out-of-steam-use-schema-extraction-and-analytics-with-sycamore", "domain": "aryn.ai", "votes": 5, "user": "mehulashah", "posted_at": "2024-04-11 18:24:07", "comments": 0, "source_title": "When RAG runs out of steam, use schema extraction and analytics with Sycamore", "source_text": "When RAG runs out of steam, use schema extraction and analytics with Sycamore\n\ntop of page\n\nGet Started\n\n  * Vinayak Thapliyal\n  *     * 1 hour ago\n\n# When RAG runs out of steam, use schema extraction and analytics with\nSycamore\n\nUnstructured data is usually free-form and schemaless. It can vary from PDFs,\nemails, and blog posts like this, to images and video files. While the types\nof unstructured data can vary massively, one can often loosely find some type\nof structure within a dataset of unstructured documents. Examples of these\ninclude scanned tax forms (deductions tables), earning reports (revenue\nstatistics), and emails (sender and receiver names).\n\nSycamore is an LLM-powered search and analytics platform for unstructured\ndata, built on OpenSearch. With Sycamore, you can easily implement RAG\npipelines, but simple RAG has limitations. For example, RAG assumes that the\nanswer lies the top-ranked results because that's what is passed to LLMs for\ngeneration. To improve RAG quality, Sycamore allows you to run analytics (e.g.\nfiltering and aggregation) to ensure the top-ranked results contain the right\ninformation, giving better quality. Furthermore, you can run analytics\nindependently to get answers to complex questions that RAG simply cannot\naddress.\n\nTo use analytics on your data in Sycamore, you will first need to extract\nstructured metadata as attributes for these operations. When ingesting data,\nSycamore Data Prep enables you to easily and automatically detect patterns\nacross unstructured documents, extract structured metadata, and use those\nfields for analytics. In this post, we walk you through how to use Sycamore\nData Prep\u2019s unsupervised schema extraction and population, and then use this\nmetadata for analytics when querying your data. This blog post is a follow on\nto our previous post on LUnA (LLM-powered Unstructured Analytics) and\ndescribes the data layer of Sycamore, which offers a 100% OpenSearch\ncompatible API.\n\n## When RAG is not enough\n\nIn this post, we\u2019ll be using aviation incident reports from The National\nTransportation Safety Board (NTSB). The NTSB is a U.S. government\ninvestigative agency for civil transport incident investigations. Each\naviation incident is reported in a PDF that contains a mix of specific\nrequired fields and free-form analysis of the incident in text. An example\nreport can be found here.\n\nWhen building a conversational interface over this data, a traditional RAG\napproach can be quite limiting. RAG assumes the answer is contained within the\ntop K results (to feed the LLM context window), and it places all of the\nburden on keywords and vector embeddings to return the right passages for an\narbitrary query. Many times, that breaks down, leading a RAG pipeline to\nproduce an incomplete or incorrect answer.\n\nBut, there is a way to deal with this! In this example, we\u2019ll go beyond\ntraditional RAG by using metadata, analytics operations, and hybrid search to\nreturn answers that are elusive to a simple RAG approach.\n\n## Answering questions where traditional RAG fails\n\nWith this in mind, let\u2019s get more specific on what we can do with the NTSB\ndataset when using metadata in our queries.\n\n  * Answer questions that need more information than can fit in the LLM context: An aggregation query is a simple example of this, e.g. \u201cWhat types of planes were involved in incidents in California?\u201d LLMs aren\u2019t accurate with these aggregation style questions, and especially when all of the unique values aren\u2019t in the top K results. This is a problem much better suited for analytics-style queries. Extracting values for \u201clocation\u201d and \u201caircraft type\u201d is sufficient to answer the question using aggregations.\n\n  * Answer questions that don\u2019t retrieve the correct documents in the top K results: Many datasets have a lot of similar looking information, making it hard for even the best vector embedding models and hybrid search techniques to retrieve the required documents in the top K results. For instance, the NTSB dataset has 100s of incidents in Washington over several years, and there is a burden on the vector embedding model to separate these incidents by time. If your question is about a specific year, e.g. \u201cWere there any incidents in the last three days of January 2023 in Washington?\u201d, there is no guarantee your vector or hybrid search will retrieve the documents from that year. As more constraints are added to the question, the probability of errors in filtering only by vector search (or term searches) increases, and this results in your relevant documents being pushed outside the top K. Instead, by having the fields \u201clocation\u201d and \u201cyear\u201d available as structured metadata, you can easily construct reliable filters to get the right documents by year.\n\nAs the above examples show, using analytics and filters with this metadata\nenables you to retrieve the right information for a query, which then gives\nbetter input into a RAG pipeline or other downstream operations. However,\npreparing data for this can be difficult to do, especially when dealing with\nunstructured, varying documents.\n\nBut if you're using Sycamore, it's a breeze! When ingesting data, Sycamore\nData Prep lets you automatically detect and extract structured data from\nunstructured documents using LLMs. You can then use this metadata with filters\nand analytics in your search queries, using the OpenSearch query language. In\nyour data prep job, you only need to provide a class type to guide the data\nextraction to select relevant fields, and the rest is done automatically.\n\nFor the NTSB data, we will prompt Sycamore to infer a schema and populate it\nfor each report, and then query these fields after ingestion using analytics\noperations in our search queries.\n\n## Let's build it\n\nYou can find a notebook with the full code for the Sycamore Data Prep job\nhere. In the sections below, we will describe some key functions.\n\n### NTSB dataset: document format\n\nThe screenshot below is an example NTSB incident report. It contains some\nmetadata (e.g. date, location, aircraft type), followed by an incident summary\nin free-form text.\n\n### Defining and extracting structured metadata\n\nIn this section, we will discuss a few parts of the job in more detail.\nSycamore Data Prep uses a data structures called DocSets, distributed\ncollections of documents bundled together for processing. There are a variety\nof transformations on DocSets to help customers easily handle unstructured\ndata and prepare it for search and analytics in Sycamore.\n\nIn this data prep job, we will use DocSet transformations that call LLMs to\ndefine the metadata schema for each Document and then extract the values for\nthat schema.\n\nExtract_batch_schema accepts an entity class name (e.g. FlightAccidentReport),\nand attempts to extract field names as a schema from a document that relate to\nthe given class. Given a schema (either generated by extract_batch_schema, or\nprovided explicitly by a user), extract_properties then extracts values for\nthe properties in the schema for a given Document.\n\nWriting a data prep job with these transforms is straightforward. The\nfollowing code ingests and partitions the Documents in the DocSet, then\nextracts metadata belonging to the \u201cFlightAccidentReport\u201d class. By default,\nthe extract_batch_schema transform only extracts the 7 most important\nattributes of the class, but you can adjust this in the configuration. Also,\nwe set the num_of_elements to 10, which indicates that only the first 10\nelements (passages) of each Document will be used for schema generation and\nproperties, respectively.\n\nFor a DocSet named \"docs\", we run these transforms:\n\n    \n    \n    docs = docs.extract_batch_schema(schema_extractor=OpenAISchemaExtractor(\"FlightAccidentReport\", llm=llm, num_of_elements=10)\\ .extract_properties(property_extractor=OpenAIPropertyExtractor(llm=llm, num_of_elements=10))\n\nNext, we can explore the generated schema and extracted properites by showing\nthe fields for a Document in the DocSet.\n\n    \n    \n    taken_docs = docs.take() # executes the DAG or plan shown above. Sycamore Data Prep uses lazy evaluation. taken_docs[0].properties['_schema'] # shows the schema for the first doc.\n\nIt will return the schema:\n\n    \n    \n    {'type': 'object', 'properties': {'location': {'type': 'string'}, 'dateAndTime': {'type': 'string'}, 'aircraft': {'type': 'string'}, 'definingEvent': {'type': 'string'}, 'probableCause': {'type': 'string'}, 'accidentNumber': {'type': 'string'}, 'aircraftDamage': {'type': 'string'}}}\n\nNext, we'll show the values for this schema for a Document:\n\n    \n    \n    taken_docs[0].properties['entity'] # entity is a top-level document property.\n\nAnd it returns:\n\n    \n    \n    {'location': 'Brashear, Texas', 'dateAndTime': 'January 15, 2023, 09:30 Local', 'aircraft': 'CHEEK CARROLL HATZ/CHEEK', 'definingEvent': 'Fuel related', 'probableCause': 'A partial loss of engine power due to the formation of carburetor ice, which resulted in the inability to maintain altitude and the subsequent hard forced landing.', 'accidentNumber': 'CEN23LA083', 'aircraftDamage': 'Substantial'}\n\nTo populate these fields, Sycamore prompts the LLM with the schema and text\nfrom the Document, and the LLM extracts the information.\n\n### Formatting structured metadata\n\nSycamore Data Prep mixes LLM-powered functions with arbitrary Turing complete\nPython code, giving you the best of both worlds when preparing and enriching\ndata. In this example, we have to format the output of the dateAndTime field.\nThe dateAndTime field has been inferred as a string, and there are cases in\nour dataset where the time component is missing (e.g. dateAndTime = \u201cJanuary\n20, 2022\u201d). In the job, we write a simple function called convert_timestamp\nand run it on our DocSet to convert it to a Python date object.\n\nYou can find the rest of the Sycamore Data Prep code in the notebook. This\nnotebook includes other data prep steps to partition the documents, create\nvector embeddings, and load the Sycamore indexes. After you run the data prep\njob and load the data, we can now run some queries.\n\n## Adding analytics to search queries\n\nWith our enriched dataset, we can now augment our search queries and RAG\npipelines with filters and aggregations that use the structured metadata.\n\n### Using aggregations\n\nTo go back to the example earlier in the post, relying on the usual top K\nresults would not answer \u201cWhat types of planes were involved in incidents in\nCalifornia?\u201d correctly. This is because LLMs do not reliably aggregate\ninformation, and even if they did, you cannot fit all of the sample set in an\nLLMs context window.\n\nInstead, we can use Sycamore's analytics operations on the dataset. We will\nuse a filter on the metadata field \"location\" to select for incidents in\nCalifornia. Next, we will use an aggregation function on the metadata field\n\u201caircraftType\u201d to find the unique plane types. Sycamore uses the rich\nOpenSearch query language to express queries, and information on this syntax\nis found here.\n\nTo query your Sycamore stack using the OpenSearch query language, you can post\nthe search request below to localhost:9200/[index-name]/_search . If you ran\nthe notebook as-is, your index name is ntsb.\n\n    \n    \n    { \"size\": 0, \"query\": { \"bool\": { \"must\": [ { \"match_all\": {} } ], \"filter\": [ { \"match_phrase\": { \"properties.entity.location\": \"California\" } } ] } }, \"aggs\": { \"unique_aircraft_types\": { \"terms\": { \"field\": \"properties.entity.aircraftType.keyword\" } } } }\n\nThis query will return a list of list of unique airplane types:\n\n    \n    \n    {... \"aggregations\": { \"unique_aircraft_types\": { \"doc_count_error_upper_bound\": 0, \"sum_other_doc_count\": 0, \"buckets\": [ { \"key\": \"Piper PA-28-180\", \"doc_count\": 9 }, { \"key\": \"Cessna 195A\", \"doc_count\": 8 }, { \"key\": \"Cessna 172\", \"doc_count\": 7 }, { \"key\": \"Cessna 180K\", \"doc_count\": 7 } ] } }\n\nThe doc_count field refers to the number of document chunks (passages) from\nincident reports that list that airplane type, not the number of incident\nreports. An incident report is segmented into chunks during the data prep job.\n\nIf you want to find the number of unique incident reports for each airplane\ntype in the query, you can add a nested aggregation (in the aggregation for\nairplane type) that counts the number of unique incident reports tagged by\nparent_id:\n\n    \n    \n    \"aggs\": { \"unique_incidents\": { \"cardinality\": { \"field\": \"parent_id.keyword\" } } }\n\n### Using filters\n\nWhen asking the question \u201cWere there any incidents in the last three days of\nJanuary 2023 in Washington?\u201d, we need to ensure all of the documents related\nto Seattle are retrieved in the top K results. With Sycamore, we can utilize\nfilters to ensure we are only sending relevant documents to our RAG pipeline.\n\nSycamore uses OpenSearch\u2019s Search Pipeline feature set to define RAG\npipelines, and you can visit here for more information on the syntax. It\nincludes a hybrid search query, that is broken up into the vector (neural) and\nkeyword searches, and the generative_qa parameters to configure the generative\nstep with an LLM (in this example GPT-4).\n\nWe will use the Sycamore's default RAG pipeline. Use the index name from the\nprior example, and provide the embedding model ID. Once you have this info,\nrun this search pipeline query:\n\nIn the above query, there is a filter to the vector/neural and keyword\nsections of the hybrid search query, and it filters on location (Washington)\nand the date range. This ensures that only documents that have these\nattributes are retrieved and used in the RAG pipeline. Here is the excerpt:\n\n    \n    \n    \"filter\": { \"bool\": { \"filter\": [ { \"match\": { \"properties.entity.location\": \"Washington\" } }, { \"range\": { \"properties.entity.day\": { \"gte\": \"2023-01-29\", \"lte\": \"2023-01-31\" } } } ] }\n\nRun the above RAG pipeline with and without this filter clause. Without the\nclause, RAG could not generate the answer correctly because hybrid search\ncould not retrieve the incident in the top K results. But, with these filters,\nwe get the correct answer:\n\n    \n    \n    \"ext\": { \"retrieval_augmented_generation\": { \"answer\": \"Yes, there was an incident on January 29, 2023, in Dallesport, Washington. An amphibious float-equipped airplane, CUB CRAFTERS INC CC18-180, collided with a channel marker during a step taxi in preparation for takeoff on the water. The left wing and left side float separated from the fuselage, resulting in substantial damage. The National Transportation Safety Board determined the probable cause to be the flight instructor's failure to see and avoid the channel marker [3].\" } }\n\n# Conclusion\n\nIn this post, we used Sycamore to extract and populate schemas for our\nunstructured documents. We then loaded this data into our indexes, and used\nqueries with aggregations and filters to ensure we retrieve the correct and\ncomprehensive results directly. This approach solves problems found in simple\nRAG pipelines where a possibly incomplete or incorrect set of top K results\nget passed to the LLM for a generated answer.\n\nBy using structured metadata, you can use analytics alongside search to ask\ndifferent and more complex questions on your data. If you have any questions\non using structured metadata in Sycamore, join our Slack and get in touch!\n\n## Recent Posts\n\nSee All\n\nNear-Duplicate Detection in Sycamore: What Is It Good For?\n\nRAG is a band-aid; we need LLM-powered Unstructured Analytics \u2014 LUnA\n\nAnswer questions on tables with Sycamore's table extraction transform\n\nContact\n\n756 California St., Unit A\n\nMountain View, CA 94041\n\nContact us: info@aryn.ai\n\nQuick Links\n\nBlog\n\nLinkedIn\n\nDocs\n\nGitHub\n\n\u00a9 2024 by Aryn.\n\nbottom of page\n\n", "frontpage": false}
