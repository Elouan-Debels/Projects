{"aid": "40054463", "title": "Learning Soccer Skills for a Bipedal Robot", "url": "https://sites.google.com/view/op3-soccer", "domain": "sites.google.com", "votes": 1, "user": "sega_sai", "posted_at": "2024-04-16 17:03:14", "comments": 0, "source_title": "OP3 Soccer", "source_text": "OP3 Soccer\n\nSearch this site\n\nOP3 Soccer\n\n## Learning Agile Soccer Skills for a Bipedal Robot with Deep Reinforcement\nLearning\n\npaper\n\nOP3 Soccer Team, DeepMind\n\n##\n\nSoccer players can tackle, get up, kick and chase a ball in one seamless\nmotion. How could robots master these agile motor skills?\n\n###\n\nMovie 1: Project overview\n\nWe investigated the application of Deep Reinforcement Learning (Deep RL) for\nlow-cost, miniature humanoid hardware in a dynamic environment, showing the\nmethod can synthesize sophisticated and safe movement skills making up complex\nbehavioral strategies in a simplified one-versus-one (1v1) soccer game.\n\nOur agents, with 20 actuated joints, were trained in simulation using the\nMuJoCo physics engine, and transferred zero-shot to real robots. The agents\nuse proprioception and game state features as observations. The trained soccer\nplayers exhibit robust and dynamic movement skills such as rapid fall\nrecovery, walking, turning, kicking and more. They transition between these\nemergent skills automatically in a smooth, stable, and efficient manner, going\nbeyond what might intuitively be expected from the platform. The agents also\ndeveloped a basic strategic understanding of the game, learning to anticipate\nball movements and to block opponent shots.\n\n###\n\nMovie 2: Behavior and skill highlights\n\nRecurring skills and strategies selected from typical one-versus-one play. The\nagent demonstrates agile skills including getting up and turning; reactive\nbehavior including kicking a moving ball; object interaction including ball\ncontrol; dynamic defensive blocking; strategical play including defensive\npositioning. The agent also quickly transitions between skills (turning,\nchasing, controlling, then kicking, for example), and combines them\n(frequently turning and kicking, for example).\n\n###\n\nMovie 3: Comparison to scripted baseline controllers\n\nCertain key locomotion behaviors, including getting up, kicking, walking, and\nturning are available for the OP3 robot. This movie illustrates the baselines\nand a side-by-side comparison with the corresponding behaviors from the deep\nRL agent.\n\n###\n\nMovie 4: Turning and kicking behaviors in simulation and in the real\nenvironment\n\nOne of the agile behaviors we see during soccer play is the turning skill\ndiscovered by the agent, shown here in slow motion. It pivots on the corner of\none foot and takes 2-3 steps to turn a 180 degrees. Although learned entirely\nin simulation, this behavior is successful on the OP3 after zero-shot transfer\nto the real robot, with perhaps surprisingly low sim-to-real gap given the\nhighly optimized nature of the behavior. The agent's kicking behavior is also\nshown here in slow motion.\n\n###\n\nMovie S1: Training in simulation\n\nWe first trained individual skills in isolation, in simulation, and then\ncomposed those skills end-to-end in a self-play setting. We found that a\ncombination of sufficiently high-frequency control and targeted dynamics\nrandomization and perturbations during training in simulation enabled good-\nquality transfer to the robot.\n\n###\n\nMovie S2: 1v1 matches\n\n5 one-versus-one matches. These matches are representative of the typical\nbehavior and gameplay of the fully trained soccer agent.\n\n###\n\nMovie S3: Set pieces in simulation and in the real environment\n\nWe analysed the agent's performance in two set-pieces, to gauge the\nreliability of getting up and shooting behaviors and to measure the\nperformance gap between the simulation and the real environment. We also\ncompared behaviors with scripted baseline skills. In experiments they walked\n156% faster, took 63% less time to get up, and kicked 24% faster than a\nscripted baseline.\n\n###\n\nMovie S4: Robustness and recovery from pushes\n\nAlthough the robots are inherently fragile, minor hardware modifications\ntogether with basic regularization of the behavior during training lead to\nsafe and effective movements while still being able to perform in a dynamic\nand agile way.\n\n##\n\nPreliminary Results: Learning from vision\n\nWe conducted a preliminary investigation of whether deep RL agents can learn\ndirectly from raw egocentric vision. In this context the agent must learn to\ncontrol its camera and integrate information over a window of egocentric\nviewpoints to predict various game aspects. Our initial analysis indicates\nthat deep RL is a promising approach to this challenging problem. We conducted\na simpler set-piece using fixed walker and ball positions and found our agent\nscored 10 goals in simulation and 6 goals on the real robot over 10 trials.\n\nWe hope the challenge of integrating the get-up skill and learning vision-\nguided exploration and multi-agent strategies will be tackled by future work.\n\n###\n\nMovie S5: Preliminary vision based agents\n\n##\n\nMore information\n\nPlease read the paper for more information about this research.\n\nPage updated\n\nGoogle Sites\n\nReport abuse\n\nThis site uses cookies from Google to deliver its services and to analyze\ntraffic. Information about your use of this site is shared with Google. By\nusing this site, you agree to its use of cookies.\n\nLearn more\n\nGot it\n\n", "frontpage": false}
