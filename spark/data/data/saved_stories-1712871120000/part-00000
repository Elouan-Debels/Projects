{"aid": "40003275", "title": "Instructor Is All You Need: My Review of the Instructor LLM Library", "url": "https://www.felixvemmer.com/en/blog/instructor-llm-framework-reviewed", "domain": "felixvemmer.com", "votes": 1, "user": "sebg", "posted_at": "2024-04-11 15:30:54", "comments": 0, "source_title": "Instructor is All You Need: My Review of the Instructor LLM Library by Jason Liu", "source_text": "Instructor is All You Need: My Review of the Instructor LLM Library by Jason\nLiu\n\nFelix Vemmer\n\nReviews\n\nTypescript\n\nPython\n\nLLMs\n\nFeatured\n\n# Instructor is All You Need: My Review of the Instructor LLM Library by Jason\nLiu\n\nWith numerous LLM libraries available, such as Langchain, CrewAI, Haystack,\nand Marvin AI, the question arises: what makes Instructor stand out as an llm\nlibrary? Find out in this blog post.\n\nFelix Vemmer\n\nApril 10, 2024\n\nFollowing the latest AI Tinkeres Berlin Meetup, where I witnessed a speedrun\nof very cool AI demos, I couldn't help but dive back into the chaotic world of\nLLM libraries. It's a realm where developers like me are on a never-ending\nquest to find the best library to help them build the first 1 billion dollar\ncompany.\n\nAfter trying Langchain, CrewAi and Marvin AI, I started to like Instructor so\nmuch that I wanted to share my thoughts on why I personally started using it\nmore and more for all my personal projects like BacklinkGPT.\n\n## What is Instructor?\n\nLet's start by understanding what the instructor library is, by checking out\nthe \"H1\" of the docs.\n\n> Instructor makes it easy to reliably get structured data like JSON from\n> Large Language Models (LLMs) like GPT-3.5, GPT-4, GPT-4-Vision, including\n> open source models like Mistral/Mixtral from Together, Anyscale, Ollama, and\n> llama-cpp-python.\n\nBy leveraging Pydantic, Instructor provides a simple, transparent, and user-\nfriendly API to manage validation, retries, and streaming responses.\n\nLet's see this in action.\n\n### Hello World Instructor\n\nA simple hello_world example would look something like this:\n\n  1. Install Instructor\n\n    \n    \n    pip install instructor\n\n  2. Import Instructor and instantiate the client.\n\n    \n    \n    from typing import Literal import instructor from openai import OpenAI from pydantic import BaseModel, Field client = instructor.from_openai(OpenAI())\n\n  3. Define a Pydantic model for the output and call the completions endpoint.\n\n    \n    \n    class Sentiment(BaseModel): sentiment: Literal[\"positive\", \"negative\", \"neutral\"] = Field( ..., description=\"Sentiment of the text\" )\n\n  4. Call the completions endpoint and simply pass in the Sentiment model:\n\n    \n    \n    sentiment = client.chat.completions.create( model=\"gpt-4-turbo-preview\", response_model=Sentiment, messages=[ {\"role\": \"system\", \"content\": \"What is the sentiment of the given text?\"}, {\"role\": \"user\", \"content\": \"The instructor llm library is awesome!\"}, ], ) print(sentiment) # sentiment='positive'\n\n### Programming Language Ports\n\nTo further enhance its accessibility, Instructor extends its support beyond\nPython, embracing a variety of programming language ports:\n\n  * Typescript: Instructor.js\n  * Elixir: instructor_ex\n  * PHP: instructor-php\n\nSo, if you believe like Guillermo Rauch that the upcoming AI applications will\nbe frontend-first and built by AI Typescript Engineers, Instructor has you\ncovered:\n\n  1. Install Instructor and its dependencies\n\n    \n    \n    pnpm add @instructor-ai/instructor zod openai\n\n  2. Import all dependencies and define the client\n\n    \n    \n    import Instructor from '@instructor-ai/instructor' import OpenAI from 'openai' import { z } from 'zod' const oai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY ?? undefined, organization: process.env.OPENAI_ORG_ID ?? undefined, }) const client = Instructor({ client: oai, mode: 'FUNCTIONS', })\n\n  3. Define the SentimentSchema this time using zod.\n\n    \n    \n    const SentimentSchema = z.object({ // Description will be used in the prompt sentiment: z .literal('positive') .or(z.literal('negative').or(z.literal('neutral'))) .describe('The sentiment of the text'), })\n\n  4. Again call the completions endpoint with the SentimentSchema:\n\n    \n    \n    async function main() { const sentiment = await client.chat.completions.create({ messages: [ { role: 'system', content: 'What is the sentiment of the given text?' }, { role: 'user', content: 'The instructor llm library is awesome!' }, ], model: 'gpt-3.5-turbo', response_model: { schema: SentimentSchema, name: 'Sentiment', }, }) console.log({ sentiment }) } main() // { sentiment: { sentiment: 'positive' } }\n\nI'm not totally sold on the idea that future AI engineers will be TypeScript\nengineers, but I have to say Instructor.js worked like a charm with Next.js.\nIf you're wondering why I went with Next.js in the first place, I've got a\nwhole post about it:\n\n## The Rise of Next.js: Why It's the Full-Stack Framework of Choice for Modern\nWebsites\n\nWhen selecting a frontend framework, reliability is paramount for my clients.\nDespite exploring options like SvelteKit, \"Why Next.js?\" remains a frequent\nquery. In this article, I unpack why Next.js stands out as a dependable choice\nand its promising future.\n\nNovember 1, 2023\n\n## Why Use Instructor\n\nIf you look at the vanity metric of popularity via GitHub Stars, you'll see\nthat Instructor has around 4.4k stars, making it a hidden champion compared to\nmore popular libraries like Langchain or CrewAi.\n\nWhile Instructor may not be the most popular library at the moment, its\nsimplicity, data validation capabilities, and growing community make it a\ncompelling choice for developers looking to integrate LLMs into their\nprojects. Let's dive deeper into the reasons why Instructor is a promising\ntool and why you might want to consider using it.\n\n### Simplicity\n\nSimplicity is one of Instructor's key advantages. Although libraries such as\nLangchain and CrewAi offer quick starts with pre-built prompts and\nabstractions, they can become harder to customize as your needs evolve.\n\nInstructor takes a different approach by patching the core OpenAI API. This\nmakes it intuitive for developers already familiar with the OpenAI SDK, while\nproviding flexibility and control as projects scale.\n\nInstructor is to the OpenAI API what Drizzle ORM is to SQL. It enhances the\nstandard OpenAI API to be more developer-friendly and adaptable. By building\non existing standards, Instructor flattens the learning curve, letting\ndevelopers focus on creating powerful LLM applications.\n\n## Unveiling the Power of Drizzle ORM: Key Features that Skyrocketed My\nProductivity\n\nWant to supercharge your dev productivity? Get a glimpse into how Drizzle ORM,\nwith its well-structured docs and powerful features, could be a game-changer\nfor your projects.\n\nJune 28, 2023\n\nJust as reference implementing the Sentiment classifyer in Langchain just\nfeels more verbose:\n\n    \n    \n    from langchain.output_parsers import PydanticOutputParser from langchain.prompts import PromptTemplate from langchain_core.pydantic_v1 import BaseModel, Field, validator from langchain_openai import OpenAI model = OpenAI(temperature=0.0) class Sentiment(BaseModel): sentiment: Literal[\"positive\", \"negative\", \"neutral\"] = Field( ..., description=\"Sentiment of the text\" ) parser = PydanticOutputParser(pydantic_object=Sentiment) prompt = PromptTemplate( template=\"What is the sentiment of the given text?\\n{format_instructions}\\n{query}\\n\", input_variables=[\"query\"], partial_variables={\"format_instructions\": parser.get_format_instructions()}, ) # And a query intended to prompt a language model to populate the data structure. chain = prompt | model | parser output = chain.invoke({\"query\": \"The instructor llm library is awesome!\"}) sentiment = parser.parse(output) print(sentiment)\n\n### Data Validation = Production Readiness\n\nData validation is crucial for production-ready LLM applications. Instructor\nleverages powerful libraries like Pydantic and Zod to ensure that the outputs\nfrom language models conform to predefined schemas. This guarantees a level of\nreliability that is essential when deploying LLMs in real-world scenarios.\n\nBy validating the structure and content of LLM responses, developers can have\nconfidence that the results are not hallucinated or inconsistent. Instructor's\nvalidation capabilities help catch potential issues early in the development\nprocess, reducing the risk of unexpected behavior in production.\n\nAs highlighted in the article \"Good LLM Validation is Just Good Validation\",\nInstructor's client.instructor.from_openai(OpenAI()) method provides a\nconvenient way to integrate validation into your LLM pipeline. This approach\nensures that your application can handle the dynamic nature of LLM outputs\nwhile maintaining the necessary safeguards for reliable operation.\n\n#### Showcase SEO Meta Descriptions\n\nOne practical example where I used this is for creating SEO Meta Descriptions.\n\nAs a reference, here are the optimal length requirements:\n\n  * Meta Title: 50-60 characters\n  * Meta Description: 150-160 characters\n\nWith these requirements in mind I can use Pydantic to generate a meta\ndescription schemafor a blog post:\n\n    \n    \n    class Metadata(BaseModel): meta_title: str = Field( ..., min_length=50, max_length=60, description=\"Meta Title between 50-60 characters. Should be concise, descriptive and include primary keyword.\", ) meta_description: str = Field( ..., min_length=150, max_length=160, description=\"Meta Description between 150-160 characters. Should be compelling, summarize the page content and include relevant keywords naturally.\", ) meta_descrioption = client.chat.completions.create( model=\"gpt-4\", max_retries=3, # Retry the request 3 times response_model=Metadata, messages=[ {\"role\": \"system\", \"content\": content}, ], )\n\nCalling this code without the max_retries=3 parameter resulted in the\nfollowing error, showing that the LLM did not exactly follow the given\nrequirements after validating the output with Pydantic.\n\n    \n    \n    ValidationError: 1 validation error for Metadata meta_description String should have at most 160 characters [type=string_too_long, input_value='Discover what sets Instr... features and benefits.', input_type=str] For further information visit https://errors.pydantic.dev/2.7/v/string_too_long\n\nAfter adding the max_retries=3 parameter, the LLM was able to meet the\nrequirements with automatic retries and produced the following output:\n\n    \n    \n    { \"meta_title\": \"Review: Why Instructor Stands Out Among LLM Libraries\", \"meta_description\": \"Explore the unique features of Instructor LLM library and what differentiates it from other libraries like Langchain, CrewAI, Haystack, and Marvin AI.\" }\n\nFor another practical use case, check out this example that shows you how to\nensure LLMs don't hallucinate by citing resources.\n\n### Code/Prompt Organization\n\nOne of the key benefits of using Pydantic with Instructor is the ability to\ncreate modular, reusable output schemas. By encapsulating prompts and their\nexpected outputs into Pydantic models, you can easily organize and reuse them\nacross your codebase. This makes managing and sharing prompts a lot easier and\nmore organized than having to copy and paste them around.\n\nLet's look at the famous chain-of-thought prompting technique, a powerful\ntechnique that allows LLMs to break down complex problems into smaller, more\nmanageable steps. Here's an example of how you can leverage chain-of-thought\nprompting with Instructor:\n\n    \n    \n    from pydantic import BaseModel, Field class Role(BaseModel): chain_of_thought: str = Field( ..., description=\"Think step by step to determine the correct title\" ) title: str class UserDetail(BaseModel): age: int name: str role: Role\n\nFor more tips on prompt engineering checkout their docs here.\n\n### Documentaiton & Cookbok with Real World Examples\n\nTalking about docs, Instructor just excels at documentation, which is crucial\nfor developer adoption. It provides comprehensive, well-structured API\ndocumentation and an extensive cookbook. The cookbook is a valuable resource\nwith recipes for common use cases, serving as a practical reference for\ndevelopers.\n\nWhile other frameworks have great examples, Instructor's cookbook stands out\nby showcasing \"real world\" examples of how it solves certain problems. I think\nquite a lot of this is probably due to the fact that the creator, Jason Liu,\nalso works as a freelancer where he's using Instructor himself to solve actual\nbusiness problems. Here's the list of examples:\n\n  1. How are single and multi-label classifications done using enums?\n  2. How is AI self-assessment implemented with llm_validator?\n  3. How to do classification in batch from user provided classes.\n  4. How are exact citations retrieved using regular expressions and smart prompting?\n  5. How are search queries segmented through function calling and multi-task definitions?\n  6. How are knowledge graphs generated from questions?\n  7. How are complex queries decomposed into subqueries in a single request?\n  8. How are entities extracted and resolved from documents?\n  9. How is Personally Identifiable Information sanitized from documents?\n  10. How are action items and dependencies generated from transcripts?\n  11. How to enable OpenAI's moderation\n  12. How to extract tables using GPT-Vision?\n  13. How to generate advertising copy from image inputs\n  14. How to use local models from Ollama\n  15. How to store responses in a database with SQLModel\n  16. How to use groqcloud api\n\n### Retries and Open Source Models\n\nInstructor's retry functionality is a game-changer when it comes to using\nopen-source models for extraction tasks. Without Instructor, these models may\nnot be reliable enough to trust their output directly. However, by leveraging\nInstructor's retry capabilities, you can essentially brute-force these models\nto provide the correct answer, making them viable options for extraction while\nstill respecting privacy.\n\nThe retry mechanism works by automatically re-prompting the model if the\noutput fails to meet the specified validation criteria. This process continues\nuntil a valid output is obtained or the maximum number of retries is reached.\nBy incorporating this functionality, Instructor enables developers to utilize\nopen source models that may not be as powerful as their commercial\ncounterparts, but can still produce accurate results through the power of\nretries.\n\n### Reliable Agentic Workflows?\n\nFinally, all the latest rage is about building agentic workflows, as discussed\nby Andrew Ng in this video:\n\nCrew.ai is an exciting new library for building AI agents designed for real-\nworld use cases. One of the main challenges I had though is that I just\ncouldn't get the agents to produce consistent output without going off the\nrails.\n\nCrew.ai provides a great abstraction with an easy-to-use API, but it can still\nbe challenging to reliably steer agents and handle failures gracefully. If a\ntask in a crew fails, there's no straightforward way to pause execution and\nrestart debugging from the latest task.\n\nIn contrast, Instructor requires more manual effort to define the desired\nlogic and flow, but it offers greater control and flexibility to build\nreliable agentic workflows. By carefully designing the task flow and error\nhandling, you can create more predictable and robust agents.\n\nIt will be interesting to see how Crew.ai evolves in the future, potentially\nadopting techniques like memory and other enhancements to improve agent\npredictability.\n\nI'm excited to explore building more agent-like flows with Instructor in the\nfuture to see if it can be used in a similar way to Crew.ai and to understand\nthe level of effort required to emulate Crew.ai's functionality. This\nexploration will provide valuable insights into the capabilities and\nlimitations of Instructor for creating agentic workflows and help determine\nits suitability for such use cases.\n\nIf you enjoyed this post make sure to check the rest of my blog and follow me\non Twitter for more posts like this.\n\nSee all posts\n\nBuilt by Felix Vemmer. The source code is available on GitHub.\n\n", "frontpage": false}
