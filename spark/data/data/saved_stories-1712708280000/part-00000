{"aid": "39982375", "title": "AI Visionary Eliezer Yudkowsky on the Singularity", "url": "https://www.scientificamerican.com/blog/cross-check/ai-visionary-eliezer-yudkowsky-on-the-singularity-bayesian-brains-and-closet-goblins/", "domain": "scientificamerican.com", "votes": 2, "user": "udev4096", "posted_at": "2024-04-09 18:16:02", "comments": 0, "source_title": "AI Visionary Eliezer Yudkowsky on the Singularity, Bayesian Brains and Closet Goblins", "source_text": "AI Visionary Eliezer Yudkowsky on the Singularity, Bayesian Brains and Closet Goblins | Scientific American\n\nSkip to main content\n\nScientific American\n\nSign Up for Our Daily Newsletter\n\nMarch 1, 2016\n\n#\n\nAI Visionary Eliezer Yudkowsky on the Singularity, Bayesian Brains and Closet\nGoblins\n\n\u201cDecision theorist\u201d Eliezer Yudkowsky spells out his idiosyncratic vision of\nthe Singularity.\n\nBy John Horgan\n\nThis article was published in Scientific American\u2019s former blog network and\nreflects the views of the author, not necessarily those of Scientific American\n\nI\u2019m perpetually astonished by smart people who believe things I find\npreposterous. For example, geneticist and National Institutes of Health\nDirector Francis Collins, who believes Jesus rose from the dead. Or\nartificial-intelligence theorist Eliezer Yudkowsky, who believes machines...\nWell, I should let Yudkowsky say what he believes. I interviewed him on\nBloggingheads.tv in 2008, and it didn\u2019t go well, because I assumed he was a\ndisciple of Singularity guru Ray Kurzweil. Yudkowsky, who never attended\ncollege, is no one\u2019s follower. He is a stubbornly original theorist of\nintelligence, both human and artificial. His writings (such as this essay,\nwhich helped me grok, or gave me the illusion of grokking, Bayes\u2019s Theorem)\nexude the arrogance of the autodidact, edges undulled by formal education, but\nthat\u2019s part of his charm. Even when he\u2019s annoying, Yudkowsky is funny, fresh,\nprovocative. For more on his background and interests, see his personal\nwebsite or the site of the Machine Intelligence Research Institute, which he-\ncofounded. And read the following Q&A, which includes a bonus: comments from\nhis wife Brienne.\n\nHorgan: When someone at a party asks what you do, what do you tell her?\n\nYudkowsky: Depending on the venue: \"I'm a decision theorist\", or \"I'm a\ncofounder of the Machine Intelligence Research Institute\", or if it wasn't\nthat kind of party, I'd talk about my fiction.\n\nHorgan: What\u2019s your favorite AI film and why?\n\nYudkowsky: AI in film is universally awful. Ex Machina is as close to being an\nexception to this rule as it is realistic to ask.\n\nHorgan: Is college overrated?\n\nYudkowsky: It'd be very surprising if college were underrated, given the\nsocial desirability bias of endorsing college. So far as I know, there's no\nreason to disbelieve the economists who say that college has mostly become a\npositional good, and that previous efforts to increase the volume of student\nloans just increased the cost of college and the burden of graduate debt.\n\nHorgan: Why do you write fiction?\n\nYudkowsky: To paraphrase Wondermark, \"Well, first I tried not making it, but\nthen that didn't work.\"\n\nBeyond that, nonfiction conveys knowledge and fiction conveys experience. If\nyou want to understand a proof of Bayes's Rule, I can use diagrams. If I want\nyou to feel what it is to use Bayesian reasoning, I have to write a story in\nwhich some character is doing that.\n\nHorgan: Are you religious in any way?\n\nYudkowsky: No. When you make a mistake, you need to avoid the temptation to go\ndefensive, try to find some way in which you were a little right, look for a\nsilver lining in the cloud. It's much wiser to just say \"Oops\", admit you were\nnot even a little right, swallow the whole bitter pill in one gulp, and get on\nwith your life. That's the attitude humanity should take toward religion.\n\n## Related Stories\n\n  * ### No Spoilers, Please! Why Curiosity Makes Us Patient\n\nAbby Hsiung, Jia-Hou Poh, Scott Huettel & Alison Adcock\n\n  * ### Feeling Angry? Chilling Out Helps More Than Blowing Off Steam\n\nSophie L. Kjaervik, Brad Bushman & The Conversation US\n\n  * ### The Dunning-Kruger Effect Shows that People Don\u2019t Know What They Don\u2019t Know\n\nCorey S. Powell & OpenMind Magazine\n\n  * ### My Synesthesia Transforms Speech into Text I \u2018See\u2019 in My Head\n\nEmily Makowski\n\nHorgan: If you were King of the World, what would top your \u201cTo Do\u201d list?\n\nYudkowsky: I once observed, \"The libertarian test is whether, imagining that\nyou've gained power, your first thought is of the laws you would pass, or the\nlaws you would repeal.\" I'm not an absolute libertarian, since not everything\nI want would be about repealing laws and softening constraints. But when I\nthink of a case like this, I imagine trying to get the world to a condition\nwhere some unemployed person can offer to drive you to work for 20 minutes, be\npaid five dollars, and then nothing else bad happens to them. They don't have\ntheir unemployment insurance phased out, have to register for a business\nlicense, lose their Medicare, be audited, have their lawyer certify compliance\nwith OSHA rules, or whatever. They just have an added $5.\n\nI'd try to get to the point where employing somebody was once again as easy as\nit was in 1900. I think it can make sense nowadays to have some safety nets,\nbut I'd try to construct every safety net such that it didn't disincent or add\npaperwork to that simple event where a person becomes part of the economy\nagain.\n\nI'd try to do all the things smart economists have been yelling about for a\nwhile but that almost no country ever does. Replace investment taxes and\nincome taxes with consumption taxes and land value tax. Replace minimum wages\nwith negative wage taxes. Institute NGDP level targeting regimes at central\nbanks and let the too-big-to-fails go hang. Require loser-pays in patent law\nand put copyright back to 28 years. Eliminate obstacles to housing\nconstruction. Copy and paste from Singapore's healthcare setup. Copy and paste\nfrom Estonia's e-government setup. Try to replace committees and elaborate\nprocess regulations with specific, individual decision-makers whose decisions\nwould be publicly documented and accountable. Run controlled trials of\ndifferent government setups and actually pay attention to the results. I could\ngo on for literally hours.\n\nAll this might not matter directly from the perspective of two hundred million\nyears later. But the goodwill generated by the resulting economic boom might\nstand my government in good stead when I tried to figure out what the heck to\ndo about Artificial Intelligence. The obvious thing, I guess, would be a\nManhattan Project on an island somewhere, with pay competitive with top hedge\nfunds, where people could collaborate on researching parts of the Artificial\nGeneral Intelligence problem without the publication of their work\nautomatically moving us closer to the end of the world. We'd still be working\nto an unknown deadline, and I wouldn't feel relaxed at that point. Unless we\npostulate that I have literally magical powers or an utterly unshakeable\nregime, I don't see how any law I could reasonably decree could delay AI\ntimelines for very long on a planet where computers are already ubiquitous.\n\nAll of this is an impossible thought experiment in the first place, and I see\nroughly zero hope of it ever coming to pass in real life.\n\nHorgan: What\u2019s so great about Bayes\u2019 Theorem?\n\nYudkowsky: For one thing, Bayes's Theorem is incredibly deep. So it's not easy\nto give a brief answer to that.\n\nI might answer that Bayes's Theorem is a kind of Second Law of Thermodynamics\nfor cognition. If you obtain a well-calibrated posterior belief that some\nproposition is 99% probable, whether that proposition is milk being available\nat the supermarket or global warming being anthropogenic, then you must have\nprocessed some combination of sufficiently good priors and sufficiently strong\nevidence. That's not a normative demand, it's a law. In the same way that a\ncar can't run without dissipating entropy, you simply don't get an accurate\nmap of the world without a process that has Bayesian structure buried\nsomewhere inside it, even if the process doesn't explicitly represent\nprobabilities or likelihood ratios. You had strong-enough evidence and a good-\nenough prior or you wouldn't have gotten there.\n\nThank you for signing up!\n\nCheck out our other newsletters\n\nOn a personal level, I think the main inspiration Bayes has to offer us is\njust the fact that there are rules, that there are iron laws that govern\nwhether a mode of thinking works to map reality. Mormons are told that they'll\nknow the truth of the Book of Mormon through feeling a burning sensation in\ntheir hearts. Let's conservatively set the prior probability of the Book of\nMormon at one to a billion (against). We then ask about the likelihood that,\nassuming the Book of Mormon is false, someone would feel a burning sensation\nin their heart after being told to expect one. If you understand Bayes's Rule\nyou can see at once that the improbability of the evidence is not commensurate\nwith the improbability of the hypothesis it's trying to lift. You don't even\nhave to make up numbers to see that the numbers don't add up - as Philip\nTetlock found in his study of superforecasters, superforecasters often know\nBayes's Rule but they rarely make up specific probabilities. On some level,\nit's harder to be fooled if you just realize on a gut level that there is\nmath, that there is some math you'd do to arrive at the exact strength of the\nevidence and whether it sufficed to lift the prior improbability of the\nhypothesis. That you can't just make stuff up and believe what you want to\nbelieve because that doesn't work. [See also \u201cBayes\u2019s Rule: Guide.\u201d]\n\nHorgan: Does the Bayesian-brain hypothesis impress you?\n\nYudkowsky: I think some of the people in that debate may be talking past each\nother. Asking whether the brain is a Bayesian algorithm is like asking whether\na Honda Accord runs on a Carnot heat engine. If you have one person who's\ntrying to say, \"Every car is a thermodynamic process that requires fuel and\ndissipates waste heat\" and the person on the other end hears, \"If you draw a\ndiagram of a Carnot heat engine and show it to a mechanic, they should agree\nthat it looks like the inside of a Honda Accord\" then you are going to have\nsome fireworks.\n\nSome people will also be really excited when they open up the internal\ncombustion engine and find the cylinders and say, \"I bet this converts heat\ninto pressure and helps drive the car forward!\" And they'll be right, but then\nyou're going to find other people saying, \"You're focusing on what's merely a\nsingle component in a much bigger library of car parts; the catalytic\nconverter is also important and that doesn't appear anywhere on your diagram\nof a Carnot heat engine. Why, sometimes we run the air conditioner, which\noperates in the exact opposite way of how you say a heat engine works.\"\n\nI don't think it would come as much of a surprise that I think the people who\nadopt a superior attitude and say, \"You are clearly unfamiliar with modern car\nrepair; you need a toolbox of diverse methods to build a car engine, like\nsparkplugs and catalytic convertors, not just these thermodynamic processes\nyou keep talking about\" are missing a key level of abstraction.\n\nBut if you want to know whether the brain is literally a Bayesian engine, as\nopposed to doing cognitive work whose nature we can understand in a Bayesian\nway, then my guess is \"Heck, no.\" There might be a few excitingly Bayesian\ncylinders in that engine, but a lot more of it is going to look like weird ad-\nhoc seat belts and air conditioning. None of which is going to change the fact\nthat to correctly identify an apple based on sensory evidence, you need to do\nsomething that's ultimately interpretable as resting on an inductive prior\nthat can learn the apple concept, and updating on evidence that distinguishes\napples from nonapples.\n\nHorgan: Can you be too rational?\n\nYudkowsky: You can run into what we call \"The Valley of Bad Rationality.\" If\nyou were previously irrational in multiple ways that balanced or canceled out,\nthen becoming half-rational can leave you worse off than before. Becoming\nincrementally more rational can make you incrementally worse off, if you\nchoose the wrong place to invest your skill points first.\n\nBut I would not recommend to people that they obsess over that possibility too\nmuch. In my experience, people who go around talking about cleverly choosing\nto be irrational strike me as, well, rather nitwits about it, to be frank.\nIt's hard to come up with a realistic non-contrived life situation where you\nknow that it's a good time to be irrational and you don't already know the\ntrue answer. I think in real life, you just tell yourself the truth as best\nyou know it, and don't try to be clever.\n\nOn an entirely separate issue, it's possible that being an ideal Bayesian\nagent is ultimately incompatible with living the life best-lived from a fun-\ntheoretic perspective. But we're a long, long, long way from that being a\nbigger problem than our current self-destructiveness.\n\nHorgan: How does your vision of the Singularity differ from that of Ray\nKurzweil?\n\nYudkowsky:\n\n\\- I don't think you can time AI with Moore's Law. AI is a software problem.\n\n\\- I don't think that humans and machines \"merging\" is a likely source for the\nfirst superhuman intelligences. It took a century after the first cars before\nwe could even begin to put a robotic exoskeleton on a horse, and a real car\nwould still be faster than that.\n\n\\- I don't expect the first strong AIs to be based on algorithms discovered by\nway of neuroscience any more than the first airplanes looked like birds.\n\n\\- I don't think that nano-info-bio \"convergence\" is probable, inevitable,\nwell-defined, or desirable.\n\n\\- I think the changes between 1930 and 1970 were bigger than the changes\nbetween 1970 and 2010.\n\n\\- I buy that productivity is currently stagnating in developed countries.\n\n\\- I think extrapolating a Moore's Law graph of technological progress past\nthe point where you say it predicts smarter-than-human AI is just plain weird.\nSmarter-than-human AI breaks your graphs.\n\n\\- Some analysts, such as Illka Tuomi, claim that Moore's Law broke down in\nthe '00s. I don't particularly disbelieve this.\n\n\\- The only key technological threshold I care about is the one where AI,\nwhich is to say AI software, becomes capable of strong self-improvement. We\nhave no graph of progress toward this threshold and no idea where it lies\n(except that it should not be high above the human level because humans can do\ncomputer science), so it can't be timed by a graph, nor known to be near, nor\nknown to be far. (Ignorance implies a wide credibility interval, not being\ncertain that something is far away.)\n\n\\- I think outcomes are not good by default - I think outcomes can be made\ngood, but this will require hard work that key actors may not have immediate\nincentives to do. Telling people that we're on a default trajectory to great\nand wonderful times is false.\n\n\\- I think that the \"Singularity\" has become a suitcase word with too many\nmutually incompatible meanings and details packed into it, and I've stopped\nusing it.\n\nHorgan: Do you think you have a shot at becoming a superintelligent cyborg?\n\nYudkowsky: The conjunction law of probability theory says that P(A&B) <= P(A)\n- the probability of both A and B happening is less than the probability of A\nalone happening. Experimental conditions that can get humans to assign P(A&B)\n> P(A) for some A&B are said to exhibit the \"conjunction fallacy\" - for\nexample, in 1982, experts at the International Congress on Forecasting\nassigned higher probability to \"A Russian invasion of Poland, and a complete\nbreakdown of diplomatic relations with the Soviet Union\" than a separate group\ndid for \"A complete breakdown of diplomatic relations with the Soviet Union\".\nSimilarly, another group assigned higher probability to \"An earthquake in\nCalifornia causing a flood that causes over a thousand deaths\" than another\ngroup assigned to \"A flood causing over a thousand deaths somewhere in North\nAmerica.\" Even though adding on additional details necessarily makes a story\nless probable, it can make the story sound more plausible. I see understanding\nthis as a kind of Pons Asinorum of serious futurism - the distinction between\ncarefully weighing each and every independent proposition you add to your\nburden, asking if you can support that detail independently of all the rest,\nversus making up a wonderful vivid story.\n\nI mention this as context for my reply, which is, \"Why the heck are you\ntacking on the 'cyborg' detail to that? I don't want to be a cyborg.\" You've\ngot to be careful with tacking on extra details to things.\n\nHorgan: Do you have a shot at immortality?\n\nYudkowsky: What, literal immortality? Literal immortality seems hard. Living\nsignificantly longer than a few trillion years requires us to be wrong about\nthe expected fate of the expanding universe. Living longer than, say, a\ngoogolplex years, requires us to be wrong about the basic character of\nphysical law, not just the details.\n\nEven if some of the wilder speculations are true and it's possible for our\nuniverse to spawn baby universes, that doesn't get us literal immortality. To\nlive significantly past a googolplex years without repeating yourself, you\nneed computing structures containing more than a googol elements, and those\nwon't fit inside a single Hubble volume.\n\nAnd a googolplex is hardly infinity. To paraphrase Martin Gardner, Graham's\nNumber is still relatively small because most finite numbers are very much\nlarger. Look up the fast-growing hierarchy if you really want to have your\nmind blown, well, eternity is longer than that. Only weird and frankly\nterrifying anthropic theories would let you live long enough to gaze, perhaps\nknowingly and perhaps not, upon the halting of the longest-running halting\nTuring machine with 100 states.\n\nBut I'm not sure that living to look upon the 100th Busy Beaver Number feels\nto me like it matters very much on a deep emotional level. I have some\nimaginative sympathy with myself a subjective century from now. That me will\nbe in a position to sympathize with their future self a subjective century\nlater. And maybe somewhere down the line is someone who faces the prospect of\ntheir future self not existing at all, and they might be very sad about that;\nbut I'm not sure I can imagine who that person will be. \"I want to live one\nmore day. Tomorrow I'll still want to live one more day. Therefore I want to\nlive forever, proof by induction on the positive integers.\" Even my desire for\nmerely physical-universe-containable longevity is an abstract want by\ninduction; it's not that I can actually imagine myself a trillion years later.\n\nHorgan: I\u2019ve described the Singularity as an \u201cescapist, pseudoscientific\u201d\nfantasy that distracts us from climate change, war, inequality and other\nserious problems. Why am I wrong?\n\nYudkowsky: Because you're trying to forecast empirical facts by\npsychoanalyzing people. This never works.\n\nSuppose we get to the point where there's an AI smart enough to do the same\nkind of work that humans do in making the AI smarter; it can tweak itself, it\ncan do computer science, it can invent new algorithms. It can self-improve.\nWhat happens after that - does it become even smarter, see even more\nimprovements, and rapidly gain capability up to some very high limit? Or does\nnothing much exciting happen?\n\nIt could be that, (A), self-improvements of size delta tend to make the AI\nsufficiently smarter that it can go back and find new potential self-\nimprovements of size k*delta and that k is greater than 1, and this continues\nfor a sufficiently extended regime that there's a rapid cascade of self-\nimprovements leading up to superintelligence; what I. J. Good called the\nintelligence explosion. Or it could be that, (B), k is less than one or that\nall regimes like this are small and don't lead up to superintelligence, or\nthat superintelligence is impossible, and you get a fizzle instead of an\nexplosion. Which is true, A or B? If you actually built an AI at some\nparticular level of intelligence and it actually tried to do that, something\nwould actually happen out there in the empirical real world, and that event\nwould be determined by background facts about the landscape of algorithms and\nattainable improvements.\n\nYou can't get solid information about that event by psychoanalyzing people.\nIt's exactly the sort of thing that Bayes's Theorem tells us is the equivalent\nof trying to run a car without fuel. Some people will be escapist regardless\nof the true values on the hidden variables of computer science, so observing\nsome people being escapist isn't strong evidence, even if it might make you\nfeel like you want to disaffiliate with a belief or something.\n\nThere is a misapprehension, I think, of the nature of rationality, which is to\nthink that it's rational to believe \"there are no closet goblins\" because\nbelief in closet goblins is foolish, immature, outdated, the sort of thing\nthat stupid people believe. The true principle is that you go in your closet\nand look. So that in possible universes where there are closet goblins, you\nend up believing in closet goblins, and in universes with no closet goblins,\nyou end up disbelieving in closet goblins.\n\nIt's difficult but not impossible to try to sneak peeks through the crack of\nthe closet door, to ask the question, \"What would look different in the\nuniverse now if you couldn't get sustained returns on cognitive investment\nlater, such that an AI trying to improve itself would fizzle? What other facts\nshould we observe in a universe like that?\"\n\nSo you have people who say, for example, that we'll only be able to improve AI\nup to the human level because we're human ourselves, and then we won't be able\nto push an AI past that. I think that if this is how the universe looks in\ngeneral, then we should also observe, e.g., diminishing returns on investment\nin hardware and software for computer chess past the human level, which we did\nnot in fact observe. Also, natural selection shouldn't have been able to\nconstruct humans, and Einstein's mother must have been one heck of a\nphysicist, etcetera.\n\nYou have people who say, for example, that it should require more and more\ntweaking to get smarter algorithms and that human intelligence is around the\nlimit. But this doesn't square up with the anthropological record of human\nintelligence; we can know that there were not diminishing returns to brain\ntweaks and mutations producing improved cognitive power. We know this because\npopulation genetics says that mutations with very low statistical returns will\nnot evolve to fixation at all.\n\nAnd hominids definitely didn't need exponentially vaster brains than\nchimpanzees. And John von Neumann didn't have a head exponentially vaster than\nthe head of an average human.\n\nAnd on a sheerly pragmatic level, human axons transmit information at around a\nmillionth of the speed of light, even when it comes to heat dissipation each\nsynaptic operation in the brain consumes around a million times the minimum\nheat dissipation for an irreversible binary operation at 300 Kelvin, and so\non. Why think the brain's software is closer to optimal than the hardware?\nHuman intelligence is privileged mainly by being the least possible level of\nintelligence that suffices to construct a computer; if it were possible to\nconstruct a computer with less intelligence, we'd be having this conversation\nat that level of intelligence instead.\n\nBut this is not a simple debate and for a detailed consideration I'd point\npeople at an old informal paper of mine, \"Intelligence Explosion\nMicroeconomics\", which is unfortunately probably still the best source out\nthere. But these are the type of questions one must ask to try to use our\ncurrently accessible evidence to reason about whether or not we'll see what's\ncolloquially termed an \"AI FOOM\" - whether there's an extended regime where\ndelta improvement in cognition, reinvested into self-optimization, yields\ngreater than delta further improvements.\n\nAs for your question about opportunity costs:\n\nThere is a conceivable world where there is no intelligence explosion and no\nsuperintelligence. Or where, a related but logically distinct proposition, the\ntricks that machine learning experts will inevitably build up for controlling\ninfrahuman AIs carry over pretty well to the human-equivalent and superhuman\nregime. Or where moral internalism is true and therefore all sufficiently\nadvanced AIs are inevitably nice. In conceivable worlds like that, all the\nwork and worry of the Machine Intelligence Research Institute comes to nothing\nand was never necessary in the first place, representing some lost number of\nmosquito nets that could otherwise have been bought by the Against Malaria\nFoundation.\n\nThere's also a conceivable world where you work hard and fight malaria, where\nyou work hard and keep the carbon emissions to not much worse than they are\nalready (or use geoengineering to mitigate mistakes already made). And then it\nends up making no difference because your civilization failed to solve the AI\nalignment problem, and all the children you saved with those malaria nets grew\nup only to be killed by nanomachines in their sleep. (Vivid detail warning! I\ndon't actually know what the final hours will be like and whether nanomachines\nwill be involved. But if we're happy to visualize what it's like to put a\nmosquito net over a bed, and then we refuse to ever visualize in concrete\ndetail what it's like for our civilization to fail AI alignment, that can also\nlead us astray.)\n\nI think that people who try to do thought-out philanthropy, e.g., Holden\nKarnofsky of Givewell, would unhesitatingly agree that these are both\nconceivable worlds we prefer not to enter. The question is just which of these\ntwo worlds is more probable as the one we should avoid. And again, the central\nprinciple of rationality is not to disbelieve in goblins because goblins are\nfoolish and low-prestige, or to believe in goblins because they are exciting\nor beautiful. The central principle of rationality is to figure out which\nobservational signs and logical validities can distinguish which of these two\nconceivable worlds is the metaphorical equivalent of believing in goblins.\n\nI think it's the first world that's improbable and the second one that's\nprobable. I'm aware that in trying to convince people of that, I'm swimming\nuphill against a sense of eternal normality - the sense that this transient\nand temporary civilization of ours that has existed for only a few decades,\nthat this species of ours that has existed for only an eyeblink of\nevolutionary and geological time, is all that makes sense and shall surely\nlast forever. But given that I do think the first conceivable world is just a\nfond dream, it should be clear why I don't think we should ignore a problem\nwe'll predictably have to panic about later. The mission of the Machine\nIntelligence Research Institute is to do today that research which, 30 years\nfrom now, people will desperately wish had begun 30 years earlier.\n\nHorgan: Does your wife Brienne believe in the Singularity?\n\nBrienne replies: \"If someone asked me whether I 'believed in the singularity',\nI'd raise an eyebrow and ask them if they 'believed in' robotic trucking. It's\nkind of a weird question. I don't know a lot about what the first fleet of\nrobotic cargo trucks will be like, or how long they'll take to completely\nreplace contemporary ground shipping. And if there were a culturally loaded\nsuitcase term 'robotruckism' that included a lot of specific technological\nclaims along with whole economic and sociological paradigms, I'd be hesitant\nto say I 'believed in' driverless trucks. I confidently forecast that\ndriverless ground shipping will replace contemporary human-operated ground\nshipping, because that's just obviously where we're headed if nothing really\nweird happens. Similarly, I confidently forecast an intelligence explosion.\nThat's obviously where we're headed if nothing really weird happens. I'm less\nsure of the other items in the 'singularity' suitcase.\" (Eliezer adds: \u201cTo\navoid prejudicing the result, Brienne composed her reply without seeing my\nother answers. We're just well-matched.\u201d)\n\nHorgan: Can we create superintelligences without knowing how our brains work?\n\nYudkowsky: Only in the sense that you can make airplanes without knowing how a\nbird flies. You don't need to be an expert in bird biology, but at the same\ntime, it's difficult to know enough to build an airplane without realizing\nsome high-level notion of how a bird might glide or push down air with its\nwings. That's why I write about human rationality in the first place - if you\npush your grasp on machine intelligence past a certain point, you can't help\nbut start having ideas about how humans could think better too.\n\nHorgan: What would superintelligences want? Will they have anything resembling\nsexual desire?\n\nYudkowsky: Think of an enormous space of possibilities, a giant\nmultidimensional sphere. This is Mind Design Space, the set of possible\ncognitive algorithms. Imagine that somewhere near the bottom of that sphere is\na little tiny dot representing all the humans who ever lived - it's a tiny dot\nbecause all humans have basically the same brain design, with a cerebral\ncortex, a prefrontal cortex, a cerebellum, a thalamus, and so on. It's\nconserved even relative to chimpanzee brain design. Some of us are weird in\nlittle ways, you could say it's a spiky dot, but the spikes are on the same\ntiny scale as the dot itself; no matter how neuroatypical you are, you aren't\nrunning on a different cortical algorithm.\n\nAsking \"what would superintelligences want\" is a Wrong Question.\nSuperintelligences are not this weird tribe of people who live across the\nwater with fascinating exotic customs. \"Artificial Intelligence\" is just a\nname for the entire space of possibilities outside the tiny human dot. With\nsufficient knowledge you might be able to reach into that space of\npossibilities and deliberately pull out an AI that wanted things that had a\ncompact description in human wanting-language, but that wouldn't be because\nthis is a kind of thing that those exotic superintelligence people naturally\nwant, it would be because you managed to pinpoint one part of the design\nspace.\n\nWhen it comes to pursuing things like matter and energy, we may tentatively\nexpect partial but not total convergence - it seems like there should be many,\nmany possible superintelligences that would instrumentally want matter and\nenergy in order to serve terminal preferences of tremendous variety. But even\nthere, everything is subject to defeat by special cases. If you don't want to\nget disassembled for spare atoms, you can, if you understand the design space\nwell enough, reach in and pull out a particular machine intelligence that\ndoesn't want to hurt you.\n\nSo the answer to your second question about sexual desire is that if you knew\nexactly what you were doing and if you had solved the general problem of\nbuilding AIs that stably want particular things as they self-improve and if\nyou had solved the general problem of pinpointing an AI's utility functions at\nthings that seem deceptively straightforward to human intuitions, and you'd\nsolved an even harder problem of building an AI using the particular sort of\narchitecture where 'being horny' or 'sex makes me happy' makes sense in the\nfirst place, then you could perhaps make an AI that had been told to look at\nhumans, model what humans want, pick out the part of the model that was sexual\ndesire, and then want and experience that thing too.\n\nYou could also, if you had a sufficiently good understanding of organic\nbiology and aerodynamics, build an airplane that could mate with birds.\n\nI don't think this would have been a smart thing for the Wright Brothers to\ntry to do in the early days. There would have been absolutely no point.\n\nIt does seem a lot wiser to figure out how to reach into the design space and\npull out a special case of AI that will lack the default instrumental\npreference to disassemble us for spare atoms.\n\nHorgan: I like to think superintelligent beings would be nonviolent, because\nthey will realize that violence is stupid. Am I naive?\n\nYudkowsky: I think so. As David Hume might have told you, you're making a type\nerror by trying to apply the 'stupidity' predicate to an agent's terminal\nvalues or utility function. Acts, choices, policies can be stupid given some\nset of preferences over final states of the world. If you happen to be an\nagent that has meta-preferences you haven't fully computed, you might have a\nplatform on which to stand and call particular guesses at the derived object-\nlevel preferences as 'stupid'.\n\nA paperclip maximizer is not making a computational error by having a\npreference order on outcomes that prefers outcomes with more paperclips in\nthem. It is not standing from within your own preference framework and\nchoosing blatantly mistaken acts, nor is it standing within your meta-\npreference framework and making mistakes about what to prefer. It is computing\nthe answer to a different question than the question that you are asking when\nyou ask, \"What should I do?\" A paperclip maximizer just outputs the action\nleading to the greatest number of expected paperclips.\n\nThe fatal scenario is an AI that neither loves you nor hates you, because\nyou're still made of atoms that it can use for something else. Game theory,\nand issues like cooperation in the Prisoner's Dilemma, don't emerge in all\npossible cases. In particular, they don't emerge when something is\nsufficiently more powerful than you that it can disassemble you for spare\natoms whether you try to press Cooperate or Defect. Past that threshold,\neither you solved the problem of making something that didn't want to hurt\nyou, or else you've already lost.\n\nHorgan: Will superintelligences solve the \u201chard problem\u201d of consciousness?\n\nYudkowsky: Yes, and in retrospect the answer will look embarrassingly obvious\nfrom our perspective.\n\nHorgan: Will superintelligences possess free will?\n\nYudkowsky: Yes, but they won't have the illusion of free will.\n\nHorgan: What\u2019s your utopia?\n\nYudkowsky: I refer your readers to my nonfiction Fun Theory Sequence, since I\nhave not as yet succeeded in writing any novel set in a fun-theoretically\noptimal world.\n\nFurther Reading:\n\nBayes's Theorem: What's the Big Deal?\n\nAre Brains Bayesian?\n\nCan the Singularity Solve the Valentine's Day Dilemma?\n\nDo Big New Brain Projects Make Sense When We Don't Even Know the \"Neural\nCode\"?\n\nTwo More Reasons Why Big Brain Projects Are Premature.\n\nArtificial brains are imminent... not!\n\nWhat\u2019s the Biggest Science News? We\u2019re Still Human, for Ill or Good.\n\nCan We Improve Predictions? Q&A with Philip \"Superforecasting\" Tetlock.\n\nChristof Koch on Free Will, the Singularity and the Quest to Crack\nConsciousness.\n\nThe Many Minds of Marvin Minsky (R.I.P.)\n\nRights & Permissions\n\nJohn Horgan, who has written for Scientific American since 1986, comments on\nscience on his free online journal Cross-Check. He has also posted his books\nMind-Body Problems and My Quantum Experiment online. Horgan teaches at Stevens\nInstitute of Technology.\n\nMore by John Horgan\n\n# Popular Stories\n\nAstronomy February 2, 2024\n\n##\n\nHere Are the Best Places to View the 2024 Total Solar Eclipse\n\nWeather predictions and population statistics show the best spots to see the\ntotal solar eclipse over North America this April\n\nKatie Peek\n\nClimate Change April 4, 2024\n\n##\n\nGeoengineering Test Quietly Launches Salt Crystals into Atmosphere\n\nA solar geoengineering experiment in San Francisco could lead to brighter\nclouds that reflect sunlight. The risks are numerous\n\nCorbin Hiar, E&E News\n\nConservation April 4, 2024\n\n##\n\nDecades-old Cans of Salmon Reveal Changes in Ocean Health\n\nResearchers used tinned fish to reconstruct parasitic population change,\ngiving new meaning to the phrase \u201copening a can of worms\u201d\n\nRachel Nuwer\n\nPublic Health April 2, 2024\n\n##\n\nAre Your Solar Eclipse Glasses Fake? Here\u2019s How to Check\n\nYou\u2019re going to want to guard your eyes from the sun using legitimate\nprotective gear\n\nSarah Sloat\n\nAstronomy April 8, 2024\n\n##\n\nWhat Were the Red Dots around the Total Solar Eclipse?\n\nDuring the total solar eclipse, skywatchers saw ruby-colored prominences\nsticking out of the moon's shadow. Here's the science of those red dots\n\nMeghan Bartels\n\nAstronomy March 29, 2024\n\n##\n\nSee Where Future U.S. Eclipses Will be Visible\n\nJust how rare are total solar eclipses, which require a perfect alignment of\nthe sun and moon?\n\nShuyao Xiao\n\n## Expand Your World with Science\n\nLearn and share the most exciting discoveries, innovations and ideas shaping\nour world today.\n\nSubscribeSign up for our newslettersSee the latest storiesRead the latest\nissue\n\nFollow Us:\n\n  * Return & Refund Policy\n  * About\n  * Press Room\n  * Accessibility Statement\n\n  * FAQs\n  * Contact Us\n  * International Editions\n\n  * Advertise\n  * SA Custom Media\n  * Terms of Use\n\n  * Privacy Policy\n  * California Consumer Privacy Statement\n  * Use of cookies/Do not sell my data\n\nScientific American is part of Springer Nature, which owns or has commercial\nrelations with thousands of scientific publications (many of them can be found\nat www.springernature.com/us). Scientific American maintains a strict policy\nof editorial independence in reporting developments in science to our readers.\n\n\u00a9 2024 SCIENTIFIC AMERICAN, A DIVISION OF SPRINGER NATURE AMERICA, INC. ALL\nRIGHTS RESERVED.\n\n## We Value Your Privacy\n\nWe use cookies to enhance site navigation, analyze site usage & personalize\ncontent to provide social media features and to improve our marketing efforts.\nWe also share information about your use of our site with our social media,\nadvertising and analytics partners. By clicking \u201cAccept All Cookies\u201d, you\nagree to the storing of cookies on your device for the described purposes.\nView Our Privacy Policy\n\n## Privacy Preference Center\n\n  * ### Your Privacy\n\n  * ### Performance Cookies\n\n  * ### Social Media Cookies\n\n  * ### Targeting Cookies\n\n  * ### Strictly Necessary Cookies\n\n  * ### Functional Cookies\n\n  * ### Targeting (1st Party)\n\n  * ### Google & IAB TCF 2 Purposes of Processing\n\n  * ### Targeting (3rd Party)\n\n#### Your Privacy\n\nWhen you visit any website, it may store or retrieve information on your\nbrowser, mostly in the form of cookies. This information might be about you,\nyour preferences or your device and is mostly used to make the site work as\nyou expect it to. The information does not usually directly identify you, but\nit can give you a more personalized web experience. Because we respect your\nright to privacy, you can choose not to allow some types of cookies. Click on\nthe different category headings to find out more and change our default\nsettings. However, blocking some types of cookies may impact your experience\nof the site and the services we are able to offer.\n\n#### Performance Cookies\n\nThese cookies allow us to count visits and traffic sources so we can measure\nand improve the performance of our site. They help us to know which pages are\nthe most and least popular and see how visitors move around the site. All\ninformation these cookies collect is aggregated and therefore anonymous. If\nyou do not allow these cookies we will not know when you have visited our\nsite, and will not be able to monitor its performance.\n\n#### Social Media Cookies\n\nThese cookies are set by a range of social media services that we have added\nto the site to enable you to share our content with your friends and networks.\nThey are capable of tracking your browser across other sites and building up a\nprofile of your interests. This may impact the content and messages you see on\nother websites you visit. If you do not allow these cookies you may not be\nable to use or see these sharing tools.\n\n#### Targeting Cookies\n\nThese cookies may be set through our site by our advertising partners. They\nmay be used by those companies to build a profile of your interests and show\nyou relevant adverts on other sites. They do not store directly personal\ninformation, but are based on uniquely identifying your browser and internet\ndevice. If you do not allow these cookies, you will experience less targeted\nadvertising.\n\n#### Strictly Necessary Cookies\n\nAlways Active\n\nThese cookies are necessary for the website to function and cannot be switched\noff in our systems. They are usually only set in response to actions made by\nyou which amount to a request for services, such as setting your privacy\npreferences, logging in or filling in forms. You can set your browser to block\nor alert you about these cookies, but some parts of the site will not then\nwork. These cookies do not store any personally identifiable information.\n\n#### Functional Cookies\n\nThese cookies enable the website to provide enhanced functionality and\npersonalisation. They may be set by us or by third party providers whose\nservices we have added to our pages. If you do not allow these cookies then\nsome or all of these services may not function properly.\n\n#### Targeting (1st Party)\n\nThese cookies may be set through our site by ourselves. They may be used by\nourselves to build a profile of your interests and show you relevant content\nor adverts on our sites. They do not store directly personal information, but\nare based on uniquely identifying your browser and internet device. If you do\nnot allow these cookies, you may experience less personalised content and/or\nadvertising.\n\n#### Google & IAB TCF 2 Purposes of Processing\n\nAllowing third-party ad tracking and third-party ad serving through Google and\nother vendors to occur. Please see more information on Google Ads\n\n#### Targeting (3rd Party)\n\nThese cookies may be set through our site by our advertising partners. They\nmay be used by those companies to build a profile of your interests and show\nyou relevant adverts on other sites. They do not store directly personal\ninformation, but are based on uniquely identifying your browser and internet\ndevice. If you do not allow these cookies, you will experience less targeted\nadvertising.\n\n### Back\n\nConsent Leg.Interest\n\nlabel\n\nlabel\n\nlabel\n\n  *     * Name\n\ncookie name\n\nlabel\n\n", "frontpage": false}
