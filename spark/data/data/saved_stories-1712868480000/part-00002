{"aid": "40002783", "title": "Zephyr-orpo-141B-A35B: Mixtral 8x22B fine-tune by HuggingFace", "url": "https://huggingface.co/HuggingFaceH4/zephyr-orpo-141b-A35b-v0.1", "domain": "huggingface.co", "votes": 2, "user": "tosh", "posted_at": "2024-04-11 14:46:42", "comments": 0, "source_title": "HuggingFaceH4/zephyr-orpo-141b-A35b-v0.1 \u00b7 Hugging Face", "source_text": "HuggingFaceH4/zephyr-orpo-141b-A35b-v0.1 \u00b7 Hugging Face\n\nHugging Face\n\n#\n\nHuggingFaceH4\n\n/\n\nzephyr-orpo-141b-A35b-v0.1\n\nText Generation Transformers TensorBoard Safetensors\n\nmixtral trl orpo generated_from_trainer conversational Inference Endpoints\ntext-generation-inference\n\nModel card Files Files and versions Metrics Training metrics Community\n\nEdit model card\n\n# Model Card for Zephyr 141B-A35B\n\nZephyr is a series of language models that are trained to act as helpful\nassistants. Zephyr 141B-A35B is the latest model in the series, and is a fine-\ntuned version of mistral-community/Mixtral-8x22B-v0.1 that was trained using a\nnovel alignment algorithm called Odds Ratio Preference Optimization (ORPO)\nwith 7k instances for 1.3 hours on 4 nodes of 8 x H100s. ORPO does not require\nan SFT step to achieve high performance and is thus much more computationally\nefficient than methods like DPO and PPO. To train Zephyr-141B-A35B, we used\nthe argilla/distilabel-capybara-dpo-7k-binarized preference dataset, which\nconsists of synthetic, high-quality, multi-turn preferences that have been\nscored via LLMs.\n\n> This model was trained collaboratively between Argilla, KAIST, and Hugging\n> Face\n\n## Model Details\n\n### Model Description\n\n  * Model type: A Mixture of Experts (MoE) model with 141B total parameters and 35B active parameters. Fine-tuned on a mix of publicly available, synthetic datasets.\n  * Language(s) (NLP): Primarily English.\n  * License: Apache 2.0\n  * Finetuned from model: mistral-community/Mixtral-8x22B-v0.1\n\n### Model Sources\n\n  * Repository: https://github.com/huggingface/alignment-handbook\n  * Dataset: https://huggingface.co/datasets/argilla/distilabel-capybara-dpo-7k-binarized\n\n## Performance\n\nZephyr 141B-A35B was trained to test the effectiveness of ORPO at scale and\nthe underlying dataset contains a mix of general chat capabilities. It\nachieves strong performance on chat benchmarks like MT Bench and IFEval. The\nscores reported below were obtained using the LightEval evaluation suite and\neach prompt has been formatted with the model's corresponding chat template to\nsimulate real-world usage. This is why some scores may differ from those\nreported in technical reports or on the Open LLM Leaderboard.\n\nModel| MT Bench| IFEval| BBH| AGIEval  \n---|---|---|---|---  \nzephyr-orpo-141b-A35b-v0.1| 8.17| 65.06| 58.96| 44.16  \ndatabricks/dbrx-instruct| 8.26| 52.13| 48.50| 41.16  \nmistralai/Mixtral-8x7B-Instruct-v0.1| 8.30| 55.08| 45.31| 47.68  \n  \n## Intended uses & limitations\n\nThe model was fine-tuned on a blend of chat, code, math, and reasoning data.\nHere's how you can run the model using the pipeline() function from \ud83e\udd17\nTransformers:\n\n    \n    \n    # pip install 'transformers>=4.39.3' # pip install accelerate import torch from transformers import pipeline pipe = pipeline( \"text-generation\", model=\"HuggingFaceH4/zephyr-orpo-141b-A35b-v0.1\", device_map=\"auto\", torch_dtype=torch.bfloat16, ) messages = [ { \"role\": \"system\", \"content\": \"You are Zephyr, a helpful assistant.\", }, {\"role\": \"user\", \"content\": \"Explain how Mixture of Experts work in language a child would understand.\"}, ] outputs = pipe( messages, max_new_tokens=512, do_sample=True, temperature=0.7, top_k=50, top_p=0.95, ) print(outputs[0][\"generated_text\"][-1][\"content\"])\n\n## Bias, Risks, and Limitations\n\nZephyr 141B-A35B has not been aligned to human preferences for safety within\nthe RLHF phase or deployed with in-the-loop filtering of responses like\nChatGPT, so the model can produce problematic outputs (especially when\nprompted to do so). It is also unknown what the size and composition of the\ncorpus was used to train the base model (mistral-\ncommunity/Mixtral-8x22B-v0.1), however it is likely to have included a mix of\nWeb data and technical sources like books and code. See the Falcon 180B model\ncard for an example of this.\n\n## Training procedure\n\n### Training hyperparameters\n\nThe following hyperparameters were used during training:\n\n  * learning_rate: 5e-06\n  * train_batch_size: 1\n  * eval_batch_size: 8\n  * seed: 42\n  * distributed_type: multi-GPU\n  * num_devices: 32\n  * total_train_batch_size: 32\n  * total_eval_batch_size: 256\n  * optimizer: Adam with betas=(0.9,0.999) and epsilon=1e-08\n  * lr_scheduler_type: inverse_sqrt\n  * lr_scheduler_warmup_steps: 100\n  * num_epochs: 3\n\n### Training results\n\n### Framework versions\n\n  * Transformers 4.39.3\n  * Pytorch 2.1.2+cu121\n  * Datasets 2.18.0\n  * Tokenizers 0.15.1\n\n## Citation\n\nIf you find Zephyr 141B-A35B is useful in your work, please cite the ORPO\npaper:\n\n    \n    \n    @misc{hong2024orpo, title={ORPO: Monolithic Preference Optimization without Reference Model}, author={Jiwoo Hong and Noah Lee and James Thorne}, year={2024}, eprint={2403.07691}, archivePrefix={arXiv}, primaryClass={cs.CL} }\n\nYou may also wish to cite the creators of this model:\n\n    \n    \n    @misc{zephyr_141b, author = {Alvaro Bartolome and Jiwoo Hong and Noah Lee and Lewis Tunstall}, title = {Zephyr 141B A35B}, year = {2024}, publisher = {Hugging Face}, journal = {Hugging Face repository}, howpublished = {\\url{https://huggingface.co/HuggingFaceH4/zephyr-orpo-141b-A35b-v0.1}} }\n\nDownloads last month\n\n    0\n\nSafetensors\n\nModel size\n\n141B params\n\nTensor type\n\nBF16\n\n\u00b7\n\n##\n\nFinetuned from\n\n## Dataset used to train HuggingFaceH4/zephyr-orpo-141b-A35b-v0.1\n\n## Collection including HuggingFaceH4/zephyr-orpo-141b-A35b-v0.1\n\n#### Zephyr ORPO\n\nCollection\n\nModels and datasets to align LLMs with Odds Ratio Preference Optimisation\n(ORPO). Recipes here: https://github.com/huggingface/alignment-handbook \u2022 3\nitems \u2022 Updated about 1 hour ago \u2022 5\n\n## Evaluation results\n\nMetadata error: specify a dataset to view leaderboard\n\n", "frontpage": false}
