{"aid": "40044293", "title": "Near-duplicate detection critical for RAG apps at scale", "url": "https://www.aryn.ai/post/near-duplicate-detection-in-sycamore-what-is-it-good-for", "domain": "aryn.ai", "votes": 3, "user": "jonfritz", "posted_at": "2024-04-15 18:54:30", "comments": 0, "source_title": "Near-Duplicate Detection in Sycamore: What Is It Good For?", "source_text": "Near-Duplicate Detection in Sycamore: What Is It Good For?\n\ntop of page\n\nGet Started\n\n  * Alex Meyer\n  *     * Apr 2\n\n# Near-Duplicate Detection in Sycamore: What Is It Good For?\n\nRecently, we added near-duplicate-detection (NDD) support to Sycamore. Back in\nthe late 1990s, in the competition between various web search engines vying\nfor market dominance, NDD was an important technique for improving relevance.\nSince Andrei Broder published his landmark paper in 2000, NDD has found its\nway into many aspects of computing. As the world turns toward generative AI,\nwe believe that NDD continues to deliver value.\n\nOne application that NDD can improve is retrieval-augmented generation (RAG).\nThis refers to the use of a large language model (LLM) to answer questions\nbeyond the scope of what the LLM was trained upon. RAG starts by retrieving\ndocuments from a traditional search engine. Then, it sends the query, along\nwith a limited number of top-scoring documents as context, to an LLM. The\nresult is an answer synthesized by the LLM from the documents. Because the\neffective context size is limited, it\u2019s important for us to fill it with as\nrich a set of documents as possible. This is where near-duplicate removal is\nhelpful.\n\nIn order to show how NDD improves retrieval and RAG in the real world, we\nchose a readily-available dataset from data.gov. It contains marketing\nagreements between credit card companies and colleges for branded cards from\n2011 through 2019. As one might imagine, a lot of these documents are rather\nsimilar, both across years and across cards. Here are some examples:\n\nGenerally, NDD consists of two phases: tagging each document with a \u201csketch\u201d,\nand eliminating or grouping documents based on sketch comparisons. These days,\nmany search systems, including Sycamore, actually cut documents into smaller\nchunks in order to improve relevance. One reason is that individual topics can\nget lost in longer documents. Another is that vector embedding models tend to\noperate on a limited number of tokens. In RAG, longer documents can overload\nan LLM\u2019s context size. We\u2019ll use the words \u201cdocument\u201d and \u201cchunk\u201d\ninterchangeably.\n\nIn Sycamore, we use a transform called Sketcher to generate a sketch for each\ndocument. The current implementation uses hashing to generate a set of numbers\ncalled a \u201cshingle\u201d. The shingle is an attribute of the document and can be\nstored in OpenSearch during ingestion.\n\nWhile it\u2019s possible to drop near-duplicate documents at ingest-time, there are\nproblems with this approach that impact recall and relevance. We can sidestep\nthese problems if we eliminate (or collapse) near-duplicates at query-time. To\nillustrate the problem, consider the two documents below, represented as\nkeywords. Let\u2019s assume they meet the threshold to be considered duplicates. If\nwe use an ingestion-time transform like SketchUniquify to de-duplicate the\nincoming batch of documents, one of the documents will win and one will lose.\nLet\u2019s assume Amex wins by virtue of alphabetical order.\n\nVenn diagram of documents with words in common\n\nNow, the index contains the one document \u201cAmex penalty fee interest rate\narbitration\u201d. If we try to query for \u201cVisa interest rate\u201d, there are zero\ndocuments matching the query. This is poor relevance, and entirely self-\ninflicted, as we had the matching document and dropped it.\n\nIt\u2019s better to de-duplicate the result set at query-time. In this case,\nretrieval would have yielded only the Visa document and no NDD would have come\ninto play at all.\n\n## A Duplicative Example\n\nBefore we get into RAG, let\u2019s take a brief detour. We want to illustrate the\nvalue of NDD in pure text retrieval. We used Sycamore to ingest the credit\ncard PDF files into OpenSearch. In total, there were 1911 files from which we\ncould extract meaningful text without resorting to optical character\nrecognition (OCR). Our ingestion used the thenlper/gte-small embedding model\nfor vector indexing. We also used the BAAI/bge-reranker-base model to do query\nreranking.\n\nThen, we decided to research the various types of arbitration language these\ndocuments contained. We used OpenSearch to ask \u201cSummarize the rules for\nsummary judgment and penalties in arbitration.\u201d Our top ten results looked\nlike the following.\n\n\"Should an arbitrator refuse or be unable to proceed with arbitration\nproceedings as called  \n---  \n\"Should an arbitrator refuse or be unable to proceed with arbitration\nproceedings as called  \n\"The Parties shall cooperate with each other in causing the arbitration to be\nheld in\\n(c)  \n\"The Parties shall cooperate with each other in causing the arbitration to be\nheld in\\n(c)  \n\"The Parties shall cooperate with each other in causing the arbitration to be\nheld in\\n(c)  \n\"Should an arbitrator refuse or be unable to proceed with arbitration\nproceedings as called  \n\"(1) Section 14, in no event will either party be responsible to the other for\nany incidental  \n\"(1) Section 14, in no event will either party be responsible to the other for\nany incidental  \n\" Except as otherwise provided in this Section, the arbitration shall be\nPursuant to the Code  \n\" Except as otherwise provided in this Section, the arbitration shall be\nPursuant to the Code  \n  \nResults like these are problematic, as many of the lines are the same. At\nfirst glance, it appears we really got only 4 useful chunks even though we\nwanted 10. Getting 10 different chunks may involve manually sifting through\nmany more results. Further, the task of de-duplicating the result set by eye\nis error-prone and biased toward the front of each chunk. Eventually our eyes\nwill glaze over.\n\n## NDD to the Rescue\n\nFor this demonstration, we used a cool piece of technology developed at Aryn.\nIt\u2019s called a remote search processor and it allows one to implement search\npipeline logic in Python outside OpenSearch. We wrote a bit of code to drop\nnear-duplicates from a result set as part of query processing. For any pair of\nnear-duplicate documents, the higher-scoring document survives.\n\nUsing our fancy pipeline and the same question as above, we were able to get\nthe following results:\n\n\"The Parties shall cooperate with each other in causing the arbitration to be\nheld in\\n(c)  \n---  \n\" Except as otherwise provided in this Section, the arbitration shall be\nPursuant to the Code  \n\"Should an arbitrator refuse or be unable to proceed with arbitration\nproceedings as called  \n\" The Arbitrator or Arbitration Panel is 3pecifically authorized in proceeding\npursuant to  \n\"If the parties are unable to resolve any Dispute as contemplated above, such\nDispute shall  \n\"Any award rendered by the arbitrator or Arbitration Panel will be final,\nconclusive and  \n\"At the time of granting or denying a motion of summary judgment as provided\nfor in (e)  \n\"If the amount of the controversy set forth in either the claim or\ncounterclaim is equal  \n\"Agreement (including any breach, termination, validity or enforceability of\nany provision  \n\"Should an arbitrator refuse or be unable to proceed with arbiLration\nproceedings as called  \n  \nWhat we see here is 9 different chunks, which is much better than 4. Looking\ncarefully at the chunks, it seems there is a spurious difference caused by\noptical character recognition (OCR) errors. There could also be chunking\nboundary differences. These are outside the scope of NDD to solve. That said,\nthe aggressiveness of NDD can be tuned at query-time.\n\nFor the above example, the remote \"dedup-response\" processor was configured\nwith a distance \"threshold\" of 0.4, which dropped 73% of the 100 chunks that\nwe retrieved. Thresholds closer to 1.0 drop more documents, but the rate of\nfalse positives increases and the likelihood of retrieving sufficient relevant\ndocuments decreases. In the example below, a distance threshold of 0.15\ndropped 60% of chunks.\n\n\"The Parties shall cooperate with each other in causing the arbitration to be\nheld in\\n(c)  \n---  \n\" Except as otherwise provided in this Section, the arbitration shall be\nPursuant to the Code  \n\"Should an arbitrator refuse or be unable to proceed with arbitration\nproceedings as called  \n\" The Arbitrator or Arbitration Panel is 3pecifically authorized in proceeding\npursuant to  \n\"The arbitration hearing shall be held in such neutral location as the parties\nmay mutually  \n\"Should an arbitrator refuse or be unable to proceed with arbitration\nproceedings as called  \n\"The arbitration hearini! shall be held in such neutral location as the\nparties may MUILIally  \n\"(b) To initiate arbitration, the Disputing Party shall notify the other party\nin writing (the  \n\"If the parties are unable to resolve any Dispute as contemplated above, such\nDispute shall  \n\"Any award rendered by the arbitrator or Arbitration Panel will be final,\nconclusive and  \n  \nOf these top-10 chunks, 8 are unique. Compared to the previous result, these\nresults are missing the chunk that starts with \u201cAgreement\u201d.\n\n## Consequences for RAG\n\nAs mentioned above, RAG uses LLMs to generate answers from provided documents.\nOne of the challenges with RAG is the limited effective \u201ccontext\u201d, which\nrefers to how much data can be given to the LLM in order to generate the\nsummary. If the context is 10 documents, it makes sense that having 9 unique\ndocuments will lead to a more comprehensive summary than 4 unique documents.\nFor this reason, NDD is a valuable complement to RAG.\n\nTo show this, we ran some RAG queries on the college credit card data. The\nfirst question is still \u201cSummarize the rules for summary judgment and\npenalties in arbitration.\u201d that we used above. We used a basic RAG pipeline\nwith GPT-4, with a context size of 5 chunks to ensure the data fits in the LLM\ncontext window. Without using NDD, the RAG pipeline yields this answer:\n\n    \n    \n    \"In arbitration, the arbitrator or Arbitration Panel grants or denies a motion of summary judgment within fifteen days after the arbitration hearing concludes. They prepare and distribute a written document detailing their findings of facts and conclusions of law, including reasons for granting or denying any award [1]. The arbitrator or Arbitration Panel can issue monetary sanctions against a party if they unreasonably delay the proceeding [1]. The arbitrator has no authority to award punitive damages [3]. Any award rendered by the arbitrator or Arbitration Panel is final and binding [1].\"\n\nThis answer misses many details needed to answer the question. It does mention\nthat an arbitrator writes a document with facts and conclusion, a fifteen day\nwindow, a penalty for delaying the proceeding, and awards are final and\nbinding. But, this information doesn't really get deeper into the rules for\nthe actual arbitration process.\n\nEnabling NDD in the same RAG pipeline adds 3 unique documents in the context\n(removing 3 duplicates), and provides the LLM more information from which to\ngenerate an answer. Here is the result:\n\n    \n    \n    \"The rules of arbitration include: cooperation between parties to make the process efficient and expeditious [1]; initiation of arbitration by notifying the other party in writing, describing the dispute, stating the claim amount, and specifying the requested relief [2]; replacement of an arbitrator who refuses or is unable to proceed [3]; the arbitrator or panel's responsibility to schedule discovery and procedural steps promptly [3]; the finality of any award rendered by the arbitrator or panel [3]; each party bearing a pro rata share of all fees, costs, and expenses of the arbitrators [3]; and the application of the Federal Rules of Evidence to the arbitration hearing [4]. The arbitration process is subject to the confidentiality provisions of the agreement [1][4].\"\n\nNot only is this a more comprehensive answer, but it reads better as a\nsummary. Beyond the finality of award, it explains what an arbitration goal\nis, describes the details needed to initiate it, the costs shared by each\nparty, the application of Federal Rules of Evidence, and a confidentiality\nprovision. Overall, the second answer includes more rules and a better\ndescription of the arbitration process.\n\nAnother example using the same dataset is the question \u201cSummarize the highest\nand lowest interest rates, penalties, and fees.\u201d The basic answer looks like\nthis, which seems like it\u2019s for one specific credit card:\n\n    \n    \n    \"The highest and lowest interest rates for purchases and cash advances range from 9.99% to 17.99% when you open your account, then vary with the market based on the Prime Rate plus a margin [1]. The minimum monthly payment is $25.00 [1]. Penalties include a late payment fee of $10.00 or 10% of the outstanding balance, whichever is less, and a return check fee of $4.50 [1]. ATM transactions incur a 50-cent charge for each deposit or withdrawal exceeding 15 a month, a 25-cent charge on each balance inquiry, and $4 for each ATM deposit adjustment [2]. International transactions may incur a 1% fee [2].\"\n\nWith NDD, after dropping 77% of chunks, we get the answer below, which more\nsuccinctly answers the actual question and also includes the $50 emergency\nreplacement fee which is missing above.\n\n    \n    \n    \"The highest interest rate is 17.99% and the lowest is 9.99% [1]. The highest penalty fee is $50 for an emergency replacement card, while the lowest is $4.50 for a return check fee [1]. The highest fee is a 1% foreign transaction fee [3], and the lowest is a 25-cent charge for each balance inquiry [2].\"\n\n## Conclusion\n\nQuery-time near-duplicate detection can make query results simultaneously more\ninformative, useful, and pleasing to users. In the first examples above, the\nnumber of good results in the top 10 more than doubled. The later examples\nshow that RAG applications benefit from more good results. The NDD process\nconsists of ingestion-time and query-time processing. Both can be tuned in\nvarious ways if defaults aren\u2019t suitable to a particular dataset. The\nprocessing overhead involved is not large, especially considering the use of\nembeddings and vector search in a typical search stack.\n\nWe have provided an NDD Jupyter notebook which is available from within the\nSycamore Docker containers. The notebook provides instructions and code to\nwalk through an NDD example inspired by this blog post.\n\n## Recent Posts\n\nSee All\n\nWhen RAG runs out of steam, use schema extraction and analytics with Sycamore\n\nRAG is a band-aid; we need LLM-powered Unstructured Analytics \u2014 LUnA\n\nAnswer questions on tables with Sycamore's table extraction transform\n\nContact\n\n756 California St., Unit A\n\nMountain View, CA 94041\n\nContact us: info@aryn.ai\n\nQuick Links\n\nBlog\n\nLinkedIn\n\nDocs\n\nGitHub\n\n\u00a9 2024 by Aryn.\n\nbottom of page\n\n", "frontpage": false}
