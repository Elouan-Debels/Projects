{"aid": "40055122", "title": "We solved RevenueCat's biggest challenges on data ingestion into Snowflake", "url": "https://www.revenuecat.com/blog/engineering/data-ingestion-snowflake/", "domain": "revenuecat.com", "votes": 1, "user": "elwatto", "posted_at": "2024-04-16 17:57:34", "comments": 0, "source_title": "Solving RevenueCat's data ingestion challenges into Snowflake", "source_text": "Solving RevenueCat's data ingestion challenges into Snowflake\n\nRevenueCat\n\nLog InSign Up\n\nEngineering\n\n# How we solved RevenueCat\u2019s biggest challenges on data ingestion into\nSnowflake\n\nChallenges, solutions, and insights from optimizing our data ingestion\npipeline.\n\nJes\u00fas S\u00e1nchez\n\nPublishedApril 15, 2024\n\nAt RevenueCat, we replicate our production data to Snowflake (a cloud-based\ndata warehouse) for data exports, reports, charts and other needs.\n\nOur infrastructure is pretty standard:\n\n  * Primary production DB: Aurora Postgres.\n  * Debezium to capture the change events from the WAL.\n  * A schema registry (Apicurio) keeps track of schema changes, versioning, etc.\n  * Kafka to process the events (Avro) and write them into S3. We chose parquet files (in S3) to achieve good compression and performance.\n  * A system to get the changes in S3 into Snowflake and consolidate the create/update/delete events into DML. We don\u2019t keep the original change stream in Snowflake since we have that in S3, our data lake, and Snowflake the lakehouse.\n  * We use Fivetran for third-party API-based sources, and data is pushed directly to the consolidated layer. We skip the data lake layer because it\u2019s convenient.\n  * Once the data is consolidated, we use dbt to transform and enrich data between layers.\n  * Airflow mainly orchestrates dbt runs, as we don\u2019t process data in Airflow.\n  * Data is queried on Snowflake (using proper clustering columns configuration) or consumed as data exports.\n\nThe diagram (simplified) looks more or less like this:\n\nIn this blog, I\u2019ll delve into the intricacies of our data management\npractices, specifically focusing on the journey of our data from its origins\nto its final destination in Snowflake. We\u2019ll explore the challenges we faced,\nthe solutions we devised, and the insights we gained through the process of\noptimizing our data ingestion pipeline.\n\n## Ingestion process: from S3 to Snowflake\n\nWe used some commercial solutions to replicate the Aurora Postgresql WAL to\nSnowflake for a while, but we ran into many problems. TOAST fields were not\nproperly handled, schema evolution was flaky, our volume was too large,\ninconsistencies were hard to diagnose, and the general opaqueness of the\nsystem was a big problem for us.\n\nWe decided to experiment with replacing it with a more standard Debezium +\nKafka + S3 to capture the changes and find our own way to load from S3 to\nSnowflake. Why didn\u2019t we choose the Snowflake Connector for Kafka? We wanted\nto build a data lake with agnostic parquet files that can be used for other\npurposes or tools such as Spark.\n\nAfter testing multiple alternatives, we finally developed a quite novel model\nleveraging Snowflake external tables and streams.\n\nSnowflake has the concept of the external table, just a virtual table pointing\nto S3 files. Snowflake runs the computing, but the storage is provided by AWS\n(or any other cloud provider). It\u2019s a \u201cread-only table\u201d (not even a table;\nit\u2019s just a way of querying parquet), so you can\u2019t perform DDL operations, as\nSnowflake only holds file metadata and gives you access using SQL as an\ninterface.\n\nYou can create a Snowflake stream on an external table, which is how Snowflake\nnames its CDC feature. The fancy thing here is that Snowflake supports\n\u201cinsert-only streams on external tables,\u201d which basically maintains the state\nof all the new rows coming from any new file uploaded to S3. This was huge for\nus, as we saved many engineering cycles on building an ingestion system that\nkeeps track of all the files ingested (or not) in our lake house.\n\nAs the incoming rows from files in S3 already have the operation column from\nDebezium (insert/update/delete), a Snowflake task just picks the batch and\napplies the merge to the consolidated table accordingly.\n\nAs a result, our ingestion pipeline only uses Snowflake to collect, track, and\nconsolidate changes coming from S3 into our consolidated layer.\n\nWith hundreds of tables, configuring the external tables, streams, and merge\nqueries is quite complex and error-prone, so we created a tool to configure\nand manage all these. This tool proved critical in allowing us to evolve and\nexperiment with alternative pipelines in an agile and flexible way.\n\nThe historical data in S3, viewable as external tables, provides a great tool\nto inspect the previous row versions, for debugging or troubleshooting data\nconsistency issues. It is like having git history for your full database, or\nin classic database terms, an easy to query copy of the entire WAL since then\nwe rolled out the tool \ud83d\ude42\n\nThis is a simplified version of the ingestion pipeline:\n\n## The problem of continuous consolidation\n\nOne unique aspect of RevenueCat\u2019s data is that many tables are heavily\nupdated; we don\u2019t have much immutable, event-like data.\n\nWhen we started tracking ingestion execution times, we realized that multi-\nbillion row tables often took multiple hours to consolidate (merge) in\nSnowflake, even using a large warehouse. This was not acceptable in terms of\ncost and latency.\n\nMost OLAP engines typically struggle with updates. To understand why, you can\nsimply look at some of the most common data lake file formats, such as\nparquet. The data is split into file chunks and heavily compressed. To change\njust one row, you must read the whole row set, modify the single row, write it\nback in full, and recompress it. Other file formats are row-based, like Avro,\nbut mostly meant for optimizing partial/selective reads; modifying a row still\nrequires reading and writing back the whole file. If you have a huge table\nwith many chunks and want to perform updates that are scattered across the\ntable, you will end up causing most of the chunks to be rewritten. Inserts, on\nthe other hand, just mean creating a single new chunk file with many rows.\nDeletes could be costly if actually performed, but OLAP typically implements\nthem as \u201cinserted\u201d tombstone marks or look-aside tables.\n\nWhile we don\u2019t know what storage mechanism Snowflake uses internally,\nempirical evidence shows that it follows the same pattern. Inserts and deletes\n(with low concurrency) are fast, but the cost of scattered updates grows\nlinearly with the table size.\n\nThere weren\u2019t easy solutions:\n\n  * Changing how we use our data and moving to a more immutable, event-based model was not a viable short-term solution we could adopt.\n  * Changing all derived pipelines to use the replication stream data without merging will speed up ingestion but slow all queries on the massive unmerged data.\n\nFortunately, we came up with one idea, using a hybrid solution, that could fix\nboth problems, and we set on to test if it would work for us.\n\n## Consolidate once a day while having fresh data\n\nLet\u2019s say that we have a table named \u201cusers.\u201d This table contains ~15B rows,\nand we process ~35M changes daily (inserts, updates, and deletes). Half of\nthem are updates, which makes ~70k updates per hour. You can try to\nconsolidate changes hourly, but in our tests, it can take up to 2 hours to\nfinish, making the pipeline fall behind.\n\nThe ingestion warehouse was running for the whole day, representing more than\n1\u20443 of the total cost in Snowflake. While using an even larger cluster helped,\nwe clearly had a problem. The resources it took to consolidate the table were\na factor of its size. The time to update 70k, 200k, or 2M rows was virtually\nthe same and mostly related to the full table size.\n\nThe only solution was to consolidate less often, but how could that be done\nwhile maintaining a low replication lag?\n\nWe started researching, implementing, and testing what we named a low-latency\nview. The idea is simple: if the number of changes is orders of magnitude\nlower than the actual table size (0.1% for the example table), we can create a\nview that yields an on-the-fly consolidated version of the data without\nactually consolidating the last small batch of changes in the storage.\n\nConceptually, we split the table into two subsets:\n\n  1. The consolidated table, with one single physical row for each logical row, contains just the last version of the row (as far as the consolidation has seen)\n  2. The ingestion table, with the set of incoming changes, as create, insert, and delete events, might contain several occurrences of the row for each logical row.\n\nThe view abstracts these sources, allowing users of the view to see the last\nversion for each row. Most often, the logical row comes from the consolidated\ntable. But if it has been modified recently, it will come from the last\nversion seen in the ingestion table.\n\nIn practice, it is just a UNION, but as always, the devil is in the details,\nand this UNION of these two subsets is more complex than it seems. For\nexample, TOAST fields have to be specially cased, and it is hard to ensure\nthat the execution plan will be optimal. It doesn\u2019t matter if the small\ningestion table is full-scanned for all queries. However, the consolidated\ntables have billions of rows, and the query predicates need to be properly\npushed down to this part of the query execution to minimize the partitions\nscanned and the number of bytes sent over the network during the Snowflake\ncompute.\n\nThe new ingestion process is as follows:\n\n  1. Changes coming from S3 are tracked using an external table and a stream (same as before).\n  2. A Snowflake task runs every X minutes, consumes the Snowflake stream, and inserts data into the ingestion table. This is what we call the \u201ccontinuous ingestion process.\u201d This ingestion is blazing fast; it inserts a small number of rows into an equally small table, which takes seconds in an extra-small cluster.\n  3. The consolidation process runs once a day (can be tuned depending on how fast the ingestion table grows). It merges the changes from the ingestion table into the consolidated table and then empties the ingestion table, ensuring that it does not balloon in size. This consolidation rewrites the underlying table, requiring a large cluster (like before), but now it runs only once daily.\n  4. As an additional benefit, auto-reclustering can be enabled in the consolidated tables, improving query efficiency. Before, this wasn\u2019t practical, as updates constantly modified the consolidated tables, and the clustering process was always running, never finishing, and substantially increasing the bill.\n\nDuring the process, the low-latency view always shows an up-to-date data\nversion. Visually, both processes look like this:\n\nSo, now we have the view and a mechanism that orchestrates the continuous\ningestion and consolidation processes.\n\nThe next challenge was coordinating this work for the more than 100 tables\nbeing ingested in the lakehouse. Luckily, as mentioned already, we spent\nengineering time building good tooling to configure and manage the different\npieces of the ingestion pipeline. Thanks to this tool, generating all the\nviews, tasks, and SQL code related to the continuous and consolidation\nprocesses takes a few minutes. It allowed us to test, iterate, and refine at\ngreat speed. In contrast, a manual approach would have taken us days, if not\nweeks, to assemble.\n\n## Conclusion and takeaways\n\nThe team is happy with the results as we achieved the goals we aimed for:\n\n  * Reduce the ingestion latency: We went from 2-3 hours to (potentially) a few minutes. In reality, we are consuming data from the stream every 30 minutes. We could run a task every minute to get the freshest data, but it does not make sense as our extract, transform, and load (ETL) processes only run every 1-4 hours.\n  * Reduce cost: We reduced the cost by approximately 75%. Previously, our large warehouse was operational nearly all day. Now, we\u2019ve streamlined it to activate just once daily for the consolidation process. This image speaks for itself:\n\nKey takeaways from this work include:\n\n  * Consolidating change events (inserts, deletes, updates) into tables can turn very slow and costly if you have a lot of updates. insert/delete events are easier to consolidate. Hybrid approaches like those described here can help ingest update-heavy change streams with low latency and cost.\n  * Invest in tooling that helps you move faster in the long run. We would not have been able to complete this project if we hadn\u2019t invested early in the tooling and took the easy path to configure things manually or in a declarative (but manual) format.\n  * Whenever possible, try to design your data to be immutable, denormalized, and event-like, as it simplifies data operations, replication, storage, and consolidation\n  * Debezium and Kafka are a solid tandem for replication needs; they outperformed commercial solutions in cost, reliability, and flexibility.\n  * Snowflake external tables and streams can simplify data ingestion from S3, hiding many complexities by providing a reliable, high-level stream of changes.\n  * Having a data lake of the replication stream provides massive value for investigations, both product and data replication issues. We chose to keep this in S3 but (for convenience) queryable as Snowflake external tables. Remember to compact and partition the files into larger ones so that the data compresses better and is accessed faster with fewer S3 requests. Doing so allows indefinite retention and lowers the bill when accessing historical data. We used Spark to compact the small files created by Kafka into ~250MB parquet files (as per Snowflake doc recommendation).\n  * Dealing with Postgres TOASTed values from Debezium introduced an unexpected amount of complexity. If you are using Postgresql and logical replication, this post is worth a read\n\n## In-App Subscriptions Made Easy\n\nSee why thousands of the world's tops apps use RevenueCat to power in-app\npurchases, analyze subscription data, and grow revenue on iOS, Android, and\nthe web.\n\nGET STARTED FOR FREETALK TO SALES\n\n## Related posts\n\nHow RevenueCat handles errors in Google Play\u2019s Billing Library\n\nEngineering\n\n### How RevenueCat handles errors in Google Play\u2019s Billing Library\n\nLessons on Billing Library error handling from RevenueCat's engineering team\n\nCesar de la Vega\n\nApril 5, 2024\n\nUse cases for RevenueCat Billing\n\nEngineering\n\n### Use cases for RevenueCat Billing\n\n3 ways you can use the new RevenueCat Billing beta today.\n\nCharlie Chapman\n\nMarch 21, 2024\n\nVision Pro apps powered by RevenueCat: What\u2019s available on launch day\n\nEngineering\n\n### Vision Pro apps powered by RevenueCat: What\u2019s available on launch day\n\nOver 160 of the first visionOS apps are powered by RevenueCat\n\nCharlie Chapman\n\nFebruary 2, 2024\n\n## Want to see how RevenueCat can help?\n\nTalk to salesTry It For Free\n\n> \u201cRevenueCat enables us to have one single source of truth for subscriptions\n> and revenue data.\u201d\n\nOlivier Lemari\u00e9, Photoroom\n\nRead Case Study\n\nRevenueCat\n\nRevenueCat\n\nC 2024 RevenueCat\n\n", "frontpage": false}
