{"aid": "40040856", "title": "Autonomous hardware diagnostics and recovery at scale", "url": "https://blog.cloudflare.com/autonomous-hardware-diagnostics-and-recovery-at-scale", "domain": "cloudflare.com", "votes": 2, "user": "yarapavan", "posted_at": "2024-04-15 14:19:22", "comments": 0, "source_title": "Autonomous hardware diagnostics and recovery at scale", "source_text": "Autonomous hardware diagnostics and recovery at scale\n\nGet Started Free|Contact Sales\n\n## The Cloudflare Blog\n\nSubscribe to receive notifications of new posts:\n\n# Autonomous hardware diagnostics and recovery at scale\n\n03/25/2024\n\n  * Jet Mariscal\n\n  * Aakash Shah\n\n  * Yilin Xiong\n\n8 min read\n\nCloudflare\u2019s global network spans more than 310 cities in over 120 countries.\nThat means thousands of servers geographically spread across different data\ncenters, running services that protect and accelerate our customer\u2019s Internet\napplications. Operating hardware at such a scale means that hardware can break\nanywhere and at any time. In such cases, our systems are engineered such that\nthese failures cause little to no impact. However, detecting and managing\nserver failure at scale requires automation. This blog aims to provide\ninsights into the difficulties involved in handling broken servers and how we\nwere able to simplify the process through automation.\n\n## Challenges dealing with broken servers\n\nWhen a server is found to have faulty hardware and needs to be removed from\nproduction, it is considered broken and its state is set to Repair in the\ninternal database where server status is tracked. In the past, our Data Center\nOperations team were essentially left to troubleshoot and diagnose broken\nservers on their own. They had to go through laborious tasks like performing\nqueries to locate and repair servers, conducting diagnostics, reviewing\nresults, evaluating if a server can be restored to production, and creating\nthe necessary tickets for re-enabling servers and executing operations to put\nthem back in production. Such effort can take hours for a single server alone,\nand can easily consume an engineer\u2019s entire day.\n\nAs you can see, addressing server repairs was a labor-intensive process\nperformed manually, Additionally, a lot of these servers remained powered on\nwithin the racks, wasting energy. With our fleet expanding rapidly, the\nattention of Data Center Operations is primarily devoted to supporting this\ngrowth, leaving less time to handle servers in need of repair.\n\nIt was clear that our infrastructure was growing too fast for us to be able to\nhandle repairs and recovery, so we had to find a better way to handle these\nsorts of inefficiencies in our operations. This would allow our engineers to\nfocus on the growth of our footprint while not abandoning repair and recovery\n\u2013 after all, these are still huge CapEx investments and wasted capacity that\notherwise would have been fully utilized.\n\n## Using automation as an autonomous system\n\nAs members of the Infrastructure Software Systems and Automation team at\nCloudflare, we primarily work on building tools and automation that help\nreduce excess work in order to ease the pressure on our operations teams,\nincrease productivity, and enable people to execute operations with the\nhighest efficiency.\n\nOur team continuously strives to challenge our existing processes and systems,\nfinding ways we can evolve them and make significant improvements \u2013 one of\nwhich is to build not just a typical automated system but an autonomous one.\nBuilding autonomous automations means creating systems that can operate\nindependently, without the need for constant human intervention or oversight \u2013\na perfect example of this is Phoenix.\n\n## Introducing Phoenix\n\nPhoenix is an autonomous diagnostics and recovery automation that runs at\nregular intervals to discover Cloudflare data centers with servers that are\nbroken, performing diagnostics on detection, recovering those that pass\ndiagnostics by re-provisioning, and ultimately re-enabling those that have\nsuccessfully been re-provisioned in the safest and most unobtrusive way\npossible \u2013 all without requiring any human intervention! Should a server fail\nat any point in the process, Phoenix will take care of updating relevant\ntickets, even pinpointing the cause of the failure, and reverting the state of\nthe server accordingly when needed \u2013 again, all without any human\nintervention!\n\nThe image below illustrates the whole process:\n\nTo better understand exactly how Phoenix works, let\u2019s dive into some details\nabout its core functionality.\n\n### Discovery\n\nDiscovery runs at a regular interval of 30 minutes, selecting a maximum of two\nCloudflare data centers that have broken or repair state servers in its fleet,\nwhich are all configurable depending on business and operational needs,\nagainst which it can immediately execute diagnostics. At this rate, Phoenix is\nable to discover and operate on all broken servers in the fleet in about 3\ndays. On each run, it also detects data centers that may have broken servers\nalready queued for recovery, and takes care of ensuring that the Recovery\nphase is executed immediately.\n\n### Diagnostics\n\nDiagnostics takes care of running various tests across the broken servers of a\nselected data center in a single run, verifying viability of the hardware\ncomponents, and identifying the candidates for recovery.\n\nA diagnostic operation includes running the following:\n\n  * Out-of-Band connectivity check This check determines the reachability of a device via out-of-band network. We employ IPMI (Intelligent Platform Management Interface) to ensure proper physical connectivity and accessibility of devices. This allows for effective monitoring and management of hardware components, enhancing overall system reliability and performance. Only devices that pass this check can progress to the Node Acceptance Testing phase.\n  * Node Acceptance Tests We leverage an existing internally-built tool called INAT (Integrated Node Acceptance Testing) that runs various tests suites/cases (Hardware Validation, Performance, etc.).\n\nFor every server that needs to be diagnosed, Phoenix will send relevant system\ninstructions to have it boot into a custom Linux boot image, internally called\nINAT-image. Built into this image are the various tests that need to run when\nthe server boots up, publishing the results to an internal resource in both\nhuman-readable (HTML) and machine-readable (JSON) formats, with the latter\nconsumed and interpreted by Phoenix. Upon completion of the boot diagnostics,\nthe server is powered off again to ensure it is not wasting energy.\n\nOur node acceptance tests encompass a range of evaluations, including but not\nlimited to benchmark testing, CPU/Memory/Storage checks, drive wiping, and\nvarious other assessments. Look out for an upcoming in-depth blog post\ncovering INAT.\n\nA summarized diagnostics result is immediately added to the tracking ticket,\nincluding pinpointing the exact cause of a failure.\n\n### Recovery\n\nRecovery executes what we call an expansion operation, which in its first\nphase will provision the servers that pass diagnostics. The second phase is to\nre-enable the successfully provisioned servers back to production, where only\nthose that have been re-enabled successfully will start receiving production\ntraffic again.\n\nOnce the diagnostics are passed and the broken servers move on towards the\nfirst phase of recovery, we change their statuses from Repair to Pending\nProvision. If the servers don't fully recover, for example, because there are\nserver configuration errors or issues enabling services, Phoenix assesses the\nsituation. In such cases, it returns those servers to the Repair state for\nadditional evaluation. Additionally, if the diagnostics indicate that the\nservers need any faulty components replaced, then Phoenix notifies our Data\nCenter operation team for manual repairs as required, ensuring that the server\nis not repeatedly selected until the required part replacement is completed.\nThis ensures any necessary human intervention can be applied promptly, making\nthe server ready for Phoenix to rediscover in its next iteration.\n\nAn autonomous recovery operation requires infusing intelligence into the\nautomated system so that we can fully trust that it\u2019s able to execute an\nexpansion operation in the safest way possible and handle situations on its\nown without any human interventions. To do this, we\u2019ve made sure Phoenix is\nautomation-aware \u2013 this means that it knows when there are other automations\nexecuting certain operations such as expansions, and will only execute an\nexpansion when there are no ongoing provisioning operations in the target data\ncenter. This ability to execute only when it\u2019s safe to do so is to ensure that\nthe recovery operation will not interfere with any other ongoing operations in\nthe data center. We\u2019ve also adjusted its tolerance with faulty hardware \u2013 this\nmeans it\u2019s able to gracefully deal with misbehaving servers by letting these\nquickly drop out of the recovery candidate list upon misbehavior that prevents\nblocking the operation.\n\n### Visibility\n\nWhile our autonomous system, Phoenix, seamlessly handles operations without\nhuman intervention, it doesn't mean we sacrifice visibility. Transparency is a\nkey feature of Phoenix. It meticulously logs every operation, from executing\ntasks to providing progress updates, and shares this information in\ncommunication channels like chat rooms and Jira tickets. This ensures a clear\nunderstanding of what Phoenix is doing at all times.\n\nTracking of actions taken by automation as well as the state transitions of a\nserver keeps us in the loop and gives us a better understanding of what these\nactions were and when they were executed, essentially giving us valuable\ninsights that will help us improve not only the system but our processes as\nwell. Having this operational data allows us to generate dashboards that let\nvarious teams monitor automation activities and measure their success. We are\nable to generate dashboards to guide business decisions and even answer common\noperational questions related to repair and recovery.\n\n## Balancing automation and empathy: Error Budgets\n\nWhen we launched Phoenix, we were well aware that not every broken server can\nbe re-enabled and successfully returned to production, and more importantly,\nthere's no 100% guarantee that a recovered server will be as stable as the\nones with no repair history \u2013 there's a risk that these servers could fail and\nend up back in Repair status again.\n\nAlthough there's no guarantee that these recovered servers won't fail again,\ncausing additional work for SRE\u2019s due to the monitoring alerts that get\ntriggered, what we can guarantee is that Phoenix immediately stops recoveries\nwithout any human intervention if a certain number of failures for a server\nare reached in a given time window \u2013 this is where we applied the concept of\nan Error Budget.\n\nThe Error Budget is the amount of error that automation can accumulate over a\ncertain period of time before our SRE\u2019s start being unhappy due to the\nexcessive server failures or unreliability of the system. It is empathy\nembedded in automation.\n\nIn the figure above, the y-axis represents the error budget. In this context,\nthe error budget applies to the number of recovered servers that failed and\nwere moved back to Repair state again. The x-axis represents the time unit\nallocated to the error budget \u2013 in this case, 24 hours. To ensure that Phoenix\nis strict enough in mitigating possible issues, we divide the time unit into\nthree consecutive buckets of the same duration \u2013 representing the three\n\u201cfollow the sun\u201d SRE shifts in a day. With this, Phoenix can only execute\nrecoveries if the number of server failures is no more than 2. Additionally,\nPhoenix will also have to compensate succeeding time buckets by deducting the\nerror budget of any excess failures in a given time bucket.\n\nPhoenix will immediately stop recoveries if it exhausts its error budget\nprematurely. In this context, prematurely means before the end of the time\nunit for which the error budget was granted. Regardless of the error budget\ndepletion rate within a time unit, the error budget is fully replenished at\nthe beginning of each time unit, meaning the budget resets every day.\n\nThe Error Budget has helped us define and manage our tolerance for hardware\nfailures without causing significant harm to the system or too much noise for\nSREs, and gave us opportunities to improve our diagnostics system. It provides\na common incentive that allows both the Infrastructure Engineering and SRE\nteams to focus on finding the right balance between innovation and\nreliability.\n\n## Where we go from here\n\nWith Phoenix, we\u2019ve not only witnessed the significant and far-reaching\npotential of having an autonomous automated system in our infrastructure,\nwe\u2019re actually reaping its benefits as well. It provides a win-win situation\nby successfully recovering hardware and ensuring that broken devices are\npowered off, thus preventing them from consuming unnecessary power while being\nidle in our racks. This not only reduces energy wastage but also contributes\nto sustainability efforts and cost savings. Automated processes that operate\nindependently have not only freed our colleagues on various Infrastructure\nteams from doing mundane and repetitive tasks, allowing them to focus more on\nareas where they can use their skill sets for more interesting and productive\nwork, but have also led us to evolving our old processes for handling hardware\nfailures and repairs, making us much more efficient than ever.\n\nAutonomous automation is a reality that is now beginning to shape the future\nof how we are building better and smarter systems here at Cloudflare, and we\nwill continue to invest engineering time for these initiatives.\n\nA huge thank you to Elvin Tan for his awesome work on INAT, and to Graeme,\nDarrel and David for INAT\u2019s continuous improvements.\n\nWe protect entire corporate networks, help customers build Internet-scale\napplications efficiently, accelerate any website or Internet application, ward\noff DDoS attacks, keep hackers at bay, and can help you on your journey to\nZero Trust.\n\nVisit 1.1.1.1 from any device to get started with our free app that makes your\nInternet faster and safer.\n\nTo learn more about our mission to help build a better Internet, start here.\nIf you're looking for a new career direction, check out our open positions.\n\nDiscuss on Hacker News\n\nHardwareAutomation\n\nFollow on X\n\nCloudflare|@cloudflare\n\nRelated posts\n\nMarch 19, 2024 1:00 PM\n\n## Redefining fleet management at Cloudflare\n\nGrowing pains were inevitable given the sheer pace of Cloudflare\u2019s growth.\nProcesses around server provisioning, maintenance windows, repairs, and\ndiagnostics reporting were reaching their limits...\n\nBy\n\n  * Ryan De Lap,\n\n  * Donald Gary,\n\n  * Dwayn Matthies\n\nHardware\n\nDecember 07, 2023 2:00 PM\n\n## A look inside the Cloudflare ML Ops platform\n\nTo help our team continue to innovate efficiently, our MLOps effort has\ncollaborated with Cloudflare\u2019s data scientists to implement the following best\npractices...\n\nBy\n\n  * Keith Adler,\n\n  * Rio Harapan Pangihutan\n\nAI, Data, Developers, Machine Learning, MLops, Hardware\n\nDecember 01, 2023 6:45 PM\n\n## Cloudflare Gen 12 Server: bigger, better, cooler in a 2U1N form factor\n\nCloudflare Gen 12 Compute servers are moving to 2U1N form factor to optimize\nthe thermal design to accommodate both high-power CPUs (>350W) and GPUs\neffectively while maintaining performance and reliability...\n\nBy\n\n  * JQ Lau,\n\n  * Syona Sarma\n\nAMD, Hardware, Cloudflare Network\n\nOctober 16, 2023 5:53 PM\n\n## Introducing the Project Argus Datacenter-ready Secure Control Module design\nspecification\n\nThe DC-SCM (Datacenter-ready Secure Control Module) decouples server\nmanagement from the server motherboard. It provides flexibility to implement\nmultiple server management and security solutions with the same server\nmotherboard design...\n\nBy\n\n  * Xiaomin Shen,\n\n  * JQ Lau\n\nHardware, Security\n\n  * Sales\n  * Enterprise Sales\n  * Become a Partner\n\nContact Sales:\n\n+1 (888) 993-5273\n\n  * Getting Started\n  * Pricing\n  * Case Studies\n  * White Papers\n  * Webinars\n  * Learning Center\n\n  * Community\n  * Community Hub\n  * Project Galileo\n  * Athenian Project\n  * Cloudflare TV\n\n  * Developers\n  * Developer Hub\n  * Developers Discord\n  * Cloudflare Workers\n  * Integrations\n\n  * Tools\n  * Cloudflare Radar\n  * Speed Test\n  * Is BGP Safe Yet?\n  * RPKI Toolkit\n  * Certificate Transparency\n\n  * Support\n  * Support\n  * Cloudflare Status\n  * Compliance\n  * GDPR\n\n  * Company\n  * About Cloudflare\n  * Our Team\n  * Press\n  * Analysts\n  * Careers\n  * Logo\n  * Network Map\n\n\u00a9 2024 Cloudflare, Inc. | Privacy Policy | Terms of Use |Cookie Preferences | Trust & Safety | Trademark\n\n", "frontpage": false}
