{"aid": "40006554", "title": "LLM Powered Development for VSCode", "url": "https://github.com/huggingface/llm-vscode", "domain": "github.com/huggingface", "votes": 1, "user": "tosh", "posted_at": "2024-04-11 20:37:13", "comments": 0, "source_title": "GitHub - huggingface/llm-vscode: LLM powered development for VSCode", "source_text": "GitHub - huggingface/llm-vscode: LLM powered development for VSCode\n\nSkip to content\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nhuggingface / llm-vscode Public\n\n  * Notifications\n  * Fork 117\n  * Star 1.1k\n\nLLM powered development for VSCode\n\n### License\n\nApache-2.0 license\n\n1.1k stars 117 forks Branches Tags Activity\n\nStar\n\nNotifications\n\n# huggingface/llm-vscode\n\nThis commit does not belong to any branch on this repository, and may belong\nto a fork outside of the repository.\n\n5 Branches\n\n1 Tags\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\nHennerMadd template for deepseek coder (#123)c9e1faf \u00b7\n\n## History\n\n22 Commits  \n  \n### .github/workflows\n\n|\n\n### .github/workflows\n\n| fix(ci): remove i686-windows bin (#130)  \n  \n### .vscode\n\n|\n\n### .vscode\n\n| feat: integrate llm-ls (#70)  \n  \n### src\n\n|\n\n### src\n\n| add template for deepseek coder (#123)  \n  \n### .eslintignore\n\n|\n\n### .eslintignore\n\n| chore: squash all history  \n  \n### .eslintrc.js\n\n|\n\n### .eslintrc.js\n\n| chore: squash all history  \n  \n### .eslintrc.json\n\n|\n\n### .eslintrc.json\n\n| feat: integrate llm-ls (#70)  \n  \n### .gitignore\n\n|\n\n### .gitignore\n\n| chore: squash all history  \n  \n### .prettierignore\n\n|\n\n### .prettierignore\n\n| chore: squash all history  \n  \n### .prettierrc.json\n\n|\n\n### .prettierrc.json\n\n| chore: squash all history  \n  \n### .vscodeignore\n\n|\n\n### .vscodeignore\n\n| feat: integrate llm-ls (#70)  \n  \n### LICENSE\n\n|\n\n### LICENSE\n\n| feat: integrate llm-ls (#70)  \n  \n### README.md\n\n|\n\n### README.md\n\n| feat: 0.5.x (#129)  \n  \n### package-lock.json\n\n|\n\n### package-lock.json\n\n| feat: 0.5.x (#129)  \n  \n### package.json\n\n|\n\n### package.json\n\n| add template for deepseek coder (#123)  \n  \n### small_logo.png\n\n|\n\n### small_logo.png\n\n| chore: squash all history  \n  \n### tsconfig.json\n\n|\n\n### tsconfig.json\n\n| feat: integrate llm-ls (#70)  \n  \n## Repository files navigation\n\n# LLM powered development for VSCode\n\nllm-vscode is an extension for all things LLM. It uses llm-ls as its backend.\n\nWe also have extensions for:\n\n  * neovim\n  * jupyter\n  * intellij\n\nPreviously huggingface-vscode.\n\nNote\n\nWhen using the Inference API, you will probably encounter some limitations.\nSubscribe to the PRO plan to avoid getting rate limited in the free tier.\n\nhttps://huggingface.co/pricing#pro\n\n## Features\n\n### Code completion\n\nThis plugin supports \"ghost-text\" code completion, \u00e0 la Copilot.\n\n### Choose your model\n\nRequests for code generation are made via an HTTP request.\n\nYou can use the Hugging Face Inference API or your own HTTP endpoint, provided\nit adheres to the APIs listed in backend.\n\nThe list of officially supported models is located in the config template\nsection.\n\n### Always fit within the context window\n\nThe prompt sent to the model will always be sized to fit within the context\nwindow, with the number of tokens determined using tokenizers.\n\n### Code attribution\n\nHit Cmd+shift+a to check if the generated code is in The Stack. This is a\nrapid first-pass attribution check using stack.dataportraits.org. We check for\nsequences of at least 50 characters that match a Bloom filter. This means\nfalse positives are possible and long enough surrounding context is necesssary\n(see the paper for details on n-gram striding and sequence length). The\ndedicated Stack search tool is a full dataset index and can be used for a\ncomplete second pass.\n\n## Installation\n\nInstall like any other vscode extension.\n\nBy default, this extension uses bigcode/starcoder & Hugging Face Inference API\nfor the inference.\n\n#### HF API token\n\nYou can supply your HF API token (hf.co/settings/token) with this command:\n\n  1. Cmd/Ctrl+Shift+P to open VSCode command palette\n  2. Type: Llm: Login\n\nIf you previously logged in with huggingface-cli login on your system the\nextension will read the token from disk.\n\n## Configuration\n\nYou can check the full list of configuration settings by opening your settings\npage (cmd+,) and typing Llm.\n\n### Backend\n\nYou can configure the backend to which requests will be sent. llm-vscode\nsupports the following backends:\n\n  * huggingface: The Hugging Face Inference API (default)\n  * ollama: Ollama\n  * openai: any OpenAI compatible API (e.g. llama-cpp-python)\n  * tgi: Text Generation Inference\n\nLet's say your current code is this:\n\n    \n    \n    import numpy as np import scipy as sp {YOUR_CURSOR_POSITION} def hello_world(): print(\"Hello world\")\n\nThe request body will then look like:\n\n    \n    \n    const inputs = `{start token}import numpy as np\\nimport scipy as sp\\n{end token}def hello_world():\\n print(\"Hello world\"){middle token}` const data = { inputs, ...configuration.requestBody }; const model = configuration.modelId; let endpoint; switch(configuration.backend) { case \"huggingface\": let url; if (configuration.url === null) { url = \"https://api-inference.huggingface.co\"; } else { url = configuration.url; } endpoint = `${url}/models/${model}`; break; case \"ollama\": case \"openai\": case \"tgi\": endpoint = configuration.url; break; } const res = await fetch(endpoint, { body: JSON.stringify(data), headers, method: \"POST\" }); const json = await res.json() as { generated_text: string };\n\nNote that the example above is a simplified version to explain what is\nhappening under the hood.\n\n### Suggestion behavior\n\nYou can tune the way the suggestions behave:\n\n  * llm.enableAutoSuggest lets you choose to enable or disable \"suggest-as-you-type\" suggestions.\n  * llm.documentFilter lets you enable suggestions only on specific files that match the pattern matching syntax you will provide. The object must be of type DocumentFilter | DocumentFilter[]:\n\n    * to match on all types of buffers: llm.documentFilter: { pattern: \"**\" }\n    * to match on all files in my_project/: llm.documentFilter: { pattern: \"/path/to/my_project/**\" }\n    * to match on all python and rust files: llm.documentFilter: { pattern: \"**/*.{py,rs}\" }\n\n### Keybindings\n\nllm-vscode sets two keybindings:\n\n  * you can trigger suggestions with Cmd+shift+l by default, which corresponds to the editor.action.inlineSuggest.trigger command\n  * code attribution is set to Cmd+shift+a by default, which corresponds to the llm.attribution command\n\n### llm-ls\n\nBy default, llm-ls is bundled with the extension. When developing locally or\nif you built your own binary because your platform is not supported, you can\nset the llm.lsp.binaryPath setting to the path of the binary.\n\n### Tokenizer\n\nllm-ls uses tokenizers to make sure the prompt fits the context_window.\n\nTo configure it, you have a few options:\n\n  * No tokenization, llm-ls will count the number of characters instead:\n\n    \n    \n    { \"llm.tokenizer\": null }\n\n  * from a local file on your disk:\n\n    \n    \n    { \"llm.tokenizer\": { \"path\": \"/path/to/my/tokenizer.json\" } }\n\n  * from a Hugging Face repository, llm-ls will attempt to download tokenizer.json at the root of the repository:\n\n    \n    \n    { \"llm.tokenizer\": { \"repository\": \"myusername/myrepo\", \"api_token\": null, } }\n\nNote: when api_token is set to null, it will use the token you set with Llm:\nLogin command. If you want to use a different token, you can set it here.\n\n  * from an HTTP endpoint, llm-ls will attempt to download a file via an HTTP GET request:\n\n    \n    \n    { \"llm.tokenizer\": { \"url\": \"https://my-endpoint.example.com/mytokenizer.json\", \"to\": \"/download/path/of/mytokenizer.json\" } }\n\n### Code Llama\n\nTo test Code Llama 13B model:\n\n  1. Make sure you have the latest version of this extension.\n  2. Make sure you have supplied HF API token\n  3. Open Vscode Settings (cmd+,) & type: Llm: Config Template\n  4. From the dropdown menu, choose hf/codellama/CodeLlama-13b-hf\n\nRead more here about Code LLama.\n\n### Phind and WizardCoder\n\nTo test Phind/Phind-CodeLlama-34B-v2 and/or WizardLM/WizardCoder-\nPython-34B-V1.0 :\n\n  1. Make sure you have the latest version of this extension.\n  2. Make sure you have supplied HF API token\n  3. Open Vscode Settings (cmd+,) & type: Llm: Config Template\n  4. From the dropdown menu, choose hf/Phind/Phind-CodeLlama-34B-v2 or hf/WizardLM/WizardCoder-Python-34B-V1.0\n\nRead more about Phind-CodeLlama-34B-v2 here and WizardCoder-15B-V1.0 here.\n\n## Developing\n\n  1. Clone llm-ls: git clone https://github.com/huggingface/llm-ls\n  2. Build llm-ls: cd llm-ls && cargo build (you can also use cargo build --release for a release build)\n  3. Clone this repo: git clone https://github.com/huggingface/llm-vscode\n  4. Install deps: cd llm-vscode && npm ci\n  5. In vscode, open Run and Debug side bar & click Launch Extension\n  6. In the new vscode window, set the llm.lsp.binaryPath setting to the path of the llm-ls binary you built in step 2 (e.g. /path/to/llm-ls/target/debug/llm-ls)\n  7. Close the window and restart the extension with F5 or like in 5.\n\n## Community\n\nRepository| Description  \n---|---  \nhuggingface-vscode-endpoint-server| Custom code generation endpoint for this\nrepository  \nllm-vscode-inference-server| An endpoint server for efficiently serving\nquantized open-source LLMs for code.  \n  \n## About\n\nLLM powered development for VSCode\n\n### Resources\n\nReadme\n\n### License\n\nApache-2.0 license\n\nActivity\n\nCustom properties\n\n### Stars\n\n1.1k stars\n\n### Watchers\n\n19 watching\n\n### Forks\n\n117 forks\n\nReport repository\n\n## Releases 1\n\nv0.1.0 Latest\n\nSep 26, 2023\n\n## Packages 0\n\nNo packages published\n\n## Contributors 41\n\n\\+ 27 contributors\n\n## Languages\n\n  * TypeScript 91.7%\n  * JavaScript 8.3%\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": false}
