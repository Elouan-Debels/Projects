{"aid": "40043571", "title": "AI or Ain't: LLMs", "url": "https://zserge.com/posts/ai-llm/", "domain": "zserge.com", "votes": 1, "user": "warkanlock", "posted_at": "2024-04-15 17:48:35", "comments": 0, "source_title": "AI or ain't: LLMs", "source_text": "AI or ain't: LLMs\n\n# AI or ain't: LLMs\n\nPreviously we covered early chatbots, bots talking gibberish, and self-taught\nnumber crunchers.\n\nBut what we got so far is still boring. AI was promised to overthrow the world\norder and not just classify arrays of floats. Can we have a chat?\n\n## GPT\n\nAs unimpressive as it is, neural networks take arrays of numbers and return\narrays of numbers. Just like our brain takes electric signals and emits\nelectric signals. It\u2019s only a matter of how we encode such inputs and outputs.\nIn a human body our brain is taught how to \u201csense\u201d pictures, temperature,\ntouch. It learns how to move legs and walk, how to write and speak. It takes\nyears for our brain to learn such rudimentary signal encodings.\n\nA neural network can also be \u201cconnected\u201d to some webcam and treat pixels as\narrays of numbers. With enough training data and time a network can learn how\nto recognise images, how to read handwriting or how to tell Chihuahua from a\nmuffin. By encoding inputs as MIDI notes we can teach a network to compose\nmusic. By encoding inputs as words we can teach it a language and some common\nknowledge written in that language.\n\nGPT models, currently popularised with OpenAI\u2019s ChatGPT, are general-purpose\ntransformers (that\u2019s network architecture) and are used to generate text by\nthe given input. We\u2019ll start with GPT-2 models published by OpenAI a while\nago, since they are fairly small and easy to work with. Code examples will be\nin Go.\n\n## Tokens\n\nIn all the previous parts we simply split a sentence by whitespace. Despite\nits simplicity, this approach is not the best: it treats \u201chello\u201d and \u201cHello!\u201d\nas two different words, it assumes that words \u201cgreet\u201d and \u201cgreeting\u201d are\ncompletely unrelated etc.\n\nA more advanced solution would be store a list of known tokens that could\nrepresent a complete word or a certain part of the word. Our tokenisation\nprocess then would have to find the longest possible token for each part of\nthe sentence. In a list of tokens each token would have a unique index, so\ntokenisation essentially converts a string of words to an array of integers.\n\nOur GPT-2 model knows ~50K words. They are stored in tokens.dat file as a\nsequence of null-terminated strings.\n\n    \n    \n    // load tokens from a file tokens := []string{} b, _ := ioutil.ReadFile(\"tokens.dat\") for len(b) > 0 { i := bytes.IndexByte(b, 0) tokens = append(tokens, string(b[:i])) b = b[i+1:] } func findToken(s string) (index, overlap int) { for i, t := range tokens { j := 0 for ; j < len(s) && j < len(t) && s[j] == t[j]; j++ { } if j > overlap || (j == overlap && j == len(t)) { overlap, index = j, i } } return index, overlap } func tokenise(s string) (context []int) { for len(s) > 0 { t, n := findToken(s) if t < 0 { return context } context = append(context, t) s = s[n:] } return context } tokenise(\"Paris is the capital of\") // [40313 318 262 3139 286] tokenise(\"The capital of Germany is\") // [464 3139 286 4486 318]\n\nSee that \u201ccapital\u201d is token #3139, \u201cis\u201d - #318, \u201cof\u201d - #286. \u201cThe\u201d and \u201cthe\u201d\nare two different tokens, since they may indicate the start of the sentence\nand thus may have different meaning.\n\n## Word vectors\n\nHowever it would be very difficult to train a network to understand what the\nwords mean if they are represented by token indices. For example, \u201cParis\u201d and\n\u201ccapital\u201d are semantically very close to each other, but their token IDs are\nvery much apart: 40313 and 3139. The word \u201cGermany\u201d is closer to \u201ccapital\u201d\nthan the actual capital city.\n\nThis is why GPT model has another layer of indirection that converts a token\nindex to a vector of numbers, representing the word \u201cmeaning\u201d. Many years ago\na similar Word2Vec algorithm has been invented, that assigned a vector to each\nword and the more related the words are - the smaller is the difference\nbetween their vectors. In fact, arithmetic operations on such vectors allowed\nto find associated words, i.e. \u201cKind - Man = Queen\u201d.\n\nGPT-2 model comes with a similar \u201cword token embedding\u201d matrix (WTE), in our\ncase stored in a wte.dat file. This file was created as a result of GPT-2\ntrainig phase, performed by OpenAI. We\u2019re using its contents without thinking\ntoo much how it was obtained:\n\n    \n    \n    // read a file into a slice of 32-bit floats func read(filename string) []float32 { b, _ := ioutil.ReadFile(filename) return unsafe.Slice((*float32)(unsafe.Pointer(&b[0])), len(b)/4) } // get a vector of floats for the given token in the WTE table func wordvec(wte []float32, token int) []float32 { return wte[WordVecSize*token : WordVecSize*(token+1)] }\n\nLet\u2019s pick words \u201cking\u201d, \u201cmonarch\u201d and \u201clettuce\u201d. These correspond to the\ntokens of 5822, 26464 and 39406 in tokens.dat. An AI might get an illusion\nthat \u201cking\u201d is rather a salad plant than a monarch. But if we get the word\nvectors for each token and calculate the difference between the elements we\nget 11.1 for \u201cking-monarch\u201d distance and 21.9 for \u201cking-lettuce\u201d distance.\nAlso \u201ctea+biscuit\u201d gives us 16 points, \u201ctea+coffee\u201d 9, but \u201ctea+Hare\u201d is 21,\ndespite all Caroll\u2019s work.\n\nSimilarly, you can take a word, find its vector and find a range of the most\nrelated vectors. You may see that for input token \u201cabsurd\u201d the closest matched\nwould be:\n\n    \n    \n    absurd 0 ridiculous 3.4613547 ludicrous 3.7072947 outrageous 6.004274 ridiculously 6.71082 nonsensical 6.7171283 bizarre 6.8727217 laughable 7.240555 outlandish 7.274483 silly 7.331988 insanely 7.7595344 astonishing 7.8167872 grotesque 7.818375 absurdity 7.8630023 insane 8.12205 incredible 8.283726 monstrous 8.381887 weird 8.558824 astounding 8.615417 stupid 8.698734\n\nWhich makes total sense! But processing one word at a time without having\ncontextual knowledge about other words is unlikely to give any good results\nfor natural language (consider \u201cworking hardly\u201d vs \u201chardly working\u201d).\n\nThis is where another large matrix, WPE, helps. It encodes a position of the\nword in the input context (sentence) and how this position corrects the word\nvector. We\u2019ll see how it\u2019s used in the network later.\n\n## Layers\n\nGPT-2 model is represented with a list of neuron layers. In the previous part\nwe represented each neuron as an object with scalar parameters, which was\nconvenient for training a neuron. Here we only focus on forward propagation\nand can speed up our network significantly if we treat all the weights of all\nthe neurons in a single layer as one vector. This would batch all arithmetic\noperations and reduce the number of \u201cfor\u201d loops a lot.\n\nHere is all the layer data in the decomposed GPT-2 model:\n\n    \n    \n    4.0K h1_attn_cproj_b.dat 4.0K h1_ln1_b.dat 4.0K h1_ln1_g.dat 4.0K h1_ln2_b.dat 4.0K h1_ln2_g.dat 4.0K h1_mlp_cproj_b.dat 12K h1_attn_cattn_b.dat 12K h1_mlp_cfc_b.dat 2.3M h1_attn_cproj_w.dat 6.8M h1_attn_cattn_w.dat 9.0M h1_mlp_cfc_w.dat 9.0M h1_mlp_cproj_w.dat ... 4.0K h2_... ... 4.0K lnf_b.dat 4.0K lnf_g.dat 364K tokens.dat 3.0M wpe.dat 147M wte.dat\n\nGPT-2 is a multi-layer neural network, just like the toy network we used to\ncalculate XOR or classify moon-shaped points. Except for it is much, much\nlarger. In its smallest variant (\u201c124M\u201d) it comes with 12 layers, where each\nlayer is in fact a combination of a few smaller layers with their own weights\nand biases.\n\nEach vector or matrix within the layer is stored in an individual file,\nextracted from the original GPT-2 model. Simply reading that as a sequence of\nfloat32 numbers would fill in the parameter data into each layer.\n\nHere\u2019s the complete code for loading a model:\n\n    \n    \n    type Model struct { dir string lnf_g []float32 lnf_b []float32 wte []float32 // word token embeddings wpe []float32 // word position embeddings layers []Layer } type Layer struct { ln1_b []float32 ln1_g []float32 ln2_b []float32 ln2_g []float32 mlp_cfc_b []float32 mlp_cfc_w []float32 mlp_cproj_b []float32 mlp_cproj_w []float32 attn_cattn_b []float32 attn_cattn_w []float32 attn_cproj_b []float32 attn_cproj_w []float32 k []float32 v []float32 } func LoadModel(dir string) (m Model) { m.dir = dir m.lnf_g = m.read(\"lnf_g.dat\") m.lnf_b = m.read(\"lnf_b.dat\") m.wte = m.read(\"wte.dat\") m.wpe = m.read(\"wpe.dat\") m.layers = make([]Layer, NumLayers) for i := range m.layers { l := &m.layers[i] l.ln1_g = m.read(fmt.Sprintf(\"h%d_ln1_g.dat\", i)) l.ln1_b = m.read(fmt.Sprintf(\"h%d_ln1_b.dat\", i)) l.ln2_g = m.read(fmt.Sprintf(\"h%d_ln2_g.dat\", i)) l.ln2_b = m.read(fmt.Sprintf(\"h%d_ln2_b.dat\", i)) l.mlp_cfc_w = m.read(fmt.Sprintf(\"h%d_mlp_cfc_w.t\", i)) l.mlp_cfc_b = m.read(fmt.Sprintf(\"h%d_mlp_cfc_b.dat\", i)) l.mlp_cproj_w = m.read(fmt.Sprintf(\"h%d_mlp_cproj_w.t\", i)) l.mlp_cproj_b = m.read(fmt.Sprintf(\"h%d_mlp_cproj_b.dat\", i)) l.attn_cproj_w = m.read(fmt.Sprintf(\"h%d_attn_cproj_w.t\", i)) l.attn_cproj_b = m.read(fmt.Sprintf(\"h%d_attn_cproj_b.dat\", i)) l.attn_cattn_w = m.read(fmt.Sprintf(\"h%d_attn_cattn_w.t\", i)) l.attn_cattn_b = m.read(fmt.Sprintf(\"h%d_attn_cattn_b.dat\", i)) l.k = make([]float32, ContextSize*WordVecSize) l.v = make([]float32, ContextSize*WordVecSize) } return m }\n\nNow we have lots and lots of numbers. But what kind of operations should we\nperform to make it compose a sentence?\n\nSome arrays end with \u201cw\u201d and some with \u201cb\u201d, those are weights and biases, so\nwe probably would end up doing the usual multiply-and-add math a lot here. All\nlayers having \u201cproj\u201d in their names perform this operation (also known as\nprojection from one vector into another vector space).\n\nThere are also \u201cg\u201d and \u201cb\u201d pairs. These stand for \u201cgamma\u201d and \u201cbeta\u201d from the\nlayer normalisation process (will be described below). They are used to scale\nthe values by... multiplying them by \u201cg\u201d and adding \u201cb\u201d to that.\n\n## Self-attention\n\nSo far we\u2019ve figured out the roles of the following blocks (in the order they\nare applied to the input):\n\n  * wpe and wte are for input token encoding\n  * for each layer we apply:\n\n    * ln1 gamma and beta are for the initial normalisation in each layer.\n    * k and v\n    * attn_cproj weights and biases\n    * attn_cattn weights and biases\n    * ln2 gamma and beta are for the final normalisation in each layer.\n    * mlp_cfc and mlp_cproj weights and biases form a two-layer dense network, like the one we\u2019ve implemented before.\n  * lnf gamma and beta and are for final normalisation of the whole network output.\n\nThe only unknowns are k, v and the blocks related to \u201cattention\u201d projection.\nBut what is attention anyway?\n\nJust like with us, human beings, attention helps the model to focus on certain\nwords in the sentence rather than the others. Self-attention means that it\nonly uses words from the same input context (sentence).\n\nThe main components of the self-attention layer are: a query q, which\nrepresents the current word, keys k that represent other words in the current\ncontext and values v that are added to the current word data if the associated\nkey looks relevant to the query.\n\nMultiplying query vector by each of the key vectors gives us the score of how\nrelevant the word with the given key is to the query. Multiplying\ncorresponding values to their scores and summing them up results in a self-\nattention vector that can be used to give the model more context about the\ninput data.\n\nImagine a team meeting, where every team member is talking about their own\nwork. However, other team members adjust their attention depending on how\nrelevant each topic is. At the end, the team collectively focuses on the most\nimportant topics for the whole team. Now, replace \u201cteam members\u201d with\nindividual words/tokens in the current sentence (context). Self-attention\nhelps to score individual tokens based on their importance to the whole\ncontext.\n\n## Do the math\n\nAt this point we\u2019ve figured out how to load model data into float32 slices,\nhow to read tokens and how to tokenise input prompt. What\u2019s missing is a few\nhelper math functions that could simplify layer operations.\n\nFirst, we\u2019ll need the good old linear X\u00b7W+b function:\n\n    \n    \n    func lin(x, w []float32, b float32) float32 { for i := range x { b += x[i] * w[i] } return b }\n\nWe also will need a slightly different activation function. Instead of ReLU,\nwhich we\u2019ve used so far, GPT-2 model requires us to use GeLU function, which\nis similar in shape but has a more complex implementation:\n\n    \n    \n    func gelu(x float32) float32 { return 0.5 * x * (1 + float32(math.Tanh(0.7978845676080871*float64(x+0.044715*x*x*x)))) }\n\nTo handle normalisation within layers we would also need a separate function.\nIt multiplies every element from X by a mean square and gamma, then adds beta\nvalue:\n\n    \n    \n    func norm(x, beta, gamma []float32) []float32 { mean, sqmean := float32(0.0), float32(0.0) for i := range x { mean += x[i] } mean = mean / float32(len(x)) for _, xi := range x { sqmean += (xi - mean) * (xi - mean) } sqmean = float32(math.Max(float64(sqmean/float32(len(x))), 0.0000001)) m := float32(math.Sqrt(1.0 / float64(sqmean))) out := make([]float32, len(x)) for i, xi := range x { out[i] = (xi-mean)*m*gamma[i] + beta[i] } return out }\n\nFinally, we will need \u201csoftmax\u201d, that converts a vector X into a probability\ndistribution of possible values, so that each value is in the range [0..1] and\nthe sum of them equals 1:\n\n    \n    \n    func softmax(x []float32) []float32 { out := make([]float32, len(x)) max, sum := float32(math.Inf(-1)), float32(0) for i := range x { if x[i] > max { max = x[i] } } for i := range x { x[i] = float32(math.Exp(float64(x[i] - max))) sum += x[i] } for i := range x { out[i] = x[i] / sum } return out }\n\nThat\u2019s all the math we need to run a GPT-2 model.\n\n## Running a single layer\n\nA single layer takes a vector as an input, as well as a slot pointer. First of\nall it handles the self-attention. It normalises the input vector and using\nthe attention weight+bias it calculates (lin(x, cattn_w, cattn_b)) the query\nq, key k, and value v vectors. Followed by a softmax(q*k) it results in a\nvector of scores for each key.\n\nIn GPT-2 self-attention happens simultaneously a number of times. Each\ncalculation happens more or less independently from the others. The smalles\nGPT-2 model has 12 \u201cheads\u201d and each head has its own query, key, and value\nvectors resulting in its own set of scores. Concatenating all the scores from\nall the heads we have a vector that represents the multi-head attention of the\nwhole layer.\n\nHowever, simply forwarding this vector to the next layer would not give good\nresults. We need another operation that would project the self-attention\nresults into a more suitable vector. This part is called \u201cprojecting\u201d and is\nimplemented as another matrix multiplication lin(attn, cproj_w, cproj_b).\n\nFinally there are two fully-connected dense sub-layers, one few times larger\nthan the word vector length, another reducing it back to the word vector\nlength (786 for GPT-2). Why two of them and why are they different? The more\nneurons the network has - the more \u201cknowledge\u201d it can contain, but all this\nknowledge needs to be compressed back to fit the dimensionality of the\nfollowing layer.\n\nPerhaps, code speaks better than words, here\u2019s how a single layer operates:\n\n    \n    \n    func (m Model) runLayer(x []float32, layer, slot int) { l := m.layers[layer] xn := norm(x, l.ln1_b, l.ln1_g) q := make([]float32, WordVecSize) for i := 0; i < WordVecSize*3; i++ { a := lin(xn, l.attn_cattn_w[WordVecSize*i:WordVecSize*(i+1)], l.attn_cattn_b[i]) if i < WordVecSize { q[i] = a } else if i < WordVecSize*2 { l.k[slot*WordVecSize+(i-WordVecSize)] = a } else { l.v[(i-WordVecSize*2)*ContextSize+slot] = a } } const headSize = 64 tmp := make([]float32, WordVecSize) for h := 0; h < NumHeads; h++ { att := make([]float32, slot+1) for i := 0; i <= slot; i++ { att[i] = lin(q[h*headSize:(h+1)*headSize], l.k[i*WordVecSize+h*headSize:], 0) / 8 } att = softmax(att) for j := 0; j < headSize; j++ { tmp[h*headSize+j] = lin(att, l.v[(j+h*headSize)*ContextSize:], 0) } } for i := 0; i < WordVecSize; i++ { x[i] += lin(tmp, l.attn_cproj_w[WordVecSize*i:], l.attn_cproj_b[i]) } xn = norm(x, l.ln2_b, l.ln2_g) mlp := make([]float32, WordVecSize*4) for i := 0; i < WordVecSize*4; i++ { mlp[i] = gelu(lin(xn, l.mlp_cfc_w[WordVecSize*i:], l.mlp_cfc_b[i])) } for i := 0; i < WordVecSize; i++ { x[i] += lin(mlp, l.mlp_cproj_w[WordVecSize*4*i:], l.mlp_cproj_b[i]) } }\n\nThis was the most complicated part of GPT-2, the rest is just calling runLayer\nin a loop for every layer:\n\n    \n    \n    func (m Model) Run(context []int, slot int) []float32 { x := make([]float32, WordVecSize) wv := m.WordVec(context[slot]) for i := range x { x[i] = m.wpe[i+WordVecSize*slot] x[i] += wv[i] } for i := range m.layers { m.runLayer(x, i, slot) } return norm(x, m.lnf_b, m.lnf_g) }\n\nYou might wonder why do we need a slot parameter. Self-attention tends to\nconsider the complete input vector, treating all its \u201cslots\u201d equally. But as\nwe feed input token by token into the network - the slots are filled one after\nthe other and no slots after the current one have any meaning yet. So we give\na hint to the network that it should the consider the previous slots but\nignore the following ones. This is called \u201cmasked self-attention\u201d.\n\n## Decoding\n\nThe final layer of the network returns another 786-element vector of numbers.\nHow can be translate it into a word?\n\nFor every known token from tokens.dat we multiply the output vector by the\nword vector. Result would be a single number, that indicates how good the\ntoken is to become the next one in the sentence. We choose a certain subset of\nsuch candidates (the most suitable ones) and randomly pick one. The resulting\ntoken is being added to the context and the whole context is being fed into\nthe network again. The output of the network creates candidates for the\nfollowing token and so on. The process continues as long as needed, or until\nthe network returns a special \u201cend of text\u201d token, meaning that it had rest\nits case.\n\nTime to test our network. GPT-2 comes in different model sizes, the one with\n124M parameters is in the repo, the rest can be converted from the publicly\navailable GPT-2 weights.\n\nWe can ask the network to continue phrases and see how it copes with the task:\n\n    \n    \n    \"We finish each other's...\" > sentences > work > meals \"Berlin is a...\" > small, but important city. > major center. > major transit destination \"Politicians are...\" > divided at their view of Russia > now calling a referendum for May > already looking forward, saying they want better services, more choice > not opposed to abortion rights, nor can their views be a... > concerned about China's growing political sophistication \"To be or not to be?\" > Are al your lives in vain? > This isn't the final question we got > And yet, it is the greatest of virtues\n\nThe whole code is available on GitHub and it\u2019s all under 300 LOC!\n\n## AI?\n\nClearly, the model is hallucinating. Berlin is not a small city at all, not\nthere is referendum in May. But for an absolutely tiny model having 124M\nparameters (about the size of a mouse brain) it can produce rather meaningful\ntext. If you print a list of top candidates for each token you might be\nsurprised to see know much knowledge the model has about the world.\n\nThe final decision whether such models are \u201cintelligent\u201d or not is left to the\nreader, but the results prove that large networks trained on large data sets\nis a way to go for the artificial intelligence today. Or is it?\n\nNext: TinyStories (Coming soon!)\n\nI hope you\u2019ve enjoyed this article. You can follow \u2013 and contribute to \u2013 on\nGithub, Mastodon, Twitter or subscribe via rss.\n\nJan 04, 2024\n\nSee also: AI or ain't: Neural Networks and more.\n\n\u00a92012\u20132023 \u00b7 Serge Zaitsev \u00b7 hello@zserge.com \u00b7 @zserge@mastodon.social\n\n", "frontpage": false}
