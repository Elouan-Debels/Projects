{"aid": "40068411", "title": "AI Product Needs Evals", "url": "https://hamel.dev/blog/posts/evals/", "domain": "hamel.dev", "votes": 1, "user": "jamesblonde", "posted_at": "2024-04-17 18:26:33", "comments": 0, "source_title": "Your AI Product Needs Evals", "source_text": "\\- Your AI Product Needs Evals\n\n# Your AI Product Needs Evals\n\nLLMs\n\nHow to construct domain-specific LLM evaluation systems.\n\nAuthor\n\nHamel Husain\n\nPublished\n\nMarch 29, 2024\n\n## Motivation\n\nI started working with language models five years ago when I led the team that\ncreated CodeSearchNet, a precursor to GitHub CoPilot. Since then, I\u2019ve seen\nmany successful and unsuccessful approaches to building LLM products. I\u2019ve\nfound that unsuccessful products almost always share a common root cause: a\nfailure to create robust evaluation systems.\n\nI\u2019m currently an independent consultant who helps companies build domain-\nspecific AI products. I hope companies can save thousands of dollars in\nconsulting fees by reading this post carefully. As much as I love making\nmoney, I hate seeing folks make the same mistake repeatedly.\n\nThis post outlines my thoughts on building evaluation systems for LLMs-powered\nAI products.\n\n# Iterating Quickly == Success\n\nLike software engineering, success with AI hinges on how fast you can iterate.\nYou must have processes and tools for:\n\n  1. Evaluating quality (ex: tests).\n  2. Debugging issues (ex: logging & inspecting data).\n  3. Changing the behavior or the system (prompt eng, fine-tuning, writing code)\n\nMany people focus exclusively on #3 above, which prevents them from improving\ntheir LLM products beyond a demo.^1 Doing all three activities well creates a\nvirtuous cycle differentiating great from mediocre AI products (see the\ndiagram below for a visualization of this cycle).\n\nIf you streamline your evaluation process, all other activities become easy.\nThis is very similar to how tests in software engineering pay massive\ndividends in the long term despite requiring up-front investment.\n\nTo ground this post in a real-world situation, I\u2019ll walk through a case study\nin which we built a system for rapid improvement. I\u2019ll primarily focus on\nevaluation as that is the most critical component.\n\n# Case Study: Lucy, A Real Estate AI Assistant\n\nRechat is a SaaS application that allows real estate professionals to perform\nvarious tasks, such as managing contracts, searching for listings, building\ncreative assets, managing appointments, and more. The thesis of Rechat is that\nyou can do everything in one place rather than having to context switch\nbetween many different tools.\n\nRechat\u2019s AI assistant, Lucy, is a canonical AI product: a conversational\ninterface that obviates the need to click, type, and navigate the software.\nDuring Lucy\u2019s beginning stages, rapid progress was made with prompt\nengineering. However, as Lucy\u2019s surface area expanded, the performance of the\nAI plateaued. Symptoms of this were:\n\n  1. Addressing one failure mode led to the emergence of others, resembling a game of whack-a-mole.\n  2. There was limited visibility into the AI system\u2019s effectiveness across tasks beyond vibe checks.\n  3. Prompts expanded into long and unwieldy forms, attempting to cover numerous edge cases and examples.\n\n## Problem: How To Systematically Improve The AI?\n\nTo break through this plateau, we created a systematic approach to improving\nLucy centered on evaluation. Our approach is illustrated by the diagram below.\n\nThis diagram is a best-faith effort to illustrate my mental model for\nimproving AI systems. In reality, the process is non-linear and can take on\nmany different forms that may or may not look like this diagram.\n\nI discuss the various components of this system in the context of evaluation\nbelow.\n\n# The Types Of Evaluation\n\nRigorous and systematic evaluation is the most important part of the whole\nsystem. That is why \u201cEval and Curation\u201d is highlighted in yellow at the center\nof the diagram. You should spend most of your time making your evaluation more\nrobust and streamlined.\n\nThere are three levels of evaluation to consider:\n\n  * Level 1: Unit Tests\n  * Level 2: Model & Human Eval (this includes debugging)\n  * Level 3: A/B testing\n\nThe cost of Level 3 > Level 2 > Level 1. This dictates the cadence and manner\nyou execute them. For example, I often run Level 1 evals on every code change,\nLevel 2 on a set cadence and Level 3 only after significant product changes.\nIt\u2019s also helpful to conquer a good portion of your Level 1 tests before you\nmove into model-based tests, as they require more work and time to execute.\n\nThere isn\u2019t a strict formula as to when to introduce each level of testing.\nYou want to balance getting user feedback quickly, managing user perception,\nand the goals of your AI product. This isn\u2019t too dissimilar from the balancing\nact you must do for products more generally.\n\n## Level 1: Unit Tests\n\nUnit tests for LLMs are assertions (like you would write in pytest). Unlike\ntypical unit tests, you want to organize these assertions for use in places\nbeyond unit tests, such as data cleaning and automatic retries (using the\nassertion error to course-correct) during model inference. The important part\nis that these assertions should run fast and cheaply as you develop your\napplication so that you can run them every time your code changes. If you have\ntrouble thinking of assertions, you should critically examine your traces and\nfailure modes. Also, do not shy away from using an LLM to help you brainstorm\nassertions!\n\n### Step 1: Write Scoped Tests\n\nThe most effective way to think about unit tests is to break down the scope of\nyour LLM into features and scenarios. For example, one feature of Lucy is the\nability to find real estate listings, which we can break down into scenarios\nlike so:\n\nFeature: Listing Finder\n\nThis feature to be tested is a function call that responds to a user request\nto find a real estate listing. For example, \u201cPlease find listings with more\nthan 3 bedrooms less than $2M in San Jose, CA\u201d\n\nThe LLM converts this into a query that gets run against the CRM. The\nassertion then verifies that the expected number of results is returned. In\nour test suite, we have three user inputs that trigger each of the scenarios\nbelow, which then execute corresponding assertions (this is an oversimplified\nexample for illustrative purposes):\n\nScenario| Assertions  \n---|---  \nOnly one listing matches user query| len(listing_array) == 1  \nMultiple listings match user query| len(listing_array) > 1  \nNo listings match user query| len(listing_array) == 0  \n  \nThere are also generic tests that aren\u2019t specific to any one feature. For\nexample, here is the code for one such generic test that ensures the UUID is\nnot mentioned in the output:\n\n    \n    \n    const noExposedUUID = message => { // Remove all text within double curly braces const sanitizedComment = message.comment.replace(/\\{\\{.*?\\}\\}/g, '') // Search for exposed UUIDs const regexp = /[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}/ig const matches = Array.from(sanitizedComment.matchAll(regexp)) expect(matches.length, 'Exposed UUIDs').to.equal(0, 'Exposed UUIDs found') }\n\nCRM results returned to the LLM contain fields that shouldn\u2019t be surfaced to\nthe user; such as the UUID associated with an entry. Our LLM prompt tells the\nLLM to not include UUIDs. We use a simple regex to assert that the LLM\nresponse doesn\u2019t include UUIDs.\n\nRechat has hundreds of these unit tests. We continuously update them based on\nnew failures we observe in the data as users challenge the AI or the product\nevolves. These unit tests are crucial to getting feedback quickly when\niterating on your AI system (prompt engineering, improving RAG, etc.). Many\npeople eventually outgrow their unit tests and move on to other levels of\nevaluation as their product matures, but it is essential not to skip this\nstep!\n\n### Step 2: Create Test Cases\n\nTo test these assertions, you must generate test cases or inputs that will\ntrigger all scenarios you wish to test. I often utilize an LLM to generate\nthese inputs synthetically; for example, here is one such prompt Rechat uses\nto generate synthetic inputs for a feature that creates and retrieves\ncontacts:\n\n    \n    \n    Write 50 different instructions that a real estate agent can give to his assistant to create contacts on his CRM. The contact details can include name, phone, email, partner name, birthday, tags, company, address and job. For each of the instructions, you need to generate a second instruction which can be used to look up the created contact. . The results should be a JSON code block with only one string as the instruction like the following: [ [\"Create a contact for John (johndoe@apple.com)\", \"What's the email address of John Smith?\"] ]\n\nUsing the above prompt, we generate test cases like below:\n\n    \n    \n    [ [ 'Create a contact for John Smith (johndoe@apple.com) with phone number 123-456-7890 and address 123 Apple St.', 'What\\'s the email address of John Smith?' ], [ 'Add Emily Johnson with phone 987-654-3210, email emilyj@email.com, and company ABC Inc.', 'What\\'s the phone number for Emily Johnson?' ], [ 'Create a contact for Tom Williams with birthday 10/20/1985, company XYZ Ltd, and job title Manager.', 'What\\'s Tom Williams\\' job title?' ], [ 'Add a contact for Susan Brown with partner name James Brown, and email susanb@email.com.', 'What\\'s the partner name of Susan Brown?' ], ... ]\n\nFor each of these test cases, we execute the first user input to create the\ncontact. We then execute the second query to fetch that contact. If the CRM\ndoesn\u2019t return exactly 1 result then we know there was a problem either\ncreating or fetching the contact. We can also run generic assertions like the\none to verify UUIDs are not in the response. You must constantly update these\ntests as you observe data through human evaluation and debugging. The key is\nto make these as challenging as possible while representing users\u2019\ninteractions with the system.\n\nYou don\u2019t need to wait for production data to test your system. You can make\neducated guesses about how users will use your product and generate synthetic\ndata. You can also let a small set of users use your product and let their\nusage refine your synthetic data generation strategy. One signal you are\nwriting good tests and assertions is when the model struggles to pass them -\nthese failure modes become problems you can solve with techniques like fine-\ntuning later on.\n\nOn a related note, unlike traditional unit tests, you don\u2019t necessarily need a\n100% pass rate. Your pass rate is a product decision, depending on the\nfailures you are willing to tolerate.\n\n### Step 3: Run & Track Your Tests Regularly\n\nThere are many ways to orchestrate Level 1 tests. Rechat has been leveraging\nCI infrastructure (e.g., GitHub Actions, GitLab Pipelines, etc.) to execute\nthese tests. However, the tooling for this part of the workflow is nascent and\nevolving rapidly.\n\nMy advice is to orchestrate tests that involve the least friction in your tech\nstack. In addition to tracking tests, you need to track the results of your\ntests over time so you can see if you are making progress. If you use CI, you\nshould collect metrics along with versions of your tests/prompts outside your\nCI system for easy analysis and tracking.\n\nI recommend starting simple and leveraging your existing analytics system to\nvisualize your test results. For example, Rechat uses Metabase to track their\nLLM test results over time. Below is a screenshot of a dashboard Rechat built\nwith Metabase:\n\nThis screenshot shows the prevalence of a particular error (shown in yellow)\nin Lucy before (left) vs after (right) we addressed it.\n\n## Level 2: Human & Model Eval\n\nAfter you have built a solid foundation of Level 1 tests, you can move on to\nother forms of validation that cannot be tested by assertions alone. A\nprerequisite to performing human and model-based eval is to log your traces.\n\n### Logging Traces\n\nA trace is a concept that has been around for a while in software engineering\nand is a log of a sequence of events such as user sessions or a request flow\nthrough a distributed system. In other words, tracing is a logical grouping of\nlogs. In the context of LLMs, traces often refer to conversations you have\nwith a LLM. For example, a user message, followed by an AI response, followed\nby another user message, would be an example of a trace.\n\nThere are a growing number of solutions for logging LLM traces.^2 Rechat uses\nLangSmith, which logs traces and allows you to view them in a human-readable\nway with an interactive playground to iterate on prompts. Sometimes, logging\nyour traces requires you to instrument your code. In this case, Rechat was\nusing LangChain which automatically logs trace events to LangSmith for you.\nHere is a screenshot of what this looks like:\n\nI like LangSmith - it doesn\u2019t require that you use LangChain and is intuitive\nand easy to use. Searching, filtering, and reading traces are essential\nfeatures for whatever solution you pick. I\u2019ve found that some tools do not\nimplement these basic functions correctly!\n\n### Looking At Your Traces\n\nYou must remove all friction from the process of looking at data. This means\nrendering your traces in domain-specific ways. I\u2019ve often found that it\u2019s\nbetter to build my own data viewing & labeling tool so I can gather all the\ninformation I need onto one screen. In Lucy\u2019s case, we needed to look at many\nsources of information (trace log, the CRM, etc) to understand what the AI\ndid. This is precisely the type of friction that needs to be eliminated. In\nRechat\u2019s case, this meant adding information like:\n\n  1. What tool (feature) & scenario was being evaluated.\n  2. Whether the trace resulted from a synthetic input or a real user input.\n  3. Filters to navigate between different tools and scenario combinations.\n  4. Links to the CRM and trace logging system for the current record.\n\nI\u2019ve built different variations of this tool for each problem I\u2019ve worked on.\nSometimes, I even need to embed another application to see what the user\ninteraction looks like. Below is a screenshot of the tool we built to evaluate\nRechat\u2019s traces:\n\nAnother design choice specific to Lucy is that we noticed that many failures\ninvolved small mistakes in the final output of the LLM (format, content, etc).\nWe decided to make the final output editable by a human so that we could\ncurate & fix data for fine-tuning.\n\nThese tools can be built with lightweight front-end frameworks like Gradio,\nStreamlit, Panel, or Shiny in less than a day. The tool shown above was built\nwith Shiny for Python. Furthermore, there are tools like Lilac which uses AI\nto search and filter data semantically, which is incredibly handy for finding\na set of similar data points while debugging an issue.\n\nI often start by labeling examples as good or bad. I\u2019ve found that assigning\nscores or more granular ratings is more onerous to manage than binary ratings.\nThere are advanced techniques you can use to make human evaluation more\nefficient or accurate (e.g., active learning, consensus voting, etc.), but I\nrecommend starting with something simple. Finally, like unit tests, you should\norganize and analyze your human-eval results to assess if you are progressing\nover time.\n\nAs discussed later, these labeled examples measure the quality of your system,\nvalidate automated evaluation, and curate high-quality synthetic data for\nfine-tuning.\n\n#### How much data should you look at?\n\nI often get asked how much data to examine. When starting, you should examine\nas much data as possible. I usually read traces generated from ALL test cases\nand user-generated traces at a minimum. You can never stop looking at data\u2014no\nfree lunch exists. However, you can sample your data more over time, lessening\nthe burden. ^3\n\n### Automated Evaluation w/ LLMs\n\nMany vendors want to sell you tools that claim to eliminate the need for a\nhuman to look at the data. Having humans periodically evaluate at least a\nsample of traces is a good idea. I often find that \u201ccorrectness\u201d is somewhat\nsubjective, and you must align the model with a human.\n\nYou should track the correlation between model-based and human evaluation to\ndecide how much you can rely on automatic evaluation. Furthermore, by\ncollecting critiques from labelers explaining why they are making a decision,\nyou can iterate on the evaluator model to align it with humans through prompt\nengineering or fine-tuning. However, I tend to favor prompt engineering for\nevaluator model alignment.\n\nI love using low-tech solutions like Excel to iterate on aligning model-based\neval with humans. For example, I sent my colleague Phillip the following\nspreadsheet every few days to grade for a different use-case involving a\nnatural language query generator. This spreadsheet would contain the following\ninformation:\n\n  1. model response: this is the prediction made by the LLM.\n  2. model critique: this is a critique written by a (usually more powerful) LLM about your original LLM\u2019s prediction.\n  3. model outcome: this is a binary label the critique model assigns to the model response as being \u201cgood\u201d or \u201cbad.\u201d\n\nPhillip then fills out his version of the same information - meaning his\ncritique, outcome, and desired response for 25-50 examples at a time (these\nare the columns prefixed with \u201cphillip_\u201d below):\n\nThis information allowed me to iterate on the prompt of the critique model to\nmake it sufficiently aligned with Phillip over time. This is also easy to\ntrack in a low-tech way in a spreadsheet:\n\nThis is a screenshot of a spreadsheet where we recorded our attempts to align\nmodel-based eval with a human evaluator.\n\nGeneral tips on model-based eval:\n\n  * Use the most powerful model you can afford. It often takes advanced reasoning capabilities to critique something well. You can often get away with a slower, more powerful model for critiquing outputs relative to what you use in production.\n  * Model-based evaluation is a meta-problem within your larger problem. You must maintain a mini-evaluation system to track its quality. I have sometimes fine-tuned a model at this stage (but I try not to).\n  * After bringing the model-based evaluator in line with the human, you must continue doing periodic exercises to monitor the model and human agreement.\n\nMy favorite aspect about creating a good evaluator model is that its critiques\ncan be used to curate high-quality synthetic data, which I will touch upon\nlater.\n\n## Level 3: A/B Testing\n\nFinally, it is always good to perform A/B tests to ensure your AI product is\ndriving user behaviors or outcomes you desire. A/B testing for LLMs compared\nto other types of products isn\u2019t too different. If you want to learn more\nabout A/B testing, I recommend reading the Eppo blog (which was created by\ncolleagues I used to work with who are rock stars in A/B testing).\n\nIt\u2019s okay to put this stage off until you are sufficiently ready and convinced\nthat your AI product is suitable for showing to real users. This level of\nevaluation is usually only appropriate for more mature products.\n\n## Evaluating RAG\n\nAside from evaluating your system as a whole, you can evaluate sub-components\nof your AI, like RAG. Evaluating RAG is beyond the scope of this post, but you\ncan learn more about this subject in a post by Jason Liu.\n\n# Eval Systems Unlock Superpowers For Free\n\nIn addition to iterating fast, eval systems unlock the ability to fine-tune\nand debug, which can take your AI product to the next level.\n\n## Fine-Tuning\n\nRechat resolved many failure modes through fine-tuning that were not possible\nwith prompt engineering alone. Fine-tuning is best for learning syntax, style,\nand rules, whereas techniques like RAG supply the model with context or up-to-\ndate facts.\n\n99% of the labor involved with fine-tuning is assembling high-quality data\nthat covers your AI product\u2019s surface area. However, if you have a solid\nevaluation system like Rechat\u2019s, you already have a robust data generation and\ncuration engine! I will expand more on the process of fine-tuning in a future\npost.^4\n\n### Data Synthesis & Curation\n\nTo illustrate why data curation and synthesis come nearly for free once you\nhave an evaluation system, consider the case where you want to create\nadditional fine-tuning data for the listing finder mentioned earlier. First,\nyou can use LLMs to generate synthetic data with a prompt like this:\n\n    \n    \n    Imagine if Zillow was able to parse natural language. Come up with 50 different ways users would be able to search listings there. Use real names for cities and neighborhoods. You can use the following parameters: <ommitted for confidentiality> Output should be a JSON code block array. Example: [ \"Homes under $500k in New York\" ]\n\nThis is almost identical to the exercise for producing test cases! You can\nthen use your Level 1 & Level 2 tests to filter out undesirable data that\nfails assertions or that the critique model thinks are wrong. You can also use\nyour existing human evaluation tools to look at traces to curate traces for a\nfine-tuning dataset.\n\n## Debugging\n\nWhen you get a complaint or see an error related to your AI product, you\nshould be able to debug this quickly. If you have a robust evaluation system,\nyou already have:\n\n  * A database of traces that you can search and filter.\n  * A set of mechanisms (assertions, tests, etc) that can help you flag errors and bad behaviors.\n  * Log searching & navigation tools that can help you find the root cause of the error. For example, the error could be RAG, a bug in the code, or a model performing poorly.\n  * The ability to make changes in response to the error and quickly test its efficacy.\n\nIn short, there is an incredibly large overlap between the infrastructure\nneeded for evaluation and that for debugging.\n\n# Conclusion\n\nEvaluation systems create a flywheel that allows you to iterate very quickly.\nIt\u2019s almost always where people get stuck when building AI products. I hope\nthis post gives you an intuition on how to go about building your evaluation\nsystems. Some key takeaways to keep in mind:\n\n  * Remove ALL friction from looking at data.\n  * Keep it simple. Don\u2019t buy fancy LLM tools. Use what you have first.\n  * You are doing it wrong if you aren\u2019t looking at lots of data.\n  * Don\u2019t rely on generic evaluation frameworks to measure the quality of your AI. Instead, create an evaluation system specific to your problem.\n  * Write lots of tests and frequently update them.\n  * LLMs can be used to unblock the creation of an eval system. Examples include using a LLM to:\n\n    * Generate test cases and write assertions\n    * Generate synthetic data\n    * Critique and label data etc.\n  * Re-use your eval infrastructure for debugging and fine-tuning.\n\nI\u2019d love to hear from you if you found this post helpful or have any\nquestions. My email is hamel@parlance-labs.com.\n\nThis article is an adaptation of this conversation I had with Emil Sedgh and\nHugo Browne-Anderson on the Vanishing Gradients podcast. Thanks to Jeremy\nHoward, Eugene Yan, Shreya Shankar, Jeremy Lewi, and Joseph Gleasure for\nreviewing this article.\n\n## Footnotes\n\n  1. This is not to suggest that people are lazy. Many don\u2019t know how to set up eval systems and skip these steps.\u21a9\ufe0e\n\n  2. Some examples include arize, human loop, openllmetry and honeyhive.\u21a9\ufe0e\n\n  3. A reasonable heuristic is to keep reading logs until you feel like you aren\u2019t learning anything new.\u21a9\ufe0e\n\n  4. If you cannot wait, I\u2019ll be teaching this course on fine-tuning soon.\u21a9\ufe0e\n\n  * Edit this page\n\n", "frontpage": false}
