{"aid": "40041986", "title": "Pile-T5", "url": "https://blog.eleuther.ai/pile-t5/", "domain": "eleuther.ai", "votes": 18, "user": "tosh", "posted_at": "2024-04-15 15:37:34", "comments": 5, "source_title": "Pile-T5", "source_text": "Pile-T5 | EleutherAI Blog\n\nEleutherAI\n\n# Pile-T5\n\nTrained T5 on the Pile\n\nApril 15, 2024 \u00b7 Lintang Sutawika, Aran Komatsuzaki, Colin Raffel\n\nThe T5 model (Raffel et al, 2019) has been a widely used model in the NLP\ncommunity. With downloads of its base model from HF being in the millions,\nit's no doubt that these models have been a community favorite. However, T5's\ntokenizer omits important code-related tokens and subsequent pretraining\ndatasets have been released with higher quality in its filtering and diverse\ndomain. In this blogpost, we introduce a new version of T5; Pile-T5 that is\ntrained on the Pile (Gao et al, 2020) and uses the LLAMA tokenizer (Touvron et\nal, 2023) that intends to improve upon those weaknesses.\n\n## Model Description#\n\nOur alternative version consists of replacing the pretrained dataset with the\nPile and switching out the original T5 tokenizer with the LLAMA tokenizer.\nPile-T5 was trained to 2 million steps or 2 trillion tokens in total, twice\nwhat the original T5 model was trained for. We train with the original span\ncorruption method and observe that improvements for finetuning on downstream\ntasks that users would want to for their usecases. We find that our models\nsubstantially outperform the most widely used T5 models (called T5-v1.1) even\nin token-matched settings. In particular, Pile-T5 performs much better on code\ntasks which would benefit extension towards code tasks. Our released models\nwere trained on the same hyperparameters as the original T5, utilizing T5x. We\nrelease the our experiments scripts here.\n\nThese models are accessible from EleutherAI's huggingface page. A notable\ndifference from the original T5 is that the Pile-T5 uses the transformer\nimplementation for UMT5 (Chung, Constant, Garcia et al, 2023) because it uses\nthe same scalable implementation in T5x. Inspired by Pythia (Biderman and\nShoelkopf et al 2023), we release intermediate checkpoints that span every\n10,000 steps with the goal of empowering researchers to study the evolution of\nour models over time. The main branch for these models in their resepective\nhuggingface page is the 2 million step version, with other branches containing\nthe partially trained checkpoints. In addition, we release the T5x versions of\nthe checkpoints here.\n\n## Going Beyond 1 Trillion Tokens#\n\nThe Pile-T5 models were evaluated on SuperGLUE, CodeXGLUE, as well as MMLU and\nBigbench Hard. The Pile-T5 models are compared with the T5v1.1 where both were\nfinetuned over the same amount of tokens. We also compare Pile-T5 models\nagaints the Flan-T5 models for MMLU and BBH as a loose comparison. All\nevaluations were done with the LM-Evaluation Harness (Gao et al, 2023) to\nreport model performance over the benchmarks we present here. We release the\nfinetuned checkpoints for Base, Large, XL, and XXL.\n\n### Performance on SuperGLUE#\n\nTo asses performance on SuperGLUE, we finetune the Pile-T5 (Both the 1\ntrillion tokens version and the final 2 trillion tokens version) and T5v1.1\nmodels for a batch size of 128 for 263k steps matching the original T5 paper.\nWith all models except for Large, we observe substansial performance increase.\nNote that Pile-T5 (1T) already outpreforms T5-v1.1 and that performance is\nfurther increased by further training.\n\nSize| Variant| Average| boolq| cb| copa| multirc| record| rte| wic| wsc  \n---|---|---|---|---|---|---|---|---|---|---  \nacc| f1| acc| acc| f1| em| em| f1| acc| acc| acc  \nBase| T5-v1.1| 71.33| 79.36| 83.63| 87.5| 63| 73.45| 33.26| 69.7| 68.75|\n78.34| 65.83| 75.96  \nPile-T5 (1T)| 74.85| 81.46| 93.69| 94.64| 65| 77.75| 40.50| 76.97| 76.49|\n80.86| 67.39| 74.03  \nPile-T5| 76.13| 82.45| 96.07| 94.64| 72| 77.74| 39.56| 77.64| 76.88| 83.03|\n67.24| 73.08  \nLarge| T5-v1.1| 81.11| 85.96| 93.21| 96.43| 82| 81.71| 48.37| 82.18| 81.71|\n85.92| 71.47| 81.73  \nPile-T5 (1T)| 79.18| 83.70| 91.85| 94.64| 79| 82.36| 47.85| 82.72| 82.14|\n83.03| 65.2| 81.73  \nPile-T5| 79.67| 85.71| 88.96| 94.64| 74| 82.60| 50.47| 84.1| 83.70| 85.19|\n68.49| 81.73  \nXL| T5-v1.1| 81.76| 86.79| 81.18| 91.07| 84| 84.03| 52.89| 83.92| 83.5| 90.25|\n73.04| 81.73  \nPile-T5 (1T)| 86.09| 89.76| 90.6| 94.64| 96| 88.17| 63.90| 91.58| 91.36|\n93.50| 72.73| 86.54  \nPile-T5| 89.00| 90.4| 93.1| 96.43| 96| 88.63| 65.16| 92.21| 91.96| 92.78|\n75.24| 96.15  \nXXL| T5-v1.1| 82.43| 88.29| 93.61| 94.64| 86| 75.22| 51.00| 84.67| 84.55|\n89.17| 72.41| 81.73  \nPile-T5 (1T)| 87.11| 90.46| 94.3| 96.43| 93| 80.81| 56.77| 91.36| 91.18|\n92.42| 70.38| 95.19  \nPile-T5| 90.08| 90.98| 98.68| 98.21| 95| 89.28| 67.68| 93.04| 92.7| 93.5|\n75.24| 96.15  \n  \n### Performance on CodeXGLUE#\n\nSince one of our major goals is to improve the ability of models to understand\ncode, we also evaluated on the Code-to-Text subtask of CodeXGLUE (Su et al,\n2021). All models were finetune on each programming language variant for 10\nepochs with the same method as detailed in the original repo.\n\nSize| Version| Average| Python| PHP| Go| Java| JavaScript| Ruby  \n---|---|---|---|---|---|---|---|---  \nBase| T5-v1.1| 14.34| 15.55| 21.72| 14.71| 14.89| 9.25| 9.90  \nPile-T5 (1T)| 15.90| 17.20| 22.90| 16.75| 16.24| 11.23| 11.10  \nPile-T5| 16.37| 17.78| 23.12| 16.70| 16.68| 11.89| 12.06  \nLarge| T5-v1.1| 11.53| 12.18| 14.17| 12.37| 12.30| 8.85| 9.32  \nPile-T5 (1T)| 15.74| 17.09| 22.80| 17.16| 16.33| 10.75| 10.31  \nPile-T5| 16.28| 17.72| 22.95| 17.07| 16.41| 12.05| 11.45  \nXL| T5-v1.1| 16.17| 17.36| 21.91| 16.69| 17.74| 11.08| 12.25  \nPile-T5 (1T)| 18.01| 18.61| 23.75| 19.04| 18.43| 14.27| 13.93  \nPile-T5| 18.68| 19.25| 24.37| 19.42| 19.15| 15.1| 14.81  \nXXL| T5-v1.1| 17.67| 17.89| 23.21| 18.54| 19.17| 13.85| 13.33  \nPile-T5 (1T)| 18.55| 19.53| 24.11| 19.27| 18.52| 15.11| 14.75  \nPile-T5| 18.72| 19.27| 24.49| 19.60| 18.96| 15.10| 14.92  \n  \nDue to both the Pile inlcuding code-based data and the LLAMA tokenizer\nincluding characters frequently used in code, we observe a sharp improvement\nin performance. Note that even though Pile-T5-large performs worse than\nT5-v1.1 in general, it substantially outpreforms it on these coding\nbenchmarks. This appears to be primarily driven by the very poor performance\nof T5-v1.1-large, which substantially underperforms T5-v1.1-base! By contrast,\nPile-T5-large performs similarly to Pile-T5-base.\n\n## Using Flan Instruction Tuning#\n\nWe continue by finetuning Pile-T5 models on Flan (Chung, Hou, Longpre et all,\n2022) with same training hyperparameters and evaluate on MMLU (Hendrycks et\nal, 2021) and BigBench Hard (Suzgun et al, 2022). When comparing to the\nFlan-T5 model, we found our Pile-T5 falls short by a small but meaningful\namount. After following up with the authors we learned that not all of the\nfinetuning data used to produce Flan-T5 was publicly released, which may\nexplain the performance difference. For a more fair comparison, we also\nfinetuned T5-v1.1 checkpoints with the same procedure and data that we used on\nthe Pile-T5 models. We specifically use the 2 trillion tokens versions of\nPile-T5, making the comparison with T5-v1.1 reflect both the increased\ntraining data scale and the change in data and tokenizer.\n\n### Performance on Held-In#\n\nWe observe competitive performance over held-in tasks (tasks that were\nincluded in the Flan Instruction Tuning dataset) with a dip in performance at\nthe Large variant similar to SuperGLUE.\n\nSize| Version| Average| ANLI R1| ANLI R2| ANLI R3| Arc Easy| Arc Challange|\nBoolQ| RTE  \n---|---|---|---|---|---|---|---|---|---  \nBase| T5-v1.1| 46.50| 39.90| 34.96| 37.33| 38.12| 28.23| 70.26| 76.73  \nPile-T5| 46.37| 39.32| 35.28| 37.53| 36.61| 30.67| 71.87| 73.28  \nLarge| T5-v1.1| 54.90| 52.46| 39.67| 42.53| 50.60| 39.99| 78.56| 80.50  \nPile-T5| 36.97| 33.00| 33.03| 32.98| 29.27| 21.21| 56.36| 52.95  \nXL| T5-v1.1| 56.40| 53.82| 40.22| 41.01| 56.31| 39.08| 80.66| 83.71  \nPile-T5| 64.41| 64.36| 48.02| 49.18| 66.56| 58.28| 85.16| 79.30  \nXXL| T5-v1.1| 69.99| 71.63| 55.81| 57.41| 75.56| 62.30| 86.53| 80.71  \nPile-T5| 69.21| 71.16| 55.92| 55.19| 70.85| 59.82| 87.55| 83.96  \n  \n### Performance on MMLU#\n\nModels are evaluated on 2 prompts versions; the original prompt (Hendrycks et\nal, 2021) and (Chung, Hou, Longpre et all, 2022). In addition, evaluation is\nnot only done in a greedy generation format but also by taking the highest\nloglikelihood of the available answer choices.\n\nMMLU Prompt\n\n    \n    \n    The following are multiple choice questions (with answers) about abstract algebra. Find the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q. A. 0 B. 4 C. 2 D. 6 Answer:\n\nFlan Prompt\n\n    \n    \n    The following are multiple choice questions (with answers) about abstract algebra. Q: Find the degree for the given field extension Q(sqrt(2), sqrt(3), sqrt(18)) over Q. (A) 0 (B) 4 (C) 2 (D) 6 A:\n\nWe see performance gains when using Pile-T5. For MMLU both likelihood-based\nand generative-based versions are evaluated on. With loglikelihood benifiting\nmostly zero-shot prompting. Using loglikelihood tends to help model when\nprompted with 0-shot while the models can struggle to output a proper response\nwhen using 0-shot greedy generation. This is due to using strict evaluation\nand the model frequently generating outputs such as the full answer instead of\nonly the letters. Performance on greedy generation improves when using 5-shot\nprompting that provides the models example of the correct way to answer. It\nshould be noted that performance can vary significantly depending on the\nprompt format. Averaging across all variations show that Pile-T5 improves upon\nv1.1 and is competitive against Flan T5 variants.\n\nSize| Variant| Average| Highest Loglikelihood| Greedy Generation  \n---|---|---|---|---  \nOriginal Prompt| Flan Prompt| Original Prompt| Flan Prompt  \n0-Shot| 5-Shot| 0-Shot| 5-Shot| 0-Shot| 5-Shot| 0-Shot| 5-Shot  \nXL| Flan-T5| 42.45| 47.37| 49.17| 47.83| 49.43| 6.63| 48.8| 40.98| 49.39  \nT5-v1.1| 36.58| 38.59| 39.52| 40.64| 39.79| 25.95| 38.84| 29.66| 39.67  \nPile-T5| 40.82| 46.04| 48.71| 47.13| 48.18| 3.61| 48.58| 35.53| 48.77  \nXXL| Flan-T5| 46.94| 51.47| 54.28| 53.31| 53.85| 2.69| 53.93| 52.15| 53.85  \nT5-v1.1| 45.76| 51.03| 51.15| 46.72| 50.77| 31.00| 50.72| 33.90| 50.78  \nPile-T5| 48.27| 50.88| 53.35| 52.22| 53.06| 35.8| 53.13| 33.85| 53.84  \n  \n### Performance on BigBench Hard (BBH)#\n\nPile-T5 performs substantially better compared to T5v1.1 on BBH on both Few-\nshot and Zero-shot settings and comparatively well even against Flan-T5.\n\nSize| Variant| Greedy Generation  \n---|---|---  \nZero-Shot| Few-Shot  \nXL| Flan-T5| 24.71| 40.36  \nT5-v1.1| 28.67| 33.06  \nPile-T5| 29.98| 41.49  \nXXL| Flan-T5| 43.06| 44.72  \nT5-v1.1| 35.14| 39.84  \nPile-T5| 41.61| 46.71  \n  \n## Conclusion#\n\nWe observe improvements on finetuned benchmarks such as SuperGLUE, CodeXGLUE,\nMMLU and BBH. Althought Pile-T5 when finetuned on the Flan mixture lags behind\nFlan-T5, it still performs better compared to T5v1.1. We conclude that Pile-T5\nwould be a better model for future multitask finetuning and other tasks that\nbenefit from the encoder-decoder architecture. With performance on the Pile-T5\nLarge being unexpectedly lagging in benchmarks such as SuperGLUE and Flan\nHeld-In tasks, we believe that there may have been a bug and advise users to\ntake caution when using it. In addition, we believe that the intermediate\ncheckpoint release would wide benefit the community for research areas such as\ninterpretability.\n\n## Acknowledgments#\n\nWe are grateful to Stability AI for providing the compute required to train\nthese models, and to the TRC Program for providing compute to finetune some of\nthe models.\n\n## Citation#\n\n    \n    \n    @misc{2024PileT5, author = {Lintang Sutawika and Aran Komatsuzaki and Colin Raffel}, title = {Pile-T5}, year = {2024}, url = {https://blog.eleuther.ai/pile-t5/}, note = {Blog post}, }\n\n## References#\n\n  1. Biderman, Stella, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad Aflah Khan, et al. Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling. arXiv [Cs.CL], 2023. arXiv. http://arxiv.org/abs/2304.01373.\n  2. Chung, Hyung Won, Noah Constant, Xavier Garcia, Adam Roberts, Yi Tay, Sharan Narang, and Orhan Firat. UniMax: Fairer and More Effective Language Sampling for Large-Scale Multilingual Pretraining. arXiv [Cs.CL], 2023. arXiv. http://arxiv.org/abs/2304.09151.\n  3. Chung, Hyung Won, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, et al. Scaling Instruction-Finetuned Language Models. arXiv [Cs.LG], 2022. arXiv. http://arxiv.org/abs/2210.11416.\n  4. Gao, Leo, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, et al. \u2018The Pile: An 800GB Dataset of Diverse Text for Language Modeling\u2019. arXiv [Cs.CL], 2020. arXiv. http://arxiv.org/abs/2101.00027.\n  5. Gao, Leo, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, et al. \u2018A Framework for Few-Shot Language Model Evaluation\u2019. Zenodo, 12 2023. https://doi.org/10.5281/zenodo.10256836.\n  6. Hendrycks, Dan, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. \u2018Measuring Massive Multitask Language Understanding\u2019. arXiv [Cs.CY], 2021. arXiv. http://arxiv.org/abs/2009.03300.\n  7. Longpre, Shayne, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, et al. The Flan Collection: Designing Data and Methods for Effective Instruction Tuning. arXiv [Cs.AI], 2023. arXiv. http://arxiv.org/abs/2301.13688.\n  8. Lu, Shuai, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy, Ambrosio Blanco, Colin Clement, et al. \u2018CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation\u2019. arXiv [Cs.SE], 2021. arXiv. http://arxiv.org/abs/2102.04664.\n  9. Suzgun, Mirac, Nathan Scales, Nathanael Sch\u00e4rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, et al. \u2018Challenging BIG-Bench Tasks and Whether Chain-of-Thought Can Solve Them\u2019. arXiv [Cs.CL], 2022. arXiv. http://arxiv.org/abs/2210.09261.\n  10. Touvron, Hugo, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, et al. \u2018LLaMA: Open and Efficient Foundation Language Models\u2019. arXiv [Cs.CL], 2023. arXiv. http://arxiv.org/abs/2302.13971.\n\n\u00a9 2024 EleutherAI\n\n", "frontpage": true}
