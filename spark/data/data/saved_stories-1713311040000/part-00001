{"aid": "40054914", "title": "We won't be replaced by AI", "url": "https://supermaven.com/blog/no-we-will-not-be-replaced-by-ai", "domain": "supermaven.com", "votes": 1, "user": "ajhofmann", "posted_at": "2024-04-16 17:41:44", "comments": 0, "source_title": "No, we won't be replaced by AI", "source_text": "No, we won't be replaced by AI\n\n# No, we won't be replaced by AI\n\nJacob Jackson\n\nTue Apr 16 2024\n\nThere's been a lot of talk about AIs replacing programmers recently. The\nNational Post: \u201cAI is coming after the tech bros and their easy money\u201d. Emad\nMostaque, CEO of Stability AI: \u201cThere Will Be No Programmers in Five Years\u201d.\nSome people are changing their career decisions as a result. Investors see the\npotential too: companies have raised hundreds of millions of dollars promising\nto replace human developers. Given the salaries that skilled developers can\nearn, there\u2019s a lot of money in finding a way to cut us out of the picture.\n\nBut it\u2019s not going to happen. We\u2019re not going to be replaced by the machines.\n\nNow, I\u2019m not going to fall into the trap of past AI detractors by predicting\ndeep learning is going to \u201chit a wall\u201d and being proven wrong 6 months later.\nI think we\u2019ll continue to see progress as AI systems get more intelligent and\nnew applications are discovered, and I think AI will create a lot of economic\nvalue. (That\u2019s why I\u2019m working on an AI company.) And a lot of arguments for\nwhy humans will stay relevant boil down to \u201cAI doesn\u2019t work today, so it won\u2019t\nwork in the future\u201d. That\u2019s not reassuring because the whole reason people are\nworried about this is the rapid rate of progress. But the predictions that\nthis progress will lead to humans being replaced are incorrect - here\u2019s why.\n\nAn AI can only learn a task when we have a way for the AI to perform the task\nand then be judged on whether it did the task correctly. The two main tasks\nthat are currently used to train AI are:\n\n  1. Next-word prediction: the AI is given a document of text from the internet and asked to predict the next word in the document.\n  2. Short response generation (RLHF): the AI is given a query or prompt (eg. a user asking for coding help) and asked to generate a response (eg. a solution to the user\u2019s problem). The response is scored by humans on its helpfulness and accuracy.\n\nTraining AIs on these tasks has led to extremely useful products like ChatGPT,\nand ChatGPT is great at helping developers in day-to-day work, so it\u2019s\nreasonable that people think systems like ChatGPT might soon eclipse us.\n\nBut software development is not just about writing individual functions and\nshort responses. A key part of software development is making technical\ndecisions, like software architecture or project prioritization, that will pay\noff over the long-term on the scale of 6 months to 5 years. To train AI to do\nthis, we would need a way for it to make technical decisions and a way to\njudge those decisions for their long-term soundness. We don\u2019t know how to do\nthat, and we especially don\u2019t know how to do that at the scale required to\ntrain a large AI model. By definition these sorts of decisions are hard to\njudge, with no objective criteria for success, only difficult tradeoffs.\n\nThe performance of AI is always going to be limited by the tasks where it can\nhave a feedback signal, and this makes long-term planning difficult for AI\nbecause there is no naturally occurring training data (unlike next-word\nprediction) and there is no way to quickly assess whether a decision was good\nor bad (unlike short response generation/RLHF). Training a good next-word\nprediction model requires a loop of hundreds of thousands of training steps,\nwhich is possible because next-word prediction is quick to evaluate, but\nrunning the same number of training steps on decisions where you have to wait\n1 year to see if the decision was good or bad would take hundreds of thousands\nof years. We humans are good at this because our minds have been shaped by\nmillions of years of evolution, but AI doesn\u2019t have that time.\n\nSome people think we can avoid difficulties like this by having the AI judge\nits own responses. While this can work to bring a weaker model up to the level\nof a stronger model, it can\u2019t work to have a strong model improve itself,\nbecause if it worked it would imply the ability for a model to continually\nself-improve in isolation without interacting with the external world. For\nlearning to occur, there has to be a feedback signal coming from external\ndata.\n\nThe future of AI is continued progress on tasks where we have a good way to\ntrain AI to solve them, and continued lack of progress on the tasks where we\ndon\u2019t. Instead of worrying about AI replacing software developers, the right\nmove is to integrate AI tools into your workflow and use AI for what it\u2019s good\nat while reserving high-level, long-term decisions to be decided by humans,\nnot AI.\n\nSee all posts\n\nFollow on Twitter\n\n\u00a9 2024 Supermaven, Inc.\n\n  * Contact Us\n  * Privacy Policy\n  * Code Policy\n  * Terms of Service\n\n", "frontpage": false}
