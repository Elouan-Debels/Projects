{"aid": "39978846", "title": "Koboldcpp-1.62.1 adds support for Command-R+", "url": "https://github.com/LostRuins/koboldcpp/releases/tag/v1.62.1", "domain": "github.com/lostruins", "votes": 1, "user": "tosh", "posted_at": "2024-04-09 12:48:09", "comments": 0, "source_title": "Release koboldcpp-1.62.1 \u00b7 LostRuins/koboldcpp", "source_text": "Release koboldcpp-1.62.1 \u00b7 LostRuins/koboldcpp \u00b7 GitHub\n\nSkip to content\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\nLostRuins / koboldcpp Public\n\nforked from ggerganov/llama.cpp\n\n  * Notifications\n  * Fork 262\n  * Star 3.6k\n\n# koboldcpp-1.62.1\n\nLatest\n\nLatest\n\nLostRuins released this 09 Apr 07:01\n\nv1.62.1\n\ndf596ae\n\n# koboldcpp-1.62.1\n\nThere and back again edition\n\n  * NEW: Img2Img is now supported when generating images using KoboldCpp. An A1111 compatible endpoint /sdapi/v1/img2img is now emulated. When using Kobold Lite, you can now click an existing image, and generate a new image based off it with Img2Img.\n  * NEW: OpenAI Chat Completions adapters can now be specified on load with --chatcompletionsadapter. This allows you to use any instruct tag format you want via the Chat Completions API. Please refer to the wiki for documentation. The instruct tags should now also handle all stop sequences correctly and not overflow past them when using OpenAI Chat Completions API.\n  * Added automatic cleanup of old orphaned koboldcpp pyinstaller temp directories.\n  * Added more usage statistics available in /api/extra/perf/\n  * Do not display localhost url if using remote tunnel\n  * Added /docs endpoint which is an alias for /api, containing API documentation\n  * Embedded Horde Worker job polling URL changed to aihorde.net\n  * Embedded Horde Workers will now give priority to the local user, and pause/unpause themselves briefly whenever generating on a local active client, and then returning to full speed when idle. This should allow you to comfortably run a busy horde worker, even when you want to use KoboldCpp locally at the same time.\n  * Try to fix SSL cert directory not found by specifying a default path.\n  * Fixed old quant tools not compiling\n  * Pulled and merged new model support, improvements and fixes from upstream.\n  * Updated Kobold Lite with some layout fixes, support for Cohere API, Claude Haiku and Gemini 1.5 API, and Img2Img features for local and horde.\n\nHotfix 1.62.1 - Merged command R plus from upstream. I cannot confirm if it\nworks correctly as CR+ is too big for me to run locally.\n\nTo use, download and run the koboldcpp.exe, which is a one-file pyinstaller.\nIf you don't need CUDA, you can use koboldcpp_nocuda.exe which is much\nsmaller. If you're using AMD, you can try koboldcpp_rocm at YellowRoseCx's\nfork here\n\nRun it from the command line with the desired launch parameters (see --help),\nor manually select the model in the GUI. and then once loaded, you can connect\nlike this (or use the full koboldai client): http://localhost:5001\n\nFor more information, be sure to run the program from command line with the\n--help flag.\n\n9 people reacted\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": false}
