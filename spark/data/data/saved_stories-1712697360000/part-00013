{"aid": "39978835", "title": "Benchmark Command R vs. GPT/Claude on your own data", "url": "https://www.promptfoo.dev/docs/guides/cohere-command-r-benchmark/", "domain": "promptfoo.dev", "votes": 2, "user": "typpo", "posted_at": "2024-04-09 12:46:21", "comments": 0, "source_title": "Command R vs GPT vs Claude: create your own benchmark | promptfoo", "source_text": "Command R vs GPT vs Claude: create your own benchmark | promptfoo\n\nSkip to main content\n\n# Command R vs GPT vs Claude: create your own benchmark\n\nWhile public benchmarks provide a general sense of capability, the only way to\ntruly understand which model will perform best for your specific application\nis to run your own custom evaluation.\n\nThis guide will show you how to perform a custom benchmark on Cohere's\nCommand-R/Command-R Plus, comparing it to GPT-4 and Claude Opus on the use\ncases that matter most to you.\n\nThe end result is a side-by-side comparison view that looks like this:\n\n## Requirements\n\n  * Cohere API key for Command-R\n  * OpenAI API key for GPT-4\n  * Anthropic API key for Claude Opus\n  * Node 16+\n\n## Step 1: Initial Setup\n\nCreate a new promptfoo project:\n\n    \n    \n    npx promptfoo@latest init cohere-benchmark cd cohere-benchmark\n\n## Step 2: Configure the models\n\nEdit promptfooconfig.yaml to specify the models to compare:\n\n    \n    \n    providers: - id: cohere:command-r # or command-r-plus - id: openai:gpt-4-0125-preview - id: anthropic:messages:claude-3-opus-20240229\n\nSet the API keys:\n\n    \n    \n    export COHERE_API_KEY=your_cohere_key export OPENAI_API_KEY=your_openai_key export ANTHROPIC_API_KEY=your_anthropic_key\n\nOptionally configure model parameters like temperature and max tokens:\n\n    \n    \n    providers: - id: cohere:command-r config: temperature: 0 - id: openai:gpt-4-0125-preview config: temperature: 0 - id: anthropic:messages:claude-3-opus-20240229 config: temperature: 0\n\nSee Cohere, OpenAI, and Anthropic docs for more detail.\n\n## Step 3: Set up prompts\n\nDefine the prompt to test. Get creative - this is your chance to see how the\nmodels handle queries unique to your application!\n\nFor example, let's see how well each model can summarize key points from a\nlegal contract:\n\n    \n    \n    prompts: - | Extract the 3 most important clauses from this contract, and explain each one in plain English:\n    \n    {{contract}}\n\n## Step 4: Add test cases\n\nProvide test case inputs and expected outputs to evaluate performance:\n\n    \n    \n    tests: - vars: contract: | Seller agrees to convey the property located at 123 Main St to Buyer for a total purchase price of $500,000. Closing to occur on or before June 30, 2023. Sale is contingent upon Buyer obtaining financing and the property appraising for at least the purchase price. Seller to provide a clear title free of any liens or encumbrances... assert: - type: llm-rubric value: | The summary should cover: - The purchase price of $500,000 - The closing deadline of June 30, 2023 - The financing and appraisal contingencies - Seller's responsibility to provide clear title - type: javascript value: output.length < 500\n\n## Step 5: Run the evaluation\n\nRun the benchmark:\n\n    \n    \n    npx promptfoo@latest eval\n\nAnd view the results:\n\n    \n    \n    npx promptfoo@latest view\n\nYou'll see the following:\n\nClick into a cell to view details on the inference job:\n\n## Analysis\n\nUse the view and the assertion results to make an informed decision about\nwhich model will deliver the best experience for your app.\n\nIn this specific case, Command-R underperformed, passing only 16.67% of test\ncases instead of the 50% pass rate from GPT-4 and Claude Opus. It doesn't mean\nit's a bad model - it just means it may not be the best for this use case.\n\nOf note, Command-R was 5-8 times as fast as Claude Opus and GPT-4\nrespectively, and it cost much less. Every model brings tradeoffs.\n\nSee Getting Started to set up your own local evals and learn more.\n\nEdit this page\n\n  * Requirements\n  * Step 1: Initial Setup\n  * Step 2: Configure the models\n  * Step 3: Set up prompts\n  * Step 4: Add test cases\n  * Step 5: Run the evaluation\n  * Analysis\n\nDocs\n\n  * Intro\n  * Command line\n  * Node package\n  * Privacy policy\n\nGuides\n\n  * Running benchmarks\n  * Evaluating factuality\n  * Evaluating RAGs\n  * Minimizing hallucinations\n\nCommunity\n\n  * GitHub\n  * Discord\n\n\u00a9 2024 promptfoo\n\n", "frontpage": false}
