{"aid": "39978876", "title": "How to Build a Data Product with Databricks", "url": "https://www.datamesh-architecture.com/howto/build-a-dataproduct-with-databricks", "domain": "datamesh-architecture.com", "votes": 1, "user": "simonharrer", "posted_at": "2024-04-09 12:52:35", "comments": 0, "source_title": "Build a Data Product with Databricks", "source_text": "Build a Data Product with Databricks\n\n# How To Build a Data Product with Databricks\n\nBy Jochen Christ April 3, 2024\n\nToday's data engineering shifted from building monolithic data pipeline\nstructures to modular data products.\n\nData Product Components\n\nA data product is the deliverable that contains everything around a business\nconcept to fulfill a data consumer's need:\n\n  * tables to actually store data\n  * code that transform data\n  * tests to verify and monitor that data is correct\n  * output ports to make data accessable\n  * input ports to ingest data from source systems or access other data products\n  * data contracts to describe the API\n  * documentation\n  * meta information, such as ownership\n\nA data product is usually managed in one Git repository.\n\nDatabricks is one of the most popular modern data platforms, now how can we\nengineer a professional data product with Databricks?\n\nA workflow job deployed as a Databricks Asset Bundle\n\nIn this article, we will use Data Contracts and the new Databricks Asset\nBundles that are a great fit to implement data products. All source code of\nthis example project is available on GitHub.\n\n## Define the Data Contract\n\nBefore we start implementing, let's discuss and define the business\nrequirements. What does our data consumer need from us, what is their use\ncase, what do they expect as a data model. And we need to make sure, that we\nunderstand and share the same semantics, quality expectations, and expected\nservice levels.\n\n> We call this approach contract-first. We start designing the interface of\n> the provided data model and its metadata as a data contract. We use the data\n> contract to drive the implementation.\n\nIn our example, the COO of an e-commerce company wants to know if there is an\nissue with articles that are not sold for a longer period, i.e., articles with\nno sale during the last three months, the so-called shelf warmers.\n\nIn collaboration with the data consumer, we define a data contract as YAML,\nusing the Data Contract Specification:\n\n    \n    \n    dataContractSpecification: 0.9.3 id: urn:datacontract:fulfillment:stock-last-sales info: title: Last Sales version: 1.0.0 description: | The data model contains all articles that are in stock. For every article the last sale timestamp is defined. owner: Fulfillment contact: name: John Doe (Data Product Owner) url: https://teams.microsoft.com/l/channel/19%3Ad7X0bzOrUrZ-QAGu0nTxlWACe5HOQ-8Joql71A_00000%40thread.tacv2/General?groupId=4d213734-d5a1-4130-8024-00000000&tenantId=b000e7de-5c4d-41a2-9e67-00000000 servers: development: type: databricks host: dbc-abcdefgh-1234.cloud.databricks.com catalog: acme schema: stock-last-sales terms: usage: > Data can be used for reports, analytics and machine learning use cases. Order may be linked and joined by other tables limitations: > Not suitable for real-time use cases. billing: free noticePeriod: P3M models: articles: description: One record per article that is currently in stock type: table fields: sku: description: The article number (stock keeping unit) type: string primary: true pattern: ^[A-Za-z0-9]{8,14}$ minLength: 8 maxLength: 14 example: \"96385074\" quantity: description: The total amount of articles that are currently in stock in all warehouses. type: long minimum: 1 required: true last_sale_timestamp: description: The business timestamp in UTC when there was the last sale for this article. Null means that the article was never sold. type: timestamp processing_timestamp: description: The technical timestamp in UTC when this row was updated type: timestamp required: true servicelevels: availability: percentage: 99.9% retention: period: 1 year freshness: threshold: 25 hours timestampField: articles.processing_timestamp frequency: description: Data is updated once a day type: batch cron: 0 0 * * * examples: - type: csv data: | sku,quantity,last_sale_timestamp,processing_timestamp 1234567890123,5,2024-02-25T16:16:30.171798,2024-03-25T16:16:30.171807 2345678901234,10,,2024-03-25T15:16:30.171811 3456789012345,15,2024-03-02T12:16:30.171814,2024-03-25T14:16:30.171816 4567890123456,20,,2024-03-25T13:16:30.171817 5678901234567,25,2024-03-08T08:16:30.171819,2024-03-25T12:16:30.171821 6789012345678,30,,2024-03-25T11:16:30.171823 7890123456789,35,2024-03-14T04:16:30.171824,2024-03-25T10:16:30.171826 8901234567890,40,,2024-03-25T09:16:30.171830 9012345678901,45,2024-03-20T00:16:30.171833,2024-03-25T08:16:30.171835 0123456789012,50,,2024-03-25T07:16:30.171837 quality: type: SodaCL specification: checks for articles: - row_count > 1000\n\ndatacontract.yaml\n\nThe dataset will contain all articles that currently are in stock and it\nincludes the last_sale_timestamp, the attribute that is most relevant for the\nCOO. The COO can easily filter in their BI tool (such as PowerBI, redash, ...)\nfor articles with last_sale_timestamp older than three months. Terms and\nservice Level attributes make it clear that the dataset is update daily at\nmidnight.\n\n## Create the Databricks Asset Bundle\n\nNow it is time to develop a data product that implements this data contract.\nDatabricks recently added the concept of Databricks Asset Bundles that are a\ngreat fit to structure and develop data products. As time of writing in March\n2024, they are in Public Preview, meaning ready for production-use.\n\nDatabricks Asset Bundles include all the infrastructure and code files to\nactually deploy data transformations to Databricks:\n\n  * Infrastructure resources\n  * Workspace configuration\n  * Source files, such as notebooks and Python scripts\n  * Unit tests\n\nThe Databricks CLI bundles these assets and deploys them to Databricks\nPlatform, internally it uses Terraform. Asset Bundles are well-integrated into\nthe Databricks Platform, e.g., it is not possible to edit code or jobs\ndirectly in Databricks, which enables a strict version control of all code and\npipeline configuration.\n\nBundles are extremely useful, when you have multiple environments, such as\ndev, staging, and production. You can deploy the same bundle to multiple\ntargets with different configurations.\n\nTo create a bundle, let's init in a new bundle:\n\n    \n    \n    databricks bundle init\n\nWe use this configuration:\n\n  * Template to use: default-python\n  * Unique name for this project: stock_last_sales\n  * Include a stub (sample) notebook in 'stock_last_sales/src': yes\n  * Include a stub (sample) Delta Live Tables pipeline in 'stock_last_sales/src': no\n  * Include a stub (sample) Python package in 'stock_last_sales/src': yes\n\nWhen we look into the bundle structure, let's have a quick look at the most\nrelevant files:\n\n  * databricks.yml The bundle configuration and deployment targets\n  * src/ The folder for the transformation code\n  * tests/ The folder to place unit tests\n  * resources/ The job definition for the workflow definition\n\n> Note: We recommend to maintain an internal bundle as template that\n> incorporates the company's naming conventions, global policies, best\n> practices, and integrations.\n\nWith asset bundles, we can write our code locally in our preferred IDE, such\nas VS Code (using the Databricks extension for Visual Studio Code), PyCharm,\nor IntelliJ IDEA (using Databricks Connect).\n\nDatabricks Asset Bundle in IntelliJ IDEA\n\nTo set up a local Python environment, we can use venv and install the\ndevelopment dependencies:\n\n    \n    \n    python3.10 -m venv .venv source .venv/bin/activate pip install -r requirements-dev.txt\n\n## Generate Unity Catalog Table\n\nHow do we organize the data for our data product? In this example, we use\nUnity Catalog to manage storage as managed tables. On an isolation level, we\ndecide that one data product should represent one schema in Unity Catalog.\n\nWe can leverage the data contract YAML to generate infrastructure code:\n\nThe model defines the table structure of the target data model. With the Data\nContract CLI tool, we can generate the SQL DDL code for the CREATE TABLE\nstatement.\n\n    \n    \n    datacontract export --format sql datacontract.yaml -- Data Contract: urn:datacontract:fulfillment:stock-last-sales -- SQL Dialect: databricks CREATE OR REPLACE TABLE acme.stock_last_sales.articles ( sku STRING primary key, quantity BIGINT not null, last_sale_timestamp TIMESTAMP, processing_timestamp TIMESTAMP not null );\n\nThe Data Contract CLI tool is also available as a Python Library datacontract-\ncli. So let's add it to the requirements-dev.txt and use it in directly in a\nDatabricks notebook to actually create the table in Unity Catalog:\n\nUse Data Contract CLI to create the Unity Catalog Table\n\nThe Unity Catalog tables is a managed tables that internally uses the Delta\nformat for efficient storage.\n\nThe created table in Unity Catalog\n\n## Develop Transformation Code\n\nNow, let's write the core transformation logic. With Python-based Databricks\nAsset Bundles, we can develop our data pipelines as:\n\n  * Databricks Notebooks,\n  * Delta Live Tables, or\n  * Python files\n\nIn this data product, we'll write plain Python files for our core\ntransformation logic that will be deployed as Wheel packages.\n\nOur transformation takes all available stocks that we get from an input port,\nsuch as the operational system that manages the current stock data, and left-\njoins the dataframe with the latest sales timestamp for every sku. The sales\ninformation are also an input port, e.g., another upstream data product\nprovided by the checkout team. We store the resulting dataframe in the\npreviously generated table structure.\n\nTransformation code as plain Python code\n\nWith that option, the code remains reusable, easy to test with unit tests, and\nwe can run it on our local machines. As professional data engineers, we make\nsure that the calculate_last_sales() function works as expected by writing\ngood unit tests.\n\nUnit Tests\n\nWe update the job configuration to run the Python code as a python_wheel_task\nand configure the scheduler and the appropriate compute cluster.\n\n    \n    \n    # The main job for stock_last_sales. resources: jobs: stock_last_sales_job: name: stock_last_sales_job schedule: # Run every day at midnight quartz_cron_expression: '0 0 0 * * ?' timezone_id: Europe/Amsterdam tasks: - task_key: create_unity_catalog_table job_cluster_key: job_cluster notebook_task: notebook_path: ../src/create_unity_catalog_table.ipynb libraries: - pypi: package: datacontract-cli - task_key: main_task depends_on: - task_key: create_unity_catalog_table job_cluster_key: job_cluster python_wheel_task: package_name: stock_last_sales entry_point: main libraries: - whl: ../dist/*.whl job_clusters: - job_cluster_key: job_cluster new_cluster: spark_version: 13.3.x-scala2.12 node_type_id: i3.xlarge autoscale: min_workers: 1 max_workers: 4\n\nresources/stock_last_sales_job.yml\n\nWhen we are confident, we can deploy the bundle to our Databricks dev\ninstances (manually for now):\n\n    \n    \n    databricks bundle deploy\n\nAnd let's trigger a manual run of our workflow:\n\n    \n    \n    databricks bundle run stock_last_sales_job\n\nIn Databricks, we can see that the workflow run was successful:\n\nA job in Databricks\n\nAnd we have data in our table that we created earlier.\n\nThe result of our data pipeline\n\n## Test the Data Contract\n\nWe are not quite finished with our task. How do we know, that the data is\ncorrect? While we have unit tests that give us confidence on the\ntransformation code, we also need an acceptance test to verify, that we\nimplemented the agreed data contract correctly.\n\nFor that, we can use the Data Contract CLI tool to make this check:\n\n    \n    \n    export DATACONTRACT_DATABRICKS_HTTP_PATH=/sql/1.0/warehouses/b053xxxxxxx export DATACONTRACT_DATABRICKS_TOKEN=dapia1926f7c64b7595880909xxxxxxxxxx datacontract test datacontract.yaml\n\nThe datacontract tool takes all the schema and format information from the\nmodel, the quality attributes, and the metadata, and compares them with the\nactual dataset. It reads the connection details from the servers section and\nconnects to Databricks executes all the checks and gives a comprehensive\noverview.\n\nData Contract Test Results\n\nWe want to execute this test with every pipeline run, so once again, let's\nmake a Notebook task for the test:\n\nData Contract Test as Notebook\n\n## Deploy with CI/CD\n\nTo automatically test, deploy the Asset Bundle to Databricks, and finally run\nthe job once, we set up a CI/CD pipeline in GitHub, using a GitHub Action.\n\n    \n    \n    name: \"Deploy Databricks Assets Bundle\" on: workflow_dispatch: push: branches: [ \"main\" ] jobs: test: name: \"Run unit tests\" runs-on: ubuntu-latest steps: - uses: actions/checkout@v4 - name: Set up Python ${{matrix.python-version}} uses: actions/setup-python@v5 with: python-version: \"3.10\" - name: Install dependencies run: | python -m pip install --upgrade pip pip install -r requirements-dev.txt - name: Test with pytest run: pytest env: DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }} DATABRICKS_TOKEN: ${{ secrets.SP_TOKEN }} DATABRICKS_CLUSTER_ID: ${{ secrets.DATABRICKS_CLUSTER_ID }} deploy: name: \"Deploy bundle to DEV\" runs-on: ubuntu-latest needs: - test steps: - uses: actions/checkout@v4 - uses: databricks/setup-cli@main - run: databricks bundle deploy working-directory: . env: DATABRICKS_TOKEN: ${{ secrets.SP_TOKEN }} DATABRICKS_BUNDLE_ENV: dev run_pipieline: name: \"Run pipeline\" runs-on: ubuntu-latest needs: - deploy steps: - uses: actions/checkout@v4 - uses: databricks/setup-cli@main - run: databricks bundle run stock_last_sales_job --refresh-all working-directory: . env: DATABRICKS_TOKEN: ${{ secrets.SP_TOKEN }} DATABRICKS_BUNDLE_ENV: dev\n\n.github/workflows/deploy.yml\n\nNow, every time we update the code, the Asset Bundle is automatically deployed\nto Databricks.\n\nCI/CD workflow in GitHub\n\n## Publish Metadata\n\nFor others to find, understand, and trust data products, we want to register\nthem in a data product registry.\n\nIn this example, we use Data Mesh Manager, a platform to register, manage, and\ndiscover data products, data contracts, and global policies.\n\nMetadata in Data Mesh Manager\n\nAgain, let's create a notebook task (or Python code task) to publish the\nmetadata to Data Mesh Manager and add the task to our workflow. We can use\nDatabricks Secrets to make the API Key available in Databricks.\n\n    \n    \n    databricks secrets create-scope datamesh_manager databricks secrets put-secret datamesh_manager api_key\n\nPublish Metadata\n\n## Summary\n\nNow, the COO can connect to this table with a BI tool (such as PowerBI,\nTableau, Redash, or withing Databricks) to answer their business question.\n\nAnalytics within Databricks\n\nDatabricks Asset Bundles are a great fit to develop professional data products\non Databricks, as it bundles all the resources and configurations (code,\ntests, storage, compute, scheduler, metadata, ...) that are needed to provide\nhigh-quality datasets to data consumers.\n\nIt is easy to integration Data Contracts for defining the requirements and the\nData Contract CLI to automate acceptance tests.\n\nFind the source code for the example project on GitHub.\n\n## Learn More\n\n  * Source Code for the examples project\n  * Official databricks documentation on Databricks Asset Bundles\n  * Databricks Tech Stack\n  * Data Contract Specification\n  * Data Contract CLI\n  * Data Mesh Manager\n\n## Need Help?\n\nINNOQ offers Data Product Engineering Consulting (Ad).\n\nWorkshop Training Legal Notice Privacy\n\n", "frontpage": false}
