{"aid": "39978872", "title": "RecurrentGemma: Mixture local attention and linear recurrences", "url": "https://github.com/google-deepmind/recurrentgemma", "domain": "github.com/google-deepmind", "votes": 1, "user": "tosh", "posted_at": "2024-04-09 12:51:36", "comments": 0, "source_title": "GitHub - google-deepmind/recurrentgemma: Open weights language model from Google DeepMind, based on Griffin.", "source_text": "GitHub - google-deepmind/recurrentgemma: Open weights language model from\nGoogle DeepMind, based on Griffin.\n\nSkip to content\n\nSign in\n\n# Search code, repositories, users, issues, pull requests...\n\nSearch syntax tips\n\nSign in\n\nSign up\n\nYou signed in with another tab or window. Reload to refresh your session. You\nsigned out in another tab or window. Reload to refresh your session. You\nswitched accounts on another tab or window. Reload to refresh your session.\nDismiss alert\n\ngoogle-deepmind / recurrentgemma Public\n\n  * Notifications\n  * Fork 0\n  * Star 9\n\nOpen weights language model from Google DeepMind, based on Griffin.\n\n### License\n\nApache-2.0 license\n\n9 stars 0 forks Branches Tags Activity\n\nStar\n\nNotifications\n\n# google-deepmind/recurrentgemma\n\nThis commit does not belong to any branch on this repository, and may belong\nto a fork outside of the repository.\n\n1 Branch\n\n0 Tags\n\n## Folders and files\n\nName| Name| Last commit message| Last commit date  \n---|---|---|---  \n  \n## Latest commit\n\nNush395Initial commit.ee7a581 \u00b7\n\n## History\n\n1 Commits  \n  \n### colabs\n\n|\n\n### colabs\n\n| Initial commit.  \n  \n### examples\n\n|\n\n### examples\n\n| Initial commit.  \n  \n### recurrentgemma\n\n|\n\n### recurrentgemma\n\n| Initial commit.  \n  \n### CONTRIBUTING.md\n\n|\n\n### CONTRIBUTING.md\n\n| Initial commit.  \n  \n### LICENSE\n\n|\n\n### LICENSE\n\n| Initial commit.  \n  \n### README.md\n\n|\n\n### README.md\n\n| Initial commit.  \n  \n### pyproject.toml\n\n|\n\n### pyproject.toml\n\n| Initial commit.  \n  \n## Repository files navigation\n\n# RecurrentGemma\n\nRecurrentGemma is a family of open-weights Language Models by Google DeepMind,\nbased on the novel Griffin architecture. This architecture achieves fast\ninference when generating long sequences by replacing global attention with a\nmixture of local attention and linear recurrences.\n\nThis repository contains the model implementation and examples for sampling\nand fine-tuning. We recommend most users adopt the Flax implementation, which\nis highly optimized. We also provide an un-optimized PyTorch implementation\nfor reference.\n\n### Learn more about RecurrentGemma\n\n  * The Griffin paper describes the model architecture.\n  * We will shortly be releasing additional details on the training pipeline for RecurrentGemma.\n\n## Quick start\n\n### Installation\n\n#### Using Poetry\n\nRecurrentGemma uses Poetry for dependency management.\n\nTo install dependencies for the full project:\n\n  * Checkout the code.\n  * poetry install -E full to create a virtual environment with all dependencies.\n  * poetry shell to activate the created virtual environment.\n\nIf you only need to install a subset of dependencies use one of the\nalternative library-specific commands below.\n\n#### Using pip\n\nIf you want to use pip instead of Poetry, then create a virtual environment\n(run python -m venv recurrentgemma-demo and . recurrentgemma-\ndemo/bin/activate) and:\n\n  * Checkout the code.\n  * pip install .[full]\n\n#### Installing library-specific packages\n\n##### JAX\n\nTo install dependencies only for the JAX pathway use: poetry install -E jax or\n(pip install .[jax]).\n\n##### PyTorch\n\nTo install dependencies only for the PyTorch pathway use: poetry install -E\ntorch (or pip install .[torch]).\n\n##### Tests\n\nTo install dependencies required for running unit tests use: poetry install -E\ntest (or pip install .[test])\n\n### Downloading the models\n\nThe model checkpoints are available through Kaggle at\nhttp://kaggle.com/models/google/recurrentgemma. Select either the Flax or\nPyTorch model variations, click the \u2913 button to download the model archive,\nthen extract the contents to a local directory.\n\nIn both cases, the archive contains both the model weights and the tokenizer.\n\n### Running the unit tests\n\nTo run the tests, install the optional [test] dependencies (e.g. using pip\ninstall .[test]) from the root of the source tree, then:\n\n    \n    \n    pytest .\n\n## Examples\n\nTo run the example sampling script, pass the paths to the weights directory\nand tokenizer:\n\n    \n    \n    python examples/sampling_jax.py \\ --path_checkpoint=/path/to/archive/contents/2b/ \\ --path_tokenizer=/path/to/archive/contents/tokenizer.model\n\n### Colab notebook tutorials\n\n  * colabs/sampling_tutorial_jax.ipynb contains a Colab notebook with a sampling example using JAX.\n\n  * colabs/sampling_tutorial_pytorch.ipynb contains a Colab notebook with a sampling example using PyTorch.\n\n  * colabs/fine_tuning_tutorial_jax.ipynb contains a Colab with a basic tutorial on how to fine-tune RecurrentGemma for a task, such as English to French translation, using JAX.\n\nTo run these notebooks you will need to have a Kaggle account and first read\nand accept the Gemma license terms and conditions from the RecurrentGemma\npage. After this you can run the notebooks, which will automatically download\nthe weights and tokenizer from there.\n\nCurrently different notebooks are supported under the following hardware:\n\nHardware| T4| P100| V100| A100| TPUv2| TPUv3+  \n---|---|---|---|---|---|---  \nSampling in Jax| \u2705| \u2705| \u2705| \u2705| \u2705| \u2705  \nSampling in PyTorch| \u2705| \u2705| \u2705| \u2705| \u2705| \u2705  \nFinetuning in Jax| \u2705| \u2705| \u2705| \u2705| \u274c| \u2705  \n  \n## System Requirements\n\nRecurrentGemma code can run on CPU, GPU or TPU. The code has been optimized\nfor running on TPU using the Flax implementation, which contains a low level\nPallas kernel to perform the linear scan in the recurrent layers.\n\n## Contributing\n\nWe are open to bug reports and issues. Please see CONTRIBUTING.md for details\non PRs.\n\n## License\n\nCopyright 2024 DeepMind Technologies Limited\n\nThis code is licensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License. You may\nobtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0.\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an AS IS BASIS, WITHOUT\nWARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\nLicense for the specific language governing permissions and limitations under\nthe License.\n\n## Disclaimer\n\nThis is not an official Google product.\n\n## About\n\nOpen weights language model from Google DeepMind, based on Griffin.\n\n### Resources\n\nReadme\n\n### License\n\nApache-2.0 license\n\nActivity\n\nCustom properties\n\n### Stars\n\n9 stars\n\n### Watchers\n\n11 watching\n\n### Forks\n\n0 forks\n\nReport repository\n\n## Releases\n\nNo releases published\n\n## Packages 0\n\nNo packages published\n\n## Contributors 3\n\n  * botev Alexander Botev\n  * SamSmithGDM\n  * Nush395 Anushan Fernando\n\n## Languages\n\n  * Python 80.9%\n  * Jupyter Notebook 19.1%\n\n## Footer\n\n\u00a9 2024 GitHub, Inc.\n\nYou can\u2019t perform that action at this time.\n\n", "frontpage": false}
