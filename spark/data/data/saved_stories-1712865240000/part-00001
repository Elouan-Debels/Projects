{"aid": "40002154", "title": "Circular Buffer Performance Trick", "url": "https://cybernetist.com/2024/04/11/circular-buffer-performance-trick/", "domain": "cybernetist.com", "votes": 2, "user": "todsacerdoti", "posted_at": "2024-04-11 13:52:09", "comments": 0, "source_title": "Circular Buffer Performance Trick", "source_text": "Circular Buffer Performance Trick - Cybernetist\n\n# Circular Buffer Performance Trick\n\n| 8 minutes | 1540 words | Milos Gajdos\n\nI have been hacking on AI agents recently for both fun and profit as part of\nthe work I\u2019m doing for one of my clients.\n\nThey\u2019re mostly text-to-speech (TTS) agents leveraging LLMs for generating text\nwhich is then turned into voice by a trained TTS model.\n\nAs you [probably] know, maintaining conversation with LLMs over a longer\nperiod of time requires maintaining the conversational context and sending it\nback to the LLM along with your follow-up prompts to prevent the LLMs from\n\u201challucinating\u201d from the get-go.\n\nWe had a need for a small agent that would act as a sort of supervisor for a\nfew other agents and we felt using one of the over-bloated LLM frameworks\nwhich tend to maintain the context and append the new prompts to it on your\nbehalf posed an unnecessary overhead for this task.\n\nSo we had to come up with a way to maintain the conversational context of\nspecific size and it needed to be reasonably fast given the volume of back and\nforth between the agents and thus consequently LLMs.\n\nMost of the agents we have are written in Python and Go, but as part of my\nside hacks I started digging into developing them in Rust along with some\nfunky new Rust crates. But this specific agent was actually written in Go,\nhence the following code is Go-based. Nevertheless, I will list the Rust\nimplementation at the end as I needed it for one of my side projects which I\u2019m\nhoping to blog about soon.\n\nAnyways, the basic requirements were pretty simple: we needed a fixed-size\nbuffer (strings will do for the demonstration purpose). We needed that buffer\nto be circular: every time a new prompt is added to it, it needs to be\nappended at the end of the buffer and the element in the front of the buffer\nneeds to be popped out so we maintain the fixed size and prevent the memory\nbloat.\n\nThe initial implementation would be something like this:\n\n    \n    \n    type History struct { data []string size int } func NewHistory(size int) *History { return &History{ data: make([]string, 0, size), size: size, } } func (h *History) Add(element string) { if len(h.data) < h.size { h.data = append(h.data, element) return } h.data = append(h.data[1:], element) } func (h *History) String() string { return strings.Join(h.data, \"\\n\") }\n\nNote, that we\u2019ve added the String method to History which plays two roles\n\n  * implements the Stringer interface for easy context inspection\n  * we use it to generate the new prompt we send to the LLM\n\nThis is hopefully straightforward, but in case it isn\u2019t just run this simple\nprogram to see it in action.\n\n    \n    \n    package main import ( \"fmt\" ) func main() { h := NewHistory(3) for _, s := range []string{\"foo\", \"bar\", \"car\", \"dar\", \"ear\"} { h.Add(s) } // prints: car\\ndar\\near fmt.Println(h) }\n\nThis works fine and does the job, but an experienced Go programmer notices\nthat if the conversation continues for a long time we end up making\nunnecessary allocations in the Add method due to append calls that might need\nto allocate new patches of memory \u2013 this also puts an extra pressure on Go GC\nwhich also isn\u2019t ideal.\n\nLet\u2019s write a simple benchmark that tests the performance for buffers of\nvarying sizes; something like this could do:\n\n    \n    \n    func BenchmarkHistory(b *testing.B) { sizes := []int{10, 100, 1000, 10000} for _, size := range sizes { b.Run(fmt.Sprintf(\"Add_Size_%d\", size), func(b *testing.B) { h := NewHistory(size) b.ResetTimer() for i := 0; i < 10*size*b.N; i++ { h.Add(\"benchmark\") _ = h.String() } }) } }\n\nNOTE: we want to benchmark Add and String together as they usually happen\naround the same time so they\u2019re in the same hot path.\n\nHere are the results\n\n    \n    \n    goos: darwin goarch: arm64 pkg: history BenchmarkHistory/Add_Size_10-12 135546 8878 ns/op 14400 B/op 111 allocs/op BenchmarkHistory/Add_Size_100-12 1818 647935 ns/op 1055979 B/op 1010 allocs/op BenchmarkHistory/Add_Size_1000-12 19 62660368 ns/op 102598794 B/op 10028 allocs/op BenchmarkHistory/Add_Size_10000-12 1 5882203041 ns/op 10122446440 B/op 100580 allocs/op PASS ok history 10.835s\n\nThis isn\u2019t horrible but we can do better!\n\nIf you think about it a bit harder you will notice that there is no need for\nextra allocations when adding a new element to the history buffer. We know\nthat the history has a fixed size and that if we add a new element to it we\nmust pop the one in the front i.e. make the room for a new entry, old man! We\nshould be able to allocate the buffer only once, at the beginning of the\nprogram, and never again.\n\nTo solve this problem, we can simply override the element that needs to be\npoped by a newly added element and maintain a cursor to its position. We also\nneed to do this in a circular fashion, hence calling it a circular buffer, so\nwe can reconstruct the full history easily. Let\u2019s demonstrate it in code to\nmake it a bit clearer what I mean:\n\n    \n    \n    type History struct { data []string size int pos int } func NewHistory(size int) *History { return &History{ data: make([]string, size), size: size, pos: 0, } } func (h *History) Add(element string) { h.data[h.pos] = element h.pos = (h.pos + 1) % h.size } func (fs *FixedSizeSlice) Get() []string { result := make([]string, fs.size) for i := 0; i < fs.size; i++ { result[i] = fs.data[(fs.pos+i)%fs.size] } return result } func (h *History) String() string { result := make([]string, h.size) for i := 0; i < h.size; i++ { result[i] = h.data[(h.pos+i)%h.size] } return strings.Join(result, \"\\n\") }\n\nNotice how we had to change the String method to accommodate the new buffer\ndesign. We couldn\u2019t just simply reuse the previous implementation as our data\nbuffer is no longer \u201clinear\u201d (contiguous?) i.e. it wouldnt return the context\nin chronological order.\n\nOk, this seems better but just to make sure it is, let\u2019s run the benchmark\nagain:\n\n    \n    \n    goos: darwin goarch: arm64 pkg: history BenchmarkHistory/Add_Size_10-12 84068 12798 ns/op 27200 B/op 200 allocs/op BenchmarkHistory/Add_Size_100-12 1359 873885 ns/op 2815989 B/op 2000 allocs/op BenchmarkHistory/Add_Size_1000-12 14 83245247 ns/op 265928059 B/op 20017 allocs/op BenchmarkHistory/Add_Size_10000-12 1 8806193542 ns/op 26552736240 B/op 200171 allocs/op PASS ok history 12.726s\n\nOooooof, there goes our performance optimisation! This is even worse. How\ncould that be? If you squint hard enough you will notice we\u2019re creating a\nresult slice every time we need to return the conversation which happens a lot\nduring the agent conversations. And that\u2019s the penalty we are ultimately\npaying here. There must be a better way! And there is, indeed!\n\nThough we won\u2019t be able to entirely ditch the memory allocations in String\nbecause we must assemble the final prompt from the collected context, but we\ncan make it a bit more performant by leveraging strings.Builder.\n\n> A Builder is used to efficiently build a string using Write methods. It\n> minimizes memory copying. The zero value is ready to use.\n\nThe new String implementation looks like this:\n\n    \n    \n    func (h *History) String() string { sb := strings.Builder{} sb.Grow(h.size * (len(h.data[0]) + 1)) idx := h.pos for i := 0; i < h.size; i++ { sb.WriteString(h.data[idx]) if i < h.size-1 { sb.WriteByte('\\n') } idx = (idx + 1) % h.size } return sb.String() }\n\nIf we run the benchmark again we get the following result:\n\n    \n    \n    goos: darwin goarch: arm64 pkg: history BenchmarkHistory/Add_Size_10-12 184761 6508 ns/op 11200 B/op 100 allocs/op BenchmarkHistory/Add_Size_100-12 1981 593697 ns/op 1024007 B/op 1000 allocs/op BenchmarkHistory/Add_Size_1000-12 20 56352919 ns/op 102400864 B/op 10009 allocs/op BenchmarkHistory/Add_Size_10000-12 1 5457632291 ns/op 10149619328 B/op 100171 allocs/op PASS ok history 10.381s\n\nThis is significantly better than the previous effort and much better than the\noriginal implementation of the buffer we started with.\n\nWe could probably spend more time on this but this is already pretty good for\nhalf an hour\u2019s worth of work and we have to move on to more interesting bits\nlike making the agents talk to each other.\n\nI mentioned at the beginning that I needed something like this for the Rust\nagent we kicked off, so here\u2019s the Rust implementation in case anyone is\nwondering about it:\n\n    \n    \n    struct History { data: Vec<String>, size: usize, pos: usize, } impl History { fn new(size: usize) -> Self { History { data: vec![String::new(); size], size, pos: 0, } } fn add(&mut self, element: String) { self.data[self.pos] = element; self.pos = (self.pos + 1) % self.size; } fn string(&self) -> String { let mut result = Vec::with_capacity(self.size); for i in 0..self.size { result.push(&self.data[(self.pos + i) % self.size][..]); } result.join(\"\\n\") } } impl fmt::Display for History { fn fmt(&self, f: &mut fmt::Formatter) -> fmt::Result { let mut result = Vec::with_capacity(self.size); for i in 0..self.size { result.push(&self.data[(self.pos + i) % self.size][..]); } write!(f, \"{}\", result.join(\"\\n\")) } }\n\nI am not going to compare the performance of the Rust implementation to the Go\none because that\u2019d be like bringing a tank to a knife fight, but feel free and\nadd a comment to this blog post.\n\nAs always with a bit more thinking and a tiny more time investment we can\navoid unnecessary performance issues that will pay off for us down the line.\nRemember, performance is a feature! Now go optimise your code!\n\nIf you are working on AI agents, we should work together! I do have some\navailability so feel free to pop an email to my GitHub email address.\n\ngo golang rust performance\n\n#### See also\n\n  * A Small Tool for Exploring Text Embeddings\n  * Fun With AI Embeddings in Go\n  * Getting Started With LDAP in Go\n  * Build a Graph of Kubernetes API Objects in Go\n  * Breadth-first search using Go standard library\n\n  * \u2190 Previous Post\n\nMilos Gajdos \u2022 \u00a9 2024 \u2022 Cybernetist\n\nHugo v0.124.1 powered \u2022 Theme Beautiful Hugo adapted from Beautiful Jekyll\n\n", "frontpage": false}
