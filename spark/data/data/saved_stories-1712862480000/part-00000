{"aid": "40001662", "title": "How to pick the best embedding model for a RAG app", "url": "https://vectorize.io/picking-the-best-embedding-model-for-rag/", "domain": "vectorize.io", "votes": 4, "user": "bytearray", "posted_at": "2024-04-11 13:06:02", "comments": 0, "source_title": "Picking the best embedding model for RAG - Vectorize", "source_text": "Picking the best embedding model for RAG - Vectorize\n\nWe use essential cookies to make our site work. With your consent, we may also\nuse non-essential cookies to improve user experience, personalize content, and\nanalyze website traffic. For these reasons, we may share your site usage data\nwith our analytics partners. By clicking \u201cAccept,\u201d you agree to our website's\ncookie use as described in our Cookie Policy. You can change your cookie\nsettings at any time by clicking \u201cPreferences.\u201d\n\n  * Use Cases\n\n    * Question Answering Systems\n    * AI Copilots\n    * Call Center Automation\n    * Content Automation\n    * Hyper-personalization\n  * Blog\n  * About\n  * Learn\n\n    * Prompt Engineering\n    * Retrieval Augmented Generation (RAG)\n    * Vector Database Guide\n  * Contact\n\nContact Us\n\n#### Be on of the first to try Vectorize!\n\nEdit Content\n\nEmbedding Models, RAG\n\n# Picking the best embedding model for RAG\n\nApril 11, 2024 Chris Latimer No comments yet\n\n## What are text embedding models?\n\nText embedding models are a type of machine learning model that are used\nheavily in the domain of natural language processing. These models typically\naccept a text input and convert that text into a numerical representation that\nencodes the semantic meaning of the text. Embedding models were instrumental\nin early machine learning use cases like sentiment analysis, classification\nand summarization just to name a few.\n\nIncreasingly, embedding models are finding themselves in the spotlight as more\ndevelopers start to build generative AI and other artificial intelligence\nfeatures into their applications. These developers typically discover\ntechniques such as retrieval augmented generation (RAG), which relies heavily\non semantic search to identify relevant context which in turn helps large\nlanguage models (LLMs) to reduce hallucinations and provide higher accuracy\nresponses to user queries.\n\nUsing RAG, developers can leverage an LLM to generate text on topics that are\ncompletely outside of its training dataset.\n\n## How are embeddings used in retrieval augmented generation (RAG)?\n\nLet\u2019s start by walking through a simple example of how RAG works and along the\nway we will highlight the role of text embedding vectors before diving deeper\ninto selecting the model that will work best for your particular use case.\n\nFor this example, let\u2019s imagine that we have a \u201cchat with our docs\u201d\napplication. Because we are releasing new versions of our product on a regular\nbasis, pre-trained foundational models like GPT or Gemma or Mistral will\nusually lack the latest product details in their training dataset. Therefore,\nwe can use RAG to provide the LLM with the missing context data and help it\ncreate higher quality outputs that can assist users in understanding how to\nuse our product.\n\n### Step 1: User Submits Query to LLM\n\nThis use case starts out like most interactions with an LLM, by the user\nsubmitting a prompt.\n\nHere you can see that our user is asking how to connect their Facebook account\nto our fictional product, however instead of asking this directly to the LLM,\nwe have application logic that will first enhance the prompt before making the\ncall to the LLM.\n\n### Step 2: RAG application generates query vector and performs similarity\nsearch\n\nIn the simplest approach to RAG, the application will use the user\u2019s query to\nperform a semantic search to retrieve relevant search results, often using a\nvector database.\n\nIn order to perform this semantic search, the RAG application will use an\nembedding model. The embedding model will take the text from the user\u2019s prompt\nand it will produce an output vector called an embedding.\n\nThe application then submits a short query to the vector database. The vector\ndatabase identifies similar documents by comparing the vector input, also\ncalled a query vector, against the embedding vectors in the appropriate search\nindex. It does this by using a distance metric, such as cosine similarity\n(most common) or dot product, to measure the similarity between the documents\nit has in the indexed dataset and the query vector being used to search\nembeddings.\n\nThis overall process is known as an approximate nearest neighbor search and is\nthe most common approach used by vector search engines. This approach to\nsemantic search is a useful technique to allow applications to get more\nrelevant search results compared to alternative methods like full text or\nkeyword searches.\n\n### Step 3: RAG application augments the prompt using semantic search results\n\nIn this step, the RAG application uses the context it retrieved in step 2 to\nproduce an augmented prompt. Using a basic form of prompt engineering, the RAG\napplication takes the original prompt then injects the context along with any\nother guidance the app wishes to provide.\n\nOnce the augmented prompt is created, it is then passed to the language model\nto generate a response.\n\n### Step 4: LLM generates a response based on the augmented prompt\n\nHere the LLM responds to the prompt, which now takes into account the\nadditional contexts which were supplied to it in step 3. This allows the LLM\nto provide information about the new release of our product even though its\ntraining datasets didn\u2019t include any documents about this version. That\u2019s kind\nof amazing when you think about it!\n\n## Choosing the best embedding model for your application\n\n### Hugging Face MTEB leaderboard\n\nMost developers have one of two default ways to decide which embedding model\nto focus on. They either use one of OpenAI\u2019s embedding model options because\nthey are using one of the GPT language models.\n\nThe other popular approach is to start exploring the MTEB leaderboard. This is\na great resource to see at a high level how various models perform on specific\nsets of standardized benchmark tasks.\n\nThese benchmarks cover a range of tasks and datasets. Some involve sentiment\nanalysis of sentences extracted from comment threads on discussion forums.\nOthers perform comparison analysis on question and answer pairs trying to\nidentify the most correct option from a set of possible answers. Some of these\nbenchmarks relate more to traditional use cases such as the ability of\nsentence transformers like BERT to assess phrases.\n\nIn retrieval augmented generation (RAG), you\u2019ll want to focus on benchmarks\nthat center on retrieval use cases. The distinction for these benchmarks is\nthat they aim to measure the performance and quality of a model\u2019s ability to\nidentify the most relevant document or documents in a dataset. Examples may\ninclude identifying the best article from a corpus of news columns, or\nperforming a search to identify the document with the closest semantic match\nwithin a large set of documents within an example dataset.\n\nMost commonly you will find embedding models that are trained to only\nunderstand English words and phrases. However, there are specialized models\nthat focus on other languages and some offer a multilingual version as well.\nAdditionally, there are even models that are trained to create embeddings from\nsoftware source code.\n\n### The challenge with text embedding model benchmarks\n\nWhile benchmarks are a good starting point, it\u2019s very common that the\nperformance of a model in a lab setting will vary considerably from the\nperformance you will experience in the real world. The solution to this\nproblem is to perform your own experiments to assess the differences between\nmodels and see which ones produce the best accuracy and similarity scores in\nyour app.\n\n### Key performance metrics to consider\n\n#### Context Relevancy (CR)\n\nThe idea behind a CR score is to assess how much of the context that you are\nproviding in your augmented prompt is relevant to answering the query\nsubmitted to the LLM. This score is calculated by evaluate each of the words\nin the prompt using a natural language processing model to determine if the\nword is relevant or not. Low CR scores can be a sign that you are not\nproviding enough information to help the LLM answer the user query with grater\naccuracy.\n\nThere is often a trade off between chunk sizes and the CR score for data\nreturned from a semantic search query. Larger chunks often contain irrelevant\ninformation, which can sometimes actually improve the accuracy of results\nsince the LLM have a greater contextual understanding to incorporate into the\ngeneration process. At the same time, larger chunks can also cause LLMs to\nstruggle when deciding which part of the chunk to pay attention to. For this\nreason, CR can be useful in determining whether your semantic search produces\nuseful information, it is not always the case that higher scores equal higher\naccuracy.\n\n#### Normalized Discounted Cumulative Gain (NDCG)\n\nNDCG is a metric that has long been used in search engines to determine if the\nsearch result data is useful and relevant for the query submitted by the user.\nCompared to CR, NDCG will tell you if the search result data returned is\nrelevant and also how well the results are ranked. It is often used in\ninformation retrieval to measure the overall quality of search results.\n\n## Using Vectorize to make a data-driven decision\n\nYou can certainly write your own code and create a script that chunks a sample\nof the data you intend to use for semantic search in your RAG application.\nOpen source frameworks like Ragas offer tools to assess the overall quality of\nthe embedding models you want to compare.\n\nHowever, this process is time consuming both to create the embeddings as well\nas to build logic to analyze the data to determine which option gives you the\nbest results. If you would rather get perform these assessments in minutes\nwith almost zero effort instead of over days with a lot of effort, you may\nwant to consider trying the Vectorize free experiment tool.\n\nNote: Vectorize is still in early-access mode, if you want to try the features\nhere and don\u2019t have access please sign up for our waitlist or if you need\nfaster access you can contact us.\n\n### Data driven experiments\n\nVectorize provides a frictionless tool for comparing the performance you can\nexpect from a set of embedding models and chunking strategies using a\nrepresentative sample of your data.\n\nExperiments will execute a vector pipeline to parse your unstructured data,\ngenerate embeddings, build a search index using either Pinecone or AstraDB as\nthe vector database engine and will then provide a quantitative assessment of\nwhich vectorization strategy will be optimal for semantic search capabilities\nin your RAG application.\n\n### Qualitative and collaborative assessments using the RAG Sandbox\n\nIt\u2019s always a good idea to verify the quantitative view with your own personal\nexperience. The Vectorize RAG Sandbox allows you to chat with your experiment\ndata. You can ask the LLM one of the questions that get generated as part of\nthe experiment, or submit a question of your own. You can also invite\nadditional team members to your Vectorize organization so you can get early\nfeedback from your users before you ever write a line of code.\n\nWith Vectorize you can take the guess work out of building relevant, accurate\nRAG applications and you can get it done in a tiny fraction of the time.\n\n## Conclusion\n\nEmbedding models are an essential component for any RAG application and there\nare many to choose from. The unique characteristics of your data and your use\ncase may produce different results than those from benchmark datasets.\n\nOpen source tools are available if you want to create your own experiments and\nanalysis, but tools like Vectorize can make that process much easier and much\nless effort intensive.\n\nWhichever approach you decide, having a strong capability around semantic\nsearch will be important to producing higher accuracy, better relevancy, and\nLLM-powered apps that your users love.\n\n### Share this:\n\n  * Twitter\n  * LinkedIn\n  * Facebook\n  * Reddit\n  * Pinterest\n  * Threads\n  * X\n\n### Related\n\n### Leave a ReplyCancel reply\n\n#### Search\n\n#### Categories\n\n#### Recent posts\n\n  * Picking the best embedding model for RAG\n\n  * 5 RAG Vector Database Traps and How to Avoid Them\n\n  * Retrieval Augmented Generation: The Easy Path To AI Relevancy\n\n#### Tags\n\nRAG Retrieval Augmented Generation\n\nThe easiest, fastest way to connect your data to your LLMs.\n\n##### Resources\n\n  * Support center\n  * Documentation\n  * Community\n  * Hosting\n\n##### Company\n\n  * About us\n  * Latest news\n  * Contact us\n  * Resources\n\n\u00a9 Vectorize AI, Inc, All Rights Reserved.\n\n  * Terms & Conditions\n  * Privacy Policy\n\n## Discover more from Vectorize\n\nSubscribe now to keep reading and get access to the full archive.\n\nContinue reading\n\nLoading Comments...\n\n", "frontpage": false}
