{"aid": "40055445", "title": "Reliable HTTP: Outsmarting the Two Generals with Webhooks", "url": "https://antman-does-software.com/reliable-http-outsmarting-the-two-generals-with-webhooks", "domain": "antman-does-software.com", "votes": 1, "user": "thunderbong", "posted_at": "2024-04-16 18:23:08", "comments": 0, "source_title": "Reliable HTTP: Outsmarting the Two Generals with Webhooks", "source_text": "Reliable HTTP: Outsmarting the Two Generals with Webhooks\n\n# Reliable HTTP: Outsmarting the Two Generals with Webhooks\n\nAnthony Manning-Franklin\n\n\u00b7Apr 15, 2024\u00b7\n\n21 min read\n\nFeatured on Hashnode\n\n## Table of contents\n\n  * Implementing Exactly-Once Processing\n\n  * Implementing Atleast Once Delivery\n\n  * Creating The Outbox\n\n  * Adding Messages To The Outbox\n\n  * Processing The Outbox\n\n  * Summary\n\nThe Two Generals Problem is a mathematical theorem proving that no messaging\nprotocol can reliably ensure that two parties share the same state.\n\nHowever, some approaches guarantee that two distributed systems will follow an\nacceptable state progression over time. That is, state changes progress\nlinearly and deterministically. It either halts or continues but never enters\nan unrecoverable state, e.g., missing one message in a series of messages.\n\nOne of the most common sources of problems in applications is the misuse of\nHTTP in machine-to-machine communication. In this article, we will examine\nsystem design patterns that significantly improve the reliability of HTTP\ncommunication between systems.\n\nThe first step to solving these problems is understanding the distinction\nbetween a delivery guarantee provided by message producers and a processing\nguarantee provided by message consumers.\n\nGuarantees come in three flavours:\n\n  * Atmost-once: 0 or 1\n\n  * Atleast-once: 1 or more\n\n  * Exactly-once: 1\n\nDelivery guarantees are either \"atmost-once\" or \"atleast-once\". A processing\nguarantee could be any one of the three, depending on the delivery guarantee\nprovided by the producer.\n\nNext, we need to understand that every HTTP request/response cycle is TWO\nmessages over one connection:\n\n  1. The connection opens\n\n  2. The requester writes their message\n\n  3. The responder writes a reply\n\n  4. The connection closes\n\nThis means the requester can receive an acknowledgement of their message via\nthe response, but critically, the responder does not know if the requester\nreceived and processed their reply.\n\nIn both scenarios depicted above, Service B does not receive a confirmation\nthat the message was processed. At best, it may receive confirmation that the\npackets were received but not confirmation that Service A successfully\nprocessed the message.\n\nThis lack of confirmation means sending important information via an HTTP\nResponse is unreliable. Suppose that HTTP Response contains the result of the\nrequester's operation to change the responding system's state. In that case,\nthe requester cannot know for certain if their internal representation of the\nresponder's system state is accurate.\n\nThat was a wordful and woefully abstract, so let's look at a concrete example.\n\n### Implementing Exactly-Once Processing\n\nIn this scenario, our system must\n\n  * Subscribe a user to a subscription plan, provided they meet the requirements for a subscription, e.g. verified email, active account, etc.\n\n  * Update a 3rd party billing platform that manages the actual charges, invoicing, etc.\n\nLet's add that in this scenario, the business logic dictates that a user can\nhave zero, one, or many active subscriptions. As you can imagine, this means\nit is essential that every subscription in the third-party billing platform be\nrecorded in our database.\n\nThe code below shows a naive implementation using a single HTTP\nRequest+Response cycle.\n\n    \n    \n    const createSubscription = async (userId: number, plan: Plan) => { const user = await userDb.get(userId); const { outcome } = validateCreateSubscriptionOperation(user, plan); if (outcome === 'SUCCESS') { const result = await billingApi.post( `/plan/${plan.id}/subscribe`, { userId }, ); await dbConnectionPool.query(` INSERT INTO subscriptions (status, external_id, user_id, plan_id) VALUES ($1, $2, $3, $4) `, ['ACTIVE', result.subscription.id, userId, plan.id] ); } }\n\nWhat happens if our server is terminated after the HTTP request to billingApi,\nbut before updating the database? Our system won't know the subscription was\ncreated, and it won't have the subscription id created by the billing system.\nUltimately, neither billingApi nor our system will be able to automatically\ndetect and correct the issue.\n\nFor the initial subscription request sent by our system, we are the producers\nof that message, and we implicitly provide atleast once delivery. If we don't\nget a response from billingApi, or we fail to process the response, then we\n(or the user) could try again until we do.\n\nFor the response, the billingApi is the producer and it only provides an\natmost-once delivery guarantee.\n\nThis can cause a serious problem for our users. They might try to subscribe to\na plan, receive an error, try again, and then be billed twice each month. When\nthey look at their active subscriptions in our system, they would only see one\nsubscription. If they cancel that subscription, they would continue being\nbilled once a month. Even if our customer support team tried to help this\nuser, they would not be able to see the extra subscription. Only someone with\naccess to the third-party billing system, such as a member of finance, or the\nbilling provider's customer support team, would be able to find the problem.\nHowever, even they might not know what they are looking for: they could have\nto check every subscription in both systems, looking for a subscription that\nonly exists in the billing service.\n\nSo how do we solve this problem? Luckily, the billingApi service also provides\nwebhooks with an atleast-once delivery guarantee. Following every HTTP request\nwe make to billingApi, they will send a webhook, a HTTP request, to our\nservice with the result of our previous request. Each webhook they send to our\nservice contains a unique messageId. If our service does not acknowlege the\nwebhook by responding 200 OK within 5 seconds, they will resend the webhook\nwith the same messageId again until we do.\n\nFirst, we need the createSubscription operation to reduce its scope to only\ndispatching the request. We will rename it dispatchSubscriptionRequest:\n\n    \n    \n    const dispatchSubscriptionRequest = async (userId: number, plan: Plan) => { const user = await userDb.get(userId); const result = validateDispatchSubscriptionRequestOperation(user, plan); if (result.outcome === 'SUCCESS') { await billingApi.post( `/plan/${plan.id}/subscribe`, { userId }, ); } return result; }\n\nInstead of receiving the subscription.id from the billing provider via the\nHTTP Response, the billing provider will send it via a webhook.\n\nHere is a condensed example of how we could handle that:\n\n    \n    \n    api.post('webhooks/billing', async (req, res) => { const { messageId, subscription: { id, userId, planId } } = req.body; try { const result = await dbConnectionPool.query(` INSERT INTO billing_inbox (message_id, subscription_id, user_id, plan_id), VALUES ($1, $2, $3, $4)`, [messageId, id, userId, planId] ); res.sendStatus(200); return; } catch (e) { const isExistingMessage = ( e instanceof DatabaseError && e.code === DbErrorCodes.UniqueViolation ); if (isExistingMessage) { res.sendStatus(200); return; } res.sendStatus(400); return; } });\n\nOf course, this isn't production grade code, for the sake of brevity it\ncrosses many levels of abstraction in one function. The critical facts here\nare that\n\n  * there is one database transaction per HTTP Request received by the webhook endpoint \u2014 a single statement is a single transaction.\n\n  * we use a unique constraint on our idempotency key, the message_id, to ensure we only save a message once, even if it is sent multiple times.\n\n  * we return 200 OK if we receive a message that we had previously saved to our inbox, allowing the webhook producer to resend a message until they successfully record our acknowledgment.\n\nNow we need to process the messages in our inbox, guaranteeing we process each\nmessage exactly once. For example:\n\n    \n    \n    function initialiseBillingInboxProcessor() { (async () => { while (lifecycle.isOpen()) { const client = await dbConnectionPool.connect(); try { await client.query('BEGIN'); const { rows: [ message ] } = await client.query(` SELECT * FROM billing_inbox WHERE processed IS FALSE LIMIT 1 FOR UPDATE SKIP LOCKED` ); if (!message) { await wait(200); continue; } await client.query(` INSERT INTO subscriptions (status, external_id, user_id, plan_id) VALUES ($1, $2, $3, $4) `, [ 'ACTIVE', message.subscription_id, message.user_id, message.plan_id ]); await client.query(` UPDATE billing_inbox SET processed = true WHERE message_id = $1`, [message.message_id], ); await client.query('COMMIT'); } catch (error) { logger.error( 'Unexpected error processing billing message', { error }, ); await client.query('ROLLBACK'); } finally { client.release(); } } })(); }\n\nWhat's happening in this code snippet? First, we define a function that is\nsynchronous, but contains an immediately invoked async function expression.\n\n    \n    \n    function initialiseBillingInboxProcessor() { (async () => { while (lifecycle.isOpen()) { } })(); }\n\nThis prevents other engineers from accidentally awaiting our infinite loop\nsince it won't resolve until the server begins a graceful shutdown.\n\nNext we begin our transaction and take an update lock on the row we select,\nwhile excluding rows that have been locked by another transaction.\n\n    \n    \n    const client = await dbConnectionPool.connect(); try { await client.query('BEGIN'); const { rows: [ message ] } = await client.query(` SELECT * FROM billing_inbox WHERE processed IS FALSE LIMIT 1 FOR UPDATE SKIP LOCKED` );\n\nThis allows us to run the inbox processor across multiple servers\nsimultaneously while preventing any two servers from ever processing the same\nmessage at the same time.\n\nIf the query returned no results, we wait 200ms before returning to the top of\nthe while loop and polling for new messages again.\n\n    \n    \n    if (!message) { await wait(200); continue; }\n\nNext we process the message by creating a new subscription record, marking the\nmessage as processed, and committing our transaction.\n\n    \n    \n    await client.query(` INSERT INTO subscriptions (status, external_id, user_id, plan_id) VALUES ($1, $2, $3, $4) `, [ 'ACTIVE', message.subscription_id, message.user_id, message.plan_id ]); await client.query(` UPDATE billing_inbox SET processed = true WHERE message_id = $1`, [message.message_id], ); await client.query('COMMIT');\n\nSo in summary, this transaction has three steps:\n\n  1. Retrieval: Get and lock an unprocessed message row, ensuring other servers don't process it.\n\n  2. Processing: Insert the new subscription record.\n\n  3. Commit: Mark the message as processed and commit the transaction.\n\nVoila! Exactly-once processing for each message. If an error occurs during\nprocessing, e.g. the server is terminated unexpectedly or the database\nconnection drops mid transaction, then the transaction will be rolled back and\nprocessing the message will be automatically retried. Of course, step two can\ninclude many more steps, as long as every database query is part of the\ntransaction.\n\n![](cdn.hashnode.com/res/hashnode/image/upload/.. align=\"center\")\n\nNow we are much more reliable message consumers because we have implemented an\nexactly once processing guarantee using the billing provider's webhooks, which\nprovide an atleast once delivery guarantee. There is still a flaw in our\nsystem design, which we will look at next.\n\n### Implementing Atleast Once Delivery\n\nWe have learnt that when a Message Producer implements Atleast Once Delivery\nwith idempotency keys we can then implement Exactly Once Processing as a\nMessage Consumer. But what about the messages we produce?\n\nTo implement Atleast Once Delivery we will need to\n\n  * Create an outbox supporting multiple destinations and providing a unique idempotency key per message\n\n  * Add messages to the outbox instead of making API requests directly\n\n  * Process messages in the outbox by sending them to third parties until we receive a response\n\nIn our example service we currently send messages to the third party billing\nprovider when our users call the new subscription endpoint. Currently, a user\ncould make a request, receive an error, then resubmit their request. However,\nthe first request error could be a false negative, meaning the billing\nprovider received the request but something went wrong while sending the\nacknowledgement, either between their server and our server or between our\nserver and our user.\n\nNow that we get the billing provider's response via a webhook and implemented\nexactly once processing, the user will be able to see and cancel the duplicate\nsubscription. However, we can make this process much more reliable using an\noutbox that will\n\n  * prevent the duplicate subscription from ever occurring\n\n  * ensure that every successful call to the new subscription endpoint results in a new subscription in the billing provider's system\n\n  * significantly increase throughput and reduce latency of the new subscription endpoint\n\n### Creating The Outbox\n\nLet's start by defining the outbox table schema\n\n    \n    \n    CREATE TYPE \"outbox_message_status\" AS ENUM ( 'pending', 'failed', 'sent' ); CREATE TABLE \"outbox\" ( \"id\" UUID PRIMARY KEY DEFAULT gen_random_uuid(), -- WARNING: https://www.2ndquadrant.com/en/blog/sequential-uuid-generators/ \"status\" \"outbox_message_status\" NOT NULL DEFAULT 'pending', \"attempts\" SMALLINT NOT NULL DEFAULT 0, \"retryLimit\" SMALLINT NOT NULL DEFAULT 0, \"retryWaitPeriodMs\" INT NOT NULL DEFAULT 1000, \"createdAt\" TIMESTAMP NOT NULL DEFAULT CLOCK_TIMESTAMP(), \"updatedAt\" TIMESTAMP, \"destination\" VARCHAR(255) NOT NULL, \"payload\" JSONB ); CREATE INDEX outbox_created_at_pending_idx ON \"outbox\" (\"status\", \"createdAt\") WHERE \"status\" = 'pending';\n\nFor the sake of simplicity I have used a random UUID as both the primary key\nand the idempotency key, however random UUID primary keys impact performance.\nThe alternatives are to either use a partially sequential UUID generator or\nuse a BIGSERIAL primary key.\n\nThis schema is, I hope, fairly intuitive. The novel decisions are:\n\n  * All columns preceding payload are fixed length. I mostly ordered these columns for readability and intentionality, but I cannot help playing a little Column Tetris to optimise for storage. Please forgive me my vices \ud83d\ude05\n\n  * using CLOCK_TIMESTAMP instead of NOW for createdAt so that two rows inserted in one transaction do not share a timestamp.\n\n  * using a partial index since we will only query for pending rows.\n\n  * using \"camelCase\" for columns to make the database repository easier to write.\n\n  * not indexing destination because in most systems it won't be very selective (not many distinct values) AND there won't be many pending rows. Even a backlog of 1,000 pending messages isn't worth indexing.\n\nHere is how we might implement a database repository for this table:\n\n    \n    \n    type OutboxRepo = { add: (msg: OutboxInsert) => Promise<Outbox['id']>; getPendingMessage: (destination?: OutboxDestination) => Promise<Outbox | undefined>; incrementAttempts: (id: Outbox['id']) => Promise<number>; setStatus: (id: Outbox['id'], status: Outbox['status']) => Promise<void>; }; type Outbox = { id: string; status: 'pending' | 'failed' | 'sent'; attempts: number; retryLimit: number; retryWaitPeriodMs: number; createdAt: Date; updatedAt: Date | null; destination: string; payload: Record<string, unknown>; }; type OutboxInsert = Pick<Outbox, 'destination' | 'payload'> & { id?: string; // for use cases where the client generates the msg id retryLimit?: number; retryWaitPeriodMs?: number; } export type OutboxDestination = 'billingProvider' | 'exampleProvider'; export const outboxRepo: OutboxRepo = { add: async (msg) => { const { keys, values } = Object.entries(msg) .reduce<EntriesOf<OutboxInsert>>((obj, [key, value]) => { if (value !== undefined) { obj.keys.push(`\"${key}\"`); obj.values.push(value); } return obj; }, {keys: [], values: []}); const client = getTransactionAwareClient(); const queryText = ` INSERT INTO \"outbox\" (${keys.join(', ')}) VALUES (${keys.map((_, i) => `$${i + 1}`)}) RETURNING id `; const { rows: [row] } = await client.query<Pick<Outbox, 'id'>>(queryText, values); return row.id; }, getPendingMessage: async (destination) => { const client = getTransactionAwareClient(); const { rows } = await client.query<Outbox>(` SELECT * FROM \"outbox\" WHERE status = 'pending' ${destination ? 'AND destination = $1' : ''} ORDER BY \"createdAt\" LIMIT 1 FOR UPDATE SKIP LOCKED; `, destination ? [destination] : []); return rows.shift(); }, incrementAttempts: async (id) => { const { rows: [row] } = await pool.query<Pick<Outbox, 'attempts'>>(` UPDATE \"outbox\" SET attempts = attempts + 1 WHERE id = $1 RETURNING attempts `, [id]); return row.attempts; }, setStatus: async (id, status) => { const client = getTransactionAwareClient(); await client.query(` UPDATE \"outbox\" SET status = $1 WHERE id = $2 `, [status, id]); }, }\n\nThe code above includes a few common utilities I always use:\n\n    \n    \n    export type ValueOf<T> = T[keyof T]; export type EntriesOf<T> = { keys: string[], values: ValueOf<T>[] };\n\nPlus, getTransactionAwareClient represents a way for database repos to\nautomatically use a transaction-bound database connection if the caller is\ncurrently mid-transaction. Typically implemented using AsyncLocalStorage.\n\nIt's worth noting that incrementAttempts does not use\ngetTransactionAwareClient. Whenever we call incrementAttempts, we want that\nchange committed separately and immediately, regardless of the outcome of the\ncaller's subsequent operations. Otherwise, during a failure, our attempt\ncounter could be rolled back with the rest of the operation, defeating the\npurpose of the column.\n\nI made destination an optional parameter when calling getPendingMessage to\nsupport use cases where we must run outbox processors dedicated to particular\ndestinations. Why? Assuming the number of concurrent outbound requests we can\nmake is limited, there are a few possible reasons:\n\n  * A destination has higher latency: We prevent our faster destinations being delayed by this slower one by running an outbox processor dedicated to the high latency destination.\n\n  * A destination is more important to the business: We prevent messages to one destination being queued behind others by running an outbox processor dedicated to the high priority destination.\n\n### Adding Messages To The Outbox\n\nWhile implementing Exactly Once Processing, we created a\ndispatchSubscriptionRequest function that validated the user's subscription\nrequest before sending it to the third-party billing provider. Now that we\nhave an outbox, we will update dispatchSubscriptionRequest to add the request\nto the outbox:\n\n    \n    \n    const enqueueSubscriptionRequest = async (userId: number, plan: Plan) => { const user = await userDb.get(userId); const result = validateEnqueueSubscriptionRequest(user, plan); if (result.outcome === 'SUCCESS') { await outboxRepo.add({ destination: 'billingProvider', payload: { path: `/plan/${plan.id}/subscribe`, body: { userId }, }, }); } return result; }\n\nWhat have we changed?\n\n  * We replaced the verb in the function name from dispatch to enqueue so callers know this function will not immediately make a request to the provider.\n\n  * We renamed validateEnqueueSubscriptionRequest to match, but it is otherwise unchanged.\n\n  * We swapped the HTTP request with a call to outboxRepo.add\n\nThere were no other changes. Importantly, we still return the result from\nvalidateEnqueueSubscriptionRequest so that users are informed if their request\ndoes not satisfy our business rules.\n\n### Processing The Outbox\n\nWe've created our outbox and added messages to it, now let's start processing\nthem. There are a few things to keep in mind while reading the next code\nsnippet:\n\n  * This is not production code: It crosses multiple layers of abstraction. In professional code we would breakup this large function to improve readability, but an article does not allow you to CMD+Click a function to jump to its definition and back.\n\n  * For concepts I explained earlier in the article, I have encapsulated them in functions. Instead of writing raw SQL we call the outboxRepo, instead of explicitly beginning, committing, and rolling back a transaction I use a withTransaction higher order function to commit on promise resolved and rollback if promise rejected.\n\n  * At 90 lines long it is our largest snippet so far, but don't worry, we will break it down piece by piece.\n\n    \n    \n    type Resolver = (v?: unknown) => void; function initialiseOutboxProcessor() { const destinationApis: Record<OutboxDestination, Axios> = { billingProvider: axios.create({ baseUrl: 'https://api.fictional-payment-platform.com/', transformRequest: [(data, headers) => { headers['Authorization'] = `Bearer ${getBillingAuthToken()}`; return data; }], httpsAgent: new https.Agent({ keepAlive: true, noDelay: true, timeout: 30_000, // milliseconds }), }), exampleProvider: axios.create({ baseUrl: 'https://fictional-service.com/api/', httpsAgent: new https.Agent({ keepAlive: true, noDelay: true, timeout: 10_000, // milliseconds }), }) } as const; (async () => { const concurrencyLimit = 20; const pollingIntervalMs = 200; let concurrentRequests = 0; while (lifecycle.isOpen()) { // Each execution of the while loop's code block is called a tick let resolveTickContinuationPromise: Resolver = () => {}; const tickContinuationPromise = new Promise((resolver) => { resolveTickContinuationPromise = resolver; }); // DO NOT AWAIT withTransaction! withTransaction(async () => { const isAtConcurrencyLimit = concurrentRequests >= concurrencyLimit; if (isAtConcurrencyLimit) { await wait(pollingIntervalMs); return; } const message = await outboxRepo.getPendingMessage(); if (!message) { await wait(pollingIntervalMs); return; } const attempts = await outboxRepo.incrementAttempts(message.id); const isAtRetryLimit = attempts > message.retryLimit; if (isAtRetryLimit) { await outboxRepo.setStatus(message.id, 'failed'); return; } concurrentRequests++; resolveTickContinuationPromise(); // resolving here so the while loop can process another // message concurrently const externalApiClient = destinationApis[message.destination]; const { status } = await externalApiClient.post( message.payload.path, message.payload.body, { headers: { 'Idempotency-Key': message.id } } ); const wasAcknowledged = status >= 200 && status < 300; if (wasAcknowledged) { await outboxRepo.setStatus(message.id, 'sent'); } /* DO NOT ADD A CATCH BLOCK! If an error is thrown, withTransaction MUST catch, log, and rollback. */ }).finally(() => { concurrentRequests--; resolveTickContinuationPromise(); ); await tickContinuationPromise; /* awaiting tickContinuationPromise instead of withTransaction allows the outbox processor to send HTTP requests concurrently because tickContinuationPromise resolves before starting the HTTP request while withTransaction resolves once the request and transaction have finished */ } })(); }\n\nLet's break this down!\n\n    \n    \n    function initialiseOutboxProcessor() { const destinationApis: Record<OutboxDestination, Axios> = { billingProvider: axios.create({ baseUrl: 'https://api.fictional-payment-platform.com/', transformRequest: [(data, headers) => { headers['Authorization'] = `Bearer ${getBillingAuthToken()}`; return data; }], httpsAgent: new https.Agent({ keepAlive: true, noDelay: true, timeout: 30_000, // milliseconds }), }), exampleProvider: axios.create({ baseUrl: 'https://fictional-service.com/api/', httpsAgent: new https.Agent({ keepAlive: true, noDelay: true, timeout: 10_000, // milliseconds }), }) } as const;\n\nFirst we declare our destination API map. By telling typescript that the value\nassigned to destinationApis must satisfy : Record<OutboxDestination, Axios>,\nwe guarantee that TypeScript will not compile if someone adds a new\nOutboxDestination but forgets to add an axios instance here.\n\nIn our Axios instance configurations we\n\n  * Set the baseUrl of the destination\n\n  * Add an Authorization header with a bearer token for the billing provider\n\n  * Set some sensible defaults for the HTTPS agent:\n\n    * keepAlive: true keeps the socket open between the requests, like we talked about earlier. This improves performance if we make multiple requests to the destination within the timeout\n\n    * noDelay: true turns off Nagle's Algorithm, improving performance.\n\n    * timeout: number determines how long the socket will stay open after the last data packet. Tweaking the timeout for different destinations can be beneficial depending on their behaviour, for example, setting a timeout shorter than the target's keep-alive timeout can help prevent ECONNRESET errors.\n\nThen we finish the object instantiation with as const to tell TypeScript that\nthe properties in this object are readonly.\n\n    \n    \n    (async () => { const concurrencyLimit = 20; const pollingIntervalMs = 200; let concurrentRequests = 0;\n\nWe begin another immediately invoked function expression like we did with our\ninbox, only this time we instantiate a few constants and a counter of running\nrequests. Each concurrent message being sent consumes a database connection,\nbut limiting the number of concurrent requests prevents exhaustion of the\nconnection pool.\n\n    \n    \n    while (lifecycle.isOpen()) { // Each execution of the while loop's code block is called a tick let resolveTickContinuationPromise: Resolver = () => {}; const tickContinuationPromise = new Promise((resolver) => { resolveTickContinuationPromise = resolver; });\n\nWe want our while loop to return to the start of its code block once we are\nabout to start our HTTP request, but allow the processing of the result to\ncontinue in the background. This will allow us to send requests for multiple\noutbox messages concurrently.\n\nHowever, our while loop is inside an async function, and the retrieval and\nprocessing of messages will occur within an anonymous function passed to a\nwithTransaction higher order function, we need an alternative to the continue\nstatement that allows us to continue the while loop from within the anonymous\nfunction but without preventing it from executing the remaining work. We solve\nthis problem by inverting control: The while loop will wait until the\nanonymous function tells it that it is ready.\n\nIn the snippet above, we create the promise the while loop will wait for,\ntickContinuationPromise, and we assign the promise resolver to a variable that\nwill be in the anonymous function's closure.\n\n    \n    \n    // DO NOT AWAIT withTransaction! withTransaction(async () => { const isAtConcurrencyLimit = concurrentRequests >= concurrencyLimit; if (isAtConcurrencyLimit) { await wait(pollingIntervalMs); return; } const message = await outboxRepo.getPendingMessage(); if (!message) { await wait(pollingIntervalMs); return; } const attempts = await outboxRepo.incrementAttempts(message.id); const isAtRetryLimit = attempts > message.retryLimit; if (isAtRetryLimit) { await outboxRepo.setStatus(message.id, 'failed'); return; } concurrentRequests++; resolveTickContinuationPromise(); // resolving here so the while loop can process another // message concurrently\n\nIn the snippet above we perform a few checks before sending the request. Just\nbefore we send the request we increment the concurrent requests counter and\nresolve the continuation promise. This allows the while loop to return to the\ntop of its code block while this function continues running in the background.\n\n    \n    \n    const externalApiClient = destinationApis[message.destination]; const { status } = await externalApiClient.post( message.payload.path, message.payload.body, { headers: { 'Idempotency-Key': message.id } } ); const wasAcknowledged = status >= 200 && status < 300; if (wasAcknowledged) { await outboxRepo.setStatus(message.id, 'sent'); } }).finally(() => { concurrentRequests--; resolveTickContinuationPromise(); });\n\nHere we grab one of the clients we prepared earlier. Crucially, we include the\nmessage ID as the Idempotency Key Header when we make a request. That way the\nrecipient, in this case the billing system, can detect if we have sent this\nmessage previously.\n\nOnce we get a response, we check the status code, and if they returned 200 we\nmark the message as sent to prevent sending it again. Then inside the finally\ncallback of the withTransaction promise we decrement the concurrent request\ncounter and resolve the continuation promise in case an error or early return\nhad occurred.\n\n    \n    \n    await tickContinuationPromise; } })(); }\n\nAnd now we wrap everything up! We await the continuation promise as the final\nstatement inside the while loop. Then we close the loop, close the immediately\ninvoked function expression, and close the initialiseOutboxProcessor function\ndefinition.\n\nNow we have a process that will send messages from our outbox. Of course, this\nis just one approach of many and it has some trade-offs to be aware of. The\nbiggest one, is that it maintains a open transaction with the database while\nmaking the request to the third-party. This might be acceptable if the third-\nparty is a good citizen who sees their messages to an inbox before processing\nthem.\n\nHowever, if they attempt to process the message before responding with 200 OK,\nthen our transaction will be held open while their system is processing our\nmessage. This makes our system extremely vulnerable to their influence.\nDatabase connections are not an infinite resource. In this case we would want\nto change the status of our message to sending and put a system in place to\ndetect messages that have been stuck in the sending status for too long so\nthat they can be resent again. That approach has a few more failure cases,\nthereby trading simplicity for performance. I decided to keep it simple for\nthis article.\n\nThe outbox in this article is unordered, but sometimes we need to send\nmessages in a particular order. In that scenario we can use a cursor to track\nour progress through an ordered set of messages.\n\n## Summary\n\nUsing HTTP, producers can implement an atleast-once delivery guarantee by\nsending a message repeatedly until the producer receives an acknowledgement\nfrom the consumer via a 200 OK response. (Along with a timeout while waiting\nfor an ack)\n\nThese relatively simple messaging guarantees can be built upon to power our\nmore robust network protocols. Yet, many services offering a REST API expect\nusers to mutate data with a HTTP POST request and then receive updated state\nvia the HTTP response. As we just learned, in this context, a HTTP Response is\nonly useful for receiving acknowledgement of our request. It is an unreliable\nmethod for receiving the result of our request.\n\nIn HTTP, only the requester knows if the message they sent was delivered. If\nyou provide a REST API where other systems can issue changes to your system,\nalways provide webhooks with an atleast-once delivery guarantee and\nidempotency key.\n\n## Subscribe to my newsletter\n\nRead articles from Antman writes software directly inside your inbox.\nSubscribe to the newsletter, and don't miss out.\n\nTypeScriptNode.jsdistributed systemPostgreSQLJavaScriptREST APIwebhooks\n\n### Written by\n\n# Anthony Manning-Franklin\n\nCTO @ Skutopia.com\n\nI write about Functional Programming, TypeScript, Node, Event Sourcing, and\nengineering leadership\n\nCTO @ Skutopia.com\n\nI write about Functional Programming, TypeScript, Node, Event Sourcing, and\nengineering leadership\n\nShare this\n\n### More articles\n\nAnthony Manning-Franklin\n\n# The Fundamental Problems of Software\n\nAs far as I can tell, there are six immutable fundamental problems faced by\nall commercial software....\n\nAnthony Manning-Franklin\n\n# The Four Quadrants of Complexity\n\nEssential complexity is where software engineers are uniquely able to deliver\nbusiness value, wherea...\n\nAnthony Manning-Franklin\n\n# Implementing the Outbox Pattern in Nodejs and Postgres\n\nAs applications scale, infrequent problems become significant. A network\nfailure for 0.1% of request...\n\n\u00a92024 Antman writes software\n\nArchive\u00b7Privacy policy\u00b7Terms\n\n", "frontpage": false}
