{"aid": "39981562", "title": "Automated Testing: How We Catch Thunderbird Bugs Before You Do", "url": "https://blog.thunderbird.net/2024/04/automated-testing-how-we-catch-thunderbird-bugs-before-you-do/", "domain": "thunderbird.net", "votes": 1, "user": "HieronymusBosch", "posted_at": "2024-04-09 17:11:10", "comments": 0, "source_title": "Automated Testing: How We Catch Thunderbird Bugs Before You Do", "source_text": "Automated Testing: How We Catch Thunderbird Bugs Before You Do\n\nDownload Thunderbird Donate\n\n  * Thunderbird\n\n# Automated Testing: How We Catch Thunderbird Bugs Before You Do\n\nApril 9, 2024 Geoff Lankow 0 responses\n\nSince the release of Thunderbird 115, a big focus has been on improving the\nstate of our automated testing. Automated testing increases the software\nquality by minimizing the number of bugs accidentally introduced by changes to\nthe code. For each change made to Thunderbird, our testing machines run a set\nof tests across Windows, macOS, and Linux to detect mistakes and unintended\nconsequences. For a single change (or a group of changes that land at the same\ntime), 60 to 80 hours of machine time is used running tests.\n\nOur code is going to be under more pressure than ever before \u2013 with a bigger\nteam making more changes, and monthly releases reducing the time code spends\non testing channels before being released.\n\nWe want to find the bugs before our users do.\n\n## Why We\u2019re Testing\n\nWe\u2019re not writing tests merely to make ourselves feel better. Tests improve\nThunderbird by:\n\n  * Preventing mistakes If we test that some code behaves in an expected way, we\u2019ll find out immediately if it no longer behaves that way. This means a shorter feedback loop, and we can fix the problem before it annoys the users.\n  * Finding out when somebody upstream breaks us Thunderbird is built from the Firefox code. The Firefox code, which we are not responsible for, is 30 to 40 times the size of the code we are responsible for. When something inevitably changes in Firefox that affects us, we want to know about it immediately so that we can respond.\n  * Freeing up human testers If we use computers to prove that the program does what it\u2019s supposed to do, particularly if we avoid tedious repetition and difficult-to-set-up tasks, then the limited human resources we have can do more things that humans are better at. For example, I\u2019ve recently added tests that check 22 ways to trigger fetching mail, and 10 circumstances fetching mail might not work. There\u2019s no way our human testers (great though they are) are testing all of them, but our automated tests can and do, several times a day.\n  * Thinking through what the code should be doing Testing forces an engineer to look at the code from a different point-of-view, and this is helpful to think about what the code is supposed to do in more circumstances. It also makes it easier to prove that the code does work in obscure circumstances.\n  * Finding existing bugs In software terms we\u2019re working with some very old code, and much of it is untested. Testing it puts a fresh set of eyes on the code and reveals some of the mistakes of the past, and where the ravages of time have broken things. It also helps the person writing the tests to understand what the code does, a lot better than just reading the code does.\n\nWe\u2019re not trying to completely cover a feature or every edge case in tests. We\nare trying to create a testing framework around the feature so that when we\nfind a bug, as well as fixing it, we can easily write a test preventing the\nbug from happening again without being noticed. For too much of the code, this\nhas been impossible without a weeks-long detour into tests.\n\n## Breaking New Ground\n\nIn the past few months we\u2019ve figured out how to make automated tests for\nthings that were previously impossible:\n\n  * Communication with mail servers using encrypted channels.\n  * OAuth2 authentication with mail servers.\n  * Communication with web servers where a specific address must be used and an unencrypted channel must not be used.\n  * Servers at any given host name or port. Previously, if we wanted to start a server for automated testing, it had to be on the local machine at a non-standard location. Now we can pretend that the server is anywhere, and using standard ports, which is needed for proper testing of account configuration features. (Actually, this was possible before, but now it\u2019s much easier.)\n\nThese new abilities are being used to wrap better testing around account set-\nup features, ahead of the new Account Hub development, so that we can be sure\nnothing breaks without being noticed. They\u2019re also helping test that\ncollecting mail works when it should, or gives the error prompts we expect\nwhen it doesn\u2019t.\n\n## Code coverage\n\nWe record every line of code that runs during our tests. Collecting all that\ndata tells what code doesn\u2019t run during our tests. If a block of code doesn\u2019t\nrun during any of our tests, nothing will tell us when it breaks until\nsomebody uses the code and complains.\n\nOur code coverage data can be viewed at coverage.thunderbird.net. You can also\nlook at Firefox\u2019s data at coverage.moz.tools.\n\nLooking at the data, you might notice that our overall number is now lower\nthan it was when we started measuring. This doesn\u2019t mean that our testing got\nworse, it actually shows where we added a lot of code (that isn\u2019t maintained\nby us) in the third_party directory. For a better reflection of the progress\nwe\u2019ve made, check out the individual directories, especially mail/base which\ncontains the most important user interface code.\n\n  * Just setting up the code coverage tools and looking at the results uncovered several memory leaks. (A memory leak is where memory is allocated for a task and not released when it is no longer needed.) We fixed these leaks and some more that existed in our test code. We now have very low levels of memory leaking in our test runs, so if we make a mistake it is easy to spot.\n  * Code coverage data can also point to code that is no longer used. We\u2019ve removed some big chunks of this dead code, which means we\u2019re not wasting time maintaining it.\n\n## Mozmill no more\n\nTowards the end of last year we finally retired an old test suite known as\nMozmill. Those tests were partially migrated to a different test suite\n(Mochitest) about four years ago, and things were mostly working fine so it\nwasn\u2019t a priority to finish. These tests now do things in a more conventional\nway instead of relying on a bunch of clever but weird tricks.\n\n## How much of the code is test code?\n\nAbout 27%. This is a very rough estimate based on the files in our code\nrepository (minus some third-party directories) and whether they are inside a\ndirectory with \u201ctest\u201d in the name or not. That\u2019s risen from about 19% in the\nlast five years.\n\nThere is no particular goal in mind, but I can imagine a future where there is\nas much test code as non-test code. If we achieve that, Thunderbird will be in\na very healthy place.\n\nLooking ahead, we\u2019ll be asking contributors to add tests to their patches more\noften. This obviously depends on the circumstance. But if you\u2019re adding or\nfixing something, that is the best time to ensure it continues to work in the\nfuture. As always, feel free to reach out if you need help writing or running\ntests, either via Matrix or Topicbox mailing lists:\n\n  * Matrix: You can join our development chat channel at #maildev:mozilla.org\n  * Topicbox mailing list: The Thunderbird Developers list a good place to raise questions about Thunderbird development.\n\nGeoff Lankow, Staff Engineer\n\nTags: community Development QA\n\n## 0 responses\n\n", "frontpage": false}
