{"aid": "40065590", "title": "Deploying Tailscale for a remote only company", "url": "https://angrydome.com/posts/2024-tailscale-deploy/", "domain": "angrydome.com", "votes": 1, "user": "yarapavan", "posted_at": "2024-04-17 14:57:46", "comments": 0, "source_title": "Deploying Tailscale for a remote only company", "source_text": "Deploying Tailscale for a remote only company :: angrydome\n\nangrydome\n\n  * Menu \u25be\n  *     * about\n    * posts\n    * trips\n\n# Deploying Tailscale for a remote only company\n\n2024-03-14mrz\n\n#tailscale #work\n\nOne of my first jobs included managing on premise infrastructure for\nbusinesses and configuring VPN systems were a constant hassle. When I moved to\nPuppet I was glad that VPN services were someone elses problem and that my\nrole didn\u2019t need access to services in the datacenter. The purpose of the VPN\nwas to extend the old school hard candy shell of physical network security to\nremote employees and offices. There was a luxury to know that to access\nspecific systems you needed physical presence in a location that could have\nsecurity controls involving badged access and 802.11x radius authentication on\nnetwork ports.\n\nFast forward to 2022 and I\u2019m deploying Tailscale for a company with no\nphysical office and all their resources are hosted on someone else\u2019s computers\n(aka \u201cthe cloud\u201d). How would very smart and well managed wireguard tunnels be\nuseful for them? They\u2019re developing a cloud based SaaS, source code is hosted\non Github, meetings done in Zoom, chat done in Slack, the CRM is Salesforce,\nthe very standard startup stack of tools.\n\nThe first use of Tailscale by the company, was for using the exit-node\nservice. They replaced individuals Surfshark and similar personal VPN\nsolutions with a handful small compute instances acting as VPN endpoints in\ndifferent regions. Employees could select a local exit node while at the\ncoffee shop and know they had a secure tunnel in a suspect network.\n\nWhen I was tasked with implementing more central security controls, I decided\nto use the fact that most people already had Tailscale for the VPN service as\nthe basis for connecting and managing the rest of the services. Different\nteams had put together their own controls and did follow best practices for\nmanaging access to sensitive services, but that left too much to chance and\nadded too much risk in terms of human error. A lot of the security was\nmaintained by very dilligent people, but from experience that is an area that\ndoesn\u2019t scale well and can suffer most with attrition and burnout. I wanted to\nget to the point were developers had access to see what resources they could\naccess from their laptops on their laptops and Tailscales access features\nreally played a crucial part in that process. Tailscale provided the\nunderlaying infrastructure and essentialy the \u201ccompany lan\u201d that I used to run\nthe rest of the access management tooling ontop of.\n\nSome patterns I found useful:\n\n  * The terraform provider lets you break up the Tailscale config into different components, so I used the google workspace provider to scrape the needed emails to populate the needed groups definitions. This also allowed for the ACL section to be a different set of files entirely, so one doesn\u2019t have to keep track of a single hujson file to make changes to (the terraform run would assemble all these together into one artifact for us). This made commit diffs easier to track in the control repo that handled Tailscale and other services around the company.\n\n  * Subnet collisions are always a problem. Since everything was developed separately and access to a resources was originally on a by purpose basis, it was common for the same IP ranges to be reused. Doing the CIDR math for others and creating lists of IP ranges folks could use by region helped a lot, since most people hate doing that work themselves. This made the subnet router feature in Tailscale much easier to implement, being able to drop in an EC2 instance for a region and expose the services by their IP range.\n\n  * CGNAT is a thing in more use than anyone thinks. It\u2019s what AWS allows users to add as a secondary IP range to every VPC (intentionally for EKS, allowing for AWS security groups to apply to pod networking), so I\u2019m really glad to see one can define CGNAT the ranges used on the tailnet now. Also if you do have to run Tailscale on a node in CGNAT, tweak the CLI to ensure it doesn\u2019t blanket drop all CGNAT traffic by accident (--netfilter-mode=off). Tailscale does per device routes anyway, but it can add a blanket \u201cDENY 100.64.0.0/10\u201d that will kill the other non Tailscale connections.\n\n  * Dig into the Autoscaler guide for your cloud of choice. For AWS, I used autoscale groups with maximum lifetimes to ensure that exit nodes and subnet router weren\u2019t running for months without patches - instead they were designed to be disposable, killed off after a week. Using a cloud-init script to handle all the patching and maintenance at boot, it meant the instances were as container like as they could be for their purpose, without all the overhead required to run a container service (and it\u2019s best to keep these services outside other kubernetes or container systems). I didn\u2019t get to implement a lifecycle hook to generate an one time use auth token on instance launch and stage it in a secret for, but I did have separate process running to rotate the auth tokens frequently, ensuring there weren\u2019t long lived credentials sitting around.\n\n  * You can use a custom private DNS entries if you want, but you can also get away with publishing route53 public entries for services that point to private IPs that are behind a subnet router. Anyone can see the DNS entry (and private DNS is leaked anyway thanks to certificate transparency lists), but only users on your Tailscale network can reach those IPs. If you do use private DNS instead, for scenarios like Route53 private zones, once they are attached to a VPC, the second from the bottom address in the range is the DNS resolver you can add to your Tailscale DNS settings, letting users resolve those private entries easily enough.\n\n  * Tailscale DNS can cause heartburn if folks don\u2019t know it is applied to their machines - the fact that DNS (and split DNS) can be implemented by the Tailscale admins can throw endusers for a loop. A sanity check to let them know if it\u2019s applying to them is they see 100.100.100.100 as their resolver in a dig lookup, it\u2019s going through MagicDNS. It is great if your org wants to keep DNS information private, but in some cases just using the public DNS records may be sufficient, since you can remove \u201cdid the user disable tailscale DNS on their machine\u201d from the troubleshooting list.\n\nSince I finished this project, Tailscale has introduced a ton of new features\nI wish I had at my disposal while there and they\u2019re definitely maturing into a\nserious enterprise ready tool. These that show they\u2019re realizing the buyers\nnow aren\u2019t individuals or teams who use the tools they\u2019re paying for, but a\ndirector or CISO who is trying to mitigate security risks for hundreds or\nthousands of users. Some of the more exciting ones for me have been:\n\n  * Tailscale client customization: being able to force DNS settings (on or off) and other behavior are the kinds of things that simplify rolling this tool out to teams. Right up there with a real MacOS package for deployment means they\u2019re building the levers possible for a larger company to adapt to Tailscale and Tailscale to adapt to their policies and needs. I\u2019d not go wild on it for where I was before, but these level of tweaks are essential when your deploying a tool to more than a handful of people and want them to have a positive experience with it.\n\n  * SSH session recording and Kubernetes operator would have allowed for the replacement of an entirely second set of tools I had to deploy to audit the last mile interactions with services. This would have been great and also allows for fully internal services to just be served up over Tailscale directly (the operator acting as an Ingress to a service is amazing) and enforce access not just at the application level but the network level with the Tailnet ACL. The other \u201czero trust\u201d access tool I used required a lot of overhead and had users change their habits when it came connecting to servers - with Tailscale ssh mrz@server-name is exactly the same as access was done before and so has much less friction for adoption. The fact that it\u2019s happening over a Tailscale tunnel with a level of logging and auditing is transparent to the enduser.\n\n  * App Connectors are a brilliant product that\u2019s repacking some things that Tailscale is already doing. A Tailnet could already have a subnet router that advertises the handful of /32 \u201csubnets\u201d that are github.com IPs, ensuring that all traffic on the tailnet going to a github.com servers originate from a specific known IP. But they\u2019ve made implementing that trivial with the feature and this alone is a huge improvement for security. Not only can one enforce traffic going to sensitive services have to go over a Tailscale network - eliminating the need for users to always have an exit node enabled, it makes it practical to use IP allowlisting for many SaaS tools. It\u2019s one of the best safe guards against leaked credentials because now the attacker also needs to be on your Tailnet not just have a users github account credentials.\n\n  * Regional Routing this is a great addition to the HA ability of the Subnet Routers. I had run into the problem of deploying transit backbones across regions to allow VPC connectivity in EU and US. Prior to this feature, the option was to either route everyone to one region to access the VPCs or split the VPC subnets into batches and then share only a regional subnet via subnet routers in both regions. The first option meant half the company had a needless roundtrip across the globe and the second option doubled the number of subnet routers and having ensure the shared subnets were kept up to date. Now one could deploy a subnet router in each region advertising identical routes and the users in the same region use the preferred node. So much easier.\n\n  * Mullvad exit-nodes would allow for throwing out a ton of redundant infrastructure and overhead managing exit nodes. I can see using these as the catch all exit nodes, with the exception of heavily regulated spaces that want internet destined traffic to go through specific proxies or firewalls in addition to going through a VPN. This also frees up admin cycles to focus on providing App Connectors and Subnet Routers to those services and not having to worry about providing VPN access to the internet at large.\n\n  * Device Posture lets you use the state and assessment of a node to determine if it should have access to your Tailscale network and what resources there in. This is amazing and something I would have loved to implement alongside Puppet/Facter\u2019s trusted data. Being able to restrict access to machines based not just on user but attributes helps minimize the risk associated with a remote office (or one following a bring your own device model). Some super basic restrictions could be limiting iPhone device access to some app connectors and exit nodes, while the same users laptop could have access to github.com and other sensitive resources.\n\nWhat I find really impressive about all of the above changes is that it\u2019s\nshowing how a tool like Tailscale really enables the a fully remote company to\noperate securely. This isn\u2019t just replacing the physical doors on a central\ndata center with a virtual abstraction, but allowing for a fine grained flow\nof traffic between systems and services in infrastructure. At Puppet we\nwouldn\u2019t have even considered putting a VPN client on a sales persons laptop,\nbut if something like Tailscale existed then, it would be trivial to deploy it\nand support it, knowing that it could be managed in a way that adds a real\nlevel of security to their work without being a huge hinderance to them doing\ntheir job. There\u2019s an added benefit for development teams because not only\ndoes it provide the security infrastructure to do their job, adding a new\ninternal service or system to the platform is possible. ACLs can be configured\nto allow a developer to access to all their own machines / services they\u2019ve\ndeployed with Tailscale first, great for testing and demoing, before then\ngetting signoff on the service being shared wider on the tailnet. Making it\npainless to developer a secure service by default is huge cultural shift for\nan org, it is an actual compelling \u201cshift security left\u201d story.\n\nSo yeah, Tailscale is a pretty great set of tools and I\u2019m excited to see what\nthey work on next.\n\nRead other posts\n\n\u2190 Tailscale Changelog Review 1 of ? Overdue update \u2192\n\n\u00a9 2024 Powered by Hugo :: Theme made by panr\n\n", "frontpage": false}
