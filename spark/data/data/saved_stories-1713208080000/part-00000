{"aid": "40040057", "title": "The Building Blocks of Interpretability (2018)", "url": "https://distill.pub/2018/building-blocks/", "domain": "distill.pub", "votes": 1, "user": "zielmicha", "posted_at": "2024-04-15 13:06:35", "comments": 0, "source_title": "The Building Blocks of Interpretability", "source_text": "The Building Blocks of Interpretability\n\nDistill\n\n# The Building Blocks of Interpretability\n\nInterpretability techniques are normally studied in isolation. We explore the\npowerful interfaces that arise when you combine them \u2014 and the rich structure\nof this combinatorial space.\n\n#### Choose an input image\n\nFor instance, by combining feature visualization (what is a neuron looking\nfor?) with attribution (how does it affect the output?), we can explore how\nthe network decides between labels like Labrador retriever and tiger cat.\n\nSeveral floppy ear detectors seem to be important when distinguishing dogs,\nwhereas pointy ears are used to classify \"tiger cat\".\n\n#### channels that most support ...\n\nfeature visualization of channel\n\nhover for attribution maps\n\n...\n\nnet evidence\n\n1.63\n\n1.51\n\n1.19\n\n1.32\n\n1.54\n\n1.72\n\nfor \"Labrador retriever\"\n\n1.22\n\n1.24\n\n1.32\n\n-0.70\n\n-1.24\n\n-0.43\n\nfor \"tiger cat\"\n\n-0.40\n\n-0.27\n\n0.13\n\n0.62\n\n0.30\n\n1.29\n\nReproduce in a Notebook\n\n### Authors\n\n### Affiliations\n\nChris Olah\n\nGoogle Brain\n\nArvind Satyanarayan\n\nGoogle Brain\n\nIan Johnson\n\nGoogle Cloud\n\nShan Carter\n\nGoogle Brain\n\nLudwig Schubert\n\nGoogle Brain\n\nKatherine Ye\n\nCMU\n\nAlexander Mordvintsev\n\nGoogle Research\n\n### Published\n\nMarch 6, 2018\n\n### DOI\n\n10.23915/distill.00010\n\nWith the growing success of neural networks, there is a corresponding need to\nbe able to explain their decisions \u2014 including building confidence about how\nthey will behave in the real-world, detecting model bias, and for scientific\ncuriosity. In order to do so, we need to both construct deep abstractions and\nreify (or instantiate) them in rich interfaces . With a few exceptions ,\nexisting work on interpretability fails to do these in concert.\n\nThe machine learning community has primarily focused on developing powerful\nmethods, such as feature visualization , attribution , and dimensionality\nreduction , for reasoning about neural networks. However, these techniques\nhave been studied as isolated threads of research, and the corresponding work\nof reifying them has been neglected. On the other hand, the human-computer\ninteraction community has begun to explore rich user interfaces for neural\nnetworks , but they have not yet engaged deeply with these abstractions. To\nthe extent these abstractions have been used, it has been in fairly standard\nways. As a result, we have been left with impoverished interfaces (e.g.,\nsaliency maps or correlating abstract neurons) that leave a lot of value on\nthe table. Worse, many interpretability techniques have not been fully\nactualized into abstractions because there has not been pressure to make them\ngeneralizable or composable.\n\nIn this article, we treat existing interpretability methods as fundamental and\ncomposable building blocks for rich user interfaces. We find that these\ndisparate techniques now come together in a unified grammar, fulfilling\ncomplementary roles in the resulting interfaces. Moreover, this grammar allows\nus to systematically explore the space of interpretability interfaces,\nenabling us to evaluate whether they meet particular goals. We will present\ninterfaces that show what the network detects and explain how it develops its\nunderstanding, while keeping the amount of information human-scale. For\nexample, we will see how a network looking at a labrador retriever detects\nfloppy ears and how that influences its classification.\n\nOur interfaces are speculative and one might wonder how reliable they are.\nRather than address this point piecemeal, we dedicate a section to it at the\nend of the article.\n\nIn this article, we use GoogLeNet, an image classification model, to\ndemonstrate our interface ideas because its neurons seem unusually\nsemantically meaningful.We\u2019re actively investigating why this is, and hope to\nuncover principles for designing interpretable models. In the meantime, while\nwe demonstrate our techniques on GoogLeNet, we provide code for you to try\nthem on other models. Although here we\u2019ve made a specific choice of task and\nnetwork, the basic abstractions and patterns for combining them that we\npresent can be applied to neural networks in other domains.\n\n## Making Sense of Hidden Layers\n\nMuch of the recent work on interpretability is concerned with a neural\nnetwork\u2019s input and output layers. Arguably, this focus is due to the clear\nmeaning these layers have: in computer vision, the input layer represents\nvalues for the red, green, and blue color channels for every pixel in the\ninput image, while the output layer consists of class labels and their\nassociated probabilities.\n\nHowever, the power of neural networks lies in their hidden layers \u2014 at every\nlayer, the network discovers a new representation of the input. In computer\nvision, we use neural networks that run the same feature detectors at every\nposition in the image. We can think of each layer\u2019s learned representation as\na three-dimensional cube. Each cell in the cube is an activation, or the\namount a neuron fires. The x- and y-axes correspond to positions in the image,\nand the z-axis is the channel (or detector) being run.\n\nThe cube of activations that a neural network for computer vision develops at\neach hidden layer. Different slices of the cube allow us to target the\nactivations of individual neurons, spatial positions, or channels.\n\nMaking sense of these activations is hard because we usually work with them as\nabstract vectors:\n\na_4,1 = [0, 0, 0, 25.2, 164.1, 0, 42.7, 4.51, 115.0, 51.3, 0, 0, ...]\n\nWith feature visualization, however, we can transform this abstract vector\ninto a more meaningful \"semantic dictionary\".\n\n{\n\n:\n\n886.\n\n,\n\n:\n\n599.\n\n,\n\n:\n\n328.\n\n,\n\n:\n\n303.\n\n, ... }\n\nThere seem to be detectors for floppy ears, dog snouts, cat heads, furry legs,\nand grass. GoogLeNet has a rich variety of ear detectors which help it\ndistinguish between 100 species of dog.Reproduce in a Notebook\n\nTo make a semantic dictionary, we pair every neuron activation with a\nvisualization of that neuron and sort them by the magnitude of the activation.\nThis marriage of activations and feature visualization changes our\nrelationship with the underlying mathematical object. Activations now map to\niconic representations, instead of abstract indices, with many appearing to be\nsimilar to salient human ideas, such as \u201cfloppy ear,\u201d \u201cdog snout,\u201d or \u201cfur.\u201d\n\nWe use optimization-based feature visualization to avoid spurious correlation,\nbut one could use other methods.\n\nSemantic dictionaries are powerful not just because they move away from\nmeaningless indices, but because they express a neural network\u2019s learned\nabstractions with canonical examples. With image classification, the neural\nnetwork learns a set of visual abstractions and thus images are the most\nnatural symbols to represent them. Were we working with audio, the more\nnatural symbols would most likely be audio clips. This is important because\nwhen neurons appear to correspond to human ideas, it is tempting to reduce\nthem to words. Doing so, however, is a lossy operation \u2014 even for familiar\nabstractions, the network may have learned a deeper nuance. For instance,\nGoogLeNet has multiple floppy ear detectors that appear to detect slightly\ndifferent levels of droopiness, length, and surrounding context to the ears.\nThere also may exist abstractions which are visually familiar, yet that we\nlack good natural language descriptions for: for example, take the particular\ncolumn of shimmering light where sun hits rippling water. Moreover, the\nnetwork may learn new abstractions that appear alien to us \u2014 here, natural\nlanguage would fail us entirely! In general, canonical examples are a more\nnatural way to represent the foreign abstractions that neural networks learn\nthan native human language.\n\nBy bringing meaning to hidden layers, semantic dictionaries set the stage for\nour existing interpretability techniques to be composable building blocks. As\nwe shall see, just like their underlying vectors, we can apply dimensionality\nreduction to them. In other cases, semantic dictionaries allow us to push\nthese techniques further. For example, besides the one-way attribution that we\ncurrently perform with the input and output layers, semantic dictionaries\nallow us to attribute to-and-from specific hidden layers. In principle, this\nwork could have been done without semantic dictionaries but it would have been\nunclear what the results meant.\n\nWhile we introduce semantic dictionaries in terms of neurons, they can be used\nwith any basis of activations. We will explore this more later.\n\n## What Does the Network See?\n\nSemantic dictionaries give us a fine-grained look at an activation: what does\neach single neuron detect? Building off this representation, we can also\nconsider an activation vector as a whole. Instead of visualizing individual\nneurons, we can instead visualize the combination of neurons that fire at a\ngiven spatial location. (Concretely, we optimize the image to maximize the dot\nproduct of its activations with the original activation vector.)\n\nActivation Vector\n\n=\n\n886.\n\n+\n\n599.\n\n+\n\n328.\n\n+\n\n303.\n\n\\+ ...\n\nChannels\n\nApplying this technique to all the activation vectors allows us to not only\nsee what the network detects at each position, but also what the network\nunderstands of the input image as a whole.\n\nReproduce in a Notebook mixed4d\n\nAnd, by working across layers (eg. \u201cmixed3a\u201d, \u201cmixed4d\u201d), we can observe how\nthe network\u2019s understanding evolves: from detecting edges in earlier layers,\nto more sophisticated shapes and object parts in the latter.\n\n#### mixed3a\n\n#### mixed4a\n\n#### mixed4d\n\n#### mixed5a\n\nThese visualizations, however, omit a crucial piece of information: the\nmagnitude of the activations. By scaling the area of each cell by the\nmagnitude of the activation vector, we can indicate how strongly the network\ndetected features at that position:\n\n#### mixed3a\n\n#### mixed4a\n\n#### mixed4d\n\n#### mixed5a\n\n## How Are Concepts Assembled?\n\nFeature visualization helps us answer what the network detects, but it does\nnot answer how the network assembles these individual pieces to arrive at\nlater decisions, or why these decisions were made.\n\nAttribution is a set of techniques that answers such questions by explaining\nthe relationships between neurons. There are a wide variety of approaches to\nattribution but, so far, there doesn\u2019t seem to be a clear right answer. In\nfact, there\u2019s reason to think that all our present answers aren\u2019t quite right\n. We think there\u2019s a lot of important research to be done on attribution\nmethods, but for the purposes of this article the exact approach taken to\nattribution doesn\u2019t matter. We use a fairly simple method, linearly\napproximating the relationshipWe do attribution by linear approximation in all\nof our interfaces. That is, we estimate the effect of a neuron on the output\nis its activation times the rate at which increasing its activation increases\nthe output. When we talk about a linear combination of activations, the\nattribution can be thought of as the linear combination of the attributions of\nthe units, or equivalently as the dot product between the activation of that\ncombination and the gradient.\n\nFor spatial attribution, we do an additional trick. GoogLeNet\u2019s strided max\npooling introduces a lot of noise and checkerboard patterns to it\u2019s gradients.\nTo avoid our interface demonstrations being dominated by this noise, we (a) do\na relaxation of the gradient of max pooling, distributing gradient to inputs\nproportional to their activation instead of winner takes all and (b) cancel\nout the checkerboard patterns.\n\nThe notebooks attached to diagrams provide reference implementations., but\ncould easily substitute in essentially any other technique. Future\nimprovements to attribution will, of course, correspondingly improve the\ninterfaces built on top of them.\n\n### Spatial Attribution with Saliency Maps\n\nThe most common interface for attribution is called a saliency map \u2014 a simple\nheatmap that highlights pixels of the input image that most caused the output\nclassification. We see two weaknesses with this current approach.\n\nFirst, it is not clear that individual pixels should be the primary unit of\nattribution. The meaning of each pixel is extremely entangled with other\npixels, is not robust to simple visual transforms (e.g., brightness, contrast,\netc.), and is far-removed from high-level concepts like the output class.\nSecond, traditional saliency maps are a very limited type of interface \u2014 they\nonly display the attribution for a single class at a time, and do not allow\nyou to probe into individual points more deeply. As they do not explicitly\ndeal with hidden layers, it has been difficult to fully explore their design\nspace.\n\nWe instead treat attribution as another user interface building block, and\napply it to the hidden layers of a neural network. In doing so, we change the\nquestions we can pose. Rather than asking whether the color of a particular\npixel was important for the \u201clabrador retriever\u201d classification, we instead\nask whether the high-level idea detected at that position (such as \u201cfloppy\near\u201d) was important. This approach is similar to what Class Activation Mapping\n(CAM) methods do but, because they interpret their results back onto the input\nimage, they miss the opportunity to communicate in terms of the rich behavior\nof a network\u2019s hidden layers.\n\nLoading\n\nThe above interface affords us a more flexible relationship with attribution.\nTo start, we perform attribution from each spatial position of each hidden\nlayer shown to all 1,000 output classes. In order to visualize this thousand-\ndimensional vector, we use dimensionality reduction to produce a multi-\ndirectional saliency map. Overlaying these saliency maps on our magnitude-\nsized activation grids provides an information scent over attribution space.\nThe activation grids allow us to anchor attribution to the visual vocabulary\nour semantic dictionaries first established. On hover, we update the legend to\ndepict attribution to the output classes (i.e., which classes does this\nspatial position most contribute to?).\n\nPerhaps most interestingly, this interface allows us to interactively perform\nattribution between hidden layers. On hover, additional saliency maps mask the\nhidden layers, in a sense shining a light into their black boxes. This type of\nlayer-to-layer attribution is a prime example of how carefully considering\ninterface design drives the generalization of our existing abstractions for\ninterpretability.\n\nWith this diagram, we have begun to think of attribution in terms of higher-\nlevel concepts. However, at a particular position, many concepts are being\ndetected together and this interface makes it difficult to split them apart.\nBy continuing to focus on spatial positions, these concepts remain entangled.\n\n### Channel Attribution\n\nSaliency maps implicitly slice our cube of activations by applying attribution\nto the spatial positions of a hidden layer. This aggregates over all channels\nand, as a result, we cannot tell which specific detectors at each position\nmost contributed to the final output classification.\n\nAn alternate way to slice the cube is by channels instead of spatial\nlocations. Doing so allows us to perform channel attribution: how much did\neach detector contribute to the final output? (This approach is similar to\ncontemporaneous work by Kim et al., who do attribution to learned combination\nof channels.)\n\nLoading\n\nThis diagram is analogous to the previous one we saw: we conduct layer-to-\nlayer attribution but this time over channels rather than spatial positions.\nOnce again, we use the icons from our semantic dictionary to represent the\nchannels that most contribute to the final output classification. Hovering\nover an individual channel displays a heatmap of its activations overlaid on\nthe input image. The legend also updates to show its attribution to the output\nclasses (i.e., what are the top classes this channel supports?). Clicking a\nchannel allows us to drill into the layer-to-layer attributions, identifying\nthe channels at lower layers that most contributed as well as the channels at\nhigher layers that are most supported.\n\nWhile these diagrams focus on layer-to-layer attribution, it can still be\nvaluable to focus on a single hidden layer. For example, the teaser figure\nallows us to evaluate hypotheses for why one class succeeded over the other.\n\nAttribution to spatial locations and channels can reveal powerful things about\na model, especially when we combine them together. Unfortunately, this family\nof approaches is burdened by two significant problems. On the one hand, it is\nvery easy to end up with an overwhelming amount of information: it would take\nhours of human auditing to understand the long-tail of channels that slightly\nimpact the output. On the other hand, both the aggregations we have explored\nare extremely lossy and can miss important parts of the story. And, while we\ncould avoid lossy aggregation by working with individual neurons, and not\naggregating at all, this explodes the first problem combinatorially.\n\n## Making Things Human-Scale\n\nIn previous sections, we\u2019ve considered three ways of slicing the cube of\nactivations: into spatial activations, channels, and individual neurons. Each\nof these has major downsides. If one only uses spatial activations or\nchannels, they miss out on very important parts of the story. For example it\u2019s\ninteresting that the floppy ear detector helped us classify an image as a\nLabrador retriever, but it\u2019s much more interesting when that\u2019s combined with\nthe locations that fired to do so. One can try to drill down to the level of\nneurons to tell the whole story, but the tens of thousands of neurons are\nsimply too much information. Even the hundreds of channels, before being split\ninto individual neurons, can be overwhelming to show users!\n\nIf we want to make useful interfaces into neural networks, it isn\u2019t enough to\nmake things meaningful. We need to make them human scale, rather than\noverwhelming dumps of information. The key to doing so is finding more\nmeaningful ways of breaking up our activations. There is good reason to\nbelieve that such decompositions exist. Often, many channels or spatial\npositions will work together in a highly correlated way and are most useful to\nthink of as one unit. Other channels or positions will have very little\nactivity, and can be ignored for a high-level overview. So, it seems like we\nought to be able to find better decompositions if we had the right tools.\n\nThere is an entire field of research, called matrix factorization, that\nstudies optimal strategies for breaking up matrices. By flattening our cube\ninto a matrix of spatial locations and channels, we can apply these techniques\nto get more meaningful groups of neurons. These groups will not align as\nnaturally with the cube as the groupings we previously looked at. Instead,\nthey will be combinations of spatial locations and channels. Moreover, these\ngroups are constructed to explain the behavior of a network on a particular\nimage. It would not be effective to reuse the same groupings on another image;\neach image requires calculating a unique set of groups.\n\nIn addition to naturally slicing a hidden layer\u2019s cube of activations into\nneurons, spatial locations, or channels, we can also consider more arbitrary\ngroupings of locations and channels.\n\nThe groups that come out of this factorization will be the atoms of the\ninterface a user works with. Unfortunately, any grouping is inherently a\ntradeoff between reducing things to human scale and, because any aggregation\nis lossy, preserving information. Matrix factorization lets us pick what our\ngroupings are optimized for, giving us a better tradeoff than the natural\ngroupings we saw earlier.\n\nThe goals of our user interface should influence what we optimize our matrix\nfactorization to prioritize. For example, if we want to prioritize what the\nnetwork detected, we would want the factorization to fully describe the\nactivations. If we instead wanted to prioritize what would change the\nnetwork\u2019s behavior, we would want the factorization to fully describe the\ngradient. Finally, if we want to prioritize what caused the present behavior,\nwe would want the factorization to fully describe the attributions. Of course,\nwe can strike a balance between these three objectives rather than optimizing\none to the exclusion of the others.\n\nIn the following diagram, we\u2019ve constructed groups that prioritize the\nactivations, by factorizing the activations Most matrix factorization\nalgorithms and libraries are set up to minimize the mean squared error of the\nreconstruction of a matrix you give them. There are ways to hack such\nlibraries to achieve more general objectives through clever manipulations of\nthe provided matrix, as we will see below. More broadly, matrix factorization\nis an optimization problem, and with custom tools you can achieve all sorts of\ncustom factorizations. with non-negative matrix factorization As the name\nsuggests, non-negative matrix factorization (NMF) constrains its factors to be\npositive. This is fine for the activations of a ReLU network, which must be\npositive as well. Our experience is that the groups we get from NMF seem more\nindependent and semantically meaningful than those without this constraint.\nBecause of this constraints, groups from NMF are a less efficient at\nrepresenting the activations than they would be without, but our experience is\nthat they seem more independent and semantically meaningful. . Notice how the\noverwhelmingly large number of neurons has been reduced to a small set of\ngroups, concisely summarizing the story of the neural network.\n\nThis figure only focuses at a single layer but, as we saw earlier, it can be\nuseful to look across multiple layers to understand how a neural network\nassembles together lower-level detectors into higher-level concepts.\n\nThe groups we constructed before were optimized to understand a single layer\nindependent of the others. To understand multiple layers together, we would\nlike each layer\u2019s factorization to be \u201ccompatible\u201d \u2014 to have the groups of\nearlier layers naturally compose into the groups of later layers. This is also\nsomething we can optimize the factorization for We formalize this\n\u201ccompatibility\u201d in a manner described below, although we\u2019re not confident it\u2019s\nthe best formalization and won\u2019t be surprised if it is superseded in future\nwork.\n\nConsider the attribution from every neuron in the layer to the set of N groups\nwe want it to be compatible with. The basic idea is to split each entry in the\nactivation matrix into N entries on the channel dimension, spreading the\nvalues proportional to the absolute value of its attribution to the\ncorresponding group. Any factorization of this matrix induces a factorization\nof the original matrix by collapsing the duplicated entries in the column\nfactors. However, the resulting factorization tries to create separate factors\nwhen the activation of the same channel has different attributions in\ndifferent places. .\n\nIn this section, we recognize that the way in which we break apart the cube of\nactivations is an important interface decision. Rather than resigning\nourselves to the natural slices of the cube of activations, we construct more\noptimal groupings of neurons. These improved groupings are both more\nmeaningful and more human-scale, making it less tedious for users to\nunderstand the behavior of the network.\n\nOur visualizations have only begun to explore the potential of alternate bases\nin providing better atoms for understanding neural networks. For example,\nwhile we focus on creating smaller numbers of directions to explain individual\nexamples, there\u2019s recently been exciting work finding \u201cglobally\u201d meaningful\ndirections \u2014 such bases could be especially helpful when trying to understand\nmultiple examples at a time, or in comparing models. The recent NIPS\ndisentangling workshop provides other promising directions. We\u2019re excited to\nsee a venue for this developing area of research.\n\n## The Space of Interpretability Interfaces\n\nThe interface ideas presented in this article combine building blocks such as\nfeature visualization and attribution. Composing these pieces is not an\narbitrary process, but rather follows a structure based on the goals of the\ninterface. For example, should the interface emphasize what the network\nrecognizes, prioritize how its understanding develops, or focus on making\nthings human-scale. To evaluate such goals, and understand the tradeoffs, we\nneed to be able to systematically consider possible alternatives.\n\nWe can think of an interface as a union of individual elements.\n\n#### Layers\n\n  * output\n  * hidden\n  * input\n\n#### Atoms\n\n  * group\n  * spatial\n  * channel\n  * neuron\n\n#### Content\n\n  * activations\n  * attribution\n\n#### Presentation\n\n  * information visualization\n  * feature visualization\n\nEach element displays a specific type of content (e.g., activations or\nattribution) using a particular style of presentation (e.g., feature\nvisualization or traditional information visualization). This content lives on\nsubstrates defined by how given layers of the network are broken apart into\natoms, and may be transformed by a series of operations (e.g., to filter it or\nproject it onto another substrate). For example, our semantic dictionaries use\nfeature visualization to display the activations of a hidden layer's neurons.\n\nOne way to represent this way of thinking is with a formal grammarExpressing\ndesign spaces as formal grammars is a technique used in human-computer\ninteraction and data visualization. We think this is a powerful technique and\npresent this grammar as an initial exploration of how it might apply to\ninterpretability interfaces.\n\nId = Int Atom = Neuron | Spatial | Channel | Group | Whole Layer = Input | Hidden Int | Out Substrate = Network Id Atom Layer | Dataset Id | Parameters Id Content = Substrate | Activation Substrate | Attribution Substrate Substrate | Transform Content Content? Element = InfoVis Content | FeatureVis Content Interface = [ Element ], but we find it helpful to think about the space visually. We can represent the network\u2019s substrate (which layers we display, and how we break them apart) as a grid, with the content and style of presentation plotted on this grid as points and connections.\n\nThis setup gives us a framework to begin exploring the space of\ninterpretability interfaces step by step. For instance, let us consider our\nteaser figure again. Its goal is to help us compare two potential\nclassifications for an input image.\n\n1\\. Feature visualization\n\nTo understand a classification, we focus on the channels of the mixed4d layer.\nFeature visualization makes these channels meaningful.\n\n2\\. Filter by output attribution\n\nNext, we filter for specific classes by calculating the output attribution.\n\n3\\. Drill down on hover\n\nHovering over channels, we get a heatmap of spatial activations.\n\nIn this article, we have only scratched the surface of possibilities. There\nare lots of combinations of our building blocks left to explore, and the\ndesign space gives us a way to do so systematically.\n\nMoreover, each building block represents a broad class of techniques. Our\ninterfaces take only one approach but, as we saw in each section, there are a\nnumber of alternatives for feature visualization, attribution, and matrix\nfactorization. An immediate next step would be to try using these alternate\ntechniques, and research ways to improve them.\n\nFinally, this is not the complete set of building blocks; as new ones are\ndiscovered, they expand the space. For example, Koh & Liang. suggest ways of\nunderstanding the influence of dataset examples on model behavior . We can\nthink of dataset examples as another substrate in our design space, thus\nbecoming another building block that fully composes with the others. In doing\nso, we can now imagine interfaces that not only allow us to inspect the\ninfluence of dataset examples on the final output classification (as Koh &\nLiang proposed), but also how examples influence the features of hidden\nlayers, and how they influence the relationship between these features and the\noutput. For example, if we consider our \u201cLabrador retriever\u201d image, we can not\nonly see which dataset examples most influenced the model to arrive at this\nclassification, but also which dataset examples most caused the \u201cfloppy ear\u201d\ndetectors to fire, and which dataset examples most caused these detectors to\nincrease the \u201cLabrador retriever\u201d classification.\n\nA new substrate.An interface to understand how dataset examples influence the\noutput classification, as presented by Koh & LiangAn interface showing how\nexamples influence the channels of hidden layers.An interface for identifying\nwhich dataset examples most caused particular detectors to increase the output\nclassification.\n\nBeyond interfaces for analyzing model behavior, if we add model parameters as\na substrate, the design space now allows us to consider interfaces for taking\naction on neural networks.Note that essentially all our interpretability\ntechniques are differentiable, so you can backprop through them. While most\nmodels today are trained to optimize simple objective functions that one can\neasily describe, many of the things we\u2019d like models to do in the real world\nare subtle, nuanced, and hard to describe mathematically. An extreme example\nof the subtle objective problem is something like \u201ccreating interesting art\u201d,\nbut much more mundane examples arise more or less whenever humans are\ninvolved. One very promising approach to training models for these subtle\nobjectives is learning from human feedback . However, even with human\nfeedback, it may still be hard to train models to behave the way we want if\nthe problematic aspect of the model doesn\u2019t surface strongly in the training\nregime where humans are giving feedback. There are lots of reasons why\nproblematic behavior may not surface or may be hard for an evaluator to give\nfeedback on. For example, discrimination and bias may be subtly present\nthroughout the model\u2019s behavior, such that it\u2019s hard for a human evaluator to\ncritique. Or the model may be making a decision in a way that has problematic\nconsequences, but those consequences never play out in the problems we\u2019re\ntraining it on. Human feedback on the model\u2019s decision making process,\nfacilitated by interpretability interfaces, could be a powerful solution to\nthese problems. It might allow us to train models not just to make the right\ndecisions, but to make them for the right reasons. (There is however a danger\nhere: we are optimizing our model to look the way we want in our interface \u2014\nif we aren\u2019t careful, this may lead to the model fooling us!Related ideas have\noccasionally been discussed under the term \u201ccognitive steganography.\u201d)\n\nAnother exciting possibility is interfaces for comparing multiple models. For\ninstance, we might want to see how a model evolves during training, or how it\nchanges when you transfer it to a new task. Or, we might want to understand\nhow a whole family of models compares to each other. Existing work has\nprimarily focused on comparing the output behavior of models but more recent\nwork is starting to explore comparing their internal representations as well.\nOne of the unique challenges of this work is that we may want to align the\natoms of each model; if we have completely different models, can we find the\nmost analogous neurons between them? Zooming out, can we develop interfaces\nthat allow us to evaluate large spaces of models at once?\n\n## How Trustworthy Are These Interfaces?\n\nIn order for interpretability interfaces to be effective, we must trust the\nstory they are telling us. We perceive two concerns with the set of building\nblocks we currently use. First, do neurons have a relatively consistent\nmeaning across different input images, and is that meaning accurately reified\nby feature visualization? Semantic dictionaries, and the interfaces that build\non top of them, are premised off this question being true. Second, does\nattribution make sense and do we trust any of the attribution methods we\npresently have?\n\nMuch prior research has found that directions in neural networks are\nsemantically meaningful. One particularly striking example of this is\n\u201csemantic arithmetic\u201d (eg. \u201cking\u201d - \u201cman\u201d + \u201cwoman\u201d = \u201cqueen\u201d). We explored\nthis question, in depth, for GoogLeNet in our previous article and found that\nmany of its neurons seem to correspond to meaningful ideas.We validated this\nin a number of ways: we visualized them without a generative model prior, so\nthat the content of the visualizations was causally linked to the neuron\nfiring; we inspected the spectrum of examples that cause the neuron to fire;\nand used diversity visualizations to try to create different inputs that cause\nthe neuron to fire.\n\nFor more details, see the article\u2019s appendix and the guided tour in @ch402\u2032s\nTwitter thread. We\u2019re actively investigating why GoogLeNet\u2019s neurons seem more\nmeaningful. Besides these neurons, however, we also found many neurons that do\nnot have as clean a meaning including \u201cpoly-semantic\u201d neurons that respond to\na mixture of salient ideas (e.g., \u201ccat\u201d and \u201ccar\u201d). There are natural ways\nthat interfaces could respond to this: we could use diversity visualizations\nto reveal the variety of meanings the neuron can take, or rotate our semantic\ndictionaries so their components are more disentangled. Of course, just like\nour models can be fooled, the features that make them up can be too \u2014\nincluding with adversarial examples . In our view, features do not need to be\nflawless detectors for it to be useful for us to think about them as such. In\nfact, it can be interesting to identify when a detector misfires.\n\nWith regards to attribution, recent work suggests that many of our current\ntechniques are unreliable. One might even wonder if the idea is fundamentally\nflawed, since a function\u2019s output could be the result of non-linear\ninteractions between its inputs. One way these interactions can pan out is as\nattribution being \u201cpath-dependent\u201d. A natural response to this would be for\ninterfaces to explicitly surface this information: how path-dependent is the\nattribution? A deeper concern, however, would be whether this path-dependency\ndominates the attribution. Clearly, this is not a concern for attribution\nbetween adjacent layers because of the simple (essentially linear) mapping\nbetween them. While there may be technicalities about correlated inputs, we\nbelieve that attribution is on firm grounding here. And even with layers\nfurther apart, our experience has been that attribution between high-level\nfeatures at the output is much more consistent than attribution to the input \u2014\nwe believe that path-dependence is not a dominating concern here.\n\nModel behavior is extremely complex, and our current building blocks force us\nto show only specific aspects of it. An important direction for future\ninterpretability research will be developing techniques that achieve broader\ncoverage of model behavior. But, even with such improvements, we anticipate\nthat a key marker of trustworthiness will be interfaces that do not mislead.\nInteracting with the explicit information displayed should not cause users to\nimplicitly draw incorrect assessments about the model (we see a similar\nprinciple articulated by Mackinlay for data visualization). Undoubtedly, the\ninterfaces we present in this article have room to improve in this regard.\nFundamental research, at the intersection of machine learning and human-\ncomputer interaction, is necessary to resolve these issues.\n\nTrusting our interfaces is essential for many of the ways we want to use\ninterpretability. This is both because the stakes can be high (as in safety\nand fairness) and also because ideas like training models with\ninterpretability feedback put our interpretability techniques in the middle of\nan adversarial setting.\n\n## Conclusion & Future Work\n\nThere is a rich design space for interacting with enumerative algorithms, and\nwe believe an equally rich space exists for interacting with neural networks.\nWe have a lot of work left ahead of us to build powerful and trustworthy\ninterfaces for interpretability. But, if we succeed, interpretability promises\nto be a powerful tool in enabling meaningful human oversight and in building\nfair, safe, and aligned AI systems.\n\n### Acknowledgments\n\nOur article was greatly strengthened thanks to the detailed feedback by Ben\nPoole, Emma Pierson, Jason Yosinski, Jeff Heer, John Backus, Martin\nWattenberg, Matt Johnson, and Tim Hwang.\n\nWe also really appreciated the conversations we\u2019ve had with Tom Brown,\nCatherine Olsson, Daniel Dewey, Ajeya Cotra, Dario Amodei, Paul Christiano on\nthe relationship of interpretability to safety; the thoughtful comments of\nMichael Nielsen, Zak Stone, Zan Armstrong and Anjuli Kannan; the support of\nWolff Dobson, Jack Clark, Charina Chou, Jason Freidenfelds, Christian Howard,\nKaroly Zsolnai-Feher in communicating our work to a broader audience; and the\nsupportive environment fostered by Greg Corrado and Jeff Dean at Google Brain.\n\nFinally, we really appreciate Justin Gilmer stepping in as acting Distill\neditor of this article, and Qiqi Yan, Guillaume Alain, and anonymous reviewer\nC for taking the time to review our article.\n\n### Author Contributions\n\nInterface Design & Prototyping. This work began with a number of exciting\ninterface demonstrations by Alex in 2015, combining feature visualizations,\nactivations, and dimensionality reduction. In early 2017, Chris generalized\nthis line of research and combined it with attribution. Katherine prototyped\nearly interfaces for spatial attribution. Ian built the neuron group Sankey\ndiagram interface. Arvind created most of the final set of interfaces in the\ndiagram, significantly improving them, with extensive design input and polish\nfrom Shan.\n\nConceptual Contributions. Many of these ideas have their earliest provenance\nwith Alex. Chris generalized and refined them, and integrated attribution.\nChris, Arvind, and Ian developed the building blocks framing. Ian and Chris\ncoined the term \u201csemantic dictionaries.\u201d Arvind and Chris crystallized this\nthinking into a grammar, and contextualized it with respect to both the\nmachine learning and human-computer interaction communities.\n\nWriting. Arvind and Chris wrote the text of the article, with significant\ninput from Ian and Shan.\n\nInfrastructure. The core library we use to visualize neural networks, Lucid,\nwas primarily written by Chris, Alex, and Ludwig. Chris wrote most of the code\nused for attribution and matrix factorization. Ludwig created distributed\nimplementations and workflows for generating our diagrams.\n\n### Discussion and Review\n\nReview 1 - Qiqi Yan Review 2 - Guillaume Alain Review 3 - Anonymous\n\n### Footnotes\n\n  1. We\u2019re actively investigating why this is, and hope to uncover principles for designing interpretable models. In the meantime, while we demonstrate our techniques on GoogLeNet, we provide code for you to try them on other models.[\u21a9]\n  2. We do attribution by linear approximation in all of our interfaces. That is, we estimate the effect of a neuron on the output is its activation times the rate at which increasing its activation increases the output. When we talk about a linear combination of activations, the attribution can be thought of as the linear combination of the attributions of the units, or equivalently as the dot product between the activation of that combination and the gradient.\n\nFor spatial attribution, we do an additional trick. GoogLeNet\u2019s strided max\npooling introduces a lot of noise and checkerboard patterns to it\u2019s gradients.\nTo avoid our interface demonstrations being dominated by this noise, we (a) do\na relaxation of the gradient of max pooling, distributing gradient to inputs\nproportional to their activation instead of winner takes all and (b) cancel\nout the checkerboard patterns.\n\nThe notebooks attached to diagrams provide reference implementations.[\u21a9]\n\n  3. Most matrix factorization algorithms and libraries are set up to minimize the mean squared error of the reconstruction of a matrix you give them. There are ways to hack such libraries to achieve more general objectives through clever manipulations of the provided matrix, as we will see below. More broadly, matrix factorization is an optimization problem, and with custom tools you can achieve all sorts of custom factorizations.[\u21a9]\n  4. As the name suggests, non-negative matrix factorization (NMF) constrains its factors to be positive. This is fine for the activations of a ReLU network, which must be positive as well. Our experience is that the groups we get from NMF seem more independent and semantically meaningful than those without this constraint. Because of this constraints, groups from NMF are a less efficient at representing the activations than they would be without, but our experience is that they seem more independent and semantically meaningful.[\u21a9]\n  5. We formalize this \u201ccompatibility\u201d in a manner described below, although we\u2019re not confident it\u2019s the best formalization and won\u2019t be surprised if it is superseded in future work.\n\nConsider the attribution from every neuron in the layer to the set of N groups\nwe want it to be compatible with. The basic idea is to split each entry in the\nactivation matrix into N entries on the channel dimension, spreading the\nvalues proportional to the absolute value of its attribution to the\ncorresponding group. Any factorization of this matrix induces a factorization\nof the original matrix by collapsing the duplicated entries in the column\nfactors. However, the resulting factorization tries to create separate factors\nwhen the activation of the same channel has different attributions in\ndifferent places. [\u21a9]\n\n  6. The recent NIPS disentangling workshop provides other promising directions. We\u2019re excited to see a venue for this developing area of research.[\u21a9]\n  7. Expressing design spaces as formal grammars is a technique used in human-computer interaction and data visualization. We think this is a powerful technique and present this grammar as an initial exploration of how it might apply to interpretability interfaces.\n\nId = Int Atom = Neuron | Spatial | Channel | Group | Whole Layer = Input | Hidden Int | Out Substrate = Network Id Atom Layer | Dataset Id | Parameters Id Content = Substrate | Activation Substrate | Attribution Substrate Substrate | Transform Content Content? Element = InfoVis Content | FeatureVis Content Interface = [ Element ][\u21a9]\n\n  8. Note that essentially all our interpretability techniques are differentiable, so you can backprop through them.[\u21a9]\n  9. An extreme example of the subtle objective problem is something like \u201ccreating interesting art\u201d, but much more mundane examples arise more or less whenever humans are involved.[\u21a9]\n  10. There are lots of reasons why problematic behavior may not surface or may be hard for an evaluator to give feedback on. For example, discrimination and bias may be subtly present throughout the model\u2019s behavior, such that it\u2019s hard for a human evaluator to critique. Or the model may be making a decision in a way that has problematic consequences, but those consequences never play out in the problems we\u2019re training it on. [\u21a9]\n  11. Related ideas have occasionally been discussed under the term \u201ccognitive steganography.\u201d[\u21a9]\n  12. We validated this in a number of ways: we visualized them without a generative model prior, so that the content of the visualizations was causally linked to the neuron firing; we inspected the spectrum of examples that cause the neuron to fire; and used diversity visualizations to try to create different inputs that cause the neuron to fire.\n\nFor more details, see the article\u2019s appendix and the guided tour in @ch402\u2032s\nTwitter thread. We\u2019re actively investigating why GoogLeNet\u2019s neurons seem more\nmeaningful.[\u21a9]\n\n### References\n\n  1. Thought as a Technology [HTML] Nielsen, M., 2016.\n  2. Visualizing Representations: Deep Learning and Human Beings [link] Olah, C., 2015.\n  3. Understanding neural networks through deep visualization [PDF] Yosinski, J., Clune, J., Nguyen, A., Fuchs, T. and Lipson, H., 2015. arXiv preprint arXiv:1506.06579.\n  4. Using Artificial Intelligence to Augment Human Intelligence [link] Carter, S. and Nielsen, M., 2017. Distill. DOI: 10.23915/distill.00009\n  5. Visualizing higher-layer features of a deep network [PDF] Erhan, D., Bengio, Y., Courville, A. and Vincent, P., 2009. University of Montreal, Vol 1341, pp. 3.\n  6. Feature Visualization [link] Olah, C., Mordvintsev, A. and Schubert, L., 2017. Distill. DOI: 10.23915/distill.00007\n  7. Deep inside convolutional networks: Visualising image classification models and saliency maps [PDF] Simonyan, K., Vedaldi, A. and Zisserman, A., 2013. arXiv preprint arXiv:1312.6034.\n  8. Deep neural networks are easily fooled: High confidence predictions for unrecognizable images [PDF] Nguyen, A., Yosinski, J. and Clune, J., 2015. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 427--436. DOI: 10.1109/cvpr.2015.7298640\n  9. Inceptionism: Going deeper into neural networks [HTML] Mordvintsev, A., Olah, C. and Tyka, M., 2015. Google Research Blog.\n  10. Plug & play generative networks: Conditional iterative generation of images in latent space [PDF] Nguyen, A., Clune, J., Bengio, Y., Dosovitskiy, A. and Yosinski, J., 2016. arXiv preprint arXiv:1612.00005.\n  11. Visualizing and understanding convolutional networks [PDF] Zeiler, M.D. and Fergus, R., 2014. European conference on computer vision, pp. 818--833.\n  12. Striving for simplicity: The all convolutional net [PDF] Springenberg, J.T., Dosovitskiy, A., Brox, T. and Riedmiller, M., 2014. arXiv preprint arXiv:1412.6806.\n  13. Grad-cam: Why did you say that? visual explanations from deep networks via gradient-based localization [PDF] Selvaraju, R.R., Das, A., Vedantam, R., Cogswell, M., Parikh, D. and Batra, D., 2016. arXiv preprint arXiv:1610.02391.\n  14. Interpretable Explanations of Black Boxes by Meaningful Perturbation [PDF] Fong, R. and Vedaldi, A., 2017. arXiv preprint arXiv:1704.03296.\n  15. PatternNet and PatternLRP--Improving the interpretability of neural networks [PDF] Kindermans, P., Schutt, K.T., Alber, M., Muller, K. and Dahne, S., 2017. arXiv preprint arXiv:1705.05598. DOI: 10.1007/978-3-319-10590-1_53\n  16. The (Un)reliability of saliency methods [PDF] Kindermans, P., Hooker, S., Adebayo, J., Alber, M., Schutt, K.T., Dahne, S., Erhan, D. and Kim, B., 2017. arXiv preprint arXiv:1711.00867.\n  17. Axiomatic attribution for deep networks [PDF] Sundararajan, M., Taly, A. and Yan, Q., 2017. arXiv preprint arXiv:1703.01365.\n  18. Visualizing data using t-SNE [PDF] Maaten, L.v.d. and Hinton, G., 2008. Journal of Machine Learning Research, Vol 9(Nov), pp. 2579--2605.\n  19. LSTMVis: A tool for visual analysis of hidden state dynamics in recurrent neural networks [PDF] Strobelt, H., Gehrmann, S., Pfister, H. and Rush, A.M., 2018. IEEE Transactions on Visualization and Computer Graphics, Vol 24(1), pp. 667--676. IEEE. DOI: 10.1109/tvcg.2017.2744158\n  20. ActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models [PDF] Kahng, M., Andrews, P.Y., Kalro, A. and Chau, D.H.P., 2018. IEEE Transactions on Visualization and Computer Graphics, Vol 24(1), pp. 88--97. IEEE. DOI: 10.1109/tvcg.2017.2744718\n  21. Do convolutional neural networks learn class hierarchy? [PDF] Bilal, A., Jourabloo, A., Ye, M., Liu, X. and Ren, L., 2018. IEEE Transactions on Visualization and Computer Graphics, Vol 24(1), pp. 152--162. IEEE. DOI: 10.1109/tvcg.2017.2744683\n  22. Going deeper with convolutions [PDF] Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., Rabinovich, A. and others,, 2015. DOI: 10.1109/cvpr.2015.7298594\n  23. Deconvolution and Checkerboard Artifacts [link] Odena, A., Dumoulin, V. and Olah, C., 2016. Distill. DOI: 10.23915/distill.00003\n  24. Learning deep features for discriminative localization [PDF] Zhou, B., Khosla, A., Lapedriza, A., Oliva, A. and Torralba, A., 2016. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2921--2929. DOI: 10.1109/cvpr.2016.319\n  25. Information foraging [link] Pirolli, P. and Card, S., 1999. Psychological review, Vol 106(4), pp. 643. American Psychological Association. DOI: 10.1037//0033-295x.106.4.643\n  26. TCAV: Relative concept importance testing with Linear Concept Activation Vectors [PDF] Kim, B., Gilmer, J., Viegas, F., Erlingsson, U. and Wattenberg, M., 2017. arXiv preprint arXiv:1711.11279.\n  27. SVCCA: Singular Vector Canonical Correlation Analysis for Deep Learning Dynamics and Interpretability [PDF] Raghu, M., Gilmer, J., Yosinski, J. and Sohl-Dickstein, J., 2017. Advances in Neural Information Processing Systems 30, pp. 6078--6087. Curran Associates, Inc.\n  28. Net2Vec: Quantifying and Explaining how Concepts are Encoded by Filters in Deep Neural Networks Fong, R. and Vedaldi, A., 2018. arXiv preprint arXiv:1801.03454.\n  29. Understanding Black-box Predictions via Influence Functions [PDF] Koh, P.W. and Liang, P., 2017. International Conference on Machine Learning (ICML).\n  30. Deep reinforcement learning from human preferences Christiano, P.F., Leike, J., Brown, T., Martic, M., Legg, S. and Amodei, D., 2017. Advances in Neural Information Processing Systems, pp. 4302--4310.\n  31. Interactive optimization for steering machine classification [PDF] Kapoor, A., Lee, B., Tan, D. and Horvitz, E., 2010. Proceedings of the SIGCHI Conference on Human Factors in Computing Systems, pp. 1343--1352. DOI: 10.1145/1753326.1753529\n  32. Interacting with predictions: Visual inspection of black-box machine learning models [PDF] Krause, J., Perer, A. and Ng, K., 2016. Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems, pp. 5686--5697. DOI: 10.1145/2858036.2858529\n  33. Modeltracker: Redesigning performance analysis tools for machine learning [PDF] Amershi, S., Chickering, M., Drucker, S.M., Lee, B., Simard, P. and Suh, J., 2015. Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems, pp. 337--346. DOI: 10.1145/2702123.2702509\n  34. Network dissection: Quantifying interpretability of deep visual representations [PDF] Bau, D., Zhou, B., Khosla, A., Oliva, A. and Torralba, A., 2017. Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on, pp. 3319--3327. DOI: 10.1109/cvpr.2017.354\n  35. Intriguing properties of neural networks [PDF] Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I. and Fergus, R., 2013. arXiv preprint arXiv:1312.6199.\n  36. Efficient estimation of word representations in vector space [PDF] Mikolov, T., Chen, K., Corrado, G. and Dean, J., 2013. arXiv preprint arXiv:1301.3781.\n  37. Unsupervised representation learning with deep convolutional generative adversarial networks [PDF] Radford, A., Metz, L. and Chintala, S., 2015. arXiv preprint arXiv:1511.06434.\n  38. Adversarial manipulation of deep representations [PDF] Sabour, S., Cao, Y., Faghri, F. and Fleet, D.J., 2015. arXiv preprint arXiv:1511.05122.\n  39. Automating the design of graphical presentations of relational information [PDF] Mackinlay, J., 1986. Acm Transactions On Graphics (Tog), Vol 5(2), pp. 110--141. ACM. DOI: 10.1145/22949.22950\n\n### Updates and Corrections\n\nIf you see mistakes or want to suggest changes, please create an issue on\nGitHub.\n\n### Reuse\n\nDiagrams and text are licensed under Creative Commons Attribution CC-BY 4.0\nwith the source available on GitHub, unless noted otherwise. The figures that\nhave been reused from other sources don\u2019t fall under this license and can be\nrecognized by a note in their caption: \u201cFigure from ...\u201d.\n\n### Citation\n\nFor attribution in academic contexts, please cite this work as\n\n    \n    \n    Olah, et al., \"The Building Blocks of Interpretability\", Distill, 2018.\n\nBibTeX citation\n\n    \n    \n    @article{olah2018the, author = {Olah, Chris and Satyanarayan, Arvind and Johnson, Ian and Carter, Shan and Schubert, Ludwig and Ye, Katherine and Mordvintsev, Alexander}, title = {The Building Blocks of Interpretability}, journal = {Distill}, year = {2018}, note = {https://distill.pub/2018/building-blocks}, doi = {10.23915/distill.00010} }\n\nDistill is dedicated to clear explanations of machine learning\n\nAbout Submit Prize Archive RSS GitHub Twitter ISSN 2476-0757\n\n", "frontpage": false}
