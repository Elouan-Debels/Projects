{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32d366ef-bccb-4d59-bd9d-6406516f4669",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model building. Build the model that will be used for the live prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "087d59c6-7ad0-4b19-9293-87a89fb792b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "\n",
    "# Helper thread to avoid the Spark StreamingContext from blocking Jupyter\n",
    "        \n",
    "class StreamingThread(threading.Thread):\n",
    "    def __init__(self, ssc):\n",
    "        super().__init__()\n",
    "        self.ssc = ssc\n",
    "    def run(self):\n",
    "        self.ssc.start()\n",
    "        self.ssc.awaitTermination()\n",
    "    def stop(self):\n",
    "        print('----- Stopping... this may take a few seconds -----')\n",
    "        self.ssc.stop(stopSparkContext=False, stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9c73dfd-74af-4b36-9bbf-ac2f85253c6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.23:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30eec04a-b51e-4108-b70c-38b761e5ae68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first, we will need to organize and clean the data\n",
    "#step 1: convert to .json (of ander formaat? -> nee, het is echt als een .json dictionary gegeven, dus zo is logisch)\n",
    "#step 2: put it in a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6467afb3-a8e5-452a-a4eb-dc6e2418b20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#script to convert all saved files to .json files, so they can actually be accessed\n",
    "import os\n",
    "\n",
    "def rename_to_json(base_dir):\n",
    "    # Function to recursively find files to rename\n",
    "    def find_files_to_rename(directory):\n",
    "        files_to_rename = []\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                if file.startswith('part-') and not file.endswith('.json'):\n",
    "                    files_to_rename.append(os.path.join(root, file))\n",
    "        return files_to_rename\n",
    "\n",
    "    # Find all files to rename\n",
    "    files_to_rename = find_files_to_rename(base_dir)\n",
    "\n",
    "    # Rename files\n",
    "    for old_path in files_to_rename:\n",
    "        # Extract the filename and directory\n",
    "        directory, filename = os.path.split(old_path)\n",
    "        # Append \".json\" to the filename\n",
    "        new_filename = filename + '.json'\n",
    "        # Construct the new path\n",
    "        new_path = os.path.join(directory, new_filename)\n",
    "        # Rename the file\n",
    "        os.rename(old_path, new_path)\n",
    "        print(f\"Renamed {filename} to {new_filename}\")\n",
    "\n",
    "# Example usage:\n",
    "base_directory = \"C:/Users/eloua/Desktop/spark/notebooks/data/data/\"\n",
    "rename_to_json(base_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "376b668d-2e07-47eb-8cc1-d58e61efec35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- aid: string (nullable = true)\n",
      " |-- comments: long (nullable = true)\n",
      " |-- domain: string (nullable = true)\n",
      " |-- frontpage: boolean (nullable = true)\n",
      " |-- posted_at: string (nullable = true)\n",
      " |-- source_text: string (nullable = true)\n",
      " |-- source_title: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- user: string (nullable = true)\n",
      " |-- votes: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#merging all the seperate .json files into one single dataframe\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def merge_json_files(base_dir):\n",
    "    # Create a SparkSession\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"model building\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    # Function to recursively find JSON files in subfolders\n",
    "    def find_json_files(directory):\n",
    "        json_files = []\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                if file.endswith('.json'):\n",
    "                    json_files.append(os.path.join(root, file))\n",
    "        return json_files\n",
    "\n",
    "    # Find all JSON files in subfolders\n",
    "    json_files = find_json_files(base_dir)\n",
    "\n",
    "    # Read each JSON file into a DataFrame and merge them\n",
    "    merged_df = None\n",
    "    for json_file in json_files:\n",
    "        if merged_df is None:\n",
    "            merged_df = spark.read.json(json_file)\n",
    "        else:\n",
    "            new_df = spark.read.json(json_file)\n",
    "            merged_df = merged_df.union(new_df)\n",
    "\n",
    "    # Show the schema of the merged DataFrame\n",
    "    merged_df.printSchema()\n",
    "\n",
    "    return merged_df\n",
    "\n",
    "#apply it to the actual data\n",
    "base_directory = \"C:/Users/eloua/Desktop/spark/notebooks/data/data_klein/\"\n",
    "merged_dataframe = merge_json_files(base_directory)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b908a55a-b2eb-4883-a734-e5ddf4ad4738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-------------+-----+\n",
      "|     aid|comments|              domain|frontpage|          posted_at|         source_text|        source_title|               title|                 url|         user|votes|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-------------+-----+\n",
      "|39977630|       0|    github.com/gvcgo|    false|2024-04-09 09:18:20|GitHub - gvcgo/ve...|GitHub - gvcgo/ve...|Show HN: A genera...|https://github.co...|      moqsien|    1|\n",
      "|39977645|       0| github.com/rrweb-io|    false|2024-04-09 09:20:17|GitHub - rrweb-io...|GitHub - rrweb-io...|Rrweb, web sessio...|https://github.co...|     brennerm|    1|\n",
      "|39977648|       0|      flexboxcss.com|    false|2024-04-09 09:20:59|Flexbox CSS - Eff...|Effortlessly crea...|Web tool to strea...|https://flexboxcs...| lovegrenoble|    1|\n",
      "|39977669|       0|    sciencedaily.com|    false|2024-04-09 09:24:50|www.sciencedaily....|www.sciencedaily.com|Study of two trib...|https://www.scien...|  GeoAtreides|    1|\n",
      "|39977671|       0|           arxiv.org|     true|2024-04-09 09:24:58|[2404.05719] Ferr...|Ferret-UI: Ground...|Apple Ferret-UI: ...|https://arxiv.org...|         tosh|    8|\n",
      "|39977673|       0|         reuters.com|     true|2024-04-09 09:25:45|     reuters.com\\n\\n|         reuters.com|Biden and Kishida...|https://www.reute...|    alephnerd|    3|\n",
      "|39977681|       0|         reuters.com|    false|2024-04-09 09:26:54|     reuters.com\\n\\n|         reuters.com|South Korea to in...|https://www.reute...|    alephnerd|    1|\n",
      "|39977704|       0|           arxiv.org|    false|2024-04-09 09:32:15|[2404.04478] Diff...|Diffusion-RWKV: S...|Diffusion-RWKV: S...|https://arxiv.org...|         tosh|    1|\n",
      "|39977741|       0|         recursal.ai|    false|2024-04-09 09:40:41|Dear VC’s, please...|Dear VC’s, please...|Please stop throw...|https://substack....|         tosh|    1|\n",
      "|39977756|       2|        swissinfo.ch|     true|2024-04-09 09:43:08|Switzerland faces...|Switzerland faces...|Switzerland faces...|https://www.swiss...|  hubraumhugo|    6|\n",
      "|39977775|       0|hague6185.wordpre...|    false|2024-04-09 09:46:03|Russian Amphibiou...|Russian Amphibiou...|Bartini Beriev VV...|https://hague6185...|   palmfacehn|    1|\n",
      "|39977778|       0|             wsj.com|    false|2024-04-09 09:46:48|         wsj.com\\n\\n|             wsj.com|'Social Order Cou...|https://www.wsj.c...|       marban|    1|\n",
      "|39977780|       0|         youtube.com|    false|2024-04-09 09:47:05|BlackRock CEO Giv...|BlackRock CEO Giv...|BlackRock CEO Giv...|https://www.youtu...|        flojo|    1|\n",
      "|39977793|       0|           dspyt.com|    false|2024-04-09 09:49:27|Mina Protocol: Un...|DSPYT: Data Scien...|Mina Protocol: Un...|https://dspyt.com...|   fedorovn19|    1|\n",
      "|39977831|       0|        upworthy.com|     true|2024-04-09 09:56:49|The beautiful thi...|The beautiful thi...|The Lonely Funera...|https://www.upwor...|  thunderbong|    3|\n",
      "|39977845|       0|              the.al|     true|2024-04-09 09:59:16|Dualshock Calibra...|Dualshock Calibra...|Show HN: DualShoc...|https://blog.the....|        al_al|    6|\n",
      "|39977859|       0| github.com/nikeedev|    false|2024-04-09 10:03:37|GitHub - nikeedev...|GitHub - nikeedev...|           Rain Lang|https://github.co...|     nikeedev|    1|\n",
      "|39977862|       4|github.com/preten...|     true|2024-04-09 10:03:53|GitHub - Pretendo...|GitHub - Pretendo...|SSSL – Hackless S...|https://github.co...| todsacerdoti|   14|\n",
      "|39977866|       0|    conradakunga.com|    false|2024-04-09 10:05:26|Dogs Will Always ...|Dogs Will Always ...|The Spiral of UX ...|https://www.conra...|davidlemayian|    1|\n",
      "|39977878|       0|  washingtonpost.com|    false|2024-04-09 10:07:57|Why this eclipse ...|Why this eclipse ...|Why this eclipse ...|https://www.washi...|          pcl|    1|\n",
      "+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "merged_dataframe.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7ab50513-db95-4283-8b47-126113c0415e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#write it to one big .parquet file\n",
    "merged_dataframe.repartition(1).write.mode('overwrite').parquet(\"C:/Users/eloua/Desktop/spark/notebooks/data/merged_data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbe731d-9df2-4d1a-bfa1-31bfba0c4b8c",
   "metadata": {},
   "source": [
    "feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cf15bb96-bb57-43b1-90da-d2ab0e242c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a variable that contains how many upvotes the user has collected in total. note that this variable should be updated regularly in a real\n",
    "#application.\n",
    "#we will make use of the HackerNews API to collect the data on the users first.\n",
    "import requests\n",
    "import json\n",
    "\n",
    "#iterate over the dataframe to collect the user_ids of the users that actually posted a news story that we're tracking\n",
    "user_ids = merged_dataframe.select(\"user\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "user_data = []\n",
    "\n",
    "# Iterate over user IDs and collect their info using the API\n",
    "for user_id in user_ids:\n",
    "    # API endpoint for user information\n",
    "    api_url = f\"https://hacker-news.firebaseio.com/v0/user/{user_id}.json\"\n",
    "    \n",
    "    # Send GET request to the API\n",
    "    response = requests.get(api_url)\n",
    "    \n",
    "    # Check if request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse JSON response\n",
    "        user_info = response.json()\n",
    "        user_data.append(user_info)\n",
    "    else:\n",
    "        print(f\"Failed to fetch data for user {user_id}\")\n",
    "\n",
    "# Save collected user data to a file\n",
    "with open(\"user_data.json\", \"w\") as f:\n",
    "    json.dump(user_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86470a8c-682f-4916-9ff4-29aabc4aa4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+------+--------------------+--------------------+\n",
      "|   created|           id| karma|           submitted|               about|\n",
      "+----------+-------------+------+--------------------+--------------------+\n",
      "|1712120068|      moqsien|     3|[40072447, 400097...|                NULL|\n",
      "|1426762276|     brennerm|    37|[39977645, 399627...|Freelance DevOps ...|\n",
      "|1687312715| lovegrenoble|   317|[40099296, 400992...|                NULL|\n",
      "|1333111965|  GeoAtreides|  1107|[40092521, 400870...|                NULL|\n",
      "|1273014226|         tosh|148859|[40100561, 401002...|https:&#x2F;&#x2F...|\n",
      "|1546925297|    alephnerd|  6301|[40098382, 400931...|Recovering Policy...|\n",
      "|1561310375|  hubraumhugo|  6695|[40087150, 400871...|Unstructured data...|\n",
      "|1694750092|   palmfacehn|   512|[40078251, 400781...|HN face palm<p>ec...|\n",
      "|1208631846|       marban| 25033|[40099299, 400981...|Mostly known for ...|\n",
      "|1276254589|        flojo|   230|[39977780, 399288...|                NULL|\n",
      "|1709037918|   fedorovn19|     3|[40090604, 400906...|                NULL|\n",
      "|1337555174|  thunderbong| 81835|[40100450, 400990...|https:&#x2F;&#x2F...|\n",
      "|1606755684|        al_al|   149|[39983553, 399818...|                NULL|\n",
      "|1705399522|     nikeedev|     1|[39977860, 39977859]|Hi!<p>I’m Nikita,...|\n",
      "|1579288749| todsacerdoti|142415|[40101007, 400996...|Founder&#x2F;CEO ...|\n",
      "|1450177018|davidlemayian|    26|[39977867, 399778...|                NULL|\n",
      "|1314329570|          pcl|  3954|[40016810, 399890...|plinskey (at) gma...|\n",
      "|1709299239|      yderose|     1|[39977893, 39977892]|                NULL|\n",
      "|1307030567|         noch|  3585|[40083465, 400635...|                NULL|\n",
      "|1689676597|       salemy|    12|[39978110, 399779...|                NULL|\n",
      "+----------+-------------+------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#save it into a dataframe\n",
    "user_info_df = spark.createDataFrame(user_data)\n",
    "user_info_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad253d0c-9911-48ae-96b2-eb9a9bb3d9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the features from the user data\n",
    "#some features will need to get binned, like karma and number of submissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f80bfba-8ece-4f9d-99f0-dd9888baa6c3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-----+------------+----------+--------------------+\n",
      "|         user|     aid|comments|              domain|frontpage|          posted_at|         source_text|        source_title|               title|                 url|votes|user_created|user_karma|user_submitted_count|\n",
      "+-------------+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-----+------------+----------+--------------------+\n",
      "|      moqsien|39977630|       0|    github.com/gvcgo|    false|2024-04-09 09:18:20|GitHub - gvcgo/ve...|GitHub - gvcgo/ve...|Show HN: A genera...|https://github.co...|    1|  1712120068|         3|                   9|\n",
      "|     brennerm|39977645|       0| github.com/rrweb-io|    false|2024-04-09 09:20:17|GitHub - rrweb-io...|GitHub - rrweb-io...|Rrweb, web sessio...|https://github.co...|    1|  1426762276|        37|                  45|\n",
      "| lovegrenoble|39977648|       0|      flexboxcss.com|    false|2024-04-09 09:20:59|Flexbox CSS - Eff...|Effortlessly crea...|Web tool to strea...|https://flexboxcs...|    1|  1687312715|       317|                 168|\n",
      "|  GeoAtreides|39977669|       0|    sciencedaily.com|    false|2024-04-09 09:24:50|www.sciencedaily....|www.sciencedaily.com|Study of two trib...|https://www.scien...|    1|  1333111965|      1107|                 279|\n",
      "|         tosh|39977671|       0|           arxiv.org|     true|2024-04-09 09:24:58|[2404.05719] Ferr...|Ferret-UI: Ground...|Apple Ferret-UI: ...|https://arxiv.org...|    8|  1273014226|    148859|               21172|\n",
      "|    alephnerd|39977673|       0|         reuters.com|     true|2024-04-09 09:25:45|     reuters.com\\n\\n|         reuters.com|Biden and Kishida...|https://www.reute...|    3|  1546925297|      6301|                2501|\n",
      "|    alephnerd|39977681|       0|         reuters.com|    false|2024-04-09 09:26:54|     reuters.com\\n\\n|         reuters.com|South Korea to in...|https://www.reute...|    1|  1546925297|      6301|                2501|\n",
      "|         tosh|39977704|       0|           arxiv.org|    false|2024-04-09 09:32:15|[2404.04478] Diff...|Diffusion-RWKV: S...|Diffusion-RWKV: S...|https://arxiv.org...|    1|  1273014226|    148859|               21172|\n",
      "|         tosh|39977741|       0|         recursal.ai|    false|2024-04-09 09:40:41|Dear VC’s, please...|Dear VC’s, please...|Please stop throw...|https://substack....|    1|  1273014226|    148859|               21172|\n",
      "|  hubraumhugo|39977756|       2|        swissinfo.ch|     true|2024-04-09 09:43:08|Switzerland faces...|Switzerland faces...|Switzerland faces...|https://www.swiss...|    6|  1561310375|      6695|                 737|\n",
      "|   palmfacehn|39977775|       0|hague6185.wordpre...|    false|2024-04-09 09:46:03|Russian Amphibiou...|Russian Amphibiou...|Bartini Beriev VV...|https://hague6185...|    1|  1694750092|       512|                 220|\n",
      "|       marban|39977778|       0|             wsj.com|    false|2024-04-09 09:46:48|         wsj.com\\n\\n|             wsj.com|'Social Order Cou...|https://www.wsj.c...|    1|  1208631846|     25033|                3803|\n",
      "|        flojo|39977780|       0|         youtube.com|    false|2024-04-09 09:47:05|BlackRock CEO Giv...|BlackRock CEO Giv...|BlackRock CEO Giv...|https://www.youtu...|    1|  1276254589|       230|                 103|\n",
      "|   fedorovn19|39977793|       0|           dspyt.com|    false|2024-04-09 09:49:27|Mina Protocol: Un...|DSPYT: Data Scien...|Mina Protocol: Un...|https://dspyt.com...|    1|  1709037918|         3|                  57|\n",
      "|  thunderbong|39977831|       0|        upworthy.com|     true|2024-04-09 09:56:49|The beautiful thi...|The beautiful thi...|The Lonely Funera...|https://www.upwor...|    3|  1337555174|     81835|               10030|\n",
      "|        al_al|39977845|       0|              the.al|     true|2024-04-09 09:59:16|Dualshock Calibra...|Dualshock Calibra...|Show HN: DualShoc...|https://blog.the....|    6|  1606755684|       149|                  15|\n",
      "|     nikeedev|39977859|       0| github.com/nikeedev|    false|2024-04-09 10:03:37|GitHub - nikeedev...|GitHub - nikeedev...|           Rain Lang|https://github.co...|    1|  1705399522|         1|                   2|\n",
      "| todsacerdoti|39977862|       4|github.com/preten...|     true|2024-04-09 10:03:53|GitHub - Pretendo...|GitHub - Pretendo...|SSSL – Hackless S...|https://github.co...|   14|  1579288749|    142415|               12139|\n",
      "|davidlemayian|39977866|       0|    conradakunga.com|    false|2024-04-09 10:05:26|Dogs Will Always ...|Dogs Will Always ...|The Spiral of UX ...|https://www.conra...|    1|  1450177018|        26|                  11|\n",
      "|          pcl|39977878|       0|  washingtonpost.com|    false|2024-04-09 10:07:57|Why this eclipse ...|Why this eclipse ...|Why this eclipse ...|https://www.washi...|    1|  1314329570|      3954|                 938|\n",
      "+-------------+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-----+------------+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#merging the collected user data with the original data\n",
    "\n",
    "# Importing required libraries\n",
    "from pyspark.sql.functions import col, size\n",
    "\n",
    "# Selecting relevant columns from user_info_df\n",
    "user_info_selected = user_info_df.select(\"id\", \"created\", \"karma\", size(\"submitted\").alias(\"submitted_count\")) #using the count of submissions as a\n",
    "#variable\n",
    "\n",
    "# Renaming columns to avoid conflicts during join and increase clarity\n",
    "user_info_selected = user_info_selected \\\n",
    "    .withColumnRenamed(\"id\", \"user\") \\\n",
    "    .withColumnRenamed(\"created\", \"user_created\") \\\n",
    "    .withColumnRenamed(\"karma\", \"user_karma\") \\\n",
    "    .withColumnRenamed(\"submitted_count\", \"user_submitted_count\")\n",
    "\n",
    "# Merging dataframes\n",
    "enriched_dataframe = merged_dataframe.join(user_info_selected, \"user\", \"left\")\n",
    "\n",
    "# Displaying the resulting DataFrame\n",
    "enriched_dataframe.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad7762ba-d9e6-4889-bb0e-c1b923fb23a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-----+------------+----------+--------------------+--------------------+-------------------------+------------------------+\n",
      "|         user|     aid|comments|              domain|frontpage|          posted_at|         source_text|        source_title|               title|                 url|votes|user_created|user_karma|user_submitted_count|  title_preprocessed|source_title_preprocessed|source_text_preprocessed|\n",
      "+-------------+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-----+------------+----------+--------------------+--------------------+-------------------------+------------------------+\n",
      "|      moqsien|39977630|       0|    github.com/gvcgo|    false|2024-04-09 09:18:20|GitHub - gvcgo/ve...|GitHub - gvcgo/ve...|Show HN: A genera...|https://github.co...|    1|  1712120068|         3|                   9|show hn a general...|     github  gvcgovers...|    github  gvcgovers...|\n",
      "|     brennerm|39977645|       0| github.com/rrweb-io|    false|2024-04-09 09:20:17|GitHub - rrweb-io...|GitHub - rrweb-io...|Rrweb, web sessio...|https://github.co...|    1|  1426762276|        37|                  45|rrweb web session...|     github  rrwebiorr...|    github  rrwebiorr...|\n",
      "| lovegrenoble|39977648|       0|      flexboxcss.com|    false|2024-04-09 09:20:59|Flexbox CSS - Eff...|Effortlessly crea...|Web tool to strea...|https://flexboxcs...|    1|  1687312715|       317|                 168|web tool to strea...|     effortlessly crea...|    flexbox css  effo...|\n",
      "|  GeoAtreides|39977669|       0|    sciencedaily.com|    false|2024-04-09 09:24:50|www.sciencedaily....|www.sciencedaily.com|Study of two trib...|https://www.scien...|    1|  1333111965|      1107|                 279|study of two trib...|       wwwsciencedailycom|    wwwsciencedailyco...|\n",
      "|         tosh|39977671|       0|           arxiv.org|     true|2024-04-09 09:24:58|[2404.05719] Ferr...|Ferret-UI: Ground...|Apple Ferret-UI: ...|https://arxiv.org...|    8|  1273014226|    148859|               21172|apple ferretui gr...|     ferretui grounded...|     ferretui grounde...|\n",
      "|    alephnerd|39977673|       0|         reuters.com|     true|2024-04-09 09:25:45|     reuters.com\\n\\n|         reuters.com|Biden and Kishida...|https://www.reute...|    3|  1546925297|      6301|                2501|biden and kishida...|               reuterscom|          reuterscom\\n\\n|\n",
      "|    alephnerd|39977681|       0|         reuters.com|    false|2024-04-09 09:26:54|     reuters.com\\n\\n|         reuters.com|South Korea to in...|https://www.reute...|    1|  1546925297|      6301|                2501|south korea to in...|               reuterscom|          reuterscom\\n\\n|\n",
      "|         tosh|39977704|       0|           arxiv.org|    false|2024-04-09 09:32:15|[2404.04478] Diff...|Diffusion-RWKV: S...|Diffusion-RWKV: S...|https://arxiv.org...|    1|  1273014226|    148859|               21172|diffusionrwkv sca...|     diffusionrwkv sca...|     diffusionrwkv sc...|\n",
      "|         tosh|39977741|       0|         recursal.ai|    false|2024-04-09 09:40:41|Dear VC’s, please...|Dear VC’s, please...|Please stop throw...|https://substack....|    1|  1273014226|    148859|               21172|please stop throw...|     dear vcs please s...|    dear vcs please s...|\n",
      "|  hubraumhugo|39977756|       2|        swissinfo.ch|     true|2024-04-09 09:43:08|Switzerland faces...|Switzerland faces...|Switzerland faces...|https://www.swiss...|    6|  1561310375|      6695|                 737|switzerland faces...|     switzerland faces...|    switzerland faces...|\n",
      "|   palmfacehn|39977775|       0|hague6185.wordpre...|    false|2024-04-09 09:46:03|Russian Amphibiou...|Russian Amphibiou...|Bartini Beriev VV...|https://hague6185...|    1|  1694750092|       512|                 220|bartini beriev vv...|     russian amphibiou...|    russian amphibiou...|\n",
      "|       marban|39977778|       0|             wsj.com|    false|2024-04-09 09:46:48|         wsj.com\\n\\n|             wsj.com|'Social Order Cou...|https://www.wsj.c...|    1|  1208631846|     25033|                3803|social order coul...|                   wsjcom|              wsjcom\\n\\n|\n",
      "|        flojo|39977780|       0|         youtube.com|    false|2024-04-09 09:47:05|BlackRock CEO Giv...|BlackRock CEO Giv...|BlackRock CEO Giv...|https://www.youtu...|    1|  1276254589|       230|                 103|blackrock ceo giv...|     blackrock ceo giv...|    blackrock ceo giv...|\n",
      "|   fedorovn19|39977793|       0|           dspyt.com|    false|2024-04-09 09:49:27|Mina Protocol: Un...|DSPYT: Data Scien...|Mina Protocol: Un...|https://dspyt.com...|    1|  1709037918|         3|                  57|mina protocol unl...|     dspyt data scienc...|    mina protocol unl...|\n",
      "|  thunderbong|39977831|       0|        upworthy.com|     true|2024-04-09 09:56:49|The beautiful thi...|The beautiful thi...|The Lonely Funera...|https://www.upwor...|    3|  1337555174|     81835|               10030|the lonely funera...|     the beautiful thi...|    the beautiful thi...|\n",
      "|        al_al|39977845|       0|              the.al|     true|2024-04-09 09:59:16|Dualshock Calibra...|Dualshock Calibra...|Show HN: DualShoc...|https://blog.the....|    6|  1606755684|       149|                  15|show hn dualshock...|     dualshock calibra...|    dualshock calibra...|\n",
      "|     nikeedev|39977859|       0| github.com/nikeedev|    false|2024-04-09 10:03:37|GitHub - nikeedev...|GitHub - nikeedev...|           Rain Lang|https://github.co...|    1|  1705399522|         1|                   2|           rain lang|     github  nikeedevr...|    github  nikeedevr...|\n",
      "| todsacerdoti|39977862|       4|github.com/preten...|     true|2024-04-09 10:03:53|GitHub - Pretendo...|GitHub - Pretendo...|SSSL – Hackless S...|https://github.co...|   14|  1579288749|    142415|               12139|sssl  hackless ss...|     github  pretendon...|    github  pretendon...|\n",
      "|davidlemayian|39977866|       0|    conradakunga.com|    false|2024-04-09 10:05:26|Dogs Will Always ...|Dogs Will Always ...|The Spiral of UX ...|https://www.conra...|    1|  1450177018|        26|                  11|the spiral of ux ...|     dogs will always ...|    dogs will always ...|\n",
      "|          pcl|39977878|       0|  washingtonpost.com|    false|2024-04-09 10:07:57|Why this eclipse ...|Why this eclipse ...|Why this eclipse ...|https://www.washi...|    1|  1314329570|      3954|                 938|why this eclipse ...|     why this eclipse ...|    why this eclipse ...|\n",
      "+-------------+--------+--------+--------------------+---------+-------------------+--------------------+--------------------+--------------------+--------------------+-----+------------+----------+--------------------+--------------------+-------------------------+------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#readablility can be relevant, since a lot of the articles will be technical, so it might be interesting to get an idea of how readability influences\n",
    "#the frontpage reaching\n",
    "#source title verwijderen, is problematisch als het bv een github pagina is die gedeeld wordt\n",
    "import re\n",
    "from textblob import TextBlob\n",
    "import textstat\n",
    "from pyspark.sql.functions import udf, size, split\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# function to preprocess text\n",
    "def preprocess_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    return text\n",
    "\n",
    "# function to calculate sentiment score using TextBlob\n",
    "def calculate_sentiment(text):\n",
    "    blob = TextBlob(str(text))\n",
    "    return blob.sentiment.polarity\n",
    "\n",
    "# function to calculate readability score using textstat\n",
    "def calculate_readability(text):\n",
    "    return textstat.flesch_kincaid_grade(text)\n",
    "\n",
    "\n",
    "# UDF for preprocessing text\n",
    "preprocess_udf = udf(preprocess_text, StringType())\n",
    "\n",
    "# Apply preprocessing and calculate word count, sentiment score, and readability score\n",
    "test_dataframe_text_analysis = enriched_dataframe.withColumn(\"title_preprocessed\", preprocess_udf(\"title\")) \\\n",
    "    .withColumn(\"source_title_preprocessed\", preprocess_udf(\"source_title\")) \\\n",
    "    .withColumn(\"source_text_preprocessed\", preprocess_udf(\"source_text\")) \\\n",
    "#    .withColumn(\"word_count_title\", size(split(col(\"title_preprocessed\"), \"\\s+\"))) \\\n",
    "#    .withColumn(\"word_count_source_title\", size(split(col(\"source_title_preprocessed\"), \"\\s+\"))) \\\n",
    "#    .withColumn(\"word_count_source_text\", size(split(col(\"source_text_preprocessed\"), \"\\s+\"))) \\\n",
    "#    .withColumn(\"sentiment_score_title\", calculate_sentiment(col(\"title_preprocessed\"))) \\\n",
    "#    .withColumn(\"sentiment_score_source_title\", calculate_sentiment(col(\"source_title_preprocessed\"))) \\\n",
    "#    .withColumn(\"sentiment_score_source_text\", calculate_sentiment(col(\"source_text_preprocessed\"))) \\\n",
    "#    .withColumn(\"readability_score_title\", calculate_readability(col(\"title_preprocessed\"))) \\\n",
    "#    .withColumn(\"readability_score_source_title\", calculate_readability(col(\"source_title_preprocessed\"))) \\\n",
    "#    .withColumn(\"readability_score_source_text\", calculate_readability(col(\"source_text_preprocessed\")))\n",
    "\n",
    "# Show the updated DataFrame\n",
    "test_dataframe_text_analysis.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95740d58-a640-4bf2-b807-2c96c17203a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('user', 'string'), ('aid', 'string'), ('comments', 'bigint'), ('domain', 'string'), ('frontpage', 'boolean'), ('posted_at', 'string'), ('source_text', 'string'), ('source_title', 'string'), ('title', 'string'), ('url', 'string'), ('votes', 'bigint'), ('user_created', 'bigint'), ('user_karma', 'bigint'), ('user_submitted_count', 'int'), ('title_preprocessed', 'string'), ('source_title_preprocessed', 'string'), ('source_text_preprocessed', 'string')]\n"
     ]
    }
   ],
   "source": [
    "# Check the data types of columns in the DataFrame\n",
    "print(test_dataframe_text_analysis.dtypes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f60424e-4c00-4071-b516-f983088a0ae1",
   "metadata": {},
   "outputs": [
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"C:\\Users\\eloua\\AppData\\Local\\Temp\\ipykernel_20592\\2961351709.py\", line 12, in preprocess_text\n  File \"C:\\Users\\eloua\\anaconda3\\envs\\sparkenvconda\\lib\\re.py\", line 210, in sub\n    return _compile(pattern, flags).sub(repl, string, count)\nTypeError: expected string or bytes-like object\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 9\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Check for null values in title_preprocessed column\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#test_dataframe_text_analysis.filter(col(\"title_preprocessed\").isNull()).show()\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#test_dataframe_text_analysis.filter(col(\"title_preprocessed\").isNull()).count()\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m \n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Check for null values in source_text_preprocessed column\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[43mtest_dataframe_text_analysis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msource_text_preprocessed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misNull\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\sql\\dataframe.py:945\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    886\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[0;32m    887\u001b[0m \n\u001b[0;32m    888\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    943\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[0;32m    944\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 945\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\Desktop\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\sql\\dataframe.py:963\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    957\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    958\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    959\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    960\u001b[0m     )\n\u001b[0;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 963\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    965\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\Desktop\\spark\\spark-3.5.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\Desktop\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\errors\\exceptions\\captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"C:\\Users\\eloua\\AppData\\Local\\Temp\\ipykernel_20592\\2961351709.py\", line 12, in preprocess_text\n  File \"C:\\Users\\eloua\\anaconda3\\envs\\sparkenvconda\\lib\\re.py\", line 210, in sub\n    return _compile(pattern, flags).sub(repl, string, count)\nTypeError: expected string or bytes-like object\n"
     ]
    }
   ],
   "source": [
    "# Check for null values in title_preprocessed column\n",
    "#test_dataframe_text_analysis.filter(col(\"title_preprocessed\").isNull()).show()\n",
    "#test_dataframe_text_analysis.filter(col(\"title_preprocessed\").isNull()).count()\n",
    "\n",
    "# Check for null values in source_title_preprocessed column\n",
    "#test_dataframe_text_analysis.filter(col(\"source_title_preprocessed\").isNull()).count()\n",
    "\n",
    "# Check for null values in source_text_preprocessed column\n",
    "test_dataframe_text_analysis.filter(col(\"source_text_preprocessed\").isNull()).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97f02850-09eb-41cd-94a7-980381f1e1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user: string (nullable = true)\n",
      " |-- aid: string (nullable = true)\n",
      " |-- comments: long (nullable = true)\n",
      " |-- domain: string (nullable = true)\n",
      " |-- frontpage: boolean (nullable = true)\n",
      " |-- posted_at: string (nullable = true)\n",
      " |-- source_text: string (nullable = true)\n",
      " |-- source_title: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- votes: long (nullable = true)\n",
      " |-- user_created: long (nullable = true)\n",
      " |-- user_karma: long (nullable = true)\n",
      " |-- user_submitted_count: integer (nullable = true)\n",
      " |-- title_preprocessed: string (nullable = true)\n",
      " |-- source_title_preprocessed: string (nullable = true)\n",
      " |-- source_text_preprocessed: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_dataframe_text_analysis.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9426ab5c-ab2c-466c-be50-a1677d5cbe06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\eloua\\Desktop\\spark\\spark-3.5.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"C:\\Users\\eloua\\Desktop\\spark\\spark-3.5.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"C:\\Users\\eloua\\anaconda3\\envs\\sparkenvconda\\lib\\socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmerged_dataframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepartition\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:/Users/eloua/Desktop/spark/notebooks/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\spark\\spark-3.5.1-bin-hadoop3\\python\\pyspark\\sql\\readwriter.py:1721\u001b[0m, in \u001b[0;36mDataFrameWriter.parquet\u001b[1;34m(self, path, mode, partitionBy, compression)\u001b[0m\n\u001b[0;32m   1719\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartitionBy(partitionBy)\n\u001b[0;32m   1720\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(compression\u001b[38;5;241m=\u001b[39mcompression)\n\u001b[1;32m-> 1721\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\spark\\spark-3.5.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[1;32m~\\Desktop\\spark\\spark-3.5.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[1;34m(self, command, retry, binary)\u001b[0m\n\u001b[0;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[0;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[1;32m~\\Desktop\\spark\\spark-3.5.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[1;34m(self, command)\u001b[0m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[0;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[0;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sparkenvconda\\lib\\socket.py:704\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 704\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    706\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_dataframe.repartition(1).write.mode('overwrite').parquet(\"C:/Users/eloua/Desktop/spark/notebooks/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452200c3-d431-4133-aadd-9f4dff1a6b1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
